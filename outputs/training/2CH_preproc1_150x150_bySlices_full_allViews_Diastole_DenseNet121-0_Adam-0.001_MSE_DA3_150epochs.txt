nohup: ignoring input
Running experiment 2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3
Going to train with the GPU in the slot 1 -> device model: GeForce RTX 2080
Model architecture:
 DenseNet121_0(
  (first_conv): Sequential(
    (0): Conv2d(30, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (pretrained_block): Sequential(
    (0): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (1): _Transition(
      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (2): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (3): _Transition(
      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (4): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer13): _DenseLayer(
        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer14): _DenseLayer(
        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer15): _DenseLayer(
        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer16): _DenseLayer(
        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer17): _DenseLayer(
        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer18): _DenseLayer(
        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer19): _DenseLayer(
        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer20): _DenseLayer(
        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer21): _DenseLayer(
        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer22): _DenseLayer(
        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer23): _DenseLayer(
        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer24): _DenseLayer(
        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (5): _Transition(
      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (6): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer13): _DenseLayer(
        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer14): _DenseLayer(
        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer15): _DenseLayer(
        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer16): _DenseLayer(
        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (reduction_block): Sequential(
    (0): AdaptiveAvgPool2d(output_size=(1, 1))
    (1): Flatten()
  )
  (dense_block): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.4, inplace=False)
    (3): Linear(in_features=512, out_features=1, bias=True)
  )
) 


############################
# TRAIN PHASE:  150 epochs #
############################

Epoch 0: 
Train batch 1/32 - 76.9ms/batch - loss: 2007.30786 - diff: 162.29mlTrain batch 2/32 - 48.3ms/batch - loss: 1869.08380 - diff: 160.77mlTrain batch 3/32 - 91.8ms/batch - loss: 2095.40051 - diff: 171.61mlTrain batch 4/32 - 58.4ms/batch - loss: 2171.43320 - diff: 175.28mlTrain batch 5/32 - 58.5ms/batch - loss: 2122.75208 - diff: 173.90mlTrain batch 6/32 - 59.6ms/batch - loss: 2128.16994 - diff: 173.01mlTrain batch 7/32 - 58.0ms/batch - loss: 2035.17950 - diff: 168.62mlTrain batch 8/32 - 57.3ms/batch - loss: 2039.72981 - diff: 168.86mlTrain batch 9/32 - 59.7ms/batch - loss: 1997.49486 - diff: 167.53mlTrain batch 10/32 - 57.3ms/batch - loss: 2003.94821 - diff: 167.72mlTrain batch 11/32 - 57.2ms/batch - loss: 1939.69957 - diff: 165.02mlTrain batch 12/32 - 56.8ms/batch - loss: 1914.76089 - diff: 164.32mlTrain batch 13/32 - 58.3ms/batch - loss: 1883.26065 - diff: 163.12mlTrain batch 14/32 - 56.4ms/batch - loss: 1855.05486 - diff: 161.94mlTrain batch 15/32 - 59.6ms/batch - loss: 1835.02410 - diff: 161.20mlTrain batch 16/32 - 56.6ms/batch - loss: 1819.89085 - diff: 160.41mlTrain batch 17/32 - 58.9ms/batch - loss: 1823.30304 - diff: 160.27mlTrain batch 18/32 - 57.6ms/batch - loss: 1788.58274 - diff: 158.33mlTrain batch 19/32 - 58.7ms/batch - loss: 1749.41577 - diff: 156.33mlTrain batch 20/32 - 58.4ms/batch - loss: 1707.81291 - diff: 154.10mlTrain batch 21/32 - 58.5ms/batch - loss: 1664.28973 - diff: 151.61mlTrain batch 22/32 - 58.4ms/batch - loss: 1640.94868 - diff: 150.24mlTrain batch 23/32 - 60.6ms/batch - loss: 1595.66917 - diff: 147.73mlTrain batch 24/32 - 59.8ms/batch - loss: 1564.38372 - diff: 145.85mlTrain batch 25/32 - 60.0ms/batch - loss: 1524.71622 - diff: 143.45mlTrain batch 26/32 - 60.3ms/batch - loss: 1487.56888 - diff: 141.02mlTrain batch 27/32 - 60.1ms/batch - loss: 1453.69294 - diff: 138.84mlTrain batch 28/32 - 60.0ms/batch - loss: 1419.50949 - diff: 136.74mlTrain batch 29/32 - 59.7ms/batch - loss: 1393.09602 - diff: 135.25mlTrain batch 30/32 - 61.3ms/batch - loss: 1361.72364 - diff: 133.11mlTrain batch 31/32 - 63.5ms/batch - loss: 1376.13508 - diff: 133.21mlTrain batch 32/32 - 58.9ms/batch - loss: 1386.24333 - diff: 132.80mlTrain batch 32/32 - 12.3s 58.9ms/batch - loss: 1386.24333 - diff: 132.80ml
Test 0.5s: val_loss: 1048.00204 - diff: 113.36ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 1: current best loss = 1048.00204, at epoch 0
Train batch 1/32 - 75.1ms/batch - loss: 494.01184 - diff: 77.94mlTrain batch 2/32 - 59.8ms/batch - loss: 514.22308 - diff: 72.82mlTrain batch 3/32 - 66.6ms/batch - loss: 469.02047 - diff: 69.14mlTrain batch 4/32 - 60.3ms/batch - loss: 441.26921 - diff: 67.90mlTrain batch 5/32 - 61.9ms/batch - loss: 466.26803 - diff: 68.93mlTrain batch 6/32 - 61.5ms/batch - loss: 432.39093 - diff: 66.54mlTrain batch 7/32 - 66.8ms/batch - loss: 427.71669 - diff: 65.20mlTrain batch 8/32 - 60.3ms/batch - loss: 404.11074 - diff: 63.09mlTrain batch 9/32 - 70.2ms/batch - loss: 394.74311 - diff: 60.95mlTrain batch 10/32 - 61.2ms/batch - loss: 395.55046 - diff: 61.21mlTrain batch 11/32 - 69.8ms/batch - loss: 414.59246 - diff: 61.38mlTrain batch 12/32 - 60.9ms/batch - loss: 400.20737 - diff: 60.18mlTrain batch 13/32 - 69.1ms/batch - loss: 392.12910 - diff: 59.98mlTrain batch 14/32 - 59.8ms/batch - loss: 371.92737 - diff: 58.32mlTrain batch 15/32 - 68.7ms/batch - loss: 361.54480 - diff: 57.27mlTrain batch 16/32 - 60.0ms/batch - loss: 352.98583 - diff: 56.42mlTrain batch 17/32 - 66.6ms/batch - loss: 348.26907 - diff: 56.20mlTrain batch 18/32 - 60.5ms/batch - loss: 344.07042 - diff: 56.12mlTrain batch 19/32 - 68.1ms/batch - loss: 348.30146 - diff: 56.48mlTrain batch 20/32 - 60.1ms/batch - loss: 353.87079 - diff: 56.88mlTrain batch 21/32 - 66.8ms/batch - loss: 356.40651 - diff: 57.31mlTrain batch 22/32 - 60.7ms/batch - loss: 349.92008 - diff: 56.75mlTrain batch 23/32 - 60.8ms/batch - loss: 349.05393 - diff: 56.81mlTrain batch 24/32 - 59.8ms/batch - loss: 351.40644 - diff: 57.26mlTrain batch 25/32 - 60.3ms/batch - loss: 341.17672 - diff: 56.31mlTrain batch 26/32 - 60.2ms/batch - loss: 342.43121 - diff: 56.51mlTrain batch 27/32 - 61.8ms/batch - loss: 345.20102 - diff: 56.60mlTrain batch 28/32 - 61.5ms/batch - loss: 350.98981 - diff: 57.27mlTrain batch 29/32 - 61.8ms/batch - loss: 357.35626 - diff: 57.94mlTrain batch 30/32 - 62.1ms/batch - loss: 355.14816 - diff: 57.91mlTrain batch 31/32 - 55.1ms/batch - loss: 381.15273 - diff: 59.04mlTrain batch 32/32 - 44.4ms/batch - loss: 382.83669 - diff: 58.90mlTrain batch 32/32 - 10.5s 44.4ms/batch - loss: 382.83669 - diff: 58.90ml
Test 0.5s: val_loss: 356.10673 - diff: 58.09ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 2: current best loss = 356.10673, at epoch 1
Train batch 1/32 - 63.4ms/batch - loss: 184.82565 - diff: 40.11mlTrain batch 2/32 - 66.2ms/batch - loss: 172.00970 - diff: 40.43mlTrain batch 3/32 - 52.8ms/batch - loss: 267.05865 - diff: 46.48mlTrain batch 4/32 - 52.4ms/batch - loss: 304.92546 - diff: 50.79mlTrain batch 5/32 - 60.6ms/batch - loss: 268.70081 - diff: 46.36mlTrain batch 6/32 - 60.3ms/batch - loss: 281.40099 - diff: 48.52mlTrain batch 7/32 - 60.6ms/batch - loss: 281.95815 - diff: 48.77mlTrain batch 8/32 - 59.4ms/batch - loss: 268.44223 - diff: 47.88mlTrain batch 9/32 - 58.4ms/batch - loss: 288.47857 - diff: 49.60mlTrain batch 10/32 - 58.8ms/batch - loss: 289.42794 - diff: 50.46mlTrain batch 11/32 - 59.2ms/batch - loss: 300.85836 - diff: 51.59mlTrain batch 12/32 - 64.2ms/batch - loss: 292.55591 - diff: 51.00mlTrain batch 13/32 - 63.9ms/batch - loss: 294.09224 - diff: 51.27mlTrain batch 14/32 - 65.7ms/batch - loss: 296.62471 - diff: 51.48mlTrain batch 15/32 - 63.4ms/batch - loss: 290.88888 - diff: 50.78mlTrain batch 16/32 - 58.4ms/batch - loss: 288.90554 - diff: 50.71mlTrain batch 17/32 - 58.5ms/batch - loss: 278.69498 - diff: 49.87mlTrain batch 18/32 - 58.5ms/batch - loss: 273.35649 - diff: 49.48mlTrain batch 19/32 - 59.1ms/batch - loss: 280.73251 - diff: 50.35mlTrain batch 20/32 - 62.3ms/batch - loss: 274.95367 - diff: 49.94mlTrain batch 21/32 - 62.4ms/batch - loss: 281.78458 - diff: 50.33mlTrain batch 22/32 - 65.0ms/batch - loss: 308.83545 - diff: 51.55mlTrain batch 23/32 - 48.6ms/batch - loss: 302.66250 - diff: 50.99mlTrain batch 24/32 - 65.8ms/batch - loss: 308.35131 - diff: 51.39mlTrain batch 25/32 - 66.3ms/batch - loss: 305.37135 - diff: 51.24mlTrain batch 26/32 - 53.1ms/batch - loss: 299.89495 - diff: 50.98mlTrain batch 27/32 - 48.4ms/batch - loss: 295.31711 - diff: 50.67mlTrain batch 28/32 - 55.4ms/batch - loss: 289.46275 - diff: 50.20mlTrain batch 29/32 - 56.1ms/batch - loss: 287.28363 - diff: 49.93mlTrain batch 30/32 - 60.4ms/batch - loss: 285.90117 - diff: 49.77mlTrain batch 31/32 - 56.3ms/batch - loss: 282.04437 - diff: 49.53mlTrain batch 32/32 - 36.1ms/batch - loss: 283.50410 - diff: 49.43mlTrain batch 32/32 - 11.6s 36.1ms/batch - loss: 283.50410 - diff: 49.43ml
Test 0.5s: val_loss: 291.64618 - diff: 52.91ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 3: current best loss = 291.64618, at epoch 2
Train batch 1/32 - 70.7ms/batch - loss: 157.76227 - diff: 40.86mlTrain batch 2/32 - 60.0ms/batch - loss: 407.70267 - diff: 52.21mlTrain batch 3/32 - 59.0ms/batch - loss: 344.42196 - diff: 49.79mlTrain batch 4/32 - 59.5ms/batch - loss: 327.03310 - diff: 51.25mlTrain batch 5/32 - 61.5ms/batch - loss: 319.57220 - diff: 53.04mlTrain batch 6/32 - 59.0ms/batch - loss: 305.19033 - diff: 52.26mlTrain batch 7/32 - 61.3ms/batch - loss: 289.69757 - diff: 50.14mlTrain batch 8/32 - 62.6ms/batch - loss: 283.45784 - diff: 50.48mlTrain batch 9/32 - 60.7ms/batch - loss: 267.58169 - diff: 49.21mlTrain batch 10/32 - 59.4ms/batch - loss: 256.02305 - diff: 48.31mlTrain batch 11/32 - 60.7ms/batch - loss: 268.54790 - diff: 49.29mlTrain batch 12/32 - 62.0ms/batch - loss: 260.59421 - diff: 48.62mlTrain batch 13/32 - 62.8ms/batch - loss: 258.52609 - diff: 48.76mlTrain batch 14/32 - 61.2ms/batch - loss: 278.62872 - diff: 49.76mlTrain batch 15/32 - 61.5ms/batch - loss: 267.91301 - diff: 48.74mlTrain batch 16/32 - 60.5ms/batch - loss: 258.06894 - diff: 47.67mlTrain batch 17/32 - 60.3ms/batch - loss: 261.57719 - diff: 48.22mlTrain batch 18/32 - 60.4ms/batch - loss: 256.96517 - diff: 47.71mlTrain batch 19/32 - 59.9ms/batch - loss: 254.15111 - diff: 47.55mlTrain batch 20/32 - 59.8ms/batch - loss: 248.85678 - diff: 47.15mlTrain batch 21/32 - 60.1ms/batch - loss: 256.11057 - diff: 48.02mlTrain batch 22/32 - 61.3ms/batch - loss: 252.75430 - diff: 47.75mlTrain batch 23/32 - 61.2ms/batch - loss: 264.33281 - diff: 48.87mlTrain batch 24/32 - 61.2ms/batch - loss: 262.34498 - diff: 48.79mlTrain batch 25/32 - 60.6ms/batch - loss: 262.12821 - diff: 49.08mlTrain batch 26/32 - 65.6ms/batch - loss: 259.44330 - diff: 49.05mlTrain batch 27/32 - 65.6ms/batch - loss: 263.57493 - diff: 49.30mlTrain batch 28/32 - 66.0ms/batch - loss: 259.03056 - diff: 48.78mlTrain batch 29/32 - 61.2ms/batch - loss: 259.41092 - diff: 49.05mlTrain batch 30/32 - 60.9ms/batch - loss: 260.08666 - diff: 49.21mlTrain batch 31/32 - 50.2ms/batch - loss: 259.12155 - diff: 49.23mlTrain batch 32/32 - 54.7ms/batch - loss: 263.41493 - diff: 49.28mlTrain batch 32/32 - 11.6s 54.7ms/batch - loss: 263.41493 - diff: 49.28ml
Test 0.6s: val_loss: 235.59399 - diff: 46.94ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 4: current best loss = 235.59399, at epoch 3
Train batch 1/32 - 73.8ms/batch - loss: 166.19142 - diff: 40.00mlTrain batch 2/32 - 62.3ms/batch - loss: 216.69071 - diff: 46.32mlTrain batch 3/32 - 64.8ms/batch - loss: 209.08439 - diff: 44.57mlTrain batch 4/32 - 64.4ms/batch - loss: 180.42131 - diff: 41.91mlTrain batch 5/32 - 71.5ms/batch - loss: 208.67229 - diff: 44.20mlTrain batch 6/32 - 64.6ms/batch - loss: 201.77212 - diff: 43.57mlTrain batch 7/32 - 65.0ms/batch - loss: 196.71177 - diff: 43.66mlTrain batch 8/32 - 65.0ms/batch - loss: 249.39589 - diff: 47.33mlTrain batch 9/32 - 66.3ms/batch - loss: 239.65636 - diff: 47.07mlTrain batch 10/32 - 66.1ms/batch - loss: 237.29054 - diff: 47.49mlTrain batch 11/32 - 66.2ms/batch - loss: 223.12906 - diff: 45.66mlTrain batch 12/32 - 66.9ms/batch - loss: 237.55989 - diff: 46.90mlTrain batch 13/32 - 58.6ms/batch - loss: 238.24945 - diff: 47.05mlTrain batch 14/32 - 61.1ms/batch - loss: 241.18754 - diff: 47.29mlTrain batch 15/32 - 61.5ms/batch - loss: 232.92182 - diff: 46.41mlTrain batch 16/32 - 67.4ms/batch - loss: 231.10128 - diff: 46.39mlTrain batch 17/32 - 67.3ms/batch - loss: 265.90476 - diff: 47.73mlTrain batch 18/32 - 67.4ms/batch - loss: 256.67999 - diff: 46.73mlTrain batch 19/32 - 66.9ms/batch - loss: 250.30059 - diff: 46.37mlTrain batch 20/32 - 61.3ms/batch - loss: 251.41889 - diff: 46.89mlTrain batch 21/32 - 59.9ms/batch - loss: 244.55655 - diff: 46.30mlTrain batch 22/32 - 60.2ms/batch - loss: 241.52704 - diff: 46.14mlTrain batch 23/32 - 53.2ms/batch - loss: 239.13454 - diff: 46.08mlTrain batch 24/32 - 59.0ms/batch - loss: 234.20302 - diff: 45.65mlTrain batch 25/32 - 54.0ms/batch - loss: 238.76357 - diff: 45.84mlTrain batch 26/32 - 51.6ms/batch - loss: 240.17750 - diff: 46.18mlTrain batch 27/32 - 60.7ms/batch - loss: 248.19544 - diff: 46.89mlTrain batch 28/32 - 60.0ms/batch - loss: 248.50090 - diff: 46.98mlTrain batch 29/32 - 57.9ms/batch - loss: 246.09628 - diff: 46.92mlTrain batch 30/32 - 58.0ms/batch - loss: 246.75240 - diff: 47.28mlTrain batch 31/32 - 49.4ms/batch - loss: 258.71753 - diff: 48.17mlTrain batch 32/32 - 36.9ms/batch - loss: 261.93925 - diff: 48.07mlTrain batch 32/32 - 11.6s 36.9ms/batch - loss: 261.93925 - diff: 48.07ml
Test 0.5s: val_loss: 354.15289 - diff: 56.89ml

Epoch 5: current best loss = 235.59399, at epoch 3
Train batch 1/32 - 68.2ms/batch - loss: 392.11108 - diff: 58.91mlTrain batch 2/32 - 60.0ms/batch - loss: 322.45011 - diff: 54.84mlTrain batch 3/32 - 68.3ms/batch - loss: 296.01486 - diff: 52.45mlTrain batch 4/32 - 59.6ms/batch - loss: 264.64630 - diff: 49.28mlTrain batch 5/32 - 60.2ms/batch - loss: 270.68299 - diff: 49.19mlTrain batch 6/32 - 57.6ms/batch - loss: 240.92938 - diff: 46.20mlTrain batch 7/32 - 57.9ms/batch - loss: 222.24079 - diff: 44.50mlTrain batch 8/32 - 57.6ms/batch - loss: 222.73654 - diff: 44.44mlTrain batch 9/32 - 57.8ms/batch - loss: 239.77462 - diff: 45.50mlTrain batch 10/32 - 57.4ms/batch - loss: 249.72253 - diff: 47.04mlTrain batch 11/32 - 59.1ms/batch - loss: 268.81326 - diff: 49.30mlTrain batch 12/32 - 58.6ms/batch - loss: 261.49565 - diff: 48.78mlTrain batch 13/32 - 58.7ms/batch - loss: 259.26440 - diff: 48.70mlTrain batch 14/32 - 58.3ms/batch - loss: 251.81471 - diff: 47.95mlTrain batch 15/32 - 57.6ms/batch - loss: 246.18591 - diff: 47.78mlTrain batch 16/32 - 58.0ms/batch - loss: 246.67562 - diff: 47.95mlTrain batch 17/32 - 57.9ms/batch - loss: 243.86671 - diff: 47.68mlTrain batch 18/32 - 57.5ms/batch - loss: 236.98090 - diff: 46.98mlTrain batch 19/32 - 58.0ms/batch - loss: 232.85394 - diff: 46.72mlTrain batch 20/32 - 58.7ms/batch - loss: 229.81026 - diff: 46.43mlTrain batch 21/32 - 59.2ms/batch - loss: 223.88250 - diff: 45.72mlTrain batch 22/32 - 58.8ms/batch - loss: 225.26428 - diff: 45.97mlTrain batch 23/32 - 58.6ms/batch - loss: 220.08014 - diff: 45.43mlTrain batch 24/32 - 61.5ms/batch - loss: 219.14187 - diff: 45.44mlTrain batch 25/32 - 63.9ms/batch - loss: 241.81411 - diff: 46.63mlTrain batch 26/32 - 63.0ms/batch - loss: 248.39932 - diff: 46.91mlTrain batch 27/32 - 62.0ms/batch - loss: 249.16615 - diff: 46.93mlTrain batch 28/32 - 61.8ms/batch - loss: 246.02681 - diff: 46.81mlTrain batch 29/32 - 62.1ms/batch - loss: 243.81573 - diff: 46.84mlTrain batch 30/32 - 62.2ms/batch - loss: 243.31656 - diff: 46.84mlTrain batch 31/32 - 66.3ms/batch - loss: 237.41943 - diff: 46.15mlTrain batch 32/32 - 57.0ms/batch - loss: 246.64985 - diff: 46.34mlTrain batch 32/32 - 10.6s 57.0ms/batch - loss: 246.64985 - diff: 46.34ml
Test 0.5s: val_loss: 212.32731 - diff: 43.94ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 6: current best loss = 212.32731, at epoch 5
Train batch 1/32 - 69.2ms/batch - loss: 157.02913 - diff: 41.66mlTrain batch 2/32 - 60.3ms/batch - loss: 137.47336 - diff: 37.36mlTrain batch 3/32 - 60.6ms/batch - loss: 134.38569 - diff: 37.91mlTrain batch 4/32 - 62.0ms/batch - loss: 121.28805 - diff: 35.16mlTrain batch 5/32 - 62.0ms/batch - loss: 168.57022 - diff: 39.24mlTrain batch 6/32 - 62.1ms/batch - loss: 181.13533 - diff: 40.37mlTrain batch 7/32 - 61.7ms/batch - loss: 263.84011 - diff: 42.83mlTrain batch 8/32 - 61.2ms/batch - loss: 256.10050 - diff: 42.90mlTrain batch 9/32 - 61.1ms/batch - loss: 256.28169 - diff: 44.42mlTrain batch 10/32 - 61.7ms/batch - loss: 259.63210 - diff: 45.20mlTrain batch 11/32 - 61.3ms/batch - loss: 250.02101 - diff: 44.91mlTrain batch 12/32 - 61.4ms/batch - loss: 238.05618 - diff: 43.99mlTrain batch 13/32 - 60.7ms/batch - loss: 251.02608 - diff: 45.54mlTrain batch 14/32 - 60.9ms/batch - loss: 248.27383 - diff: 45.54mlTrain batch 15/32 - 59.4ms/batch - loss: 244.11562 - diff: 45.10mlTrain batch 16/32 - 64.1ms/batch - loss: 244.84860 - diff: 45.63mlTrain batch 17/32 - 63.7ms/batch - loss: 261.69585 - diff: 46.80mlTrain batch 18/32 - 55.4ms/batch - loss: 263.77084 - diff: 47.33mlTrain batch 19/32 - 63.7ms/batch - loss: 254.55059 - diff: 46.33mlTrain batch 20/32 - 62.7ms/batch - loss: 248.80872 - diff: 45.95mlTrain batch 21/32 - 61.0ms/batch - loss: 247.16370 - diff: 46.15mlTrain batch 22/32 - 66.7ms/batch - loss: 244.18004 - diff: 45.96mlTrain batch 23/32 - 66.4ms/batch - loss: 241.07788 - diff: 45.84mlTrain batch 24/32 - 63.1ms/batch - loss: 237.81186 - diff: 45.55mlTrain batch 25/32 - 62.5ms/batch - loss: 236.24353 - diff: 45.55mlTrain batch 26/32 - 62.5ms/batch - loss: 230.82757 - diff: 44.96mlTrain batch 27/32 - 62.5ms/batch - loss: 227.67395 - diff: 44.71mlTrain batch 28/32 - 62.6ms/batch - loss: 228.44261 - diff: 44.91mlTrain batch 29/32 - 63.2ms/batch - loss: 225.08601 - diff: 44.66mlTrain batch 30/32 - 60.5ms/batch - loss: 223.41450 - diff: 44.50mlTrain batch 31/32 - 60.2ms/batch - loss: 220.89074 - diff: 44.33mlTrain batch 32/32 - 46.2ms/batch - loss: 246.34131 - diff: 44.63mlTrain batch 32/32 - 11.2s 46.2ms/batch - loss: 246.34131 - diff: 44.63ml
Test 0.5s: val_loss: 198.38204 - diff: 41.18ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 7: current best loss = 198.38204, at epoch 6
Train batch 1/32 - 74.6ms/batch - loss: 127.47618 - diff: 34.12mlTrain batch 2/32 - 60.3ms/batch - loss: 135.86285 - diff: 33.51mlTrain batch 3/32 - 62.0ms/batch - loss: 181.65005 - diff: 40.36mlTrain batch 4/32 - 60.1ms/batch - loss: 159.93146 - diff: 38.23mlTrain batch 5/32 - 64.2ms/batch - loss: 206.75123 - diff: 42.22mlTrain batch 6/32 - 63.9ms/batch - loss: 215.74475 - diff: 43.40mlTrain batch 7/32 - 61.3ms/batch - loss: 209.57073 - diff: 42.34mlTrain batch 8/32 - 59.4ms/batch - loss: 210.36267 - diff: 42.40mlTrain batch 9/32 - 58.7ms/batch - loss: 202.48473 - diff: 41.86mlTrain batch 10/32 - 59.6ms/batch - loss: 208.50502 - diff: 42.43mlTrain batch 11/32 - 57.3ms/batch - loss: 200.50938 - diff: 41.62mlTrain batch 12/32 - 61.5ms/batch - loss: 192.61162 - diff: 41.04mlTrain batch 13/32 - 62.3ms/batch - loss: 196.90831 - diff: 41.72mlTrain batch 14/32 - 64.8ms/batch - loss: 192.84688 - diff: 41.47mlTrain batch 15/32 - 63.2ms/batch - loss: 187.87068 - diff: 41.05mlTrain batch 16/32 - 67.0ms/batch - loss: 185.07355 - diff: 41.04mlTrain batch 17/32 - 59.4ms/batch - loss: 180.01091 - diff: 40.62mlTrain batch 18/32 - 62.7ms/batch - loss: 180.93315 - diff: 40.94mlTrain batch 19/32 - 62.5ms/batch - loss: 179.42482 - diff: 40.87mlTrain batch 20/32 - 56.0ms/batch - loss: 177.27548 - diff: 40.76mlTrain batch 21/32 - 60.7ms/batch - loss: 174.30445 - diff: 40.46mlTrain batch 22/32 - 60.9ms/batch - loss: 174.34901 - diff: 40.60mlTrain batch 23/32 - 60.8ms/batch - loss: 175.14982 - diff: 40.77mlTrain batch 24/32 - 60.5ms/batch - loss: 181.73065 - diff: 41.26mlTrain batch 25/32 - 60.5ms/batch - loss: 181.09637 - diff: 41.24mlTrain batch 26/32 - 61.7ms/batch - loss: 201.10615 - diff: 41.99mlTrain batch 27/32 - 61.9ms/batch - loss: 203.71898 - diff: 42.43mlTrain batch 28/32 - 59.7ms/batch - loss: 211.99627 - diff: 43.14mlTrain batch 29/32 - 64.6ms/batch - loss: 213.67027 - diff: 43.49mlTrain batch 30/32 - 59.2ms/batch - loss: 210.24346 - diff: 43.19mlTrain batch 31/32 - 48.3ms/batch - loss: 211.93117 - diff: 43.39mlTrain batch 32/32 - 35.5ms/batch - loss: 214.46776 - diff: 43.36mlTrain batch 32/32 - 10.4s 35.5ms/batch - loss: 214.46776 - diff: 43.36ml
Test 0.5s: val_loss: 165.17242 - diff: 38.92ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 8: current best loss = 165.17242, at epoch 7
Train batch 1/32 - 73.0ms/batch - loss: 186.80130 - diff: 41.50mlTrain batch 2/32 - 64.1ms/batch - loss: 180.14726 - diff: 43.12mlTrain batch 3/32 - 62.4ms/batch - loss: 150.43960 - diff: 39.34mlTrain batch 4/32 - 64.0ms/batch - loss: 148.84522 - diff: 39.64mlTrain batch 5/32 - 58.2ms/batch - loss: 139.06562 - diff: 38.00mlTrain batch 6/32 - 57.7ms/batch - loss: 133.90472 - diff: 37.53mlTrain batch 7/32 - 57.8ms/batch - loss: 253.46921 - diff: 43.56mlTrain batch 8/32 - 55.2ms/batch - loss: 233.35700 - diff: 42.06mlTrain batch 9/32 - 59.5ms/batch - loss: 233.03032 - diff: 43.06mlTrain batch 10/32 - 58.6ms/batch - loss: 216.18009 - diff: 41.54mlTrain batch 11/32 - 58.7ms/batch - loss: 216.23033 - diff: 42.10mlTrain batch 12/32 - 59.0ms/batch - loss: 211.66248 - diff: 42.00mlTrain batch 13/32 - 58.7ms/batch - loss: 203.65400 - diff: 41.43mlTrain batch 14/32 - 58.9ms/batch - loss: 202.92837 - diff: 41.39mlTrain batch 15/32 - 64.3ms/batch - loss: 199.56589 - diff: 41.27mlTrain batch 16/32 - 49.5ms/batch - loss: 191.62442 - diff: 40.53mlTrain batch 17/32 - 53.1ms/batch - loss: 192.99426 - diff: 40.96mlTrain batch 18/32 - 64.4ms/batch - loss: 188.27944 - diff: 40.51mlTrain batch 19/32 - 63.7ms/batch - loss: 190.25195 - diff: 40.99mlTrain batch 20/32 - 48.8ms/batch - loss: 185.60949 - diff: 40.45mlTrain batch 21/32 - 59.9ms/batch - loss: 181.29377 - diff: 40.13mlTrain batch 22/32 - 59.1ms/batch - loss: 181.94148 - diff: 40.36mlTrain batch 23/32 - 64.0ms/batch - loss: 177.25084 - diff: 39.85mlTrain batch 24/32 - 62.5ms/batch - loss: 175.51935 - diff: 39.94mlTrain batch 25/32 - 62.4ms/batch - loss: 172.95598 - diff: 39.66mlTrain batch 26/32 - 62.8ms/batch - loss: 169.96671 - diff: 39.33mlTrain batch 27/32 - 62.7ms/batch - loss: 166.87685 - diff: 38.87mlTrain batch 28/32 - 62.7ms/batch - loss: 164.79200 - diff: 38.71mlTrain batch 29/32 - 63.0ms/batch - loss: 172.22651 - diff: 39.17mlTrain batch 30/32 - 62.2ms/batch - loss: 170.51584 - diff: 39.00mlTrain batch 31/32 - 65.8ms/batch - loss: 170.01737 - diff: 39.01mlTrain batch 32/32 - 49.4ms/batch - loss: 170.80190 - diff: 38.91mlTrain batch 32/32 - 11.4s 49.4ms/batch - loss: 170.80190 - diff: 38.91ml
Test 0.5s: val_loss: 205.66896 - diff: 42.45ml

Epoch 9: current best loss = 165.17242, at epoch 7
Train batch 1/32 - 73.1ms/batch - loss: 262.72485 - diff: 50.94mlTrain batch 2/32 - 62.6ms/batch - loss: 190.67370 - diff: 44.43mlTrain batch 3/32 - 62.5ms/batch - loss: 154.61716 - diff: 39.66mlTrain batch 4/32 - 63.5ms/batch - loss: 156.99271 - diff: 39.80mlTrain batch 5/32 - 64.1ms/batch - loss: 152.32907 - diff: 39.51mlTrain batch 6/32 - 64.0ms/batch - loss: 148.75825 - diff: 39.19mlTrain batch 7/32 - 63.8ms/batch - loss: 165.71631 - diff: 41.30mlTrain batch 8/32 - 63.7ms/batch - loss: 161.62615 - diff: 40.77mlTrain batch 9/32 - 64.3ms/batch - loss: 166.66020 - diff: 41.44mlTrain batch 10/32 - 64.3ms/batch - loss: 163.73805 - diff: 41.41mlTrain batch 11/32 - 63.7ms/batch - loss: 156.03569 - diff: 40.29mlTrain batch 12/32 - 62.8ms/batch - loss: 168.21505 - diff: 41.81mlTrain batch 13/32 - 62.6ms/batch - loss: 163.18575 - diff: 41.21mlTrain batch 14/32 - 58.2ms/batch - loss: 170.13893 - diff: 42.11mlTrain batch 15/32 - 58.0ms/batch - loss: 162.06420 - diff: 40.84mlTrain batch 16/32 - 48.8ms/batch - loss: 161.77638 - diff: 40.76mlTrain batch 17/32 - 46.0ms/batch - loss: 165.60867 - diff: 40.79mlTrain batch 18/32 - 57.5ms/batch - loss: 165.81984 - diff: 40.91mlTrain batch 19/32 - 58.4ms/batch - loss: 164.36698 - diff: 40.84mlTrain batch 20/32 - 60.2ms/batch - loss: 163.25805 - diff: 40.57mlTrain batch 21/32 - 57.8ms/batch - loss: 163.20318 - diff: 40.43mlTrain batch 22/32 - 57.5ms/batch - loss: 161.79215 - diff: 40.41mlTrain batch 23/32 - 59.9ms/batch - loss: 160.90269 - diff: 40.31mlTrain batch 24/32 - 60.3ms/batch - loss: 159.79153 - diff: 40.11mlTrain batch 25/32 - 60.4ms/batch - loss: 161.01360 - diff: 40.35mlTrain batch 26/32 - 67.0ms/batch - loss: 158.10691 - diff: 39.98mlTrain batch 27/32 - 66.4ms/batch - loss: 159.15124 - diff: 40.09mlTrain batch 28/32 - 66.6ms/batch - loss: 175.21816 - diff: 40.46mlTrain batch 29/32 - 60.8ms/batch - loss: 174.04712 - diff: 40.44mlTrain batch 30/32 - 58.7ms/batch - loss: 174.09296 - diff: 40.59mlTrain batch 31/32 - 49.6ms/batch - loss: 179.04581 - diff: 41.31mlTrain batch 32/32 - 50.9ms/batch - loss: 192.02944 - diff: 41.56mlTrain batch 32/32 - 11.2s 50.9ms/batch - loss: 192.02944 - diff: 41.56ml
Test 0.6s: val_loss: 170.28376 - diff: 37.90ml

Epoch 10: current best loss = 165.17242, at epoch 7
Train batch 1/32 - 73.0ms/batch - loss: 161.66971 - diff: 43.53mlTrain batch 2/32 - 63.0ms/batch - loss: 133.51293 - diff: 38.73mlTrain batch 3/32 - 63.5ms/batch - loss: 131.02776 - diff: 38.13mlTrain batch 4/32 - 63.6ms/batch - loss: 157.58573 - diff: 41.13mlTrain batch 5/32 - 62.6ms/batch - loss: 166.54674 - diff: 41.73mlTrain batch 6/32 - 62.6ms/batch - loss: 176.31575 - diff: 42.57mlTrain batch 7/32 - 69.9ms/batch - loss: 165.94769 - diff: 41.21mlTrain batch 8/32 - 62.3ms/batch - loss: 157.58063 - diff: 39.93mlTrain batch 9/32 - 62.4ms/batch - loss: 157.59912 - diff: 40.54mlTrain batch 10/32 - 62.8ms/batch - loss: 161.54592 - diff: 40.53mlTrain batch 11/32 - 66.6ms/batch - loss: 153.17805 - diff: 39.31mlTrain batch 12/32 - 60.9ms/batch - loss: 151.55890 - diff: 38.86mlTrain batch 13/32 - 60.3ms/batch - loss: 153.77463 - diff: 39.40mlTrain batch 14/32 - 67.0ms/batch - loss: 176.90458 - diff: 40.34mlTrain batch 15/32 - 57.9ms/batch - loss: 178.47631 - diff: 40.91mlTrain batch 16/32 - 58.5ms/batch - loss: 178.28162 - diff: 41.04mlTrain batch 17/32 - 58.1ms/batch - loss: 178.62860 - diff: 41.22mlTrain batch 18/32 - 58.5ms/batch - loss: 192.32146 - diff: 41.80mlTrain batch 19/32 - 61.8ms/batch - loss: 191.04141 - diff: 41.91mlTrain batch 20/32 - 61.1ms/batch - loss: 186.16839 - diff: 41.29mlTrain batch 21/32 - 60.9ms/batch - loss: 183.68797 - diff: 41.28mlTrain batch 22/32 - 60.6ms/batch - loss: 180.60780 - diff: 40.96mlTrain batch 23/32 - 66.3ms/batch - loss: 181.07551 - diff: 41.20mlTrain batch 24/32 - 65.7ms/batch - loss: 176.22240 - diff: 40.55mlTrain batch 25/32 - 67.1ms/batch - loss: 175.86079 - diff: 40.56mlTrain batch 26/32 - 66.1ms/batch - loss: 177.01024 - diff: 40.51mlTrain batch 27/32 - 67.9ms/batch - loss: 180.40323 - diff: 40.75mlTrain batch 28/32 - 61.7ms/batch - loss: 180.06829 - diff: 40.86mlTrain batch 29/32 - 67.6ms/batch - loss: 181.43325 - diff: 41.07mlTrain batch 30/32 - 68.0ms/batch - loss: 181.68072 - diff: 41.27mlTrain batch 31/32 - 66.9ms/batch - loss: 180.57701 - diff: 41.31mlTrain batch 32/32 - 51.6ms/batch - loss: 179.57040 - diff: 41.09mlTrain batch 32/32 - 11.4s 51.6ms/batch - loss: 179.57040 - diff: 41.09ml
Test 0.5s: val_loss: 155.11758 - diff: 37.16ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 11: current best loss = 155.11758, at epoch 10
Train batch 1/32 - 67.6ms/batch - loss: 246.31943 - diff: 50.81mlTrain batch 2/32 - 59.6ms/batch - loss: 185.69334 - diff: 44.23mlTrain batch 3/32 - 58.4ms/batch - loss: 189.92695 - diff: 44.01mlTrain batch 4/32 - 59.6ms/batch - loss: 164.18593 - diff: 40.85mlTrain batch 5/32 - 59.4ms/batch - loss: 150.31637 - diff: 38.79mlTrain batch 6/32 - 59.5ms/batch - loss: 146.15486 - diff: 38.28mlTrain batch 7/32 - 59.7ms/batch - loss: 139.19807 - diff: 37.12mlTrain batch 8/32 - 61.7ms/batch - loss: 140.01834 - diff: 37.26mlTrain batch 9/32 - 61.3ms/batch - loss: 143.16725 - diff: 38.00mlTrain batch 10/32 - 64.0ms/batch - loss: 141.58488 - diff: 37.79mlTrain batch 11/32 - 63.1ms/batch - loss: 140.51783 - diff: 37.73mlTrain batch 12/32 - 57.4ms/batch - loss: 148.07826 - diff: 38.14mlTrain batch 13/32 - 61.5ms/batch - loss: 153.12331 - diff: 38.35mlTrain batch 14/32 - 60.3ms/batch - loss: 158.41871 - diff: 39.14mlTrain batch 15/32 - 61.1ms/batch - loss: 166.31332 - diff: 40.14mlTrain batch 16/32 - 60.2ms/batch - loss: 166.71744 - diff: 40.06mlTrain batch 17/32 - 60.9ms/batch - loss: 161.21495 - diff: 39.39mlTrain batch 18/32 - 60.7ms/batch - loss: 160.89921 - diff: 39.41mlTrain batch 19/32 - 60.0ms/batch - loss: 161.07228 - diff: 39.52mlTrain batch 20/32 - 61.6ms/batch - loss: 160.23121 - diff: 39.42mlTrain batch 21/32 - 63.1ms/batch - loss: 158.35914 - diff: 39.23mlTrain batch 22/32 - 62.2ms/batch - loss: 157.07365 - diff: 39.20mlTrain batch 23/32 - 62.3ms/batch - loss: 155.39614 - diff: 39.09mlTrain batch 24/32 - 62.0ms/batch - loss: 151.05101 - diff: 38.42mlTrain batch 25/32 - 62.1ms/batch - loss: 150.47265 - diff: 38.46mlTrain batch 26/32 - 61.5ms/batch - loss: 148.11959 - diff: 38.11mlTrain batch 27/32 - 61.5ms/batch - loss: 146.14897 - diff: 37.90mlTrain batch 28/32 - 60.8ms/batch - loss: 165.21717 - diff: 38.54mlTrain batch 29/32 - 60.5ms/batch - loss: 165.25098 - diff: 38.37mlTrain batch 30/32 - 54.8ms/batch - loss: 165.61860 - diff: 38.41mlTrain batch 31/32 - 59.6ms/batch - loss: 164.62069 - diff: 38.38mlTrain batch 32/32 - 37.6ms/batch - loss: 164.83518 - diff: 38.22mlTrain batch 32/32 - 10.7s 37.6ms/batch - loss: 164.83518 - diff: 38.22ml
Test 0.5s: val_loss: 154.09529 - diff: 39.41ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 12: current best loss = 154.09529, at epoch 11
Train batch 1/32 - 67.1ms/batch - loss: 137.86290 - diff: 31.18mlTrain batch 2/32 - 58.1ms/batch - loss: 131.79435 - diff: 34.58mlTrain batch 3/32 - 59.5ms/batch - loss: 158.02495 - diff: 38.65mlTrain batch 4/32 - 59.5ms/batch - loss: 171.90187 - diff: 40.48mlTrain batch 5/32 - 59.6ms/batch - loss: 157.53868 - diff: 37.85mlTrain batch 6/32 - 59.0ms/batch - loss: 180.82078 - diff: 40.71mlTrain batch 7/32 - 64.3ms/batch - loss: 163.90131 - diff: 38.64mlTrain batch 8/32 - 67.5ms/batch - loss: 164.56512 - diff: 39.53mlTrain batch 9/32 - 59.0ms/batch - loss: 164.85502 - diff: 39.93mlTrain batch 10/32 - 63.9ms/batch - loss: 151.40673 - diff: 37.84mlTrain batch 11/32 - 63.7ms/batch - loss: 159.82779 - diff: 37.64mlTrain batch 12/32 - 62.9ms/batch - loss: 162.77257 - diff: 38.45mlTrain batch 13/32 - 62.6ms/batch - loss: 162.23497 - diff: 38.58mlTrain batch 14/32 - 58.2ms/batch - loss: 162.86162 - diff: 38.92mlTrain batch 15/32 - 58.1ms/batch - loss: 163.92300 - diff: 39.54mlTrain batch 16/32 - 59.5ms/batch - loss: 162.77631 - diff: 39.61mlTrain batch 17/32 - 59.3ms/batch - loss: 165.58601 - diff: 39.96mlTrain batch 18/32 - 58.3ms/batch - loss: 167.42915 - diff: 40.34mlTrain batch 19/32 - 57.6ms/batch - loss: 166.72155 - diff: 40.31mlTrain batch 20/32 - 58.6ms/batch - loss: 167.69897 - diff: 40.36mlTrain batch 21/32 - 57.4ms/batch - loss: 171.49887 - diff: 40.52mlTrain batch 22/32 - 58.7ms/batch - loss: 167.28731 - diff: 40.06mlTrain batch 23/32 - 58.5ms/batch - loss: 163.58116 - diff: 39.55mlTrain batch 24/32 - 57.8ms/batch - loss: 164.23067 - diff: 39.49mlTrain batch 25/32 - 57.9ms/batch - loss: 162.57355 - diff: 39.23mlTrain batch 26/32 - 59.8ms/batch - loss: 185.48444 - diff: 40.28mlTrain batch 27/32 - 60.9ms/batch - loss: 185.12743 - diff: 40.19mlTrain batch 28/32 - 59.6ms/batch - loss: 188.16963 - diff: 40.35mlTrain batch 29/32 - 59.7ms/batch - loss: 186.95707 - diff: 40.25mlTrain batch 30/32 - 54.2ms/batch - loss: 187.36692 - diff: 40.40mlTrain batch 31/32 - 50.1ms/batch - loss: 186.32269 - diff: 40.24mlTrain batch 32/32 - 42.5ms/batch - loss: 194.02261 - diff: 40.30mlTrain batch 32/32 - 10.6s 42.5ms/batch - loss: 194.02261 - diff: 40.30ml
Test 0.5s: val_loss: 187.30669 - diff: 40.93ml

Epoch 13: current best loss = 154.09529, at epoch 11
Train batch 1/32 - 71.4ms/batch - loss: 409.84631 - diff: 63.87mlTrain batch 2/32 - 60.6ms/batch - loss: 276.87944 - diff: 51.80mlTrain batch 3/32 - 53.7ms/batch - loss: 267.61697 - diff: 50.59mlTrain batch 4/32 - 53.2ms/batch - loss: 279.97232 - diff: 50.01mlTrain batch 5/32 - 63.4ms/batch - loss: 245.97352 - diff: 46.53mlTrain batch 6/32 - 61.1ms/batch - loss: 252.06575 - diff: 48.58mlTrain batch 7/32 - 62.4ms/batch - loss: 232.69378 - diff: 46.34mlTrain batch 8/32 - 61.4ms/batch - loss: 224.62802 - diff: 45.43mlTrain batch 9/32 - 61.2ms/batch - loss: 214.74986 - diff: 44.37mlTrain batch 10/32 - 61.1ms/batch - loss: 213.50105 - diff: 44.67mlTrain batch 11/32 - 60.1ms/batch - loss: 209.43700 - diff: 44.05mlTrain batch 12/32 - 60.4ms/batch - loss: 200.88393 - diff: 43.18mlTrain batch 13/32 - 59.5ms/batch - loss: 198.57921 - diff: 42.89mlTrain batch 14/32 - 60.0ms/batch - loss: 191.33794 - diff: 42.34mlTrain batch 15/32 - 62.5ms/batch - loss: 185.53286 - diff: 41.73mlTrain batch 16/32 - 62.4ms/batch - loss: 180.08296 - diff: 40.77mlTrain batch 17/32 - 61.2ms/batch - loss: 184.97846 - diff: 41.10mlTrain batch 18/32 - 61.2ms/batch - loss: 219.30094 - diff: 42.57mlTrain batch 19/32 - 61.5ms/batch - loss: 217.05556 - diff: 42.62mlTrain batch 20/32 - 62.9ms/batch - loss: 213.46306 - diff: 42.44mlTrain batch 21/32 - 62.2ms/batch - loss: 214.08217 - diff: 42.57mlTrain batch 22/32 - 66.1ms/batch - loss: 211.73174 - diff: 42.40mlTrain batch 23/32 - 61.1ms/batch - loss: 208.49507 - diff: 42.25mlTrain batch 24/32 - 65.2ms/batch - loss: 215.11901 - diff: 42.74mlTrain batch 25/32 - 60.6ms/batch - loss: 216.09293 - diff: 42.84mlTrain batch 26/32 - 64.4ms/batch - loss: 211.73877 - diff: 42.40mlTrain batch 27/32 - 60.4ms/batch - loss: 207.70214 - diff: 42.07mlTrain batch 28/32 - 62.9ms/batch - loss: 206.52018 - diff: 42.18mlTrain batch 29/32 - 52.9ms/batch - loss: 202.92691 - diff: 41.83mlTrain batch 30/32 - 52.1ms/batch - loss: 198.86123 - diff: 41.28mlTrain batch 31/32 - 57.6ms/batch - loss: 195.12135 - diff: 40.91mlTrain batch 32/32 - 52.6ms/batch - loss: 202.14577 - diff: 41.03mlTrain batch 32/32 - 11.2s 52.6ms/batch - loss: 202.14577 - diff: 41.03ml
Test 0.5s: val_loss: 178.20944 - diff: 43.85ml

Epoch 14: current best loss = 154.09529, at epoch 11
Train batch 1/32 - 71.2ms/batch - loss: 114.44153 - diff: 33.39mlTrain batch 2/32 - 62.6ms/batch - loss: 125.62520 - diff: 34.26mlTrain batch 3/32 - 61.0ms/batch - loss: 311.82976 - diff: 45.62mlTrain batch 4/32 - 61.2ms/batch - loss: 256.39662 - diff: 41.36mlTrain batch 5/32 - 62.6ms/batch - loss: 263.34775 - diff: 42.89mlTrain batch 6/32 - 62.6ms/batch - loss: 270.62673 - diff: 45.83mlTrain batch 7/32 - 68.3ms/batch - loss: 239.68404 - diff: 42.45mlTrain batch 8/32 - 62.4ms/batch - loss: 227.08737 - diff: 41.90mlTrain batch 9/32 - 73.3ms/batch - loss: 221.52939 - diff: 41.60mlTrain batch 10/32 - 64.8ms/batch - loss: 208.14855 - diff: 40.24mlTrain batch 11/32 - 72.6ms/batch - loss: 206.19384 - diff: 40.88mlTrain batch 12/32 - 66.1ms/batch - loss: 208.77948 - diff: 41.48mlTrain batch 13/32 - 66.8ms/batch - loss: 211.77749 - diff: 42.08mlTrain batch 14/32 - 57.3ms/batch - loss: 202.95112 - diff: 41.10mlTrain batch 15/32 - 63.7ms/batch - loss: 200.03799 - diff: 41.22mlTrain batch 16/32 - 52.0ms/batch - loss: 194.85893 - diff: 40.88mlTrain batch 17/32 - 46.1ms/batch - loss: 189.10792 - diff: 40.40mlTrain batch 18/32 - 45.1ms/batch - loss: 183.53078 - diff: 39.97mlTrain batch 19/32 - 61.8ms/batch - loss: 180.61823 - diff: 39.87mlTrain batch 20/32 - 61.1ms/batch - loss: 176.72858 - diff: 39.41mlTrain batch 21/32 - 64.5ms/batch - loss: 177.24098 - diff: 39.63mlTrain batch 22/32 - 61.8ms/batch - loss: 176.02028 - diff: 39.44mlTrain batch 23/32 - 62.3ms/batch - loss: 185.16749 - diff: 40.27mlTrain batch 24/32 - 61.4ms/batch - loss: 185.90222 - diff: 40.66mlTrain batch 25/32 - 61.2ms/batch - loss: 186.16855 - diff: 40.67mlTrain batch 26/32 - 66.6ms/batch - loss: 183.06101 - diff: 40.33mlTrain batch 27/32 - 67.0ms/batch - loss: 179.93871 - diff: 40.02mlTrain batch 28/32 - 63.5ms/batch - loss: 178.67465 - diff: 39.97mlTrain batch 29/32 - 66.0ms/batch - loss: 178.36273 - diff: 39.99mlTrain batch 30/32 - 64.0ms/batch - loss: 176.59978 - diff: 39.89mlTrain batch 31/32 - 66.3ms/batch - loss: 178.82456 - diff: 40.15mlTrain batch 32/32 - 48.6ms/batch - loss: 179.42278 - diff: 40.06mlTrain batch 32/32 - 10.5s 48.6ms/batch - loss: 179.42278 - diff: 40.06ml
Test 0.5s: val_loss: 180.46460 - diff: 42.27ml

Epoch 15: current best loss = 154.09529, at epoch 11
Train batch 1/32 - 70.3ms/batch - loss: 102.65883 - diff: 30.47mlTrain batch 2/32 - 60.2ms/batch - loss: 92.23317 - diff: 29.16mlTrain batch 3/32 - 60.7ms/batch - loss: 81.67086 - diff: 27.96mlTrain batch 4/32 - 61.8ms/batch - loss: 102.65664 - diff: 31.08mlTrain batch 5/32 - 61.7ms/batch - loss: 117.75489 - diff: 32.19mlTrain batch 6/32 - 62.3ms/batch - loss: 120.53429 - diff: 33.15mlTrain batch 7/32 - 61.6ms/batch - loss: 115.01782 - diff: 32.80mlTrain batch 8/32 - 62.3ms/batch - loss: 114.67347 - diff: 33.08mlTrain batch 9/32 - 62.1ms/batch - loss: 117.78388 - diff: 33.82mlTrain batch 10/32 - 60.4ms/batch - loss: 113.36775 - diff: 33.17mlTrain batch 11/32 - 60.6ms/batch - loss: 119.84819 - diff: 33.43mlTrain batch 12/32 - 60.7ms/batch - loss: 119.99338 - diff: 32.91mlTrain batch 13/32 - 60.1ms/batch - loss: 129.85626 - diff: 34.16mlTrain batch 14/32 - 63.0ms/batch - loss: 161.43834 - diff: 34.90mlTrain batch 15/32 - 62.6ms/batch - loss: 160.58687 - diff: 34.95mlTrain batch 16/32 - 49.7ms/batch - loss: 155.78575 - diff: 34.60mlTrain batch 17/32 - 57.7ms/batch - loss: 156.74238 - diff: 34.96mlTrain batch 18/32 - 62.5ms/batch - loss: 155.25005 - diff: 35.17mlTrain batch 19/32 - 65.3ms/batch - loss: 154.06023 - diff: 35.35mlTrain batch 20/32 - 64.6ms/batch - loss: 155.02994 - diff: 35.63mlTrain batch 21/32 - 70.4ms/batch - loss: 153.65113 - diff: 35.79mlTrain batch 22/32 - 60.5ms/batch - loss: 154.59423 - diff: 36.14mlTrain batch 23/32 - 66.6ms/batch - loss: 156.64710 - diff: 36.63mlTrain batch 24/32 - 66.2ms/batch - loss: 157.31215 - diff: 36.83mlTrain batch 25/32 - 64.3ms/batch - loss: 155.49318 - diff: 36.70mlTrain batch 26/32 - 64.7ms/batch - loss: 157.04187 - diff: 36.99mlTrain batch 27/32 - 66.2ms/batch - loss: 156.99793 - diff: 36.94mlTrain batch 28/32 - 64.1ms/batch - loss: 160.89370 - diff: 37.17mlTrain batch 29/32 - 64.6ms/batch - loss: 160.80006 - diff: 37.20mlTrain batch 30/32 - 63.7ms/batch - loss: 159.50220 - diff: 36.99mlTrain batch 31/32 - 53.7ms/batch - loss: 156.34508 - diff: 36.53mlTrain batch 32/32 - 41.6ms/batch - loss: 160.56217 - diff: 36.56mlTrain batch 32/32 - 12.0s 41.6ms/batch - loss: 160.56217 - diff: 36.56ml
Test 0.5s: val_loss: 166.50651 - diff: 38.10ml

Epoch 16: current best loss = 154.09529, at epoch 11
Train batch 1/32 - 73.7ms/batch - loss: 141.55022 - diff: 37.20mlTrain batch 2/32 - 63.4ms/batch - loss: 182.53703 - diff: 40.06mlTrain batch 3/32 - 64.8ms/batch - loss: 196.02807 - diff: 44.03mlTrain batch 4/32 - 64.3ms/batch - loss: 189.13905 - diff: 43.25mlTrain batch 5/32 - 66.4ms/batch - loss: 181.06895 - diff: 42.17mlTrain batch 6/32 - 66.6ms/batch - loss: 165.60891 - diff: 40.48mlTrain batch 7/32 - 58.9ms/batch - loss: 175.58326 - diff: 42.16mlTrain batch 8/32 - 61.8ms/batch - loss: 170.37978 - diff: 41.41mlTrain batch 9/32 - 58.8ms/batch - loss: 158.56616 - diff: 39.29mlTrain batch 10/32 - 45.7ms/batch - loss: 150.80388 - diff: 38.11mlTrain batch 11/32 - 45.7ms/batch - loss: 155.87939 - diff: 38.64mlTrain batch 12/32 - 61.7ms/batch - loss: 150.61606 - diff: 38.08mlTrain batch 13/32 - 53.4ms/batch - loss: 147.00101 - diff: 37.65mlTrain batch 14/32 - 52.6ms/batch - loss: 142.58934 - diff: 37.00mlTrain batch 15/32 - 52.5ms/batch - loss: 140.52645 - diff: 36.77mlTrain batch 16/32 - 59.1ms/batch - loss: 170.53033 - diff: 37.91mlTrain batch 17/32 - 58.3ms/batch - loss: 170.53859 - diff: 37.99mlTrain batch 18/32 - 59.0ms/batch - loss: 172.83285 - diff: 38.71mlTrain batch 19/32 - 58.2ms/batch - loss: 171.77594 - diff: 38.76mlTrain batch 20/32 - 58.0ms/batch - loss: 171.28112 - diff: 38.91mlTrain batch 21/32 - 57.4ms/batch - loss: 171.27043 - diff: 39.00mlTrain batch 22/32 - 57.9ms/batch - loss: 170.50394 - diff: 39.14mlTrain batch 23/32 - 57.0ms/batch - loss: 174.98657 - diff: 39.71mlTrain batch 24/32 - 57.8ms/batch - loss: 175.50737 - diff: 39.94mlTrain batch 25/32 - 58.1ms/batch - loss: 177.19244 - diff: 40.14mlTrain batch 26/32 - 58.6ms/batch - loss: 175.92947 - diff: 40.18mlTrain batch 27/32 - 59.0ms/batch - loss: 174.30644 - diff: 39.83mlTrain batch 28/32 - 58.5ms/batch - loss: 172.71849 - diff: 39.83mlTrain batch 29/32 - 58.4ms/batch - loss: 170.05318 - diff: 39.52mlTrain batch 30/32 - 63.3ms/batch - loss: 169.09898 - diff: 39.48mlTrain batch 31/32 - 63.1ms/batch - loss: 168.47258 - diff: 39.46mlTrain batch 32/32 - 51.3ms/batch - loss: 174.48540 - diff: 39.55mlTrain batch 32/32 - 10.7s 51.3ms/batch - loss: 174.48540 - diff: 39.55ml
Test 0.6s: val_loss: 148.24886 - diff: 36.50ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 17: current best loss = 148.24886, at epoch 16
Train batch 1/32 - 67.4ms/batch - loss: 61.19102 - diff: 24.71mlTrain batch 2/32 - 58.1ms/batch - loss: 176.38709 - diff: 40.05mlTrain batch 3/32 - 59.5ms/batch - loss: 132.94268 - diff: 33.90mlTrain batch 4/32 - 59.3ms/batch - loss: 138.31132 - diff: 35.18mlTrain batch 5/32 - 59.5ms/batch - loss: 134.35291 - diff: 35.28mlTrain batch 6/32 - 59.8ms/batch - loss: 223.34496 - diff: 40.07mlTrain batch 7/32 - 59.3ms/batch - loss: 219.93059 - diff: 41.16mlTrain batch 8/32 - 59.0ms/batch - loss: 199.95600 - diff: 39.33mlTrain batch 9/32 - 58.6ms/batch - loss: 193.51035 - diff: 39.36mlTrain batch 10/32 - 57.0ms/batch - loss: 192.67552 - diff: 39.71mlTrain batch 11/32 - 56.3ms/batch - loss: 187.37389 - diff: 39.38mlTrain batch 12/32 - 60.6ms/batch - loss: 180.75644 - diff: 38.79mlTrain batch 13/32 - 60.7ms/batch - loss: 173.04048 - diff: 37.94mlTrain batch 14/32 - 61.0ms/batch - loss: 168.11686 - diff: 37.50mlTrain batch 15/32 - 67.3ms/batch - loss: 170.22634 - diff: 37.60mlTrain batch 16/32 - 63.4ms/batch - loss: 172.97485 - diff: 38.25mlTrain batch 17/32 - 62.2ms/batch - loss: 168.90715 - diff: 37.81mlTrain batch 18/32 - 58.8ms/batch - loss: 167.61145 - diff: 37.76mlTrain batch 19/32 - 61.8ms/batch - loss: 165.31239 - diff: 37.55mlTrain batch 20/32 - 67.3ms/batch - loss: 163.14309 - diff: 37.53mlTrain batch 21/32 - 68.1ms/batch - loss: 162.85285 - diff: 37.58mlTrain batch 22/32 - 61.2ms/batch - loss: 163.82649 - diff: 37.96mlTrain batch 23/32 - 61.6ms/batch - loss: 167.01794 - diff: 38.26mlTrain batch 24/32 - 64.1ms/batch - loss: 167.17847 - diff: 38.15mlTrain batch 25/32 - 65.6ms/batch - loss: 166.89208 - diff: 38.33mlTrain batch 26/32 - 65.3ms/batch - loss: 168.12485 - diff: 38.48mlTrain batch 27/32 - 66.2ms/batch - loss: 168.49094 - diff: 38.77mlTrain batch 28/32 - 65.6ms/batch - loss: 165.17950 - diff: 38.41mlTrain batch 29/32 - 67.2ms/batch - loss: 166.03521 - diff: 38.72mlTrain batch 30/32 - 68.0ms/batch - loss: 166.25987 - diff: 38.80mlTrain batch 31/32 - 67.0ms/batch - loss: 164.27557 - diff: 38.61mlTrain batch 32/32 - 59.8ms/batch - loss: 168.26929 - diff: 38.68mlTrain batch 32/32 - 11.6s 59.8ms/batch - loss: 168.26929 - diff: 38.68ml
Test 0.5s: val_loss: 131.93384 - diff: 33.68ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 18: current best loss = 131.93384, at epoch 17
Train batch 1/32 - 73.9ms/batch - loss: 141.53085 - diff: 34.26mlTrain batch 2/32 - 59.0ms/batch - loss: 162.49121 - diff: 36.14mlTrain batch 3/32 - 69.4ms/batch - loss: 201.71606 - diff: 42.91mlTrain batch 4/32 - 60.3ms/batch - loss: 199.11290 - diff: 42.16mlTrain batch 5/32 - 70.7ms/batch - loss: 181.95525 - diff: 41.17mlTrain batch 6/32 - 61.4ms/batch - loss: 170.31382 - diff: 40.05mlTrain batch 7/32 - 70.1ms/batch - loss: 161.48028 - diff: 39.12mlTrain batch 8/32 - 60.8ms/batch - loss: 147.79128 - diff: 37.15mlTrain batch 9/32 - 68.7ms/batch - loss: 140.53037 - diff: 36.07mlTrain batch 10/32 - 60.5ms/batch - loss: 142.69262 - diff: 37.00mlTrain batch 11/32 - 65.1ms/batch - loss: 145.78589 - diff: 37.70mlTrain batch 12/32 - 55.7ms/batch - loss: 143.68668 - diff: 37.57mlTrain batch 13/32 - 58.8ms/batch - loss: 139.50057 - diff: 37.19mlTrain batch 14/32 - 60.8ms/batch - loss: 143.48403 - diff: 37.80mlTrain batch 15/32 - 62.1ms/batch - loss: 138.55531 - diff: 37.05mlTrain batch 16/32 - 65.5ms/batch - loss: 136.86465 - diff: 36.86mlTrain batch 17/32 - 64.8ms/batch - loss: 136.75628 - diff: 36.85mlTrain batch 18/32 - 58.7ms/batch - loss: 159.80530 - diff: 37.77mlTrain batch 19/32 - 57.8ms/batch - loss: 162.88140 - diff: 37.62mlTrain batch 20/32 - 57.8ms/batch - loss: 158.65548 - diff: 37.20mlTrain batch 21/32 - 58.6ms/batch - loss: 155.32846 - diff: 36.97mlTrain batch 22/32 - 59.0ms/batch - loss: 160.60700 - diff: 37.61mlTrain batch 23/32 - 58.9ms/batch - loss: 159.43213 - diff: 37.56mlTrain batch 24/32 - 58.9ms/batch - loss: 157.56991 - diff: 37.55mlTrain batch 25/32 - 57.6ms/batch - loss: 156.01479 - diff: 37.24mlTrain batch 26/32 - 57.9ms/batch - loss: 167.38864 - diff: 38.11mlTrain batch 27/32 - 58.2ms/batch - loss: 165.59476 - diff: 38.01mlTrain batch 28/32 - 58.1ms/batch - loss: 166.29000 - diff: 38.26mlTrain batch 29/32 - 58.3ms/batch - loss: 165.15578 - diff: 38.24mlTrain batch 30/32 - 56.7ms/batch - loss: 167.22078 - diff: 38.58mlTrain batch 31/32 - 48.2ms/batch - loss: 165.80387 - diff: 38.50mlTrain batch 32/32 - 37.1ms/batch - loss: 170.34822 - diff: 38.58mlTrain batch 32/32 - 10.5s 37.1ms/batch - loss: 170.34822 - diff: 38.58ml
Test 0.5s: val_loss: 146.27893 - diff: 36.62ml

Epoch 19: current best loss = 131.93384, at epoch 17
Train batch 1/32 - 78.1ms/batch - loss: 75.57954 - diff: 26.82mlTrain batch 2/32 - 61.2ms/batch - loss: 81.29459 - diff: 27.78mlTrain batch 3/32 - 63.3ms/batch - loss: 114.65825 - diff: 33.95mlTrain batch 4/32 - 62.7ms/batch - loss: 139.54448 - diff: 37.86mlTrain batch 5/32 - 64.7ms/batch - loss: 153.48903 - diff: 39.06mlTrain batch 6/32 - 65.3ms/batch - loss: 158.89563 - diff: 39.93mlTrain batch 7/32 - 64.2ms/batch - loss: 150.57019 - diff: 38.71mlTrain batch 8/32 - 64.1ms/batch - loss: 143.14159 - diff: 37.96mlTrain batch 9/32 - 58.2ms/batch - loss: 138.97981 - diff: 37.81mlTrain batch 10/32 - 60.6ms/batch - loss: 140.60963 - diff: 38.11mlTrain batch 11/32 - 61.2ms/batch - loss: 130.85496 - diff: 36.22mlTrain batch 12/32 - 60.3ms/batch - loss: 134.83883 - diff: 36.71mlTrain batch 13/32 - 49.2ms/batch - loss: 136.57601 - diff: 36.81mlTrain batch 14/32 - 52.5ms/batch - loss: 172.57485 - diff: 38.12mlTrain batch 15/32 - 62.2ms/batch - loss: 165.85850 - diff: 37.61mlTrain batch 16/32 - 69.5ms/batch - loss: 167.71829 - diff: 38.21mlTrain batch 17/32 - 62.1ms/batch - loss: 164.50108 - diff: 37.84mlTrain batch 18/32 - 68.4ms/batch - loss: 161.19833 - diff: 37.53mlTrain batch 19/32 - 60.8ms/batch - loss: 162.99280 - diff: 37.78mlTrain batch 20/32 - 67.4ms/batch - loss: 161.10645 - diff: 37.82mlTrain batch 21/32 - 61.1ms/batch - loss: 161.75286 - diff: 38.06mlTrain batch 22/32 - 58.5ms/batch - loss: 160.35673 - diff: 37.87mlTrain batch 23/32 - 63.1ms/batch - loss: 159.66986 - diff: 37.78mlTrain batch 24/32 - 62.7ms/batch - loss: 154.85103 - diff: 37.13mlTrain batch 25/32 - 62.4ms/batch - loss: 151.26311 - diff: 36.64mlTrain batch 26/32 - 61.6ms/batch - loss: 149.49952 - diff: 36.54mlTrain batch 27/32 - 61.8ms/batch - loss: 148.47770 - diff: 36.63mlTrain batch 28/32 - 61.6ms/batch - loss: 148.67287 - diff: 36.75mlTrain batch 29/32 - 60.5ms/batch - loss: 147.94058 - diff: 36.64mlTrain batch 30/32 - 60.1ms/batch - loss: 147.90608 - diff: 36.68mlTrain batch 31/32 - 52.6ms/batch - loss: 147.57583 - diff: 36.76mlTrain batch 32/32 - 45.5ms/batch - loss: 147.18903 - diff: 36.60mlTrain batch 32/32 - 11.3s 45.5ms/batch - loss: 147.18903 - diff: 36.60ml
Test 0.5s: val_loss: 143.68153 - diff: 37.81ml

Epoch 20: current best loss = 131.93384, at epoch 17
Train batch 1/32 - 77.7ms/batch - loss: 81.82216 - diff: 28.72mlTrain batch 2/32 - 65.8ms/batch - loss: 104.13335 - diff: 35.24mlTrain batch 3/32 - 63.4ms/batch - loss: 111.59188 - diff: 35.29mlTrain batch 4/32 - 64.1ms/batch - loss: 110.59908 - diff: 35.19mlTrain batch 5/32 - 58.4ms/batch - loss: 189.99674 - diff: 38.83mlTrain batch 6/32 - 58.2ms/batch - loss: 174.51705 - diff: 37.45mlTrain batch 7/32 - 63.6ms/batch - loss: 168.29795 - diff: 36.80mlTrain batch 8/32 - 62.9ms/batch - loss: 164.54684 - diff: 36.21mlTrain batch 9/32 - 61.1ms/batch - loss: 161.80874 - diff: 36.45mlTrain batch 10/32 - 61.2ms/batch - loss: 157.85044 - diff: 36.19mlTrain batch 11/32 - 60.0ms/batch - loss: 167.09933 - diff: 37.24mlTrain batch 12/32 - 60.4ms/batch - loss: 163.74231 - diff: 37.13mlTrain batch 13/32 - 56.4ms/batch - loss: 162.09546 - diff: 37.08mlTrain batch 14/32 - 58.6ms/batch - loss: 158.07613 - diff: 36.89mlTrain batch 15/32 - 65.6ms/batch - loss: 152.74266 - diff: 36.23mlTrain batch 16/32 - 65.2ms/batch - loss: 151.07292 - diff: 36.28mlTrain batch 17/32 - 66.1ms/batch - loss: 149.93030 - diff: 36.51mlTrain batch 18/32 - 63.4ms/batch - loss: 148.64247 - diff: 36.51mlTrain batch 19/32 - 66.5ms/batch - loss: 144.74913 - diff: 36.06mlTrain batch 20/32 - 59.7ms/batch - loss: 145.37388 - diff: 36.29mlTrain batch 21/32 - 61.5ms/batch - loss: 150.89807 - diff: 36.98mlTrain batch 22/32 - 58.8ms/batch - loss: 149.47672 - diff: 36.75mlTrain batch 23/32 - 58.1ms/batch - loss: 150.65151 - diff: 36.56mlTrain batch 24/32 - 58.4ms/batch - loss: 152.95256 - diff: 36.70mlTrain batch 25/32 - 59.3ms/batch - loss: 150.81732 - diff: 36.33mlTrain batch 26/32 - 58.6ms/batch - loss: 148.66144 - diff: 36.14mlTrain batch 27/32 - 58.6ms/batch - loss: 147.80667 - diff: 36.13mlTrain batch 28/32 - 57.5ms/batch - loss: 146.05047 - diff: 36.00mlTrain batch 29/32 - 61.6ms/batch - loss: 144.55075 - diff: 35.90mlTrain batch 30/32 - 54.4ms/batch - loss: 144.34607 - diff: 35.93mlTrain batch 31/32 - 64.0ms/batch - loss: 142.41342 - diff: 35.80mlTrain batch 32/32 - 45.3ms/batch - loss: 142.82121 - diff: 35.72mlTrain batch 32/32 - 11.2s 45.3ms/batch - loss: 142.82121 - diff: 35.72ml
Test 0.5s: val_loss: 121.75735 - diff: 32.49ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 21: current best loss = 121.75735, at epoch 20
Train batch 1/32 - 83.8ms/batch - loss: 156.84579 - diff: 42.31mlTrain batch 2/32 - 58.1ms/batch - loss: 166.37212 - diff: 43.57mlTrain batch 3/32 - 65.0ms/batch - loss: 131.28169 - diff: 36.95mlTrain batch 4/32 - 58.5ms/batch - loss: 120.20603 - diff: 35.36mlTrain batch 5/32 - 67.6ms/batch - loss: 123.60474 - diff: 35.67mlTrain batch 6/32 - 59.5ms/batch - loss: 138.28634 - diff: 37.18mlTrain batch 7/32 - 67.3ms/batch - loss: 146.40588 - diff: 38.68mlTrain batch 8/32 - 58.1ms/batch - loss: 144.36490 - diff: 38.27mlTrain batch 9/32 - 64.9ms/batch - loss: 140.68683 - diff: 37.67mlTrain batch 10/32 - 58.5ms/batch - loss: 137.08667 - diff: 37.22mlTrain batch 11/32 - 58.8ms/batch - loss: 133.81604 - diff: 36.67mlTrain batch 12/32 - 58.1ms/batch - loss: 176.46696 - diff: 38.98mlTrain batch 13/32 - 52.4ms/batch - loss: 183.03909 - diff: 39.55mlTrain batch 14/32 - 58.3ms/batch - loss: 185.80497 - diff: 39.96mlTrain batch 15/32 - 52.6ms/batch - loss: 179.68793 - diff: 39.26mlTrain batch 16/32 - 50.0ms/batch - loss: 174.65440 - diff: 38.57mlTrain batch 17/32 - 58.7ms/batch - loss: 171.60435 - diff: 38.34mlTrain batch 18/32 - 50.2ms/batch - loss: 166.55752 - diff: 37.51mlTrain batch 19/32 - 59.3ms/batch - loss: 167.61272 - diff: 38.02mlTrain batch 20/32 - 60.4ms/batch - loss: 167.36661 - diff: 38.44mlTrain batch 21/32 - 62.2ms/batch - loss: 165.79445 - diff: 38.31mlTrain batch 22/32 - 63.5ms/batch - loss: 168.19566 - diff: 38.78mlTrain batch 23/32 - 64.9ms/batch - loss: 167.61261 - diff: 38.82mlTrain batch 24/32 - 64.2ms/batch - loss: 165.60544 - diff: 38.62mlTrain batch 25/32 - 64.3ms/batch - loss: 163.88884 - diff: 38.44mlTrain batch 26/32 - 57.7ms/batch - loss: 160.57436 - diff: 37.93mlTrain batch 27/32 - 57.4ms/batch - loss: 157.32357 - diff: 37.62mlTrain batch 28/32 - 58.6ms/batch - loss: 155.47377 - diff: 37.55mlTrain batch 29/32 - 57.5ms/batch - loss: 155.50930 - diff: 37.76mlTrain batch 30/32 - 57.9ms/batch - loss: 155.32892 - diff: 37.85mlTrain batch 31/32 - 56.0ms/batch - loss: 157.12681 - diff: 38.13mlTrain batch 32/32 - 52.7ms/batch - loss: 160.27159 - diff: 38.16mlTrain batch 32/32 - 11.2s 52.7ms/batch - loss: 160.27159 - diff: 38.16ml
Test 0.6s: val_loss: 133.05513 - diff: 34.89ml

Epoch 22: current best loss = 121.75735, at epoch 20
Train batch 1/32 - 71.8ms/batch - loss: 98.05388 - diff: 32.23mlTrain batch 2/32 - 61.7ms/batch - loss: 137.26316 - diff: 36.04mlTrain batch 3/32 - 61.4ms/batch - loss: 128.64070 - diff: 35.23mlTrain batch 4/32 - 62.6ms/batch - loss: 131.01164 - diff: 35.23mlTrain batch 5/32 - 62.9ms/batch - loss: 122.22988 - diff: 33.80mlTrain batch 6/32 - 65.0ms/batch - loss: 122.94329 - diff: 33.94mlTrain batch 7/32 - 60.3ms/batch - loss: 172.58350 - diff: 36.09mlTrain batch 8/32 - 61.4ms/batch - loss: 171.87530 - diff: 36.89mlTrain batch 9/32 - 61.0ms/batch - loss: 161.93478 - diff: 35.78mlTrain batch 10/32 - 61.2ms/batch - loss: 154.87956 - diff: 35.31mlTrain batch 11/32 - 60.7ms/batch - loss: 145.96774 - diff: 34.26mlTrain batch 12/32 - 63.0ms/batch - loss: 144.41842 - diff: 34.63mlTrain batch 13/32 - 59.8ms/batch - loss: 160.01544 - diff: 35.69mlTrain batch 14/32 - 62.1ms/batch - loss: 157.88146 - diff: 36.07mlTrain batch 15/32 - 61.9ms/batch - loss: 155.22501 - diff: 36.03mlTrain batch 16/32 - 61.6ms/batch - loss: 148.63808 - diff: 35.38mlTrain batch 17/32 - 63.9ms/batch - loss: 148.23819 - diff: 35.59mlTrain batch 18/32 - 63.7ms/batch - loss: 146.07639 - diff: 35.45mlTrain batch 19/32 - 50.6ms/batch - loss: 145.96166 - diff: 35.71mlTrain batch 20/32 - 63.7ms/batch - loss: 144.64778 - diff: 35.76mlTrain batch 21/32 - 54.8ms/batch - loss: 143.36256 - diff: 35.69mlTrain batch 22/32 - 58.6ms/batch - loss: 140.35350 - diff: 35.34mlTrain batch 23/32 - 57.3ms/batch - loss: 136.10689 - diff: 34.69mlTrain batch 24/32 - 59.1ms/batch - loss: 134.33830 - diff: 34.64mlTrain batch 25/32 - 58.8ms/batch - loss: 139.78708 - diff: 35.32mlTrain batch 26/32 - 59.8ms/batch - loss: 138.77574 - diff: 35.14mlTrain batch 27/32 - 59.1ms/batch - loss: 137.54455 - diff: 34.92mlTrain batch 28/32 - 60.9ms/batch - loss: 135.61207 - diff: 34.79mlTrain batch 29/32 - 59.8ms/batch - loss: 135.72748 - diff: 34.95mlTrain batch 30/32 - 60.8ms/batch - loss: 133.71743 - diff: 34.62mlTrain batch 31/32 - 60.3ms/batch - loss: 135.88883 - diff: 34.90mlTrain batch 32/32 - 50.5ms/batch - loss: 136.93759 - diff: 34.85mlTrain batch 32/32 - 10.7s 50.5ms/batch - loss: 136.93759 - diff: 34.85ml
Test 0.6s: val_loss: 174.22561 - diff: 39.64ml

Epoch 23: current best loss = 121.75735, at epoch 20
Train batch 1/32 - 68.4ms/batch - loss: 144.21111 - diff: 34.49mlTrain batch 2/32 - 59.0ms/batch - loss: 116.24310 - diff: 31.12mlTrain batch 3/32 - 59.0ms/batch - loss: 102.24819 - diff: 30.72mlTrain batch 4/32 - 61.6ms/batch - loss: 114.89802 - diff: 32.82mlTrain batch 5/32 - 58.3ms/batch - loss: 102.95040 - diff: 31.28mlTrain batch 6/32 - 58.3ms/batch - loss: 116.21229 - diff: 33.63mlTrain batch 7/32 - 57.8ms/batch - loss: 122.65878 - diff: 34.88mlTrain batch 8/32 - 59.6ms/batch - loss: 120.21758 - diff: 34.84mlTrain batch 9/32 - 59.3ms/batch - loss: 121.25011 - diff: 34.88mlTrain batch 10/32 - 59.6ms/batch - loss: 125.14093 - diff: 35.45mlTrain batch 11/32 - 53.7ms/batch - loss: 125.63704 - diff: 34.91mlTrain batch 12/32 - 56.5ms/batch - loss: 128.87572 - diff: 35.67mlTrain batch 13/32 - 52.7ms/batch - loss: 127.98300 - diff: 35.61mlTrain batch 14/32 - 62.7ms/batch - loss: 128.45483 - diff: 35.33mlTrain batch 15/32 - 58.8ms/batch - loss: 129.69577 - diff: 35.71mlTrain batch 16/32 - 58.4ms/batch - loss: 128.07077 - diff: 35.44mlTrain batch 17/32 - 58.2ms/batch - loss: 124.31375 - diff: 34.84mlTrain batch 18/32 - 60.0ms/batch - loss: 122.84232 - diff: 34.38mlTrain batch 19/32 - 60.9ms/batch - loss: 137.67270 - diff: 34.69mlTrain batch 20/32 - 58.4ms/batch - loss: 135.78022 - diff: 34.54mlTrain batch 21/32 - 58.2ms/batch - loss: 133.30209 - diff: 34.40mlTrain batch 22/32 - 58.2ms/batch - loss: 140.94899 - diff: 34.84mlTrain batch 23/32 - 58.4ms/batch - loss: 138.16008 - diff: 34.51mlTrain batch 24/32 - 58.4ms/batch - loss: 141.54304 - diff: 35.11mlTrain batch 25/32 - 57.4ms/batch - loss: 140.46176 - diff: 35.15mlTrain batch 26/32 - 61.4ms/batch - loss: 145.95884 - diff: 35.63mlTrain batch 27/32 - 61.2ms/batch - loss: 145.13490 - diff: 35.63mlTrain batch 28/32 - 62.7ms/batch - loss: 144.26423 - diff: 35.70mlTrain batch 29/32 - 62.2ms/batch - loss: 145.02723 - diff: 35.91mlTrain batch 30/32 - 61.6ms/batch - loss: 144.24498 - diff: 35.75mlTrain batch 31/32 - 61.4ms/batch - loss: 143.75148 - diff: 35.82mlTrain batch 32/32 - 45.4ms/batch - loss: 144.25621 - diff: 35.72mlTrain batch 32/32 - 10.8s 45.4ms/batch - loss: 144.25621 - diff: 35.72ml
Test 0.6s: val_loss: 161.64052 - diff: 37.70ml

Epoch 24: current best loss = 121.75735, at epoch 20
Train batch 1/32 - 57.8ms/batch - loss: 72.89397 - diff: 26.42mlTrain batch 2/32 - 45.9ms/batch - loss: 90.89225 - diff: 30.48mlTrain batch 3/32 - 59.5ms/batch - loss: 88.72256 - diff: 30.81mlTrain batch 4/32 - 58.7ms/batch - loss: 88.81051 - diff: 30.75mlTrain batch 5/32 - 58.8ms/batch - loss: 170.94978 - diff: 36.42mlTrain batch 6/32 - 58.7ms/batch - loss: 165.40648 - diff: 36.38mlTrain batch 7/32 - 66.7ms/batch - loss: 157.37509 - diff: 34.97mlTrain batch 8/32 - 59.2ms/batch - loss: 155.31733 - diff: 35.70mlTrain batch 9/32 - 66.8ms/batch - loss: 157.72148 - diff: 36.39mlTrain batch 10/32 - 59.4ms/batch - loss: 152.10416 - diff: 36.24mlTrain batch 11/32 - 59.7ms/batch - loss: 150.19919 - diff: 36.05mlTrain batch 12/32 - 59.3ms/batch - loss: 145.95354 - diff: 35.94mlTrain batch 13/32 - 58.8ms/batch - loss: 144.94561 - diff: 35.62mlTrain batch 14/32 - 58.0ms/batch - loss: 142.15995 - diff: 35.39mlTrain batch 15/32 - 59.7ms/batch - loss: 157.84724 - diff: 36.48mlTrain batch 16/32 - 59.5ms/batch - loss: 158.59077 - diff: 36.43mlTrain batch 17/32 - 65.7ms/batch - loss: 157.24172 - diff: 36.42mlTrain batch 18/32 - 60.9ms/batch - loss: 153.52163 - diff: 36.11mlTrain batch 19/32 - 67.0ms/batch - loss: 148.86183 - diff: 35.69mlTrain batch 20/32 - 58.6ms/batch - loss: 146.43743 - diff: 35.60mlTrain batch 21/32 - 66.5ms/batch - loss: 145.35001 - diff: 35.53mlTrain batch 22/32 - 58.8ms/batch - loss: 148.08711 - diff: 35.73mlTrain batch 23/32 - 67.3ms/batch - loss: 145.08661 - diff: 35.50mlTrain batch 24/32 - 60.8ms/batch - loss: 143.48601 - diff: 35.34mlTrain batch 25/32 - 65.1ms/batch - loss: 147.18537 - diff: 35.59mlTrain batch 26/32 - 59.9ms/batch - loss: 146.96660 - diff: 35.76mlTrain batch 27/32 - 66.3ms/batch - loss: 146.13530 - diff: 35.80mlTrain batch 28/32 - 58.1ms/batch - loss: 146.54824 - diff: 35.95mlTrain batch 29/32 - 68.0ms/batch - loss: 145.92942 - diff: 36.01mlTrain batch 30/32 - 62.6ms/batch - loss: 144.68929 - diff: 35.95mlTrain batch 31/32 - 61.4ms/batch - loss: 141.75186 - diff: 35.55mlTrain batch 32/32 - 59.0ms/batch - loss: 147.32718 - diff: 35.62mlTrain batch 32/32 - 10.7s 59.0ms/batch - loss: 147.32718 - diff: 35.62ml
Test 0.5s: val_loss: 122.43503 - diff: 34.07ml

Epoch 25: current best loss = 121.75735, at epoch 20
Train batch 1/32 - 71.6ms/batch - loss: 139.96533 - diff: 34.71mlTrain batch 2/32 - 61.4ms/batch - loss: 140.23164 - diff: 35.75mlTrain batch 3/32 - 60.6ms/batch - loss: 153.77697 - diff: 39.09mlTrain batch 4/32 - 62.8ms/batch - loss: 151.22618 - diff: 39.23mlTrain batch 5/32 - 62.6ms/batch - loss: 159.56841 - diff: 41.37mlTrain batch 6/32 - 62.5ms/batch - loss: 159.70372 - diff: 41.94mlTrain batch 7/32 - 63.7ms/batch - loss: 146.14322 - diff: 39.72mlTrain batch 8/32 - 70.8ms/batch - loss: 136.57706 - diff: 37.62mlTrain batch 9/32 - 63.9ms/batch - loss: 140.78914 - diff: 37.78mlTrain batch 10/32 - 65.0ms/batch - loss: 136.51158 - diff: 37.11mlTrain batch 11/32 - 63.4ms/batch - loss: 136.45295 - diff: 36.86mlTrain batch 12/32 - 64.8ms/batch - loss: 140.55484 - diff: 37.29mlTrain batch 13/32 - 64.4ms/batch - loss: 138.37423 - diff: 37.01mlTrain batch 14/32 - 59.3ms/batch - loss: 139.11973 - diff: 37.05mlTrain batch 15/32 - 55.3ms/batch - loss: 136.98168 - diff: 36.77mlTrain batch 16/32 - 62.9ms/batch - loss: 135.25305 - diff: 36.40mlTrain batch 17/32 - 62.2ms/batch - loss: 131.39055 - diff: 35.89mlTrain batch 18/32 - 62.6ms/batch - loss: 131.01930 - diff: 35.81mlTrain batch 19/32 - 61.6ms/batch - loss: 126.47827 - diff: 34.92mlTrain batch 20/32 - 62.6ms/batch - loss: 128.09211 - diff: 35.09mlTrain batch 21/32 - 61.6ms/batch - loss: 124.95541 - diff: 34.58mlTrain batch 22/32 - 67.0ms/batch - loss: 130.90307 - diff: 34.86mlTrain batch 23/32 - 65.9ms/batch - loss: 134.22517 - diff: 35.40mlTrain batch 24/32 - 66.1ms/batch - loss: 134.41642 - diff: 35.46mlTrain batch 25/32 - 66.6ms/batch - loss: 136.77294 - diff: 35.79mlTrain batch 26/32 - 65.2ms/batch - loss: 136.02106 - diff: 35.71mlTrain batch 27/32 - 64.9ms/batch - loss: 133.72240 - diff: 35.41mlTrain batch 28/32 - 64.8ms/batch - loss: 131.60315 - diff: 35.19mlTrain batch 29/32 - 65.0ms/batch - loss: 137.24357 - diff: 35.53mlTrain batch 30/32 - 64.2ms/batch - loss: 138.07608 - diff: 35.71mlTrain batch 31/32 - 58.5ms/batch - loss: 148.79860 - diff: 36.16mlTrain batch 32/32 - 41.7ms/batch - loss: 155.11100 - diff: 36.30mlTrain batch 32/32 - 11.4s 41.7ms/batch - loss: 155.11100 - diff: 36.30ml
Test 0.5s: val_loss: 119.23327 - diff: 34.37ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 26: current best loss = 119.23327, at epoch 25
Train batch 1/32 - 69.4ms/batch - loss: 96.37076 - diff: 29.33mlTrain batch 2/32 - 58.1ms/batch - loss: 200.84288 - diff: 44.97mlTrain batch 3/32 - 65.0ms/batch - loss: 190.93741 - diff: 41.89mlTrain batch 4/32 - 64.2ms/batch - loss: 162.10209 - diff: 38.48mlTrain batch 5/32 - 66.0ms/batch - loss: 162.45646 - diff: 39.41mlTrain batch 6/32 - 65.7ms/batch - loss: 164.71836 - diff: 39.29mlTrain batch 7/32 - 58.4ms/batch - loss: 157.37924 - diff: 38.49mlTrain batch 8/32 - 57.7ms/batch - loss: 160.21374 - diff: 39.17mlTrain batch 9/32 - 58.9ms/batch - loss: 155.51214 - diff: 38.39mlTrain batch 10/32 - 58.3ms/batch - loss: 146.10944 - diff: 36.82mlTrain batch 11/32 - 60.0ms/batch - loss: 177.34178 - diff: 38.81mlTrain batch 12/32 - 62.8ms/batch - loss: 175.51043 - diff: 38.84mlTrain batch 13/32 - 61.1ms/batch - loss: 181.51257 - diff: 39.81mlTrain batch 14/32 - 61.1ms/batch - loss: 176.86011 - diff: 39.19mlTrain batch 15/32 - 65.0ms/batch - loss: 179.13180 - diff: 40.04mlTrain batch 16/32 - 64.5ms/batch - loss: 177.28697 - diff: 40.00mlTrain batch 17/32 - 56.0ms/batch - loss: 172.51319 - diff: 39.73mlTrain batch 18/32 - 66.6ms/batch - loss: 167.38118 - diff: 38.99mlTrain batch 19/32 - 62.3ms/batch - loss: 166.07535 - diff: 38.89mlTrain batch 20/32 - 62.2ms/batch - loss: 165.93523 - diff: 38.98mlTrain batch 21/32 - 61.2ms/batch - loss: 166.59857 - diff: 39.21mlTrain batch 22/32 - 61.1ms/batch - loss: 161.12247 - diff: 38.38mlTrain batch 23/32 - 61.4ms/batch - loss: 158.92075 - diff: 38.23mlTrain batch 24/32 - 60.0ms/batch - loss: 160.10166 - diff: 38.45mlTrain batch 25/32 - 66.6ms/batch - loss: 159.16562 - diff: 38.58mlTrain batch 26/32 - 66.7ms/batch - loss: 158.27465 - diff: 38.51mlTrain batch 27/32 - 67.6ms/batch - loss: 158.01666 - diff: 38.43mlTrain batch 28/32 - 65.8ms/batch - loss: 157.26461 - diff: 38.23mlTrain batch 29/32 - 64.8ms/batch - loss: 155.66161 - diff: 38.05mlTrain batch 30/32 - 64.7ms/batch - loss: 154.47386 - diff: 37.85mlTrain batch 31/32 - 64.3ms/batch - loss: 153.73042 - diff: 37.82mlTrain batch 32/32 - 54.1ms/batch - loss: 153.10987 - diff: 37.63mlTrain batch 32/32 - 11.0s 54.1ms/batch - loss: 153.10987 - diff: 37.63ml
Test 0.5s: val_loss: 126.21574 - diff: 34.18ml

Epoch 27: current best loss = 119.23327, at epoch 25
Train batch 1/32 - 71.6ms/batch - loss: 82.65691 - diff: 28.20mlTrain batch 2/32 - 59.8ms/batch - loss: 122.33421 - diff: 34.79mlTrain batch 3/32 - 58.6ms/batch - loss: 125.35066 - diff: 34.91mlTrain batch 4/32 - 59.8ms/batch - loss: 118.84925 - diff: 34.61mlTrain batch 5/32 - 63.7ms/batch - loss: 137.39245 - diff: 36.36mlTrain batch 6/32 - 60.1ms/batch - loss: 148.87574 - diff: 37.69mlTrain batch 7/32 - 59.9ms/batch - loss: 145.21225 - diff: 37.36mlTrain batch 8/32 - 59.6ms/batch - loss: 146.33033 - diff: 37.58mlTrain batch 9/32 - 59.5ms/batch - loss: 144.59586 - diff: 37.63mlTrain batch 10/32 - 59.5ms/batch - loss: 144.50630 - diff: 37.33mlTrain batch 11/32 - 59.8ms/batch - loss: 143.95374 - diff: 37.35mlTrain batch 12/32 - 59.6ms/batch - loss: 139.84186 - diff: 36.88mlTrain batch 13/32 - 59.2ms/batch - loss: 137.57661 - diff: 36.54mlTrain batch 14/32 - 59.2ms/batch - loss: 144.07697 - diff: 37.12mlTrain batch 15/32 - 60.4ms/batch - loss: 182.03102 - diff: 38.42mlTrain batch 16/32 - 62.5ms/batch - loss: 180.42210 - diff: 38.28mlTrain batch 17/32 - 65.7ms/batch - loss: 175.84847 - diff: 37.99mlTrain batch 18/32 - 64.6ms/batch - loss: 173.05858 - diff: 37.59mlTrain batch 19/32 - 64.4ms/batch - loss: 171.06038 - diff: 37.34mlTrain batch 20/32 - 64.6ms/batch - loss: 167.85395 - diff: 37.18mlTrain batch 21/32 - 65.2ms/batch - loss: 165.80240 - diff: 37.21mlTrain batch 22/32 - 65.2ms/batch - loss: 164.38032 - diff: 37.16mlTrain batch 23/32 - 64.3ms/batch - loss: 161.92923 - diff: 36.99mlTrain batch 24/32 - 65.0ms/batch - loss: 160.93453 - diff: 37.13mlTrain batch 25/32 - 65.4ms/batch - loss: 159.34199 - diff: 37.07mlTrain batch 26/32 - 65.3ms/batch - loss: 156.88952 - diff: 36.84mlTrain batch 27/32 - 64.1ms/batch - loss: 158.09360 - diff: 36.98mlTrain batch 28/32 - 65.1ms/batch - loss: 158.97109 - diff: 37.20mlTrain batch 29/32 - 64.1ms/batch - loss: 157.08266 - diff: 37.13mlTrain batch 30/32 - 63.9ms/batch - loss: 153.98410 - diff: 36.86mlTrain batch 31/32 - 66.1ms/batch - loss: 151.46118 - diff: 36.60mlTrain batch 32/32 - 60.0ms/batch - loss: 151.51056 - diff: 36.50mlTrain batch 32/32 - 11.0s 60.0ms/batch - loss: 151.51056 - diff: 36.50ml
Test 0.6s: val_loss: 113.63534 - diff: 32.40ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 28: current best loss = 113.63534, at epoch 27
Train batch 1/32 - 76.2ms/batch - loss: 129.53799 - diff: 34.09mlTrain batch 2/32 - 60.5ms/batch - loss: 97.18134 - diff: 30.82mlTrain batch 3/32 - 64.4ms/batch - loss: 106.89661 - diff: 33.45mlTrain batch 4/32 - 58.2ms/batch - loss: 118.22886 - diff: 33.82mlTrain batch 5/32 - 64.5ms/batch - loss: 131.17298 - diff: 35.42mlTrain batch 6/32 - 58.0ms/batch - loss: 118.02017 - diff: 33.10mlTrain batch 7/32 - 64.9ms/batch - loss: 113.53393 - diff: 32.66mlTrain batch 8/32 - 57.6ms/batch - loss: 115.99358 - diff: 32.86mlTrain batch 9/32 - 64.2ms/batch - loss: 115.00446 - diff: 33.05mlTrain batch 10/32 - 58.3ms/batch - loss: 126.45085 - diff: 33.65mlTrain batch 11/32 - 64.3ms/batch - loss: 123.95882 - diff: 33.68mlTrain batch 12/32 - 57.8ms/batch - loss: 119.45837 - diff: 32.86mlTrain batch 13/32 - 59.6ms/batch - loss: 115.66694 - diff: 32.37mlTrain batch 14/32 - 59.1ms/batch - loss: 115.34947 - diff: 32.62mlTrain batch 15/32 - 59.5ms/batch - loss: 114.25990 - diff: 32.63mlTrain batch 16/32 - 59.1ms/batch - loss: 112.53769 - diff: 32.57mlTrain batch 17/32 - 71.1ms/batch - loss: 111.00334 - diff: 32.47mlTrain batch 18/32 - 62.0ms/batch - loss: 112.23276 - diff: 32.73mlTrain batch 19/32 - 62.1ms/batch - loss: 112.95561 - diff: 32.95mlTrain batch 20/32 - 60.8ms/batch - loss: 109.57865 - diff: 32.44mlTrain batch 21/32 - 62.8ms/batch - loss: 106.13008 - diff: 31.85mlTrain batch 22/32 - 62.5ms/batch - loss: 104.53931 - diff: 31.69mlTrain batch 23/32 - 63.3ms/batch - loss: 104.44922 - diff: 31.70mlTrain batch 24/32 - 61.5ms/batch - loss: 112.76153 - diff: 32.36mlTrain batch 25/32 - 61.9ms/batch - loss: 113.98861 - diff: 32.66mlTrain batch 26/32 - 62.8ms/batch - loss: 120.66681 - diff: 33.49mlTrain batch 27/32 - 62.2ms/batch - loss: 121.70267 - diff: 33.79mlTrain batch 28/32 - 62.3ms/batch - loss: 122.65479 - diff: 33.95mlTrain batch 29/32 - 62.3ms/batch - loss: 124.56291 - diff: 34.26mlTrain batch 30/32 - 61.3ms/batch - loss: 122.80662 - diff: 34.07mlTrain batch 31/32 - 48.3ms/batch - loss: 132.54948 - diff: 34.71mlTrain batch 32/32 - 36.0ms/batch - loss: 133.30910 - diff: 34.61mlTrain batch 32/32 - 10.5s 36.0ms/batch - loss: 133.30910 - diff: 34.61ml
Test 0.6s: val_loss: 122.39632 - diff: 34.95ml

Epoch 29: current best loss = 113.63534, at epoch 27
Train batch 1/32 - 69.3ms/batch - loss: 91.37649 - diff: 29.85mlTrain batch 2/32 - 59.0ms/batch - loss: 152.79313 - diff: 37.42mlTrain batch 3/32 - 58.9ms/batch - loss: 120.30666 - diff: 33.66mlTrain batch 4/32 - 59.5ms/batch - loss: 121.27884 - diff: 34.12mlTrain batch 5/32 - 60.3ms/batch - loss: 106.89542 - diff: 31.77mlTrain batch 6/32 - 59.7ms/batch - loss: 111.52244 - diff: 31.47mlTrain batch 7/32 - 59.3ms/batch - loss: 114.16034 - diff: 31.41mlTrain batch 8/32 - 59.6ms/batch - loss: 140.14888 - diff: 32.94mlTrain batch 9/32 - 59.0ms/batch - loss: 142.88414 - diff: 33.71mlTrain batch 10/32 - 58.6ms/batch - loss: 136.78525 - diff: 33.41mlTrain batch 11/32 - 58.8ms/batch - loss: 138.73807 - diff: 33.78mlTrain batch 12/32 - 50.3ms/batch - loss: 142.85811 - diff: 34.72mlTrain batch 13/32 - 50.9ms/batch - loss: 140.68506 - diff: 34.61mlTrain batch 14/32 - 64.0ms/batch - loss: 141.65116 - diff: 34.93mlTrain batch 15/32 - 65.7ms/batch - loss: 140.30517 - diff: 34.98mlTrain batch 16/32 - 66.6ms/batch - loss: 141.72792 - diff: 35.41mlTrain batch 17/32 - 66.3ms/batch - loss: 142.51260 - diff: 35.60mlTrain batch 18/32 - 66.5ms/batch - loss: 138.65418 - diff: 35.16mlTrain batch 19/32 - 66.3ms/batch - loss: 140.87388 - diff: 35.49mlTrain batch 20/32 - 67.1ms/batch - loss: 168.14687 - diff: 36.86mlTrain batch 21/32 - 66.5ms/batch - loss: 164.25848 - diff: 36.59mlTrain batch 22/32 - 66.5ms/batch - loss: 167.36058 - diff: 37.21mlTrain batch 23/32 - 66.1ms/batch - loss: 167.83643 - diff: 37.44mlTrain batch 24/32 - 65.9ms/batch - loss: 168.87317 - diff: 37.65mlTrain batch 25/32 - 66.0ms/batch - loss: 166.83636 - diff: 37.58mlTrain batch 26/32 - 62.7ms/batch - loss: 166.19742 - diff: 37.67mlTrain batch 27/32 - 62.2ms/batch - loss: 163.49282 - diff: 37.45mlTrain batch 28/32 - 65.9ms/batch - loss: 162.62350 - diff: 37.54mlTrain batch 29/32 - 62.2ms/batch - loss: 163.18436 - diff: 37.83mlTrain batch 30/32 - 64.7ms/batch - loss: 161.57908 - diff: 37.78mlTrain batch 31/32 - 53.3ms/batch - loss: 161.72345 - diff: 37.80mlTrain batch 32/32 - 50.7ms/batch - loss: 161.42904 - diff: 37.65mlTrain batch 32/32 - 11.0s 50.7ms/batch - loss: 161.42904 - diff: 37.65ml
Test 0.6s: val_loss: 128.75194 - diff: 34.79ml

Epoch 30: current best loss = 113.63534, at epoch 27
Train batch 1/32 - 69.1ms/batch - loss: 77.12140 - diff: 27.35mlTrain batch 2/32 - 61.5ms/batch - loss: 65.95530 - diff: 26.00mlTrain batch 3/32 - 61.8ms/batch - loss: 95.62540 - diff: 31.16mlTrain batch 4/32 - 61.2ms/batch - loss: 109.21613 - diff: 33.91mlTrain batch 5/32 - 61.5ms/batch - loss: 120.12333 - diff: 36.09mlTrain batch 6/32 - 59.0ms/batch - loss: 131.30692 - diff: 37.63mlTrain batch 7/32 - 58.9ms/batch - loss: 153.29823 - diff: 38.89mlTrain batch 8/32 - 58.7ms/batch - loss: 157.41453 - diff: 39.02mlTrain batch 9/32 - 65.3ms/batch - loss: 157.01237 - diff: 38.61mlTrain batch 10/32 - 65.3ms/batch - loss: 150.44422 - diff: 38.02mlTrain batch 11/32 - 66.0ms/batch - loss: 146.08512 - diff: 37.75mlTrain batch 12/32 - 64.8ms/batch - loss: 138.86656 - diff: 36.89mlTrain batch 13/32 - 65.7ms/batch - loss: 137.54688 - diff: 36.61mlTrain batch 14/32 - 66.4ms/batch - loss: 137.37346 - diff: 36.62mlTrain batch 15/32 - 65.0ms/batch - loss: 132.88081 - diff: 35.95mlTrain batch 16/32 - 64.0ms/batch - loss: 138.77160 - diff: 36.77mlTrain batch 17/32 - 59.2ms/batch - loss: 138.34791 - diff: 36.87mlTrain batch 18/32 - 58.5ms/batch - loss: 138.47526 - diff: 36.75mlTrain batch 19/32 - 58.9ms/batch - loss: 136.01336 - diff: 36.45mlTrain batch 20/32 - 58.4ms/batch - loss: 133.14699 - diff: 36.22mlTrain batch 21/32 - 59.5ms/batch - loss: 132.37402 - diff: 36.01mlTrain batch 22/32 - 59.3ms/batch - loss: 131.42110 - diff: 35.96mlTrain batch 23/32 - 50.5ms/batch - loss: 133.06974 - diff: 35.79mlTrain batch 24/32 - 50.1ms/batch - loss: 134.20100 - diff: 35.99mlTrain batch 25/32 - 59.0ms/batch - loss: 134.68607 - diff: 36.13mlTrain batch 26/32 - 58.6ms/batch - loss: 132.95904 - diff: 35.97mlTrain batch 27/32 - 65.0ms/batch - loss: 134.47700 - diff: 35.96mlTrain batch 28/32 - 57.9ms/batch - loss: 133.16599 - diff: 35.86mlTrain batch 29/32 - 59.7ms/batch - loss: 144.91807 - diff: 36.05mlTrain batch 30/32 - 59.3ms/batch - loss: 144.42209 - diff: 36.00mlTrain batch 31/32 - 64.0ms/batch - loss: 141.69787 - diff: 35.68mlTrain batch 32/32 - 52.1ms/batch - loss: 145.71358 - diff: 35.69mlTrain batch 32/32 - 10.6s 52.1ms/batch - loss: 145.71358 - diff: 35.69ml
Test 0.6s: val_loss: 115.30711 - diff: 32.66ml

Epoch 31: current best loss = 113.63534, at epoch 27
Train batch 1/32 - 72.2ms/batch - loss: 87.70882 - diff: 30.51mlTrain batch 2/32 - 60.8ms/batch - loss: 181.05207 - diff: 36.23mlTrain batch 3/32 - 67.9ms/batch - loss: 189.43231 - diff: 38.11mlTrain batch 4/32 - 67.9ms/batch - loss: 172.23992 - diff: 36.75mlTrain batch 5/32 - 66.9ms/batch - loss: 172.69109 - diff: 37.59mlTrain batch 6/32 - 68.3ms/batch - loss: 176.42605 - diff: 38.48mlTrain batch 7/32 - 69.5ms/batch - loss: 174.25360 - diff: 39.00mlTrain batch 8/32 - 68.1ms/batch - loss: 161.92087 - diff: 37.58mlTrain batch 9/32 - 62.2ms/batch - loss: 157.79503 - diff: 37.56mlTrain batch 10/32 - 67.7ms/batch - loss: 154.68540 - diff: 37.33mlTrain batch 11/32 - 66.2ms/batch - loss: 148.55653 - diff: 36.74mlTrain batch 12/32 - 58.6ms/batch - loss: 145.49570 - diff: 36.59mlTrain batch 13/32 - 64.5ms/batch - loss: 145.64362 - diff: 36.71mlTrain batch 14/32 - 63.3ms/batch - loss: 146.73382 - diff: 36.86mlTrain batch 15/32 - 65.2ms/batch - loss: 139.70765 - diff: 35.73mlTrain batch 16/32 - 64.0ms/batch - loss: 136.31127 - diff: 35.43mlTrain batch 17/32 - 61.7ms/batch - loss: 140.45522 - diff: 36.16mlTrain batch 18/32 - 61.3ms/batch - loss: 164.22776 - diff: 37.46mlTrain batch 19/32 - 66.5ms/batch - loss: 160.18556 - diff: 37.17mlTrain batch 20/32 - 66.0ms/batch - loss: 162.60539 - diff: 37.42mlTrain batch 21/32 - 65.5ms/batch - loss: 159.58484 - diff: 37.15mlTrain batch 22/32 - 65.4ms/batch - loss: 154.59974 - diff: 36.41mlTrain batch 23/32 - 67.4ms/batch - loss: 152.54384 - diff: 36.28mlTrain batch 24/32 - 54.1ms/batch - loss: 149.86251 - diff: 36.09mlTrain batch 25/32 - 65.9ms/batch - loss: 148.37509 - diff: 35.84mlTrain batch 26/32 - 65.5ms/batch - loss: 146.75495 - diff: 35.73mlTrain batch 27/32 - 65.5ms/batch - loss: 146.22700 - diff: 35.67mlTrain batch 28/32 - 65.3ms/batch - loss: 144.48487 - diff: 35.58mlTrain batch 29/32 - 65.8ms/batch - loss: 144.55892 - diff: 35.69mlTrain batch 30/32 - 52.2ms/batch - loss: 142.61660 - diff: 35.54mlTrain batch 31/32 - 50.8ms/batch - loss: 141.54207 - diff: 35.60mlTrain batch 32/32 - 37.3ms/batch - loss: 164.00208 - diff: 36.01mlTrain batch 32/32 - 11.1s 37.3ms/batch - loss: 164.00208 - diff: 36.01ml
Test 0.5s: val_loss: 103.45886 - diff: 31.11ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 32: current best loss = 103.45886, at epoch 31
Train batch 1/32 - 68.3ms/batch - loss: 137.99345 - diff: 36.43mlTrain batch 2/32 - 59.0ms/batch - loss: 154.14030 - diff: 39.13mlTrain batch 3/32 - 58.7ms/batch - loss: 166.02767 - diff: 42.69mlTrain batch 4/32 - 58.1ms/batch - loss: 149.63921 - diff: 39.64mlTrain batch 5/32 - 60.4ms/batch - loss: 133.42686 - diff: 36.85mlTrain batch 6/32 - 60.7ms/batch - loss: 140.88238 - diff: 37.15mlTrain batch 7/32 - 61.9ms/batch - loss: 134.43596 - diff: 36.67mlTrain batch 8/32 - 61.9ms/batch - loss: 129.79202 - diff: 36.04mlTrain batch 9/32 - 53.5ms/batch - loss: 133.77311 - diff: 36.23mlTrain batch 10/32 - 60.9ms/batch - loss: 140.52331 - diff: 36.62mlTrain batch 11/32 - 61.7ms/batch - loss: 141.81890 - diff: 37.00mlTrain batch 12/32 - 62.6ms/batch - loss: 137.62665 - diff: 36.58mlTrain batch 13/32 - 53.8ms/batch - loss: 134.68799 - diff: 36.47mlTrain batch 14/32 - 58.4ms/batch - loss: 128.88563 - diff: 35.63mlTrain batch 15/32 - 69.4ms/batch - loss: 129.63725 - diff: 35.64mlTrain batch 16/32 - 63.0ms/batch - loss: 126.71291 - diff: 35.18mlTrain batch 17/32 - 69.1ms/batch - loss: 135.44229 - diff: 35.69mlTrain batch 18/32 - 63.0ms/batch - loss: 135.73099 - diff: 35.55mlTrain batch 19/32 - 71.2ms/batch - loss: 135.04128 - diff: 35.66mlTrain batch 20/32 - 64.2ms/batch - loss: 143.88481 - diff: 36.38mlTrain batch 21/32 - 64.5ms/batch - loss: 140.93195 - diff: 35.98mlTrain batch 22/32 - 70.4ms/batch - loss: 139.01713 - diff: 35.78mlTrain batch 23/32 - 63.6ms/batch - loss: 156.06560 - diff: 36.85mlTrain batch 24/32 - 69.8ms/batch - loss: 154.40469 - diff: 36.98mlTrain batch 25/32 - 63.2ms/batch - loss: 156.70292 - diff: 37.38mlTrain batch 26/32 - 70.0ms/batch - loss: 154.84724 - diff: 37.34mlTrain batch 27/32 - 63.0ms/batch - loss: 152.83388 - diff: 37.22mlTrain batch 28/32 - 68.2ms/batch - loss: 150.56079 - diff: 36.99mlTrain batch 29/32 - 66.8ms/batch - loss: 149.86024 - diff: 37.09mlTrain batch 30/32 - 63.2ms/batch - loss: 147.19664 - diff: 36.82mlTrain batch 31/32 - 61.7ms/batch - loss: 149.26347 - diff: 36.98mlTrain batch 32/32 - 52.2ms/batch - loss: 153.93414 - diff: 37.11mlTrain batch 32/32 - 11.6s 52.2ms/batch - loss: 153.93414 - diff: 37.11ml
Test 0.6s: val_loss: 122.45260 - diff: 33.86ml

Epoch 33: current best loss = 103.45886, at epoch 31
Train batch 1/32 - 66.0ms/batch - loss: 155.27744 - diff: 38.86mlTrain batch 2/32 - 46.5ms/batch - loss: 146.46307 - diff: 38.63mlTrain batch 3/32 - 60.4ms/batch - loss: 143.60186 - diff: 38.00mlTrain batch 4/32 - 59.1ms/batch - loss: 127.78063 - diff: 35.75mlTrain batch 5/32 - 67.2ms/batch - loss: 168.07729 - diff: 39.58mlTrain batch 6/32 - 67.2ms/batch - loss: 238.65855 - diff: 41.61mlTrain batch 7/32 - 57.2ms/batch - loss: 233.96780 - diff: 42.23mlTrain batch 8/32 - 56.1ms/batch - loss: 233.81543 - diff: 43.16mlTrain batch 9/32 - 58.7ms/batch - loss: 220.87649 - diff: 42.23mlTrain batch 10/32 - 61.5ms/batch - loss: 208.01039 - diff: 41.11mlTrain batch 11/32 - 61.8ms/batch - loss: 194.43596 - diff: 39.65mlTrain batch 12/32 - 60.7ms/batch - loss: 199.75810 - diff: 40.55mlTrain batch 13/32 - 54.1ms/batch - loss: 192.37173 - diff: 39.98mlTrain batch 14/32 - 59.6ms/batch - loss: 182.92872 - diff: 38.94mlTrain batch 15/32 - 48.7ms/batch - loss: 183.56234 - diff: 39.61mlTrain batch 16/32 - 48.7ms/batch - loss: 174.61647 - diff: 38.29mlTrain batch 17/32 - 67.2ms/batch - loss: 169.53280 - diff: 37.92mlTrain batch 18/32 - 65.9ms/batch - loss: 166.80636 - diff: 37.78mlTrain batch 19/32 - 63.3ms/batch - loss: 161.71212 - diff: 37.36mlTrain batch 20/32 - 63.5ms/batch - loss: 159.94319 - diff: 37.23mlTrain batch 21/32 - 66.6ms/batch - loss: 157.92836 - diff: 37.20mlTrain batch 22/32 - 65.5ms/batch - loss: 155.67980 - diff: 37.01mlTrain batch 23/32 - 65.4ms/batch - loss: 151.37290 - diff: 36.36mlTrain batch 24/32 - 64.7ms/batch - loss: 148.46284 - diff: 36.15mlTrain batch 25/32 - 64.9ms/batch - loss: 147.13421 - diff: 36.03mlTrain batch 26/32 - 64.7ms/batch - loss: 146.61205 - diff: 35.94mlTrain batch 27/32 - 65.6ms/batch - loss: 147.99825 - diff: 36.33mlTrain batch 28/32 - 65.8ms/batch - loss: 146.36760 - diff: 36.24mlTrain batch 29/32 - 63.3ms/batch - loss: 143.48031 - diff: 35.90mlTrain batch 30/32 - 63.6ms/batch - loss: 140.76634 - diff: 35.53mlTrain batch 31/32 - 66.7ms/batch - loss: 139.45490 - diff: 35.39mlTrain batch 32/32 - 58.8ms/batch - loss: 141.06708 - diff: 35.36mlTrain batch 32/32 - 11.5s 58.8ms/batch - loss: 141.06708 - diff: 35.36ml
Test 0.6s: val_loss: 110.71981 - diff: 33.16ml

Epoch 34: current best loss = 103.45886, at epoch 31
Train batch 1/32 - 69.9ms/batch - loss: 80.17312 - diff: 30.20mlTrain batch 2/32 - 60.5ms/batch - loss: 81.81078 - diff: 30.78mlTrain batch 3/32 - 61.7ms/batch - loss: 92.31773 - diff: 31.98mlTrain batch 4/32 - 67.8ms/batch - loss: 89.33084 - diff: 31.39mlTrain batch 5/32 - 45.8ms/batch - loss: 84.58899 - diff: 30.30mlTrain batch 6/32 - 45.3ms/batch - loss: 89.95846 - diff: 31.27mlTrain batch 7/32 - 50.1ms/batch - loss: 93.01186 - diff: 31.94mlTrain batch 8/32 - 48.4ms/batch - loss: 102.86957 - diff: 33.01mlTrain batch 9/32 - 61.4ms/batch - loss: 104.66506 - diff: 33.05mlTrain batch 10/32 - 60.8ms/batch - loss: 136.35427 - diff: 33.64mlTrain batch 11/32 - 63.1ms/batch - loss: 143.91027 - diff: 35.23mlTrain batch 12/32 - 61.7ms/batch - loss: 161.96327 - diff: 37.85mlTrain batch 13/32 - 54.8ms/batch - loss: 155.93595 - diff: 37.14mlTrain batch 14/32 - 64.1ms/batch - loss: 153.82866 - diff: 37.33mlTrain batch 15/32 - 61.4ms/batch - loss: 170.12546 - diff: 38.70mlTrain batch 16/32 - 60.2ms/batch - loss: 168.07355 - diff: 38.42mlTrain batch 17/32 - 61.4ms/batch - loss: 165.66965 - diff: 38.28mlTrain batch 18/32 - 61.2ms/batch - loss: 166.89993 - diff: 38.51mlTrain batch 19/32 - 62.7ms/batch - loss: 165.06051 - diff: 38.41mlTrain batch 20/32 - 61.9ms/batch - loss: 165.71263 - diff: 38.40mlTrain batch 21/32 - 62.0ms/batch - loss: 166.43311 - diff: 38.49mlTrain batch 22/32 - 61.3ms/batch - loss: 161.48283 - diff: 37.85mlTrain batch 23/32 - 67.5ms/batch - loss: 156.23644 - diff: 37.16mlTrain batch 24/32 - 61.7ms/batch - loss: 154.20906 - diff: 37.05mlTrain batch 25/32 - 67.1ms/batch - loss: 157.23035 - diff: 37.44mlTrain batch 26/32 - 60.7ms/batch - loss: 156.28101 - diff: 37.46mlTrain batch 27/32 - 68.2ms/batch - loss: 152.74784 - diff: 37.10mlTrain batch 28/32 - 60.6ms/batch - loss: 153.11001 - diff: 37.17mlTrain batch 29/32 - 65.2ms/batch - loss: 152.85724 - diff: 37.19mlTrain batch 30/32 - 61.1ms/batch - loss: 152.00061 - diff: 37.15mlTrain batch 31/32 - 52.5ms/batch - loss: 152.75364 - diff: 37.34mlTrain batch 32/32 - 46.3ms/batch - loss: 155.93032 - diff: 37.34mlTrain batch 32/32 - 10.8s 46.3ms/batch - loss: 155.93032 - diff: 37.34ml
Test 0.6s: val_loss: 110.89690 - diff: 31.91ml

Epoch 35: current best loss = 103.45886, at epoch 31
Train batch 1/32 - 89.6ms/batch - loss: 162.52029 - diff: 42.35mlTrain batch 2/32 - 68.4ms/batch - loss: 134.32464 - diff: 37.75mlTrain batch 3/32 - 76.1ms/batch - loss: 223.95340 - diff: 42.12mlTrain batch 4/32 - 68.7ms/batch - loss: 205.75944 - diff: 39.10mlTrain batch 5/32 - 70.7ms/batch - loss: 193.17849 - diff: 39.15mlTrain batch 6/32 - 66.2ms/batch - loss: 181.22766 - diff: 38.49mlTrain batch 7/32 - 74.0ms/batch - loss: 164.69056 - diff: 36.93mlTrain batch 8/32 - 66.5ms/batch - loss: 155.65746 - diff: 36.25mlTrain batch 9/32 - 73.9ms/batch - loss: 152.44898 - diff: 36.11mlTrain batch 10/32 - 66.0ms/batch - loss: 149.24741 - diff: 35.35mlTrain batch 11/32 - 67.7ms/batch - loss: 153.22456 - diff: 35.97mlTrain batch 12/32 - 58.6ms/batch - loss: 150.49406 - diff: 35.99mlTrain batch 13/32 - 65.1ms/batch - loss: 146.27839 - diff: 35.72mlTrain batch 14/32 - 64.0ms/batch - loss: 180.84981 - diff: 36.91mlTrain batch 15/32 - 71.4ms/batch - loss: 172.23586 - diff: 35.82mlTrain batch 16/32 - 60.6ms/batch - loss: 166.84322 - diff: 35.46mlTrain batch 17/32 - 71.8ms/batch - loss: 161.46096 - diff: 35.10mlTrain batch 18/32 - 65.2ms/batch - loss: 159.03835 - diff: 34.96mlTrain batch 19/32 - 68.0ms/batch - loss: 154.39003 - diff: 34.64mlTrain batch 20/32 - 60.0ms/batch - loss: 154.19570 - diff: 34.98mlTrain batch 21/32 - 58.5ms/batch - loss: 150.65885 - diff: 34.67mlTrain batch 22/32 - 59.1ms/batch - loss: 146.23182 - diff: 34.07mlTrain batch 23/32 - 59.1ms/batch - loss: 146.59392 - diff: 34.34mlTrain batch 24/32 - 58.7ms/batch - loss: 144.65158 - diff: 34.24mlTrain batch 25/32 - 61.9ms/batch - loss: 144.00546 - diff: 34.40mlTrain batch 26/32 - 50.8ms/batch - loss: 143.38988 - diff: 34.35mlTrain batch 27/32 - 66.7ms/batch - loss: 141.95451 - diff: 34.19mlTrain batch 28/32 - 62.3ms/batch - loss: 139.65666 - diff: 33.94mlTrain batch 29/32 - 69.8ms/batch - loss: 138.61036 - diff: 33.91mlTrain batch 30/32 - 62.6ms/batch - loss: 137.10529 - diff: 33.82mlTrain batch 31/32 - 53.4ms/batch - loss: 137.34793 - diff: 33.90mlTrain batch 32/32 - 41.3ms/batch - loss: 136.97865 - diff: 33.74mlTrain batch 32/32 - 10.7s 41.3ms/batch - loss: 136.97865 - diff: 33.74ml
Test 0.6s: val_loss: 115.54788 - diff: 33.69ml

Epoch 36: current best loss = 103.45886, at epoch 31
Train batch 1/32 - 77.6ms/batch - loss: 78.88484 - diff: 25.53mlTrain batch 2/32 - 68.8ms/batch - loss: 105.53922 - diff: 30.77mlTrain batch 3/32 - 74.1ms/batch - loss: 134.33604 - diff: 32.83mlTrain batch 4/32 - 67.3ms/batch - loss: 130.37155 - diff: 33.15mlTrain batch 5/32 - 77.8ms/batch - loss: 129.55969 - diff: 33.39mlTrain batch 6/32 - 68.5ms/batch - loss: 140.64040 - diff: 35.43mlTrain batch 7/32 - 75.4ms/batch - loss: 147.36960 - diff: 36.10mlTrain batch 8/32 - 67.1ms/batch - loss: 135.73289 - diff: 34.68mlTrain batch 9/32 - 73.3ms/batch - loss: 140.18202 - diff: 35.47mlTrain batch 10/32 - 64.4ms/batch - loss: 135.19029 - diff: 34.89mlTrain batch 11/32 - 74.0ms/batch - loss: 127.98533 - diff: 33.92mlTrain batch 12/32 - 64.6ms/batch - loss: 123.58058 - diff: 33.46mlTrain batch 13/32 - 67.6ms/batch - loss: 127.55446 - diff: 34.21mlTrain batch 14/32 - 65.2ms/batch - loss: 155.33697 - diff: 35.10mlTrain batch 15/32 - 73.6ms/batch - loss: 149.53963 - diff: 34.72mlTrain batch 16/32 - 64.9ms/batch - loss: 143.21574 - diff: 33.93mlTrain batch 17/32 - 74.8ms/batch - loss: 145.92412 - diff: 33.85mlTrain batch 18/32 - 65.9ms/batch - loss: 144.57887 - diff: 33.88mlTrain batch 19/32 - 65.8ms/batch - loss: 141.66093 - diff: 33.50mlTrain batch 20/32 - 48.3ms/batch - loss: 146.05420 - diff: 33.96mlTrain batch 21/32 - 59.9ms/batch - loss: 148.79929 - diff: 34.41mlTrain batch 22/32 - 58.3ms/batch - loss: 146.84190 - diff: 34.34mlTrain batch 23/32 - 59.3ms/batch - loss: 146.86920 - diff: 34.53mlTrain batch 24/32 - 58.0ms/batch - loss: 144.16786 - diff: 34.35mlTrain batch 25/32 - 58.6ms/batch - loss: 145.00928 - diff: 34.55mlTrain batch 26/32 - 58.2ms/batch - loss: 144.50826 - diff: 34.79mlTrain batch 27/32 - 67.1ms/batch - loss: 144.29206 - diff: 34.86mlTrain batch 28/32 - 64.2ms/batch - loss: 142.92027 - diff: 34.85mlTrain batch 29/32 - 65.8ms/batch - loss: 144.43390 - diff: 35.10mlTrain batch 30/32 - 63.7ms/batch - loss: 145.94230 - diff: 35.26mlTrain batch 31/32 - 61.5ms/batch - loss: 145.22977 - diff: 35.34mlTrain batch 32/32 - 47.0ms/batch - loss: 144.54297 - diff: 35.17mlTrain batch 32/32 - 10.5s 47.0ms/batch - loss: 144.54297 - diff: 35.17ml
Test 0.6s: val_loss: 120.73899 - diff: 34.36ml

Epoch 37: current best loss = 103.45886, at epoch 31
Train batch 1/32 - 71.5ms/batch - loss: 153.16638 - diff: 38.73mlTrain batch 2/32 - 58.7ms/batch - loss: 181.31139 - diff: 44.23mlTrain batch 3/32 - 63.7ms/batch - loss: 147.99536 - diff: 38.60mlTrain batch 4/32 - 58.6ms/batch - loss: 136.75427 - diff: 37.30mlTrain batch 5/32 - 65.3ms/batch - loss: 124.69070 - diff: 35.99mlTrain batch 6/32 - 58.4ms/batch - loss: 122.92319 - diff: 35.87mlTrain batch 7/32 - 59.4ms/batch - loss: 128.81270 - diff: 36.62mlTrain batch 8/32 - 57.8ms/batch - loss: 121.90952 - diff: 36.08mlTrain batch 9/32 - 61.3ms/batch - loss: 116.28949 - diff: 34.83mlTrain batch 10/32 - 59.8ms/batch - loss: 113.65636 - diff: 34.45mlTrain batch 11/32 - 59.7ms/batch - loss: 111.43551 - diff: 34.01mlTrain batch 12/32 - 59.0ms/batch - loss: 112.45469 - diff: 34.42mlTrain batch 13/32 - 61.9ms/batch - loss: 108.38726 - diff: 33.66mlTrain batch 14/32 - 61.3ms/batch - loss: 105.71226 - diff: 33.16mlTrain batch 15/32 - 62.1ms/batch - loss: 107.68041 - diff: 33.33mlTrain batch 16/32 - 61.1ms/batch - loss: 107.90486 - diff: 33.52mlTrain batch 17/32 - 63.9ms/batch - loss: 105.45436 - diff: 33.13mlTrain batch 18/32 - 60.9ms/batch - loss: 103.80116 - diff: 32.85mlTrain batch 19/32 - 61.3ms/batch - loss: 107.90319 - diff: 33.47mlTrain batch 20/32 - 62.7ms/batch - loss: 111.17150 - diff: 33.94mlTrain batch 21/32 - 58.5ms/batch - loss: 109.87630 - diff: 33.54mlTrain batch 22/32 - 59.1ms/batch - loss: 122.02025 - diff: 34.34mlTrain batch 23/32 - 58.9ms/batch - loss: 120.83922 - diff: 34.10mlTrain batch 24/32 - 58.5ms/batch - loss: 120.53259 - diff: 34.06mlTrain batch 25/32 - 59.9ms/batch - loss: 122.52052 - diff: 34.49mlTrain batch 26/32 - 59.8ms/batch - loss: 122.16982 - diff: 34.48mlTrain batch 27/32 - 59.6ms/batch - loss: 122.41316 - diff: 34.40mlTrain batch 28/32 - 65.5ms/batch - loss: 136.45770 - diff: 34.95mlTrain batch 29/32 - 59.6ms/batch - loss: 135.37059 - diff: 34.84mlTrain batch 30/32 - 65.5ms/batch - loss: 135.22408 - diff: 34.85mlTrain batch 31/32 - 56.1ms/batch - loss: 133.55113 - diff: 34.68mlTrain batch 32/32 - 49.9ms/batch - loss: 150.30253 - diff: 35.00mlTrain batch 32/32 - 10.9s 49.9ms/batch - loss: 150.30253 - diff: 35.00ml
Test 0.6s: val_loss: 126.72510 - diff: 33.79ml

Epoch 38: current best loss = 103.45886, at epoch 31
Train batch 1/32 - 69.6ms/batch - loss: 192.37126 - diff: 45.25mlTrain batch 2/32 - 60.1ms/batch - loss: 149.31773 - diff: 40.07mlTrain batch 3/32 - 59.2ms/batch - loss: 128.87394 - diff: 37.13mlTrain batch 4/32 - 64.3ms/batch - loss: 114.99864 - diff: 34.92mlTrain batch 5/32 - 64.7ms/batch - loss: 110.29981 - diff: 34.38mlTrain batch 6/32 - 64.4ms/batch - loss: 103.40420 - diff: 33.24mlTrain batch 7/32 - 64.0ms/batch - loss: 104.12730 - diff: 33.13mlTrain batch 8/32 - 64.4ms/batch - loss: 100.41202 - diff: 32.78mlTrain batch 9/32 - 63.6ms/batch - loss: 102.04767 - diff: 33.00mlTrain batch 10/32 - 63.6ms/batch - loss: 102.45921 - diff: 33.30mlTrain batch 11/32 - 63.3ms/batch - loss: 108.78065 - diff: 34.26mlTrain batch 12/32 - 63.3ms/batch - loss: 135.24429 - diff: 34.95mlTrain batch 13/32 - 63.5ms/batch - loss: 136.09259 - diff: 35.44mlTrain batch 14/32 - 62.6ms/batch - loss: 134.31625 - diff: 35.31mlTrain batch 15/32 - 51.6ms/batch - loss: 147.59218 - diff: 36.03mlTrain batch 16/32 - 53.6ms/batch - loss: 147.78323 - diff: 36.42mlTrain batch 17/32 - 55.3ms/batch - loss: 145.69822 - diff: 36.30mlTrain batch 18/32 - 60.8ms/batch - loss: 140.80047 - diff: 35.64mlTrain batch 19/32 - 60.9ms/batch - loss: 137.87065 - diff: 35.36mlTrain batch 20/32 - 61.4ms/batch - loss: 136.26114 - diff: 35.15mlTrain batch 21/32 - 60.6ms/batch - loss: 138.91544 - diff: 35.41mlTrain batch 22/32 - 62.3ms/batch - loss: 139.40251 - diff: 35.61mlTrain batch 23/32 - 62.7ms/batch - loss: 138.02033 - diff: 35.69mlTrain batch 24/32 - 62.1ms/batch - loss: 136.05104 - diff: 35.44mlTrain batch 25/32 - 61.6ms/batch - loss: 133.73477 - diff: 35.13mlTrain batch 26/32 - 61.5ms/batch - loss: 133.58971 - diff: 35.17mlTrain batch 27/32 - 60.9ms/batch - loss: 131.52129 - diff: 35.00mlTrain batch 28/32 - 65.9ms/batch - loss: 129.04570 - diff: 34.58mlTrain batch 29/32 - 60.1ms/batch - loss: 131.29981 - diff: 34.85mlTrain batch 30/32 - 52.9ms/batch - loss: 128.93247 - diff: 34.55mlTrain batch 31/32 - 65.1ms/batch - loss: 127.41028 - diff: 34.38mlTrain batch 32/32 - 59.3ms/batch - loss: 127.73225 - diff: 34.29mlTrain batch 32/32 - 10.9s 59.3ms/batch - loss: 127.73225 - diff: 34.29ml
Test 0.6s: val_loss: 94.66534 - diff: 30.76ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 39: current best loss = 94.66534, at epoch 38
Train batch 1/32 - 80.6ms/batch - loss: 129.49274 - diff: 37.21mlTrain batch 2/32 - 67.6ms/batch - loss: 114.09190 - diff: 33.91mlTrain batch 3/32 - 69.5ms/batch - loss: 132.10838 - diff: 32.36mlTrain batch 4/32 - 68.6ms/batch - loss: 150.14386 - diff: 35.10mlTrain batch 5/32 - 69.3ms/batch - loss: 157.02427 - diff: 36.20mlTrain batch 6/32 - 76.2ms/batch - loss: 151.41184 - diff: 36.48mlTrain batch 7/32 - 69.1ms/batch - loss: 143.31774 - diff: 35.27mlTrain batch 8/32 - 60.5ms/batch - loss: 142.78327 - diff: 35.02mlTrain batch 9/32 - 61.3ms/batch - loss: 140.96143 - diff: 35.64mlTrain batch 10/32 - 59.0ms/batch - loss: 131.56592 - diff: 34.14mlTrain batch 11/32 - 57.9ms/batch - loss: 130.70700 - diff: 34.35mlTrain batch 12/32 - 56.1ms/batch - loss: 135.55183 - diff: 34.57mlTrain batch 13/32 - 61.4ms/batch - loss: 131.86476 - diff: 34.14mlTrain batch 14/32 - 69.2ms/batch - loss: 130.34451 - diff: 34.30mlTrain batch 15/32 - 58.5ms/batch - loss: 137.80358 - diff: 35.44mlTrain batch 16/32 - 61.0ms/batch - loss: 134.93862 - diff: 35.28mlTrain batch 17/32 - 60.9ms/batch - loss: 136.46503 - diff: 35.61mlTrain batch 18/32 - 65.5ms/batch - loss: 133.94398 - diff: 35.26mlTrain batch 19/32 - 65.2ms/batch - loss: 135.57768 - diff: 35.58mlTrain batch 20/32 - 65.4ms/batch - loss: 133.04380 - diff: 35.29mlTrain batch 21/32 - 66.0ms/batch - loss: 139.73888 - diff: 35.85mlTrain batch 22/32 - 66.8ms/batch - loss: 145.63388 - diff: 36.69mlTrain batch 23/32 - 66.3ms/batch - loss: 144.80935 - diff: 36.79mlTrain batch 24/32 - 66.3ms/batch - loss: 143.38619 - diff: 36.68mlTrain batch 25/32 - 65.8ms/batch - loss: 143.15581 - diff: 36.81mlTrain batch 26/32 - 64.8ms/batch - loss: 140.31370 - diff: 36.53mlTrain batch 27/32 - 65.6ms/batch - loss: 138.98868 - diff: 36.47mlTrain batch 28/32 - 65.3ms/batch - loss: 141.47134 - diff: 36.96mlTrain batch 29/32 - 65.2ms/batch - loss: 139.10896 - diff: 36.58mlTrain batch 30/32 - 63.2ms/batch - loss: 137.29090 - diff: 36.42mlTrain batch 31/32 - 67.9ms/batch - loss: 135.96163 - diff: 36.31mlTrain batch 32/32 - 58.5ms/batch - loss: 136.77925 - diff: 36.24mlTrain batch 32/32 - 11.2s 58.5ms/batch - loss: 136.77925 - diff: 36.24ml
Test 0.6s: val_loss: 147.75314 - diff: 37.61ml

Epoch 40: current best loss = 94.66534, at epoch 38
Train batch 1/32 - 80.7ms/batch - loss: 104.68655 - diff: 33.59mlTrain batch 2/32 - 65.8ms/batch - loss: 132.67491 - diff: 36.21mlTrain batch 3/32 - 59.2ms/batch - loss: 119.73706 - diff: 34.52mlTrain batch 4/32 - 63.2ms/batch - loss: 100.68947 - diff: 31.16mlTrain batch 5/32 - 67.0ms/batch - loss: 117.91569 - diff: 33.64mlTrain batch 6/32 - 67.3ms/batch - loss: 124.27711 - diff: 34.77mlTrain batch 7/32 - 62.2ms/batch - loss: 117.58684 - diff: 34.28mlTrain batch 8/32 - 61.3ms/batch - loss: 113.66065 - diff: 33.50mlTrain batch 9/32 - 63.3ms/batch - loss: 132.97201 - diff: 34.65mlTrain batch 10/32 - 62.3ms/batch - loss: 130.38334 - diff: 34.55mlTrain batch 11/32 - 63.1ms/batch - loss: 128.59474 - diff: 33.97mlTrain batch 12/32 - 62.6ms/batch - loss: 126.89310 - diff: 33.97mlTrain batch 13/32 - 62.1ms/batch - loss: 128.63025 - diff: 34.06mlTrain batch 14/32 - 61.3ms/batch - loss: 124.83455 - diff: 33.75mlTrain batch 15/32 - 54.0ms/batch - loss: 122.00544 - diff: 33.47mlTrain batch 16/32 - 50.4ms/batch - loss: 119.67190 - diff: 33.23mlTrain batch 17/32 - 64.3ms/batch - loss: 115.92095 - diff: 32.80mlTrain batch 18/32 - 62.6ms/batch - loss: 115.68473 - diff: 32.85mlTrain batch 19/32 - 63.1ms/batch - loss: 113.06842 - diff: 32.32mlTrain batch 20/32 - 62.5ms/batch - loss: 112.06916 - diff: 32.24mlTrain batch 21/32 - 64.7ms/batch - loss: 114.73980 - diff: 32.74mlTrain batch 22/32 - 63.4ms/batch - loss: 120.09258 - diff: 32.86mlTrain batch 23/32 - 65.6ms/batch - loss: 117.86195 - diff: 32.56mlTrain batch 24/32 - 64.7ms/batch - loss: 119.96121 - diff: 32.93mlTrain batch 25/32 - 65.2ms/batch - loss: 120.81147 - diff: 33.16mlTrain batch 26/32 - 64.8ms/batch - loss: 119.14946 - diff: 32.95mlTrain batch 27/32 - 65.2ms/batch - loss: 119.83269 - diff: 33.06mlTrain batch 28/32 - 64.7ms/batch - loss: 120.91257 - diff: 33.28mlTrain batch 29/32 - 65.2ms/batch - loss: 122.40914 - diff: 33.44mlTrain batch 30/32 - 64.0ms/batch - loss: 121.49130 - diff: 33.29mlTrain batch 31/32 - 65.5ms/batch - loss: 120.52176 - diff: 33.22mlTrain batch 32/32 - 56.0ms/batch - loss: 120.62521 - diff: 33.11mlTrain batch 32/32 - 11.0s 56.0ms/batch - loss: 120.62521 - diff: 33.11ml
Test 0.6s: val_loss: 121.26413 - diff: 34.33ml

Epoch 41: current best loss = 94.66534, at epoch 38
Train batch 1/32 - 76.6ms/batch - loss: 208.50273 - diff: 43.67mlTrain batch 2/32 - 58.1ms/batch - loss: 176.71304 - diff: 39.84mlTrain batch 3/32 - 65.9ms/batch - loss: 157.64586 - diff: 34.46mlTrain batch 4/32 - 58.7ms/batch - loss: 250.01099 - diff: 40.44mlTrain batch 5/32 - 66.5ms/batch - loss: 232.07538 - diff: 39.20mlTrain batch 6/32 - 60.0ms/batch - loss: 216.25107 - diff: 38.53mlTrain batch 7/32 - 59.9ms/batch - loss: 200.79470 - diff: 37.91mlTrain batch 8/32 - 59.3ms/batch - loss: 195.41022 - diff: 38.46mlTrain batch 9/32 - 59.3ms/batch - loss: 187.86121 - diff: 37.81mlTrain batch 10/32 - 59.5ms/batch - loss: 187.05209 - diff: 37.99mlTrain batch 11/32 - 58.5ms/batch - loss: 177.69011 - diff: 37.15mlTrain batch 12/32 - 58.5ms/batch - loss: 169.56469 - diff: 36.56mlTrain batch 13/32 - 63.9ms/batch - loss: 162.58878 - diff: 35.93mlTrain batch 14/32 - 64.6ms/batch - loss: 157.51606 - diff: 35.63mlTrain batch 15/32 - 71.3ms/batch - loss: 155.92506 - diff: 36.08mlTrain batch 16/32 - 61.0ms/batch - loss: 153.75770 - diff: 35.98mlTrain batch 17/32 - 65.6ms/batch - loss: 148.16503 - diff: 35.31mlTrain batch 18/32 - 64.7ms/batch - loss: 144.74043 - diff: 35.03mlTrain batch 19/32 - 65.6ms/batch - loss: 143.11594 - diff: 34.95mlTrain batch 20/32 - 64.7ms/batch - loss: 140.91721 - diff: 34.77mlTrain batch 21/32 - 64.4ms/batch - loss: 140.09284 - diff: 34.84mlTrain batch 22/32 - 64.2ms/batch - loss: 137.86326 - diff: 34.69mlTrain batch 23/32 - 64.3ms/batch - loss: 136.72452 - diff: 34.73mlTrain batch 24/32 - 64.1ms/batch - loss: 133.66027 - diff: 34.38mlTrain batch 25/32 - 66.1ms/batch - loss: 132.30539 - diff: 34.21mlTrain batch 26/32 - 65.6ms/batch - loss: 130.22881 - diff: 34.03mlTrain batch 27/32 - 65.2ms/batch - loss: 128.89418 - diff: 33.98mlTrain batch 28/32 - 65.1ms/batch - loss: 129.23838 - diff: 34.32mlTrain batch 29/32 - 61.3ms/batch - loss: 128.15329 - diff: 34.16mlTrain batch 30/32 - 54.1ms/batch - loss: 129.07381 - diff: 34.37mlTrain batch 31/32 - 50.7ms/batch - loss: 128.29891 - diff: 34.27mlTrain batch 32/32 - 38.4ms/batch - loss: 134.20041 - diff: 34.44mlTrain batch 32/32 - 10.5s 38.4ms/batch - loss: 134.20041 - diff: 34.44ml
Test 0.5s: val_loss: 128.94437 - diff: 34.60ml

Epoch 42: current best loss = 94.66534, at epoch 38
Train batch 1/32 - 76.1ms/batch - loss: 177.68546 - diff: 44.84mlTrain batch 2/32 - 63.6ms/batch - loss: 118.90025 - diff: 35.81mlTrain batch 3/32 - 63.8ms/batch - loss: 138.71205 - diff: 38.04mlTrain batch 4/32 - 64.3ms/batch - loss: 144.50557 - diff: 38.67mlTrain batch 5/32 - 63.6ms/batch - loss: 136.10581 - diff: 37.44mlTrain batch 6/32 - 63.7ms/batch - loss: 136.10613 - diff: 37.07mlTrain batch 7/32 - 62.6ms/batch - loss: 151.86731 - diff: 37.21mlTrain batch 8/32 - 67.8ms/batch - loss: 147.96290 - diff: 36.75mlTrain batch 9/32 - 66.5ms/batch - loss: 149.31543 - diff: 36.96mlTrain batch 10/32 - 66.0ms/batch - loss: 143.73778 - diff: 36.25mlTrain batch 11/32 - 59.4ms/batch - loss: 136.17381 - diff: 35.07mlTrain batch 12/32 - 58.7ms/batch - loss: 133.92646 - diff: 35.03mlTrain batch 13/32 - 64.5ms/batch - loss: 133.07785 - diff: 35.12mlTrain batch 14/32 - 66.7ms/batch - loss: 126.99771 - diff: 34.28mlTrain batch 15/32 - 65.7ms/batch - loss: 127.50950 - diff: 34.37mlTrain batch 16/32 - 65.1ms/batch - loss: 128.29982 - diff: 34.77mlTrain batch 17/32 - 63.2ms/batch - loss: 125.85299 - diff: 34.60mlTrain batch 18/32 - 63.9ms/batch - loss: 125.22093 - diff: 34.57mlTrain batch 19/32 - 62.9ms/batch - loss: 122.66061 - diff: 34.28mlTrain batch 20/32 - 62.9ms/batch - loss: 121.00561 - diff: 34.01mlTrain batch 21/32 - 63.4ms/batch - loss: 125.48107 - diff: 34.33mlTrain batch 22/32 - 62.5ms/batch - loss: 125.64438 - diff: 34.30mlTrain batch 23/32 - 64.0ms/batch - loss: 124.29421 - diff: 34.07mlTrain batch 24/32 - 63.8ms/batch - loss: 123.80552 - diff: 33.86mlTrain batch 25/32 - 63.2ms/batch - loss: 125.15490 - diff: 33.98mlTrain batch 26/32 - 64.0ms/batch - loss: 121.86208 - diff: 33.38mlTrain batch 27/32 - 58.6ms/batch - loss: 121.51227 - diff: 33.41mlTrain batch 28/32 - 59.8ms/batch - loss: 119.94975 - diff: 33.25mlTrain batch 29/32 - 59.2ms/batch - loss: 123.79593 - diff: 33.72mlTrain batch 30/32 - 60.0ms/batch - loss: 124.24907 - diff: 33.82mlTrain batch 31/32 - 58.8ms/batch - loss: 123.33893 - diff: 33.67mlTrain batch 32/32 - 52.0ms/batch - loss: 122.87555 - diff: 33.50mlTrain batch 32/32 - 11.2s 52.0ms/batch - loss: 122.87555 - diff: 33.50ml
Test 0.6s: val_loss: 115.90472 - diff: 34.54ml

Epoch 43: current best loss = 94.66534, at epoch 38
Train batch 1/32 - 74.1ms/batch - loss: 37.92174 - diff: 18.19mlTrain batch 2/32 - 62.5ms/batch - loss: 57.85283 - diff: 23.75mlTrain batch 3/32 - 62.6ms/batch - loss: 87.07529 - diff: 28.77mlTrain batch 4/32 - 62.0ms/batch - loss: 91.23298 - diff: 30.20mlTrain batch 5/32 - 66.3ms/batch - loss: 90.52830 - diff: 30.40mlTrain batch 6/32 - 63.0ms/batch - loss: 102.23878 - diff: 31.44mlTrain batch 7/32 - 67.2ms/batch - loss: 101.38564 - diff: 31.77mlTrain batch 8/32 - 63.2ms/batch - loss: 129.55148 - diff: 33.27mlTrain batch 9/32 - 67.3ms/batch - loss: 132.11287 - diff: 33.67mlTrain batch 10/32 - 63.0ms/batch - loss: 127.18199 - diff: 33.48mlTrain batch 11/32 - 65.3ms/batch - loss: 131.10164 - diff: 34.09mlTrain batch 12/32 - 62.6ms/batch - loss: 126.96170 - diff: 33.78mlTrain batch 13/32 - 63.3ms/batch - loss: 136.81211 - diff: 34.93mlTrain batch 14/32 - 53.2ms/batch - loss: 162.08497 - diff: 36.57mlTrain batch 15/32 - 61.0ms/batch - loss: 159.44557 - diff: 36.25mlTrain batch 16/32 - 53.1ms/batch - loss: 157.73294 - diff: 36.36mlTrain batch 17/32 - 68.2ms/batch - loss: 155.97358 - diff: 36.26mlTrain batch 18/32 - 68.2ms/batch - loss: 150.49040 - diff: 35.50mlTrain batch 19/32 - 62.5ms/batch - loss: 148.27041 - diff: 35.19mlTrain batch 20/32 - 61.0ms/batch - loss: 147.26988 - diff: 35.26mlTrain batch 21/32 - 61.5ms/batch - loss: 145.13664 - diff: 35.14mlTrain batch 22/32 - 60.7ms/batch - loss: 144.68785 - diff: 35.32mlTrain batch 23/32 - 59.2ms/batch - loss: 146.58973 - diff: 35.60mlTrain batch 24/32 - 59.1ms/batch - loss: 143.26940 - diff: 35.17mlTrain batch 25/32 - 67.2ms/batch - loss: 140.13296 - diff: 34.80mlTrain batch 26/32 - 66.0ms/batch - loss: 141.06890 - diff: 35.08mlTrain batch 27/32 - 57.6ms/batch - loss: 141.94657 - diff: 35.26mlTrain batch 28/32 - 53.2ms/batch - loss: 138.88587 - diff: 34.81mlTrain batch 29/32 - 60.8ms/batch - loss: 137.66579 - diff: 34.67mlTrain batch 30/32 - 60.8ms/batch - loss: 136.77655 - diff: 34.69mlTrain batch 31/32 - 55.4ms/batch - loss: 134.10667 - diff: 34.38mlTrain batch 32/32 - 45.3ms/batch - loss: 134.35448 - diff: 34.28mlTrain batch 32/32 - 11.7s 45.3ms/batch - loss: 134.35448 - diff: 34.28ml
Test 0.5s: val_loss: 158.54639 - diff: 40.80ml

Epoch 44: current best loss = 94.66534, at epoch 38
Train batch 1/32 - 64.8ms/batch - loss: 95.51398 - diff: 29.97mlTrain batch 2/32 - 54.4ms/batch - loss: 107.89072 - diff: 34.52mlTrain batch 3/32 - 62.6ms/batch - loss: 123.89318 - diff: 37.58mlTrain batch 4/32 - 67.9ms/batch - loss: 121.76723 - diff: 36.80mlTrain batch 5/32 - 60.9ms/batch - loss: 123.78273 - diff: 35.43mlTrain batch 6/32 - 68.1ms/batch - loss: 119.71644 - diff: 35.14mlTrain batch 7/32 - 64.3ms/batch - loss: 128.46313 - diff: 35.42mlTrain batch 8/32 - 68.0ms/batch - loss: 142.23788 - diff: 37.22mlTrain batch 9/32 - 59.6ms/batch - loss: 132.16850 - diff: 35.76mlTrain batch 10/32 - 60.3ms/batch - loss: 128.45541 - diff: 35.32mlTrain batch 11/32 - 59.6ms/batch - loss: 128.67764 - diff: 35.50mlTrain batch 12/32 - 62.2ms/batch - loss: 134.12556 - diff: 36.12mlTrain batch 13/32 - 54.2ms/batch - loss: 132.29674 - diff: 35.81mlTrain batch 14/32 - 59.7ms/batch - loss: 127.47337 - diff: 35.25mlTrain batch 15/32 - 62.1ms/batch - loss: 129.23778 - diff: 35.25mlTrain batch 16/32 - 60.1ms/batch - loss: 128.71244 - diff: 35.41mlTrain batch 17/32 - 62.5ms/batch - loss: 126.43062 - diff: 35.18mlTrain batch 18/32 - 61.8ms/batch - loss: 128.21421 - diff: 35.08mlTrain batch 19/32 - 68.4ms/batch - loss: 125.70729 - diff: 34.75mlTrain batch 20/32 - 68.0ms/batch - loss: 122.00503 - diff: 34.15mlTrain batch 21/32 - 68.8ms/batch - loss: 122.62764 - diff: 34.14mlTrain batch 22/32 - 68.0ms/batch - loss: 122.74817 - diff: 34.27mlTrain batch 23/32 - 68.5ms/batch - loss: 122.09556 - diff: 34.17mlTrain batch 24/32 - 68.9ms/batch - loss: 124.86065 - diff: 34.43mlTrain batch 25/32 - 68.3ms/batch - loss: 123.22851 - diff: 34.20mlTrain batch 26/32 - 67.7ms/batch - loss: 121.54188 - diff: 33.98mlTrain batch 27/32 - 62.6ms/batch - loss: 122.83274 - diff: 34.28mlTrain batch 28/32 - 62.3ms/batch - loss: 143.69440 - diff: 35.20mlTrain batch 29/32 - 62.6ms/batch - loss: 141.21979 - diff: 34.98mlTrain batch 30/32 - 62.2ms/batch - loss: 141.39621 - diff: 35.05mlTrain batch 31/32 - 62.2ms/batch - loss: 144.62038 - diff: 35.43mlTrain batch 32/32 - 56.4ms/batch - loss: 150.02370 - diff: 35.51mlTrain batch 32/32 - 11.1s 56.4ms/batch - loss: 150.02370 - diff: 35.51ml
Test 0.5s: val_loss: 109.66487 - diff: 32.24ml

Epoch 45: current best loss = 94.66534, at epoch 38
Going to unfreeze the pretrained weights
Train batch 1/32 - 128.6ms/batch - loss: 154.41315 - diff: 39.33mlTrain batch 2/32 - 120.2ms/batch - loss: 195.51164 - diff: 45.34mlTrain batch 3/32 - 106.3ms/batch - loss: 313.52412 - diff: 55.17mlTrain batch 4/32 - 108.5ms/batch - loss: 424.36407 - diff: 64.70mlTrain batch 5/32 - 107.4ms/batch - loss: 444.08328 - diff: 66.48mlTrain batch 6/32 - 105.7ms/batch - loss: 412.33480 - diff: 63.86mlTrain batch 7/32 - 108.8ms/batch - loss: 397.98614 - diff: 62.05mlTrain batch 8/32 - 110.3ms/batch - loss: 381.03021 - diff: 60.83mlTrain batch 9/32 - 93.6ms/batch - loss: 384.85594 - diff: 61.32mlTrain batch 10/32 - 96.8ms/batch - loss: 376.76200 - diff: 60.88mlTrain batch 11/32 - 116.8ms/batch - loss: 375.24932 - diff: 60.75mlTrain batch 12/32 - 112.2ms/batch - loss: 360.34552 - diff: 59.25mlTrain batch 13/32 - 115.5ms/batch - loss: 345.37968 - diff: 57.94mlTrain batch 14/32 - 114.7ms/batch - loss: 329.25605 - diff: 55.96mlTrain batch 15/32 - 115.9ms/batch - loss: 314.52063 - diff: 54.47mlTrain batch 16/32 - 115.8ms/batch - loss: 312.98720 - diff: 54.67mlTrain batch 17/32 - 111.3ms/batch - loss: 304.91011 - diff: 54.09mlTrain batch 18/32 - 110.6ms/batch - loss: 299.66409 - diff: 53.70mlTrain batch 19/32 - 105.4ms/batch - loss: 292.11742 - diff: 52.78mlTrain batch 20/32 - 105.1ms/batch - loss: 287.03630 - diff: 52.45mlTrain batch 21/32 - 113.4ms/batch - loss: 288.29125 - diff: 52.53mlTrain batch 22/32 - 119.5ms/batch - loss: 283.84016 - diff: 51.93mlTrain batch 23/32 - 113.6ms/batch - loss: 304.67768 - diff: 52.49mlTrain batch 24/32 - 115.1ms/batch - loss: 298.51064 - diff: 52.00mlTrain batch 25/32 - 115.2ms/batch - loss: 289.42885 - diff: 50.94mlTrain batch 26/32 - 113.8ms/batch - loss: 283.00183 - diff: 50.07mlTrain batch 27/32 - 108.6ms/batch - loss: 280.23565 - diff: 49.98mlTrain batch 28/32 - 108.5ms/batch - loss: 274.28658 - diff: 49.46mlTrain batch 29/32 - 109.7ms/batch - loss: 269.14232 - diff: 49.11mlTrain batch 30/32 - 108.5ms/batch - loss: 267.45486 - diff: 49.08mlTrain batch 31/32 - 97.0ms/batch - loss: 262.19027 - diff: 48.58mlTrain batch 32/32 - 83.7ms/batch - loss: 274.80523 - diff: 48.76mlTrain batch 32/32 - 11.6s 83.7ms/batch - loss: 274.80523 - diff: 48.76ml
Test 0.6s: val_loss: 203.96110 - diff: 45.06ml

Epoch 46: current best loss = 94.66534, at epoch 38
Train batch 1/32 - 130.3ms/batch - loss: 123.09601 - diff: 35.40mlTrain batch 2/32 - 116.1ms/batch - loss: 174.84319 - diff: 43.17mlTrain batch 3/32 - 113.7ms/batch - loss: 210.10828 - diff: 48.31mlTrain batch 4/32 - 112.1ms/batch - loss: 188.95512 - diff: 44.94mlTrain batch 5/32 - 107.4ms/batch - loss: 174.29438 - diff: 42.69mlTrain batch 6/32 - 106.4ms/batch - loss: 176.78168 - diff: 42.27mlTrain batch 7/32 - 100.0ms/batch - loss: 183.91651 - diff: 43.22mlTrain batch 8/32 - 101.0ms/batch - loss: 179.09304 - diff: 42.44mlTrain batch 9/32 - 120.2ms/batch - loss: 181.70275 - diff: 42.78mlTrain batch 10/32 - 113.2ms/batch - loss: 173.34609 - diff: 41.48mlTrain batch 11/32 - 102.0ms/batch - loss: 172.86752 - diff: 41.39mlTrain batch 12/32 - 95.1ms/batch - loss: 220.67281 - diff: 43.25mlTrain batch 13/32 - 103.1ms/batch - loss: 222.69209 - diff: 43.58mlTrain batch 14/32 - 102.1ms/batch - loss: 212.72224 - diff: 42.75mlTrain batch 15/32 - 102.3ms/batch - loss: 210.10064 - diff: 42.66mlTrain batch 16/32 - 103.6ms/batch - loss: 201.57272 - diff: 41.79mlTrain batch 17/32 - 106.6ms/batch - loss: 215.41054 - diff: 42.47mlTrain batch 18/32 - 106.8ms/batch - loss: 218.32839 - diff: 42.95mlTrain batch 19/32 - 116.6ms/batch - loss: 224.36621 - diff: 43.16mlTrain batch 20/32 - 113.2ms/batch - loss: 224.34350 - diff: 43.18mlTrain batch 21/32 - 116.2ms/batch - loss: 216.83073 - diff: 42.34mlTrain batch 22/32 - 116.5ms/batch - loss: 214.23814 - diff: 42.28mlTrain batch 23/32 - 116.7ms/batch - loss: 209.41733 - diff: 41.87mlTrain batch 24/32 - 116.6ms/batch - loss: 203.07357 - diff: 41.21mlTrain batch 25/32 - 112.7ms/batch - loss: 197.70097 - diff: 40.55mlTrain batch 26/32 - 112.4ms/batch - loss: 195.48492 - diff: 40.52mlTrain batch 27/32 - 116.5ms/batch - loss: 193.78661 - diff: 40.34mlTrain batch 28/32 - 115.0ms/batch - loss: 189.93970 - diff: 40.00mlTrain batch 29/32 - 120.2ms/batch - loss: 186.55108 - diff: 39.79mlTrain batch 30/32 - 109.0ms/batch - loss: 184.60130 - diff: 39.72mlTrain batch 31/32 - 110.9ms/batch - loss: 182.85821 - diff: 39.73mlTrain batch 32/32 - 75.4ms/batch - loss: 183.50408 - diff: 39.62mlTrain batch 32/32 - 11.3s 75.4ms/batch - loss: 183.50408 - diff: 39.62ml
Test 0.6s: val_loss: 84.53010 - diff: 28.66ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 47: current best loss = 84.53010, at epoch 46
Train batch 1/32 - 115.3ms/batch - loss: 93.13914 - diff: 30.20mlTrain batch 2/32 - 103.2ms/batch - loss: 104.58582 - diff: 33.50mlTrain batch 3/32 - 105.8ms/batch - loss: 207.46084 - diff: 36.83mlTrain batch 4/32 - 106.7ms/batch - loss: 165.51174 - diff: 33.32mlTrain batch 5/32 - 98.5ms/batch - loss: 155.67399 - diff: 33.85mlTrain batch 6/32 - 90.6ms/batch - loss: 139.54010 - diff: 32.00mlTrain batch 7/32 - 107.3ms/batch - loss: 136.43790 - diff: 32.61mlTrain batch 8/32 - 113.9ms/batch - loss: 133.04254 - diff: 32.63mlTrain batch 9/32 - 108.4ms/batch - loss: 136.11486 - diff: 33.12mlTrain batch 10/32 - 108.9ms/batch - loss: 127.91518 - diff: 32.05mlTrain batch 11/32 - 106.4ms/batch - loss: 125.38157 - diff: 32.34mlTrain batch 12/32 - 100.8ms/batch - loss: 123.12714 - diff: 32.31mlTrain batch 13/32 - 115.0ms/batch - loss: 122.21092 - diff: 32.39mlTrain batch 14/32 - 107.4ms/batch - loss: 131.62362 - diff: 33.11mlTrain batch 15/32 - 113.9ms/batch - loss: 140.99728 - diff: 33.22mlTrain batch 16/32 - 106.9ms/batch - loss: 136.06188 - diff: 32.69mlTrain batch 17/32 - 128.0ms/batch - loss: 129.57975 - diff: 31.69mlTrain batch 18/32 - 112.1ms/batch - loss: 129.04389 - diff: 31.74mlTrain batch 19/32 - 112.0ms/batch - loss: 136.48317 - diff: 32.55mlTrain batch 20/32 - 112.5ms/batch - loss: 142.65420 - diff: 33.28mlTrain batch 21/32 - 102.1ms/batch - loss: 141.54986 - diff: 33.47mlTrain batch 22/32 - 104.7ms/batch - loss: 139.92010 - diff: 33.45mlTrain batch 23/32 - 106.9ms/batch - loss: 137.47356 - diff: 32.93mlTrain batch 24/32 - 105.7ms/batch - loss: 136.27095 - diff: 33.01mlTrain batch 25/32 - 107.2ms/batch - loss: 137.25105 - diff: 33.40mlTrain batch 26/32 - 110.0ms/batch - loss: 135.40367 - diff: 33.26mlTrain batch 27/32 - 106.3ms/batch - loss: 133.45868 - diff: 33.13mlTrain batch 28/32 - 108.3ms/batch - loss: 136.10832 - diff: 33.45mlTrain batch 29/32 - 110.0ms/batch - loss: 133.16873 - diff: 33.18mlTrain batch 30/32 - 97.0ms/batch - loss: 132.20835 - diff: 33.21mlTrain batch 31/32 - 112.2ms/batch - loss: 130.16661 - diff: 32.99mlTrain batch 32/32 - 84.5ms/batch - loss: 132.40275 - diff: 33.01mlTrain batch 32/32 - 11.4s 84.5ms/batch - loss: 132.40275 - diff: 33.01ml
Test 0.6s: val_loss: 237.86333 - diff: 50.79ml

Epoch 48: current best loss = 84.53010, at epoch 46
Train batch 1/32 - 121.5ms/batch - loss: 84.87328 - diff: 29.85mlTrain batch 2/32 - 105.3ms/batch - loss: 70.46271 - diff: 27.04mlTrain batch 3/32 - 104.1ms/batch - loss: 86.73099 - diff: 30.57mlTrain batch 4/32 - 105.6ms/batch - loss: 98.25765 - diff: 32.44mlTrain batch 5/32 - 106.3ms/batch - loss: 110.73376 - diff: 34.33mlTrain batch 6/32 - 113.3ms/batch - loss: 103.52619 - diff: 33.24mlTrain batch 7/32 - 104.8ms/batch - loss: 98.36930 - diff: 32.46mlTrain batch 8/32 - 110.5ms/batch - loss: 93.05047 - diff: 31.28mlTrain batch 9/32 - 105.9ms/batch - loss: 93.37180 - diff: 31.04mlTrain batch 10/32 - 108.9ms/batch - loss: 89.00709 - diff: 30.24mlTrain batch 11/32 - 105.1ms/batch - loss: 88.36201 - diff: 30.18mlTrain batch 12/32 - 111.9ms/batch - loss: 90.47743 - diff: 30.45mlTrain batch 13/32 - 104.7ms/batch - loss: 88.94481 - diff: 30.33mlTrain batch 14/32 - 120.1ms/batch - loss: 89.37084 - diff: 30.51mlTrain batch 15/32 - 114.5ms/batch - loss: 89.20377 - diff: 30.26mlTrain batch 16/32 - 107.1ms/batch - loss: 90.52128 - diff: 30.67mlTrain batch 17/32 - 103.4ms/batch - loss: 88.09306 - diff: 30.15mlTrain batch 18/32 - 107.8ms/batch - loss: 90.17448 - diff: 30.34mlTrain batch 19/32 - 103.2ms/batch - loss: 89.04575 - diff: 30.14mlTrain batch 20/32 - 104.3ms/batch - loss: 89.05728 - diff: 29.98mlTrain batch 21/32 - 102.9ms/batch - loss: 87.15675 - diff: 29.55mlTrain batch 22/32 - 102.6ms/batch - loss: 86.27436 - diff: 29.36mlTrain batch 23/32 - 103.4ms/batch - loss: 83.48974 - diff: 28.78mlTrain batch 24/32 - 105.2ms/batch - loss: 82.62604 - diff: 28.51mlTrain batch 25/32 - 105.2ms/batch - loss: 91.56620 - diff: 29.20mlTrain batch 26/32 - 102.9ms/batch - loss: 91.09550 - diff: 29.19mlTrain batch 27/32 - 102.7ms/batch - loss: 90.47787 - diff: 29.16mlTrain batch 28/32 - 103.3ms/batch - loss: 96.64541 - diff: 29.79mlTrain batch 29/32 - 102.9ms/batch - loss: 98.81781 - diff: 30.11mlTrain batch 30/32 - 103.3ms/batch - loss: 98.41603 - diff: 30.15mlTrain batch 31/32 - 103.3ms/batch - loss: 100.48091 - diff: 30.26mlTrain batch 32/32 - 102.0ms/batch - loss: 102.48852 - diff: 30.23mlTrain batch 32/32 - 10.8s 102.0ms/batch - loss: 102.48852 - diff: 30.23ml
Test 0.6s: val_loss: 74.69561 - diff: 25.05ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 49: current best loss = 74.69561, at epoch 48
Train batch 1/32 - 115.5ms/batch - loss: 96.87717 - diff: 30.66mlTrain batch 2/32 - 104.0ms/batch - loss: 92.11254 - diff: 29.14mlTrain batch 3/32 - 103.7ms/batch - loss: 78.98033 - diff: 25.92mlTrain batch 4/32 - 103.9ms/batch - loss: 111.82626 - diff: 29.01mlTrain batch 5/32 - 103.7ms/batch - loss: 113.48142 - diff: 30.72mlTrain batch 6/32 - 103.4ms/batch - loss: 104.20024 - diff: 29.57mlTrain batch 7/32 - 102.9ms/batch - loss: 110.51446 - diff: 29.73mlTrain batch 8/32 - 103.8ms/batch - loss: 101.70466 - diff: 28.30mlTrain batch 9/32 - 103.5ms/batch - loss: 101.32810 - diff: 28.56mlTrain batch 10/32 - 104.6ms/batch - loss: 97.89295 - diff: 28.05mlTrain batch 11/32 - 103.2ms/batch - loss: 98.34544 - diff: 27.74mlTrain batch 12/32 - 104.0ms/batch - loss: 93.91449 - diff: 27.34mlTrain batch 13/32 - 92.2ms/batch - loss: 98.24100 - diff: 28.35mlTrain batch 14/32 - 107.8ms/batch - loss: 98.72606 - diff: 28.70mlTrain batch 15/32 - 119.0ms/batch - loss: 95.98569 - diff: 28.41mlTrain batch 16/32 - 117.4ms/batch - loss: 94.56889 - diff: 28.15mlTrain batch 17/32 - 120.3ms/batch - loss: 92.27782 - diff: 27.84mlTrain batch 18/32 - 115.0ms/batch - loss: 93.27002 - diff: 28.29mlTrain batch 19/32 - 114.1ms/batch - loss: 95.02235 - diff: 28.87mlTrain batch 20/32 - 113.6ms/batch - loss: 94.63306 - diff: 29.04mlTrain batch 21/32 - 115.7ms/batch - loss: 94.25286 - diff: 28.96mlTrain batch 22/32 - 115.6ms/batch - loss: 92.07917 - diff: 28.76mlTrain batch 23/32 - 116.7ms/batch - loss: 90.12683 - diff: 28.44mlTrain batch 24/32 - 117.5ms/batch - loss: 89.86362 - diff: 28.52mlTrain batch 25/32 - 117.2ms/batch - loss: 91.42414 - diff: 28.68mlTrain batch 26/32 - 116.1ms/batch - loss: 91.61373 - diff: 28.71mlTrain batch 27/32 - 114.1ms/batch - loss: 93.30724 - diff: 29.02mlTrain batch 28/32 - 113.8ms/batch - loss: 97.34235 - diff: 29.35mlTrain batch 29/32 - 108.5ms/batch - loss: 96.66081 - diff: 29.29mlTrain batch 30/32 - 96.8ms/batch - loss: 96.09075 - diff: 29.26mlTrain batch 31/32 - 110.7ms/batch - loss: 96.24990 - diff: 29.31mlTrain batch 32/32 - 83.7ms/batch - loss: 96.65672 - diff: 29.23mlTrain batch 32/32 - 11.5s 83.7ms/batch - loss: 96.65672 - diff: 29.23ml
Test 0.6s: val_loss: 97.30965 - diff: 28.76ml

Epoch 50: current best loss = 74.69561, at epoch 48
Train batch 1/32 - 120.2ms/batch - loss: 65.07231 - diff: 25.78mlTrain batch 2/32 - 106.7ms/batch - loss: 121.43555 - diff: 34.61mlTrain batch 3/32 - 106.4ms/batch - loss: 107.01660 - diff: 32.67mlTrain batch 4/32 - 107.1ms/batch - loss: 104.63055 - diff: 32.76mlTrain batch 5/32 - 107.3ms/batch - loss: 91.63575 - diff: 30.39mlTrain batch 6/32 - 107.0ms/batch - loss: 84.26644 - diff: 29.02mlTrain batch 7/32 - 103.0ms/batch - loss: 75.32723 - diff: 27.02mlTrain batch 8/32 - 101.3ms/batch - loss: 71.89800 - diff: 26.12mlTrain batch 9/32 - 113.5ms/batch - loss: 70.04471 - diff: 25.79mlTrain batch 10/32 - 114.1ms/batch - loss: 67.87964 - diff: 25.24mlTrain batch 11/32 - 114.5ms/batch - loss: 67.72856 - diff: 25.44mlTrain batch 12/32 - 117.7ms/batch - loss: 70.44383 - diff: 26.07mlTrain batch 13/32 - 109.3ms/batch - loss: 70.72264 - diff: 26.38mlTrain batch 14/32 - 111.5ms/batch - loss: 82.69224 - diff: 26.65mlTrain batch 15/32 - 102.6ms/batch - loss: 80.04194 - diff: 25.92mlTrain batch 16/32 - 112.5ms/batch - loss: 85.34425 - diff: 26.97mlTrain batch 17/32 - 103.2ms/batch - loss: 87.10373 - diff: 27.23mlTrain batch 18/32 - 111.5ms/batch - loss: 87.72658 - diff: 27.47mlTrain batch 19/32 - 103.6ms/batch - loss: 89.12976 - diff: 27.76mlTrain batch 20/32 - 109.4ms/batch - loss: 90.64789 - diff: 28.15mlTrain batch 21/32 - 102.9ms/batch - loss: 91.23003 - diff: 28.09mlTrain batch 22/32 - 111.7ms/batch - loss: 91.52380 - diff: 28.21mlTrain batch 23/32 - 107.5ms/batch - loss: 90.18398 - diff: 28.13mlTrain batch 24/32 - 113.6ms/batch - loss: 90.93046 - diff: 28.35mlTrain batch 25/32 - 103.3ms/batch - loss: 92.82585 - diff: 28.64mlTrain batch 26/32 - 115.0ms/batch - loss: 93.39721 - diff: 28.76mlTrain batch 27/32 - 106.3ms/batch - loss: 93.76869 - diff: 28.91mlTrain batch 28/32 - 113.0ms/batch - loss: 92.86741 - diff: 28.84mlTrain batch 29/32 - 105.7ms/batch - loss: 91.23360 - diff: 28.51mlTrain batch 30/32 - 102.1ms/batch - loss: 89.47931 - diff: 28.27mlTrain batch 31/32 - 90.4ms/batch - loss: 88.81427 - diff: 28.16mlTrain batch 32/32 - 83.6ms/batch - loss: 95.62824 - diff: 28.37mlTrain batch 32/32 - 11.2s 83.6ms/batch - loss: 95.62824 - diff: 28.37ml
Test 0.5s: val_loss: 130.85038 - diff: 37.26ml

Epoch 51: current best loss = 74.69561, at epoch 48
Train batch 1/32 - 120.2ms/batch - loss: 43.23666 - diff: 20.13mlTrain batch 2/32 - 106.7ms/batch - loss: 54.79326 - diff: 23.69mlTrain batch 3/32 - 106.9ms/batch - loss: 67.74055 - diff: 26.08mlTrain batch 4/32 - 105.9ms/batch - loss: 71.86132 - diff: 26.94mlTrain batch 5/32 - 107.7ms/batch - loss: 65.52998 - diff: 25.59mlTrain batch 6/32 - 110.8ms/batch - loss: 64.85825 - diff: 25.56mlTrain batch 7/32 - 96.8ms/batch - loss: 79.53852 - diff: 27.53mlTrain batch 8/32 - 106.4ms/batch - loss: 78.97027 - diff: 28.05mlTrain batch 9/32 - 110.3ms/batch - loss: 86.69087 - diff: 29.64mlTrain batch 10/32 - 109.5ms/batch - loss: 86.61559 - diff: 29.58mlTrain batch 11/32 - 91.3ms/batch - loss: 84.99919 - diff: 29.38mlTrain batch 12/32 - 107.4ms/batch - loss: 88.60415 - diff: 29.59mlTrain batch 13/32 - 104.6ms/batch - loss: 89.45476 - diff: 29.91mlTrain batch 14/32 - 110.4ms/batch - loss: 88.18325 - diff: 29.70mlTrain batch 15/32 - 103.2ms/batch - loss: 88.88206 - diff: 29.95mlTrain batch 16/32 - 109.7ms/batch - loss: 84.47966 - diff: 28.88mlTrain batch 17/32 - 102.5ms/batch - loss: 84.33838 - diff: 28.87mlTrain batch 18/32 - 105.4ms/batch - loss: 93.15536 - diff: 29.87mlTrain batch 19/32 - 102.4ms/batch - loss: 95.86775 - diff: 30.06mlTrain batch 20/32 - 104.8ms/batch - loss: 94.12296 - diff: 29.74mlTrain batch 21/32 - 106.7ms/batch - loss: 91.61792 - diff: 29.38mlTrain batch 22/32 - 113.4ms/batch - loss: 91.49939 - diff: 29.16mlTrain batch 23/32 - 107.3ms/batch - loss: 90.47868 - diff: 29.01mlTrain batch 24/32 - 112.4ms/batch - loss: 89.24542 - diff: 28.86mlTrain batch 25/32 - 107.3ms/batch - loss: 88.34135 - diff: 28.77mlTrain batch 26/32 - 113.9ms/batch - loss: 88.18601 - diff: 28.81mlTrain batch 27/32 - 113.6ms/batch - loss: 86.41168 - diff: 28.51mlTrain batch 28/32 - 106.6ms/batch - loss: 85.76181 - diff: 28.45mlTrain batch 29/32 - 102.8ms/batch - loss: 89.91859 - diff: 28.91mlTrain batch 30/32 - 94.2ms/batch - loss: 90.65671 - diff: 29.13mlTrain batch 31/32 - 90.5ms/batch - loss: 95.63103 - diff: 29.62mlTrain batch 32/32 - 84.9ms/batch - loss: 96.80055 - diff: 29.59mlTrain batch 32/32 - 11.3s 84.9ms/batch - loss: 96.80055 - diff: 29.59ml
Test 0.5s: val_loss: 79.04657 - diff: 27.48ml

Epoch 52: current best loss = 74.69561, at epoch 48
Train batch 1/32 - 134.4ms/batch - loss: 84.81018 - diff: 28.90mlTrain batch 2/32 - 114.3ms/batch - loss: 76.09173 - diff: 26.82mlTrain batch 3/32 - 118.3ms/batch - loss: 94.30752 - diff: 28.47mlTrain batch 4/32 - 104.8ms/batch - loss: 91.93356 - diff: 28.88mlTrain batch 5/32 - 109.2ms/batch - loss: 87.71975 - diff: 28.52mlTrain batch 6/32 - 103.0ms/batch - loss: 84.27982 - diff: 28.15mlTrain batch 7/32 - 109.6ms/batch - loss: 81.41395 - diff: 27.19mlTrain batch 8/32 - 103.5ms/batch - loss: 95.09502 - diff: 29.50mlTrain batch 9/32 - 105.4ms/batch - loss: 90.06693 - diff: 28.91mlTrain batch 10/32 - 102.9ms/batch - loss: 89.90824 - diff: 28.65mlTrain batch 11/32 - 99.7ms/batch - loss: 88.34242 - diff: 28.81mlTrain batch 12/32 - 99.1ms/batch - loss: 88.56014 - diff: 28.90mlTrain batch 13/32 - 83.0ms/batch - loss: 89.74508 - diff: 29.28mlTrain batch 14/32 - 105.1ms/batch - loss: 90.29676 - diff: 28.94mlTrain batch 15/32 - 99.8ms/batch - loss: 88.33061 - diff: 28.63mlTrain batch 16/32 - 103.0ms/batch - loss: 86.93583 - diff: 28.56mlTrain batch 17/32 - 104.7ms/batch - loss: 84.75847 - diff: 28.29mlTrain batch 18/32 - 103.8ms/batch - loss: 84.90256 - diff: 28.41mlTrain batch 19/32 - 104.0ms/batch - loss: 82.11398 - diff: 27.89mlTrain batch 20/32 - 103.1ms/batch - loss: 83.64288 - diff: 28.19mlTrain batch 21/32 - 103.6ms/batch - loss: 84.19240 - diff: 28.38mlTrain batch 22/32 - 103.2ms/batch - loss: 83.00144 - diff: 28.32mlTrain batch 23/32 - 104.0ms/batch - loss: 90.88847 - diff: 28.77mlTrain batch 24/32 - 102.9ms/batch - loss: 89.75267 - diff: 28.39mlTrain batch 25/32 - 104.2ms/batch - loss: 98.09708 - diff: 29.45mlTrain batch 26/32 - 105.7ms/batch - loss: 98.62125 - diff: 29.51mlTrain batch 27/32 - 106.0ms/batch - loss: 97.22427 - diff: 29.26mlTrain batch 28/32 - 106.1ms/batch - loss: 97.49688 - diff: 29.37mlTrain batch 29/32 - 107.0ms/batch - loss: 96.99327 - diff: 29.30mlTrain batch 30/32 - 106.5ms/batch - loss: 94.93691 - diff: 28.99mlTrain batch 31/32 - 90.6ms/batch - loss: 95.00484 - diff: 28.90mlTrain batch 32/32 - 81.5ms/batch - loss: 95.08868 - diff: 28.78mlTrain batch 32/32 - 10.7s 81.5ms/batch - loss: 95.08868 - diff: 28.78ml
Test 0.6s: val_loss: 131.79500 - diff: 34.52ml

Epoch 53: current best loss = 74.69561, at epoch 48
Train batch 1/32 - 127.4ms/batch - loss: 57.53731 - diff: 26.98mlTrain batch 2/32 - 107.1ms/batch - loss: 192.87540 - diff: 35.72mlTrain batch 3/32 - 106.9ms/batch - loss: 153.12739 - diff: 33.91mlTrain batch 4/32 - 106.6ms/batch - loss: 137.77433 - diff: 33.56mlTrain batch 5/32 - 106.9ms/batch - loss: 126.87046 - diff: 32.56mlTrain batch 6/32 - 107.0ms/batch - loss: 116.94877 - diff: 31.47mlTrain batch 7/32 - 106.6ms/batch - loss: 119.36106 - diff: 31.79mlTrain batch 8/32 - 108.9ms/batch - loss: 110.41466 - diff: 30.60mlTrain batch 9/32 - 112.7ms/batch - loss: 102.98461 - diff: 29.45mlTrain batch 10/32 - 107.3ms/batch - loss: 96.85728 - diff: 28.67mlTrain batch 11/32 - 107.1ms/batch - loss: 92.14491 - diff: 28.14mlTrain batch 12/32 - 106.7ms/batch - loss: 89.81753 - diff: 27.97mlTrain batch 13/32 - 105.8ms/batch - loss: 93.15191 - diff: 28.96mlTrain batch 14/32 - 107.3ms/batch - loss: 90.59826 - diff: 28.75mlTrain batch 15/32 - 106.7ms/batch - loss: 90.13547 - diff: 28.73mlTrain batch 16/32 - 102.9ms/batch - loss: 88.99680 - diff: 28.65mlTrain batch 17/32 - 103.2ms/batch - loss: 88.63187 - diff: 28.46mlTrain batch 18/32 - 103.1ms/batch - loss: 85.57090 - diff: 27.96mlTrain batch 19/32 - 102.7ms/batch - loss: 85.47828 - diff: 28.02mlTrain batch 20/32 - 103.3ms/batch - loss: 84.18474 - diff: 27.90mlTrain batch 21/32 - 104.1ms/batch - loss: 87.21285 - diff: 28.51mlTrain batch 22/32 - 102.9ms/batch - loss: 85.81313 - diff: 28.37mlTrain batch 23/32 - 107.2ms/batch - loss: 86.08952 - diff: 28.48mlTrain batch 24/32 - 105.2ms/batch - loss: 86.10707 - diff: 28.56mlTrain batch 25/32 - 106.6ms/batch - loss: 86.92584 - diff: 28.65mlTrain batch 26/32 - 103.6ms/batch - loss: 87.07139 - diff: 28.78mlTrain batch 27/32 - 102.6ms/batch - loss: 89.94388 - diff: 29.36mlTrain batch 28/32 - 102.9ms/batch - loss: 89.05200 - diff: 29.32mlTrain batch 29/32 - 104.1ms/batch - loss: 88.86326 - diff: 29.45mlTrain batch 30/32 - 107.1ms/batch - loss: 88.77350 - diff: 29.41mlTrain batch 31/32 - 108.2ms/batch - loss: 86.95230 - diff: 29.02mlTrain batch 32/32 - 82.2ms/batch - loss: 88.81235 - diff: 29.02mlTrain batch 32/32 - 11.0s 82.2ms/batch - loss: 88.81235 - diff: 29.02ml
Test 0.5s: val_loss: 183.40929 - diff: 41.34ml

Epoch 54: current best loss = 74.69561, at epoch 48
Train batch 1/32 - 115.9ms/batch - loss: 48.48484 - diff: 23.47mlTrain batch 2/32 - 103.8ms/batch - loss: 43.12562 - diff: 21.56mlTrain batch 3/32 - 117.7ms/batch - loss: 46.31868 - diff: 21.62mlTrain batch 4/32 - 113.6ms/batch - loss: 47.40358 - diff: 22.15mlTrain batch 5/32 - 105.8ms/batch - loss: 48.72665 - diff: 22.70mlTrain batch 6/32 - 102.8ms/batch - loss: 58.41148 - diff: 24.38mlTrain batch 7/32 - 104.8ms/batch - loss: 56.34448 - diff: 23.94mlTrain batch 8/32 - 103.0ms/batch - loss: 66.86420 - diff: 25.48mlTrain batch 9/32 - 104.5ms/batch - loss: 71.42079 - diff: 26.36mlTrain batch 10/32 - 102.5ms/batch - loss: 72.57280 - diff: 27.08mlTrain batch 11/32 - 111.4ms/batch - loss: 76.41294 - diff: 27.29mlTrain batch 12/32 - 103.5ms/batch - loss: 76.78652 - diff: 27.48mlTrain batch 13/32 - 111.8ms/batch - loss: 75.77853 - diff: 27.24mlTrain batch 14/32 - 94.4ms/batch - loss: 75.76325 - diff: 27.23mlTrain batch 15/32 - 125.0ms/batch - loss: 84.26057 - diff: 28.12mlTrain batch 16/32 - 111.8ms/batch - loss: 81.40700 - diff: 27.72mlTrain batch 17/32 - 118.2ms/batch - loss: 81.38074 - diff: 27.83mlTrain batch 18/32 - 92.7ms/batch - loss: 79.96938 - diff: 27.45mlTrain batch 19/32 - 122.7ms/batch - loss: 81.29229 - diff: 27.65mlTrain batch 20/32 - 115.0ms/batch - loss: 79.69138 - diff: 27.34mlTrain batch 21/32 - 108.8ms/batch - loss: 79.02209 - diff: 27.26mlTrain batch 22/32 - 106.0ms/batch - loss: 79.37749 - diff: 27.27mlTrain batch 23/32 - 109.6ms/batch - loss: 78.68589 - diff: 27.25mlTrain batch 24/32 - 107.4ms/batch - loss: 77.99174 - diff: 27.31mlTrain batch 25/32 - 111.3ms/batch - loss: 76.82523 - diff: 27.16mlTrain batch 26/32 - 106.6ms/batch - loss: 74.85004 - diff: 26.73mlTrain batch 27/32 - 110.9ms/batch - loss: 74.69934 - diff: 26.81mlTrain batch 28/32 - 107.6ms/batch - loss: 74.64906 - diff: 26.87mlTrain batch 29/32 - 109.1ms/batch - loss: 73.72584 - diff: 26.56mlTrain batch 30/32 - 106.3ms/batch - loss: 74.26311 - diff: 26.67mlTrain batch 31/32 - 90.8ms/batch - loss: 75.08461 - diff: 26.87mlTrain batch 32/32 - 79.1ms/batch - loss: 74.64011 - diff: 26.73mlTrain batch 32/32 - 10.6s 79.1ms/batch - loss: 74.64011 - diff: 26.73ml
Test 0.5s: val_loss: 82.08342 - diff: 27.17ml

Epoch 55: current best loss = 74.69561, at epoch 48
Train batch 1/32 - 132.6ms/batch - loss: 25.90267 - diff: 15.48mlTrain batch 2/32 - 104.5ms/batch - loss: 24.83432 - diff: 15.33mlTrain batch 3/32 - 103.6ms/batch - loss: 42.52053 - diff: 19.34mlTrain batch 4/32 - 102.8ms/batch - loss: 92.92895 - diff: 26.07mlTrain batch 5/32 - 103.7ms/batch - loss: 85.55660 - diff: 25.42mlTrain batch 6/32 - 103.0ms/batch - loss: 88.62788 - diff: 26.86mlTrain batch 7/32 - 103.3ms/batch - loss: 83.93793 - diff: 26.45mlTrain batch 8/32 - 102.3ms/batch - loss: 83.92181 - diff: 26.92mlTrain batch 9/32 - 102.7ms/batch - loss: 87.09799 - diff: 27.70mlTrain batch 10/32 - 103.7ms/batch - loss: 92.61308 - diff: 28.80mlTrain batch 11/32 - 105.1ms/batch - loss: 93.34847 - diff: 29.21mlTrain batch 12/32 - 103.7ms/batch - loss: 91.80121 - diff: 29.10mlTrain batch 13/32 - 103.5ms/batch - loss: 90.10881 - diff: 29.02mlTrain batch 14/32 - 106.7ms/batch - loss: 86.82403 - diff: 28.35mlTrain batch 15/32 - 101.4ms/batch - loss: 83.72879 - diff: 27.88mlTrain batch 16/32 - 96.8ms/batch - loss: 82.10387 - diff: 27.71mlTrain batch 17/32 - 108.3ms/batch - loss: 79.46321 - diff: 27.13mlTrain batch 18/32 - 94.7ms/batch - loss: 75.72460 - diff: 26.21mlTrain batch 19/32 - 117.9ms/batch - loss: 77.98632 - diff: 26.79mlTrain batch 20/32 - 108.4ms/batch - loss: 79.52813 - diff: 27.11mlTrain batch 21/32 - 108.4ms/batch - loss: 80.53043 - diff: 27.43mlTrain batch 22/32 - 108.3ms/batch - loss: 86.12814 - diff: 28.56mlTrain batch 23/32 - 120.8ms/batch - loss: 84.55623 - diff: 28.17mlTrain batch 24/32 - 124.5ms/batch - loss: 83.29516 - diff: 27.93mlTrain batch 25/32 - 108.9ms/batch - loss: 81.93746 - diff: 27.80mlTrain batch 26/32 - 109.7ms/batch - loss: 82.07309 - diff: 27.76mlTrain batch 27/32 - 108.3ms/batch - loss: 80.67589 - diff: 27.55mlTrain batch 28/32 - 106.7ms/batch - loss: 80.38470 - diff: 27.50mlTrain batch 29/32 - 106.6ms/batch - loss: 79.46056 - diff: 27.40mlTrain batch 30/32 - 105.8ms/batch - loss: 80.60764 - diff: 27.63mlTrain batch 31/32 - 102.2ms/batch - loss: 79.99232 - diff: 27.53mlTrain batch 32/32 - 105.8ms/batch - loss: 83.56234 - diff: 27.64mlTrain batch 32/32 - 10.9s 105.8ms/batch - loss: 83.56234 - diff: 27.64ml
Test 0.6s: val_loss: 55.99484 - diff: 23.16ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 56: current best loss = 55.99484, at epoch 55
Train batch 1/32 - 113.5ms/batch - loss: 40.88574 - diff: 19.86mlTrain batch 2/32 - 103.2ms/batch - loss: 50.25409 - diff: 22.13mlTrain batch 3/32 - 111.1ms/batch - loss: 53.52653 - diff: 22.34mlTrain batch 4/32 - 102.9ms/batch - loss: 57.77555 - diff: 23.17mlTrain batch 5/32 - 117.9ms/batch - loss: 60.95465 - diff: 24.41mlTrain batch 6/32 - 115.1ms/batch - loss: 64.33190 - diff: 24.65mlTrain batch 7/32 - 114.5ms/batch - loss: 64.01485 - diff: 23.94mlTrain batch 8/32 - 103.0ms/batch - loss: 65.25249 - diff: 24.52mlTrain batch 9/32 - 103.6ms/batch - loss: 64.95576 - diff: 24.78mlTrain batch 10/32 - 104.5ms/batch - loss: 63.34395 - diff: 24.74mlTrain batch 11/32 - 105.1ms/batch - loss: 65.55803 - diff: 25.15mlTrain batch 12/32 - 106.1ms/batch - loss: 64.71382 - diff: 25.09mlTrain batch 13/32 - 104.9ms/batch - loss: 65.96357 - diff: 25.35mlTrain batch 14/32 - 114.9ms/batch - loss: 70.28036 - diff: 26.42mlTrain batch 15/32 - 108.1ms/batch - loss: 67.71984 - diff: 25.96mlTrain batch 16/32 - 116.2ms/batch - loss: 66.93083 - diff: 25.79mlTrain batch 17/32 - 108.2ms/batch - loss: 65.09819 - diff: 25.44mlTrain batch 18/32 - 119.8ms/batch - loss: 63.87315 - diff: 25.09mlTrain batch 19/32 - 120.4ms/batch - loss: 64.29438 - diff: 25.15mlTrain batch 20/32 - 111.6ms/batch - loss: 63.54818 - diff: 24.97mlTrain batch 21/32 - 108.3ms/batch - loss: 62.34573 - diff: 24.65mlTrain batch 22/32 - 109.1ms/batch - loss: 60.71340 - diff: 24.36mlTrain batch 23/32 - 117.5ms/batch - loss: 60.61858 - diff: 24.14mlTrain batch 24/32 - 120.5ms/batch - loss: 60.53551 - diff: 24.14mlTrain batch 25/32 - 113.3ms/batch - loss: 60.09058 - diff: 23.94mlTrain batch 26/32 - 113.8ms/batch - loss: 59.48826 - diff: 23.81mlTrain batch 27/32 - 113.4ms/batch - loss: 59.04155 - diff: 23.76mlTrain batch 28/32 - 112.3ms/batch - loss: 58.21653 - diff: 23.60mlTrain batch 29/32 - 112.5ms/batch - loss: 58.71938 - diff: 23.81mlTrain batch 30/32 - 111.1ms/batch - loss: 60.25179 - diff: 24.20mlTrain batch 31/32 - 112.7ms/batch - loss: 59.10968 - diff: 23.99mlTrain batch 32/32 - 92.0ms/batch - loss: 61.04206 - diff: 24.07mlTrain batch 32/32 - 12.0s 92.0ms/batch - loss: 61.04206 - diff: 24.07ml
Test 0.6s: val_loss: 64.02131 - diff: 24.55ml

Epoch 57: current best loss = 55.99484, at epoch 55
Train batch 1/32 - 130.4ms/batch - loss: 63.80681 - diff: 26.79mlTrain batch 2/32 - 108.0ms/batch - loss: 52.75648 - diff: 24.81mlTrain batch 3/32 - 108.0ms/batch - loss: 47.41305 - diff: 22.89mlTrain batch 4/32 - 106.8ms/batch - loss: 44.34659 - diff: 21.89mlTrain batch 5/32 - 107.9ms/batch - loss: 51.05373 - diff: 23.15mlTrain batch 6/32 - 106.3ms/batch - loss: 54.04734 - diff: 24.12mlTrain batch 7/32 - 107.4ms/batch - loss: 65.69218 - diff: 25.69mlTrain batch 8/32 - 106.4ms/batch - loss: 60.98136 - diff: 24.54mlTrain batch 9/32 - 115.1ms/batch - loss: 58.70363 - diff: 24.40mlTrain batch 10/32 - 114.3ms/batch - loss: 59.24117 - diff: 24.19mlTrain batch 11/32 - 115.3ms/batch - loss: 61.45864 - diff: 24.59mlTrain batch 12/32 - 114.9ms/batch - loss: 61.71788 - diff: 24.54mlTrain batch 13/32 - 102.5ms/batch - loss: 63.23547 - diff: 24.92mlTrain batch 14/32 - 103.1ms/batch - loss: 61.21646 - diff: 24.38mlTrain batch 15/32 - 92.9ms/batch - loss: 59.82150 - diff: 24.24mlTrain batch 16/32 - 90.2ms/batch - loss: 58.92405 - diff: 23.98mlTrain batch 17/32 - 81.9ms/batch - loss: 58.20824 - diff: 23.92mlTrain batch 18/32 - 82.4ms/batch - loss: 59.09185 - diff: 24.24mlTrain batch 19/32 - 114.9ms/batch - loss: 58.44136 - diff: 24.16mlTrain batch 20/32 - 120.8ms/batch - loss: 59.46463 - diff: 24.48mlTrain batch 21/32 - 112.0ms/batch - loss: 60.81771 - diff: 24.83mlTrain batch 22/32 - 118.2ms/batch - loss: 63.20478 - diff: 25.18mlTrain batch 23/32 - 106.2ms/batch - loss: 62.28572 - diff: 25.07mlTrain batch 24/32 - 114.4ms/batch - loss: 62.26313 - diff: 24.98mlTrain batch 25/32 - 111.7ms/batch - loss: 62.91045 - diff: 25.25mlTrain batch 26/32 - 117.0ms/batch - loss: 64.27646 - diff: 25.51mlTrain batch 27/32 - 112.0ms/batch - loss: 65.69197 - diff: 25.79mlTrain batch 28/32 - 115.5ms/batch - loss: 67.59470 - diff: 26.14mlTrain batch 29/32 - 110.7ms/batch - loss: 66.81168 - diff: 25.89mlTrain batch 30/32 - 113.8ms/batch - loss: 66.40008 - diff: 25.82mlTrain batch 31/32 - 100.9ms/batch - loss: 65.37664 - diff: 25.55mlTrain batch 32/32 - 79.6ms/batch - loss: 69.11675 - diff: 25.66mlTrain batch 32/32 - 11.1s 79.6ms/batch - loss: 69.11675 - diff: 25.66ml
Test 0.6s: val_loss: 57.12056 - diff: 22.67ml

Epoch 58: current best loss = 55.99484, at epoch 55
Train batch 1/32 - 118.7ms/batch - loss: 46.11900 - diff: 18.35mlTrain batch 2/32 - 103.2ms/batch - loss: 58.04617 - diff: 23.90mlTrain batch 3/32 - 104.6ms/batch - loss: 76.51867 - diff: 26.62mlTrain batch 4/32 - 105.6ms/batch - loss: 74.59636 - diff: 26.88mlTrain batch 5/32 - 105.4ms/batch - loss: 74.29536 - diff: 26.96mlTrain batch 6/32 - 105.0ms/batch - loss: 98.50259 - diff: 30.41mlTrain batch 7/32 - 113.7ms/batch - loss: 89.80257 - diff: 28.99mlTrain batch 8/32 - 113.3ms/batch - loss: 83.41018 - diff: 27.99mlTrain batch 9/32 - 118.2ms/batch - loss: 80.52128 - diff: 27.72mlTrain batch 10/32 - 117.0ms/batch - loss: 77.17704 - diff: 27.12mlTrain batch 11/32 - 117.0ms/batch - loss: 77.86030 - diff: 26.84mlTrain batch 12/32 - 114.2ms/batch - loss: 76.77360 - diff: 26.65mlTrain batch 13/32 - 117.5ms/batch - loss: 109.81740 - diff: 27.82mlTrain batch 14/32 - 105.4ms/batch - loss: 107.41793 - diff: 27.74mlTrain batch 15/32 - 102.8ms/batch - loss: 105.18457 - diff: 27.60mlTrain batch 16/32 - 101.1ms/batch - loss: 102.60232 - diff: 27.44mlTrain batch 17/32 - 90.1ms/batch - loss: 101.64737 - diff: 27.65mlTrain batch 18/32 - 112.4ms/batch - loss: 111.78000 - diff: 28.81mlTrain batch 19/32 - 106.9ms/batch - loss: 108.86616 - diff: 28.36mlTrain batch 20/32 - 108.1ms/batch - loss: 107.96404 - diff: 28.31mlTrain batch 21/32 - 105.6ms/batch - loss: 107.28023 - diff: 28.48mlTrain batch 22/32 - 111.7ms/batch - loss: 104.29767 - diff: 28.12mlTrain batch 23/32 - 115.1ms/batch - loss: 103.96136 - diff: 28.15mlTrain batch 24/32 - 117.4ms/batch - loss: 103.13367 - diff: 28.28mlTrain batch 25/32 - 115.4ms/batch - loss: 102.23773 - diff: 28.35mlTrain batch 26/32 - 120.1ms/batch - loss: 102.03925 - diff: 28.35mlTrain batch 27/32 - 113.5ms/batch - loss: 99.90350 - diff: 28.05mlTrain batch 28/32 - 122.4ms/batch - loss: 98.47752 - diff: 27.96mlTrain batch 29/32 - 113.9ms/batch - loss: 97.03271 - diff: 27.78mlTrain batch 30/32 - 105.3ms/batch - loss: 95.70583 - diff: 27.62mlTrain batch 31/32 - 107.9ms/batch - loss: 94.32039 - diff: 27.49mlTrain batch 32/32 - 89.4ms/batch - loss: 94.92091 - diff: 27.43mlTrain batch 32/32 - 11.5s 89.4ms/batch - loss: 94.92091 - diff: 27.43ml
Test 0.5s: val_loss: 149.18707 - diff: 39.21ml

Epoch 59: current best loss = 55.99484, at epoch 55
Train batch 1/32 - 135.7ms/batch - loss: 63.25358 - diff: 25.83mlTrain batch 2/32 - 114.7ms/batch - loss: 93.21333 - diff: 30.24mlTrain batch 3/32 - 106.8ms/batch - loss: 81.25992 - diff: 28.52mlTrain batch 4/32 - 103.3ms/batch - loss: 79.41415 - diff: 28.45mlTrain batch 5/32 - 103.9ms/batch - loss: 78.15896 - diff: 28.39mlTrain batch 6/32 - 103.8ms/batch - loss: 108.40528 - diff: 30.71mlTrain batch 7/32 - 105.4ms/batch - loss: 96.56198 - diff: 28.59mlTrain batch 8/32 - 112.5ms/batch - loss: 97.23679 - diff: 29.05mlTrain batch 9/32 - 107.9ms/batch - loss: 92.08303 - diff: 28.12mlTrain batch 10/32 - 105.2ms/batch - loss: 88.79980 - diff: 28.02mlTrain batch 11/32 - 104.1ms/batch - loss: 82.75656 - diff: 26.81mlTrain batch 12/32 - 112.7ms/batch - loss: 78.65886 - diff: 26.31mlTrain batch 13/32 - 111.9ms/batch - loss: 82.99895 - diff: 27.14mlTrain batch 14/32 - 118.1ms/batch - loss: 85.22252 - diff: 27.68mlTrain batch 15/32 - 119.9ms/batch - loss: 81.45303 - diff: 27.02mlTrain batch 16/32 - 118.3ms/batch - loss: 79.85223 - diff: 26.99mlTrain batch 17/32 - 117.5ms/batch - loss: 79.73811 - diff: 26.95mlTrain batch 18/32 - 113.3ms/batch - loss: 78.89186 - diff: 26.83mlTrain batch 19/32 - 121.3ms/batch - loss: 77.85920 - diff: 26.68mlTrain batch 20/32 - 115.1ms/batch - loss: 78.02751 - diff: 26.80mlTrain batch 21/32 - 128.3ms/batch - loss: 77.86780 - diff: 26.94mlTrain batch 22/32 - 116.6ms/batch - loss: 76.58788 - diff: 26.69mlTrain batch 23/32 - 107.0ms/batch - loss: 76.56744 - diff: 26.67mlTrain batch 24/32 - 102.7ms/batch - loss: 78.61752 - diff: 26.88mlTrain batch 25/32 - 110.5ms/batch - loss: 76.63292 - diff: 26.56mlTrain batch 26/32 - 117.6ms/batch - loss: 78.22553 - diff: 27.06mlTrain batch 27/32 - 112.4ms/batch - loss: 76.51442 - diff: 26.76mlTrain batch 28/32 - 112.0ms/batch - loss: 75.08726 - diff: 26.50mlTrain batch 29/32 - 117.3ms/batch - loss: 75.72468 - diff: 26.57mlTrain batch 30/32 - 114.0ms/batch - loss: 74.84410 - diff: 26.42mlTrain batch 31/32 - 116.0ms/batch - loss: 73.76027 - diff: 26.17mlTrain batch 32/32 - 104.6ms/batch - loss: 85.92336 - diff: 26.49mlTrain batch 32/32 - 11.8s 104.6ms/batch - loss: 85.92336 - diff: 26.49ml
Test 0.6s: val_loss: 86.44163 - diff: 28.23ml

Epoch 60: current best loss = 55.99484, at epoch 55
Train batch 1/32 - 127.4ms/batch - loss: 37.34764 - diff: 18.09mlTrain batch 2/32 - 106.9ms/batch - loss: 52.45391 - diff: 22.36mlTrain batch 3/32 - 100.9ms/batch - loss: 48.88492 - diff: 21.69mlTrain batch 4/32 - 113.4ms/batch - loss: 45.63547 - diff: 21.23mlTrain batch 5/32 - 110.7ms/batch - loss: 44.85782 - diff: 21.26mlTrain batch 6/32 - 103.1ms/batch - loss: 44.72172 - diff: 21.06mlTrain batch 7/32 - 104.4ms/batch - loss: 54.50726 - diff: 23.24mlTrain batch 8/32 - 104.2ms/batch - loss: 56.37132 - diff: 23.54mlTrain batch 9/32 - 103.2ms/batch - loss: 53.42144 - diff: 22.57mlTrain batch 10/32 - 104.7ms/batch - loss: 57.30915 - diff: 23.17mlTrain batch 11/32 - 104.4ms/batch - loss: 61.77099 - diff: 23.61mlTrain batch 12/32 - 104.4ms/batch - loss: 62.27677 - diff: 23.81mlTrain batch 13/32 - 100.6ms/batch - loss: 68.11286 - diff: 24.82mlTrain batch 14/32 - 113.8ms/batch - loss: 68.57363 - diff: 25.04mlTrain batch 15/32 - 106.5ms/batch - loss: 66.69058 - diff: 24.70mlTrain batch 16/32 - 107.5ms/batch - loss: 66.18810 - diff: 24.75mlTrain batch 17/32 - 107.2ms/batch - loss: 66.40448 - diff: 24.83mlTrain batch 18/32 - 109.6ms/batch - loss: 64.55903 - diff: 24.44mlTrain batch 19/32 - 115.8ms/batch - loss: 63.57032 - diff: 24.35mlTrain batch 20/32 - 117.2ms/batch - loss: 65.48948 - diff: 24.80mlTrain batch 21/32 - 107.5ms/batch - loss: 64.67339 - diff: 24.66mlTrain batch 22/32 - 106.6ms/batch - loss: 69.64797 - diff: 25.36mlTrain batch 23/32 - 106.7ms/batch - loss: 69.01611 - diff: 25.27mlTrain batch 24/32 - 107.5ms/batch - loss: 69.18600 - diff: 25.39mlTrain batch 25/32 - 106.4ms/batch - loss: 70.13081 - diff: 25.75mlTrain batch 26/32 - 107.7ms/batch - loss: 76.02712 - diff: 25.87mlTrain batch 27/32 - 107.3ms/batch - loss: 79.77293 - diff: 26.52mlTrain batch 28/32 - 106.4ms/batch - loss: 78.03847 - diff: 26.25mlTrain batch 29/32 - 96.0ms/batch - loss: 77.46682 - diff: 26.21mlTrain batch 30/32 - 92.2ms/batch - loss: 77.46386 - diff: 26.26mlTrain batch 31/32 - 115.1ms/batch - loss: 76.47029 - diff: 26.21mlTrain batch 32/32 - 110.2ms/batch - loss: 77.15444 - diff: 26.17mlTrain batch 32/32 - 11.5s 110.2ms/batch - loss: 77.15444 - diff: 26.17ml
Test 0.7s: val_loss: 86.47320 - diff: 28.18ml

Epoch 61: current best loss = 55.99484, at epoch 55
Train batch 1/32 - 127.2ms/batch - loss: 41.04446 - diff: 18.85mlTrain batch 2/32 - 113.8ms/batch - loss: 51.45865 - diff: 22.87mlTrain batch 3/32 - 113.6ms/batch - loss: 43.23341 - diff: 20.75mlTrain batch 4/32 - 118.0ms/batch - loss: 41.40584 - diff: 20.36mlTrain batch 5/32 - 116.0ms/batch - loss: 40.17224 - diff: 20.22mlTrain batch 6/32 - 117.2ms/batch - loss: 37.61348 - diff: 19.48mlTrain batch 7/32 - 115.8ms/batch - loss: 52.35953 - diff: 21.05mlTrain batch 8/32 - 115.2ms/batch - loss: 55.95559 - diff: 22.32mlTrain batch 9/32 - 113.5ms/batch - loss: 54.69759 - diff: 22.38mlTrain batch 10/32 - 102.4ms/batch - loss: 52.58134 - diff: 21.70mlTrain batch 11/32 - 103.9ms/batch - loss: 56.74551 - diff: 22.53mlTrain batch 12/32 - 100.3ms/batch - loss: 79.43560 - diff: 26.44mlTrain batch 13/32 - 110.9ms/batch - loss: 78.99439 - diff: 26.64mlTrain batch 14/32 - 105.4ms/batch - loss: 78.41691 - diff: 26.71mlTrain batch 15/32 - 106.4ms/batch - loss: 77.04749 - diff: 26.53mlTrain batch 16/32 - 106.5ms/batch - loss: 76.92114 - diff: 26.75mlTrain batch 17/32 - 105.9ms/batch - loss: 76.98131 - diff: 26.86mlTrain batch 18/32 - 106.5ms/batch - loss: 77.86269 - diff: 26.97mlTrain batch 19/32 - 106.2ms/batch - loss: 76.77305 - diff: 26.81mlTrain batch 20/32 - 103.3ms/batch - loss: 74.44038 - diff: 26.26mlTrain batch 21/32 - 103.1ms/batch - loss: 73.00935 - diff: 26.03mlTrain batch 22/32 - 103.6ms/batch - loss: 72.13467 - diff: 26.06mlTrain batch 23/32 - 104.1ms/batch - loss: 74.46456 - diff: 26.67mlTrain batch 24/32 - 104.4ms/batch - loss: 74.22307 - diff: 26.70mlTrain batch 25/32 - 103.2ms/batch - loss: 72.37417 - diff: 26.33mlTrain batch 26/32 - 103.9ms/batch - loss: 71.66312 - diff: 26.24mlTrain batch 27/32 - 103.3ms/batch - loss: 71.70535 - diff: 26.30mlTrain batch 28/32 - 102.8ms/batch - loss: 70.61550 - diff: 26.07mlTrain batch 29/32 - 104.0ms/batch - loss: 71.67011 - diff: 26.13mlTrain batch 30/32 - 103.0ms/batch - loss: 71.32340 - diff: 26.07mlTrain batch 31/32 - 103.6ms/batch - loss: 70.95119 - diff: 26.14mlTrain batch 32/32 - 92.6ms/batch - loss: 80.02700 - diff: 26.48mlTrain batch 32/32 - 11.1s 92.6ms/batch - loss: 80.02700 - diff: 26.48ml
Test 0.6s: val_loss: 76.97010 - diff: 25.89ml

Epoch 62: current best loss = 55.99484, at epoch 55
Train batch 1/32 - 120.3ms/batch - loss: 31.65691 - diff: 15.36mlTrain batch 2/32 - 109.3ms/batch - loss: 41.24043 - diff: 19.09mlTrain batch 3/32 - 113.6ms/batch - loss: 41.67627 - diff: 18.67mlTrain batch 4/32 - 102.9ms/batch - loss: 40.22560 - diff: 19.04mlTrain batch 5/32 - 105.0ms/batch - loss: 41.27222 - diff: 19.94mlTrain batch 6/32 - 102.5ms/batch - loss: 44.16348 - diff: 20.93mlTrain batch 7/32 - 108.7ms/batch - loss: 43.53412 - diff: 20.77mlTrain batch 8/32 - 113.9ms/batch - loss: 45.52252 - diff: 21.21mlTrain batch 9/32 - 106.6ms/batch - loss: 43.88986 - diff: 20.84mlTrain batch 10/32 - 106.4ms/batch - loss: 43.07189 - diff: 20.50mlTrain batch 11/32 - 89.2ms/batch - loss: 52.65708 - diff: 21.98mlTrain batch 12/32 - 81.3ms/batch - loss: 55.35122 - diff: 22.95mlTrain batch 13/32 - 118.1ms/batch - loss: 55.18626 - diff: 22.90mlTrain batch 14/32 - 114.3ms/batch - loss: 60.58422 - diff: 23.61mlTrain batch 15/32 - 122.4ms/batch - loss: 60.00562 - diff: 23.48mlTrain batch 16/32 - 113.2ms/batch - loss: 61.94506 - diff: 23.98mlTrain batch 17/32 - 102.9ms/batch - loss: 61.32788 - diff: 23.92mlTrain batch 18/32 - 102.6ms/batch - loss: 60.32635 - diff: 23.90mlTrain batch 19/32 - 105.7ms/batch - loss: 60.20753 - diff: 24.02mlTrain batch 20/32 - 105.8ms/batch - loss: 64.05952 - diff: 24.65mlTrain batch 21/32 - 107.0ms/batch - loss: 65.44268 - diff: 24.70mlTrain batch 22/32 - 107.9ms/batch - loss: 63.97349 - diff: 24.40mlTrain batch 23/32 - 106.5ms/batch - loss: 63.78733 - diff: 24.27mlTrain batch 24/32 - 106.1ms/batch - loss: 63.48158 - diff: 24.26mlTrain batch 25/32 - 106.6ms/batch - loss: 63.46684 - diff: 24.33mlTrain batch 26/32 - 105.7ms/batch - loss: 62.19528 - diff: 24.14mlTrain batch 27/32 - 102.6ms/batch - loss: 62.76609 - diff: 24.10mlTrain batch 28/32 - 103.3ms/batch - loss: 63.08512 - diff: 24.21mlTrain batch 29/32 - 97.3ms/batch - loss: 62.61601 - diff: 24.12mlTrain batch 30/32 - 87.3ms/batch - loss: 62.09519 - diff: 24.05mlTrain batch 31/32 - 101.9ms/batch - loss: 63.17102 - diff: 24.28mlTrain batch 32/32 - 80.3ms/batch - loss: 63.97877 - diff: 24.25mlTrain batch 32/32 - 11.1s 80.3ms/batch - loss: 63.97877 - diff: 24.25ml
Test 0.6s: val_loss: 62.56818 - diff: 23.55ml

Epoch 63: current best loss = 55.99484, at epoch 55
Train batch 1/32 - 119.3ms/batch - loss: 40.17008 - diff: 21.93mlTrain batch 2/32 - 104.7ms/batch - loss: 37.92760 - diff: 20.85mlTrain batch 3/32 - 106.0ms/batch - loss: 37.21251 - diff: 19.62mlTrain batch 4/32 - 104.0ms/batch - loss: 34.03701 - diff: 18.42mlTrain batch 5/32 - 109.1ms/batch - loss: 37.59841 - diff: 19.08mlTrain batch 6/32 - 102.4ms/batch - loss: 36.16498 - diff: 18.76mlTrain batch 7/32 - 106.6ms/batch - loss: 33.21468 - diff: 17.85mlTrain batch 8/32 - 103.7ms/batch - loss: 34.73343 - diff: 18.56mlTrain batch 9/32 - 103.7ms/batch - loss: 35.63785 - diff: 18.68mlTrain batch 10/32 - 103.0ms/batch - loss: 57.20235 - diff: 20.62mlTrain batch 11/32 - 103.7ms/batch - loss: 54.85166 - diff: 20.30mlTrain batch 12/32 - 103.1ms/batch - loss: 54.81174 - diff: 20.56mlTrain batch 13/32 - 107.9ms/batch - loss: 53.72359 - diff: 20.74mlTrain batch 14/32 - 105.7ms/batch - loss: 51.94485 - diff: 20.54mlTrain batch 15/32 - 107.4ms/batch - loss: 52.34278 - diff: 20.80mlTrain batch 16/32 - 105.3ms/batch - loss: 51.41712 - diff: 20.76mlTrain batch 17/32 - 103.5ms/batch - loss: 51.47667 - diff: 21.00mlTrain batch 18/32 - 90.9ms/batch - loss: 49.55065 - diff: 20.58mlTrain batch 19/32 - 106.2ms/batch - loss: 52.62665 - diff: 21.15mlTrain batch 20/32 - 106.3ms/batch - loss: 53.76052 - diff: 21.61mlTrain batch 21/32 - 106.7ms/batch - loss: 52.47412 - diff: 21.27mlTrain batch 22/32 - 106.6ms/batch - loss: 51.80109 - diff: 21.34mlTrain batch 23/32 - 103.8ms/batch - loss: 51.47475 - diff: 21.33mlTrain batch 24/32 - 102.5ms/batch - loss: 52.66054 - diff: 21.60mlTrain batch 25/32 - 103.2ms/batch - loss: 52.38924 - diff: 21.59mlTrain batch 26/32 - 103.7ms/batch - loss: 51.69933 - diff: 21.52mlTrain batch 27/32 - 103.3ms/batch - loss: 53.87045 - diff: 21.96mlTrain batch 28/32 - 102.6ms/batch - loss: 53.09667 - diff: 21.73mlTrain batch 29/32 - 105.8ms/batch - loss: 52.53331 - diff: 21.61mlTrain batch 30/32 - 98.0ms/batch - loss: 52.25991 - diff: 21.59mlTrain batch 31/32 - 95.1ms/batch - loss: 51.88799 - diff: 21.55mlTrain batch 32/32 - 87.1ms/batch - loss: 54.44079 - diff: 21.62mlTrain batch 32/32 - 10.8s 87.1ms/batch - loss: 54.44079 - diff: 21.62ml
Test 0.6s: val_loss: 71.68952 - diff: 26.30ml

Epoch 64: current best loss = 55.99484, at epoch 55
Train batch 1/32 - 119.2ms/batch - loss: 67.75839 - diff: 29.15mlTrain batch 2/32 - 104.1ms/batch - loss: 54.86275 - diff: 22.57mlTrain batch 3/32 - 104.1ms/batch - loss: 46.40158 - diff: 20.41mlTrain batch 4/32 - 103.5ms/batch - loss: 47.27510 - diff: 21.33mlTrain batch 5/32 - 115.3ms/batch - loss: 49.41330 - diff: 21.62mlTrain batch 6/32 - 106.7ms/batch - loss: 53.69359 - diff: 22.58mlTrain batch 7/32 - 93.3ms/batch - loss: 53.37889 - diff: 23.05mlTrain batch 8/32 - 107.5ms/batch - loss: 52.99172 - diff: 23.17mlTrain batch 9/32 - 108.3ms/batch - loss: 51.25558 - diff: 22.40mlTrain batch 10/32 - 106.6ms/batch - loss: 53.31169 - diff: 22.76mlTrain batch 11/32 - 116.0ms/batch - loss: 55.40500 - diff: 23.06mlTrain batch 12/32 - 115.7ms/batch - loss: 61.46591 - diff: 23.65mlTrain batch 13/32 - 120.0ms/batch - loss: 60.46327 - diff: 23.43mlTrain batch 14/32 - 97.6ms/batch - loss: 59.41612 - diff: 23.33mlTrain batch 15/32 - 104.3ms/batch - loss: 57.45425 - diff: 23.04mlTrain batch 16/32 - 102.3ms/batch - loss: 56.68203 - diff: 22.96mlTrain batch 17/32 - 103.6ms/batch - loss: 55.94199 - diff: 22.83mlTrain batch 18/32 - 103.4ms/batch - loss: 56.98794 - diff: 23.10mlTrain batch 19/32 - 103.1ms/batch - loss: 60.69470 - diff: 23.65mlTrain batch 20/32 - 102.8ms/batch - loss: 60.06925 - diff: 23.71mlTrain batch 21/32 - 111.7ms/batch - loss: 58.77435 - diff: 23.47mlTrain batch 22/32 - 112.7ms/batch - loss: 58.57056 - diff: 23.48mlTrain batch 23/32 - 120.5ms/batch - loss: 66.42198 - diff: 24.00mlTrain batch 24/32 - 112.8ms/batch - loss: 65.95221 - diff: 23.97mlTrain batch 25/32 - 111.9ms/batch - loss: 65.96999 - diff: 24.04mlTrain batch 26/32 - 112.8ms/batch - loss: 64.38349 - diff: 23.76mlTrain batch 27/32 - 109.5ms/batch - loss: 64.89153 - diff: 23.85mlTrain batch 28/32 - 117.8ms/batch - loss: 65.17064 - diff: 23.89mlTrain batch 29/32 - 115.8ms/batch - loss: 64.17799 - diff: 23.65mlTrain batch 30/32 - 124.1ms/batch - loss: 64.12575 - diff: 23.69mlTrain batch 31/32 - 117.7ms/batch - loss: 64.77377 - diff: 23.93mlTrain batch 32/32 - 103.1ms/batch - loss: 65.89832 - diff: 23.93mlTrain batch 32/32 - 11.6s 103.1ms/batch - loss: 65.89832 - diff: 23.93ml
Test 0.6s: val_loss: 63.14260 - diff: 23.76ml

Epoch 65: current best loss = 55.99484, at epoch 55
Train batch 1/32 - 124.5ms/batch - loss: 55.59812 - diff: 24.59mlTrain batch 2/32 - 109.0ms/batch - loss: 38.83869 - diff: 20.12mlTrain batch 3/32 - 109.4ms/batch - loss: 54.99843 - diff: 23.62mlTrain batch 4/32 - 110.9ms/batch - loss: 54.98282 - diff: 23.76mlTrain batch 5/32 - 108.1ms/batch - loss: 54.38639 - diff: 23.94mlTrain batch 6/32 - 105.6ms/batch - loss: 53.36306 - diff: 23.57mlTrain batch 7/32 - 105.3ms/batch - loss: 53.79339 - diff: 23.74mlTrain batch 8/32 - 107.8ms/batch - loss: 52.68535 - diff: 23.61mlTrain batch 9/32 - 104.0ms/batch - loss: 54.50189 - diff: 23.85mlTrain batch 10/32 - 108.2ms/batch - loss: 52.45036 - diff: 23.11mlTrain batch 11/32 - 107.3ms/batch - loss: 54.82099 - diff: 23.72mlTrain batch 12/32 - 104.8ms/batch - loss: 54.23369 - diff: 23.38mlTrain batch 13/32 - 105.2ms/batch - loss: 52.26191 - diff: 22.86mlTrain batch 14/32 - 88.2ms/batch - loss: 51.31222 - diff: 22.59mlTrain batch 15/32 - 122.4ms/batch - loss: 52.05117 - diff: 22.76mlTrain batch 16/32 - 105.4ms/batch - loss: 52.99284 - diff: 22.94mlTrain batch 17/32 - 110.0ms/batch - loss: 51.59506 - diff: 22.65mlTrain batch 18/32 - 107.7ms/batch - loss: 52.40556 - diff: 22.78mlTrain batch 19/32 - 107.3ms/batch - loss: 53.95451 - diff: 22.92mlTrain batch 20/32 - 106.4ms/batch - loss: 52.37342 - diff: 22.59mlTrain batch 21/32 - 107.5ms/batch - loss: 52.98394 - diff: 22.82mlTrain batch 22/32 - 108.7ms/batch - loss: 52.23177 - diff: 22.73mlTrain batch 23/32 - 110.9ms/batch - loss: 52.34919 - diff: 22.74mlTrain batch 24/32 - 109.3ms/batch - loss: 52.97820 - diff: 22.84mlTrain batch 25/32 - 107.5ms/batch - loss: 53.69614 - diff: 22.99mlTrain batch 26/32 - 116.0ms/batch - loss: 56.87466 - diff: 23.51mlTrain batch 27/32 - 103.2ms/batch - loss: 57.03225 - diff: 23.53mlTrain batch 28/32 - 106.3ms/batch - loss: 59.44948 - diff: 23.81mlTrain batch 29/32 - 94.1ms/batch - loss: 59.99776 - diff: 23.88mlTrain batch 30/32 - 100.6ms/batch - loss: 59.47290 - diff: 23.79mlTrain batch 31/32 - 101.5ms/batch - loss: 60.09154 - diff: 23.92mlTrain batch 32/32 - 87.8ms/batch - loss: 60.55454 - diff: 23.89mlTrain batch 32/32 - 11.4s 87.8ms/batch - loss: 60.55454 - diff: 23.89ml
Test 0.6s: val_loss: 111.16883 - diff: 32.98ml

Epoch 66: current best loss = 55.99484, at epoch 55
Train batch 1/32 - 116.7ms/batch - loss: 55.95901 - diff: 26.21mlTrain batch 2/32 - 103.1ms/batch - loss: 50.80371 - diff: 25.05mlTrain batch 3/32 - 103.8ms/batch - loss: 60.71944 - diff: 27.06mlTrain batch 4/32 - 105.3ms/batch - loss: 59.43704 - diff: 26.25mlTrain batch 5/32 - 103.2ms/batch - loss: 55.10260 - diff: 24.69mlTrain batch 6/32 - 104.9ms/batch - loss: 56.00320 - diff: 24.67mlTrain batch 7/32 - 102.9ms/batch - loss: 61.66894 - diff: 26.01mlTrain batch 8/32 - 104.7ms/batch - loss: 63.09082 - diff: 25.78mlTrain batch 9/32 - 104.0ms/batch - loss: 60.32602 - diff: 24.96mlTrain batch 10/32 - 104.7ms/batch - loss: 62.64847 - diff: 25.07mlTrain batch 11/32 - 102.8ms/batch - loss: 60.96663 - diff: 24.82mlTrain batch 12/32 - 82.0ms/batch - loss: 57.69947 - diff: 23.94mlTrain batch 13/32 - 110.8ms/batch - loss: 57.40150 - diff: 23.97mlTrain batch 14/32 - 106.4ms/batch - loss: 58.36411 - diff: 24.39mlTrain batch 15/32 - 106.4ms/batch - loss: 59.65873 - diff: 24.76mlTrain batch 16/32 - 106.9ms/batch - loss: 58.54421 - diff: 24.51mlTrain batch 17/32 - 115.4ms/batch - loss: 58.70531 - diff: 24.60mlTrain batch 18/32 - 114.9ms/batch - loss: 60.91039 - diff: 25.26mlTrain batch 19/32 - 116.7ms/batch - loss: 59.11558 - diff: 24.77mlTrain batch 20/32 - 115.3ms/batch - loss: 61.77420 - diff: 25.29mlTrain batch 21/32 - 115.4ms/batch - loss: 68.07860 - diff: 26.48mlTrain batch 22/32 - 115.0ms/batch - loss: 67.49505 - diff: 26.40mlTrain batch 23/32 - 115.1ms/batch - loss: 66.59041 - diff: 26.12mlTrain batch 24/32 - 115.1ms/batch - loss: 67.47266 - diff: 26.12mlTrain batch 25/32 - 115.1ms/batch - loss: 67.35398 - diff: 26.06mlTrain batch 26/32 - 115.1ms/batch - loss: 67.64198 - diff: 26.13mlTrain batch 27/32 - 106.6ms/batch - loss: 66.49988 - diff: 25.70mlTrain batch 28/32 - 106.4ms/batch - loss: 65.42702 - diff: 25.53mlTrain batch 29/32 - 108.3ms/batch - loss: 64.92945 - diff: 25.39mlTrain batch 30/32 - 107.2ms/batch - loss: 64.40440 - diff: 25.24mlTrain batch 31/32 - 107.6ms/batch - loss: 64.21995 - diff: 25.22mlTrain batch 32/32 - 102.8ms/batch - loss: 64.05302 - diff: 25.11mlTrain batch 32/32 - 10.8s 102.8ms/batch - loss: 64.05302 - diff: 25.11ml
Test 0.6s: val_loss: 56.93024 - diff: 22.34ml
Epoch    67: reducing learning rate of group 0 to 5.0000e-04.

Epoch 67: current best loss = 55.99484, at epoch 55
Train batch 1/32 - 127.5ms/batch - loss: 23.54851 - diff: 16.18mlTrain batch 2/32 - 107.4ms/batch - loss: 24.13167 - diff: 16.16mlTrain batch 3/32 - 114.7ms/batch - loss: 27.66442 - diff: 17.37mlTrain batch 4/32 - 114.0ms/batch - loss: 28.40988 - diff: 17.70mlTrain batch 5/32 - 103.5ms/batch - loss: 29.97847 - diff: 17.89mlTrain batch 6/32 - 103.1ms/batch - loss: 28.51426 - diff: 17.12mlTrain batch 7/32 - 102.8ms/batch - loss: 34.40794 - diff: 18.45mlTrain batch 8/32 - 103.8ms/batch - loss: 33.24229 - diff: 17.96mlTrain batch 9/32 - 114.0ms/batch - loss: 33.36757 - diff: 18.28mlTrain batch 10/32 - 113.7ms/batch - loss: 35.56941 - diff: 19.03mlTrain batch 11/32 - 103.3ms/batch - loss: 37.98840 - diff: 19.54mlTrain batch 12/32 - 103.4ms/batch - loss: 47.36297 - diff: 20.62mlTrain batch 13/32 - 103.5ms/batch - loss: 46.65389 - diff: 20.65mlTrain batch 14/32 - 105.0ms/batch - loss: 47.83612 - diff: 21.23mlTrain batch 15/32 - 113.4ms/batch - loss: 48.25744 - diff: 21.40mlTrain batch 16/32 - 116.3ms/batch - loss: 47.06606 - diff: 21.24mlTrain batch 17/32 - 116.8ms/batch - loss: 47.96021 - diff: 21.35mlTrain batch 18/32 - 116.9ms/batch - loss: 46.47727 - diff: 21.02mlTrain batch 19/32 - 117.6ms/batch - loss: 47.50905 - diff: 21.22mlTrain batch 20/32 - 122.7ms/batch - loss: 49.13993 - diff: 21.44mlTrain batch 21/32 - 117.3ms/batch - loss: 50.87709 - diff: 22.03mlTrain batch 22/32 - 123.1ms/batch - loss: 51.23890 - diff: 22.19mlTrain batch 23/32 - 117.0ms/batch - loss: 50.93001 - diff: 22.12mlTrain batch 24/32 - 120.3ms/batch - loss: 49.57532 - diff: 21.82mlTrain batch 25/32 - 116.9ms/batch - loss: 50.44865 - diff: 22.13mlTrain batch 26/32 - 114.0ms/batch - loss: 50.89657 - diff: 22.36mlTrain batch 27/32 - 117.3ms/batch - loss: 51.40851 - diff: 22.55mlTrain batch 28/32 - 114.2ms/batch - loss: 50.52894 - diff: 22.39mlTrain batch 29/32 - 106.9ms/batch - loss: 50.38682 - diff: 22.40mlTrain batch 30/32 - 117.8ms/batch - loss: 49.63776 - diff: 22.19mlTrain batch 31/32 - 92.5ms/batch - loss: 49.22553 - diff: 22.11mlTrain batch 32/32 - 81.0ms/batch - loss: 50.73848 - diff: 22.15mlTrain batch 32/32 - 11.2s 81.0ms/batch - loss: 50.73848 - diff: 22.15ml
Test 0.5s: val_loss: 58.48187 - diff: 23.12ml

Epoch 68: current best loss = 55.99484, at epoch 55
Train batch 1/32 - 119.3ms/batch - loss: 24.02062 - diff: 15.87mlTrain batch 2/32 - 106.8ms/batch - loss: 27.09790 - diff: 16.00mlTrain batch 3/32 - 107.6ms/batch - loss: 30.30437 - diff: 17.40mlTrain batch 4/32 - 106.3ms/batch - loss: 30.86569 - diff: 17.74mlTrain batch 5/32 - 106.1ms/batch - loss: 31.24812 - diff: 17.70mlTrain batch 6/32 - 110.1ms/batch - loss: 36.84813 - diff: 19.38mlTrain batch 7/32 - 109.8ms/batch - loss: 36.23441 - diff: 19.17mlTrain batch 8/32 - 116.6ms/batch - loss: 35.61973 - diff: 19.05mlTrain batch 9/32 - 121.2ms/batch - loss: 34.44201 - diff: 18.59mlTrain batch 10/32 - 93.1ms/batch - loss: 34.47374 - diff: 18.43mlTrain batch 11/32 - 99.6ms/batch - loss: 33.56479 - diff: 18.27mlTrain batch 12/32 - 106.1ms/batch - loss: 34.14081 - diff: 18.57mlTrain batch 13/32 - 109.7ms/batch - loss: 33.55995 - diff: 18.50mlTrain batch 14/32 - 120.0ms/batch - loss: 34.39125 - diff: 18.46mlTrain batch 15/32 - 112.3ms/batch - loss: 34.37080 - diff: 18.46mlTrain batch 16/32 - 106.3ms/batch - loss: 34.51329 - diff: 18.35mlTrain batch 17/32 - 106.7ms/batch - loss: 34.97014 - diff: 18.47mlTrain batch 18/32 - 110.6ms/batch - loss: 40.81630 - diff: 19.25mlTrain batch 19/32 - 108.4ms/batch - loss: 39.25065 - diff: 18.81mlTrain batch 20/32 - 110.9ms/batch - loss: 39.83557 - diff: 18.89mlTrain batch 21/32 - 108.0ms/batch - loss: 44.70902 - diff: 19.83mlTrain batch 22/32 - 108.7ms/batch - loss: 43.55739 - diff: 19.63mlTrain batch 23/32 - 107.8ms/batch - loss: 43.06069 - diff: 19.46mlTrain batch 24/32 - 111.8ms/batch - loss: 42.77682 - diff: 19.40mlTrain batch 25/32 - 106.6ms/batch - loss: 42.20656 - diff: 19.37mlTrain batch 26/32 - 114.0ms/batch - loss: 42.36270 - diff: 19.47mlTrain batch 27/32 - 102.7ms/batch - loss: 41.50661 - diff: 19.25mlTrain batch 28/32 - 111.8ms/batch - loss: 41.48986 - diff: 19.22mlTrain batch 29/32 - 115.1ms/batch - loss: 40.62442 - diff: 19.00mlTrain batch 30/32 - 113.6ms/batch - loss: 42.11997 - diff: 19.46mlTrain batch 31/32 - 115.9ms/batch - loss: 42.08446 - diff: 19.53mlTrain batch 32/32 - 99.2ms/batch - loss: 42.20148 - diff: 19.48mlTrain batch 32/32 - 11.5s 99.2ms/batch - loss: 42.20148 - diff: 19.48ml
Test 0.6s: val_loss: 63.25752 - diff: 23.90ml

Epoch 69: current best loss = 55.99484, at epoch 55
Train batch 1/32 - 121.8ms/batch - loss: 41.79496 - diff: 20.66mlTrain batch 2/32 - 114.4ms/batch - loss: 44.48591 - diff: 21.66mlTrain batch 3/32 - 120.3ms/batch - loss: 36.11770 - diff: 19.15mlTrain batch 4/32 - 113.8ms/batch - loss: 31.64780 - diff: 17.87mlTrain batch 5/32 - 114.2ms/batch - loss: 40.73913 - diff: 20.38mlTrain batch 6/32 - 113.7ms/batch - loss: 40.99357 - diff: 20.14mlTrain batch 7/32 - 113.0ms/batch - loss: 46.50760 - diff: 20.88mlTrain batch 8/32 - 113.7ms/batch - loss: 43.61169 - diff: 20.19mlTrain batch 9/32 - 113.5ms/batch - loss: 40.41915 - diff: 19.27mlTrain batch 10/32 - 119.0ms/batch - loss: 40.42198 - diff: 18.83mlTrain batch 11/32 - 118.1ms/batch - loss: 38.37695 - diff: 18.34mlTrain batch 12/32 - 113.3ms/batch - loss: 40.53483 - diff: 19.06mlTrain batch 13/32 - 118.5ms/batch - loss: 39.48382 - diff: 18.84mlTrain batch 14/32 - 111.1ms/batch - loss: 38.21962 - diff: 18.64mlTrain batch 15/32 - 121.7ms/batch - loss: 36.99616 - diff: 18.23mlTrain batch 16/32 - 110.8ms/batch - loss: 38.64059 - diff: 18.62mlTrain batch 17/32 - 105.4ms/batch - loss: 37.95524 - diff: 18.52mlTrain batch 18/32 - 115.3ms/batch - loss: 37.17116 - diff: 18.40mlTrain batch 19/32 - 115.0ms/batch - loss: 37.93130 - diff: 18.80mlTrain batch 20/32 - 117.7ms/batch - loss: 38.57420 - diff: 19.05mlTrain batch 21/32 - 115.9ms/batch - loss: 37.70583 - diff: 18.86mlTrain batch 22/32 - 116.2ms/batch - loss: 36.66035 - diff: 18.53mlTrain batch 23/32 - 116.0ms/batch - loss: 36.60365 - diff: 18.50mlTrain batch 24/32 - 115.7ms/batch - loss: 36.31261 - diff: 18.45mlTrain batch 25/32 - 116.2ms/batch - loss: 35.61436 - diff: 18.27mlTrain batch 26/32 - 107.8ms/batch - loss: 34.71957 - diff: 18.01mlTrain batch 27/32 - 114.8ms/batch - loss: 37.53468 - diff: 18.57mlTrain batch 28/32 - 104.9ms/batch - loss: 36.95453 - diff: 18.46mlTrain batch 29/32 - 108.4ms/batch - loss: 37.19485 - diff: 18.62mlTrain batch 30/32 - 108.1ms/batch - loss: 37.99181 - diff: 18.89mlTrain batch 31/32 - 100.0ms/batch - loss: 37.46335 - diff: 18.76mlTrain batch 32/32 - 71.4ms/batch - loss: 38.89057 - diff: 18.80mlTrain batch 32/32 - 10.8s 71.4ms/batch - loss: 38.89057 - diff: 18.80ml
Test 0.6s: val_loss: 56.07566 - diff: 21.95ml

Epoch 70: current best loss = 55.99484, at epoch 55
Train batch 1/32 - 130.5ms/batch - loss: 65.25620 - diff: 25.95mlTrain batch 2/32 - 118.8ms/batch - loss: 40.27364 - diff: 18.87mlTrain batch 3/32 - 114.4ms/batch - loss: 39.24539 - diff: 19.18mlTrain batch 4/32 - 113.0ms/batch - loss: 44.59945 - diff: 20.66mlTrain batch 5/32 - 113.6ms/batch - loss: 42.02723 - diff: 20.01mlTrain batch 6/32 - 114.0ms/batch - loss: 43.34936 - diff: 20.54mlTrain batch 7/32 - 110.9ms/batch - loss: 42.40507 - diff: 20.22mlTrain batch 8/32 - 112.8ms/batch - loss: 45.97417 - diff: 20.85mlTrain batch 9/32 - 112.7ms/batch - loss: 42.77272 - diff: 19.97mlTrain batch 10/32 - 113.0ms/batch - loss: 41.16425 - diff: 19.60mlTrain batch 11/32 - 113.0ms/batch - loss: 40.01149 - diff: 19.42mlTrain batch 12/32 - 112.9ms/batch - loss: 37.47331 - diff: 18.59mlTrain batch 13/32 - 114.2ms/batch - loss: 35.84481 - diff: 18.25mlTrain batch 14/32 - 112.8ms/batch - loss: 36.13650 - diff: 18.42mlTrain batch 15/32 - 114.5ms/batch - loss: 36.25264 - diff: 18.47mlTrain batch 16/32 - 112.7ms/batch - loss: 36.09214 - diff: 18.36mlTrain batch 17/32 - 109.7ms/batch - loss: 38.54332 - diff: 19.06mlTrain batch 18/32 - 96.0ms/batch - loss: 38.16102 - diff: 19.04mlTrain batch 19/32 - 115.5ms/batch - loss: 38.45884 - diff: 19.29mlTrain batch 20/32 - 106.8ms/batch - loss: 38.64731 - diff: 19.39mlTrain batch 21/32 - 121.9ms/batch - loss: 38.89631 - diff: 19.46mlTrain batch 22/32 - 114.3ms/batch - loss: 40.14147 - diff: 19.81mlTrain batch 23/32 - 102.2ms/batch - loss: 40.04664 - diff: 19.80mlTrain batch 24/32 - 103.7ms/batch - loss: 40.79062 - diff: 20.07mlTrain batch 25/32 - 103.4ms/batch - loss: 39.55474 - diff: 19.73mlTrain batch 26/32 - 101.9ms/batch - loss: 39.20898 - diff: 19.66mlTrain batch 27/32 - 102.3ms/batch - loss: 39.01542 - diff: 19.72mlTrain batch 28/32 - 104.3ms/batch - loss: 39.18977 - diff: 19.84mlTrain batch 29/32 - 114.7ms/batch - loss: 39.54703 - diff: 19.99mlTrain batch 30/32 - 112.4ms/batch - loss: 39.31102 - diff: 20.00mlTrain batch 31/32 - 106.0ms/batch - loss: 39.48273 - diff: 19.99mlTrain batch 32/32 - 86.0ms/batch - loss: 40.18124 - diff: 19.99mlTrain batch 32/32 - 11.7s 86.0ms/batch - loss: 40.18124 - diff: 19.99ml
Test 0.6s: val_loss: 68.05927 - diff: 25.63ml

Epoch 71: current best loss = 55.99484, at epoch 55
Train batch 1/32 - 123.3ms/batch - loss: 19.85712 - diff: 14.39mlTrain batch 2/32 - 111.8ms/batch - loss: 30.76921 - diff: 18.67mlTrain batch 3/32 - 103.2ms/batch - loss: 31.08156 - diff: 18.51mlTrain batch 4/32 - 106.0ms/batch - loss: 29.36909 - diff: 17.91mlTrain batch 5/32 - 106.6ms/batch - loss: 33.43416 - diff: 18.73mlTrain batch 6/32 - 106.4ms/batch - loss: 31.97227 - diff: 18.15mlTrain batch 7/32 - 107.5ms/batch - loss: 31.09202 - diff: 17.78mlTrain batch 8/32 - 110.8ms/batch - loss: 31.83789 - diff: 18.28mlTrain batch 9/32 - 109.6ms/batch - loss: 34.04135 - diff: 18.60mlTrain batch 10/32 - 108.1ms/batch - loss: 33.08913 - diff: 18.34mlTrain batch 11/32 - 108.2ms/batch - loss: 43.73117 - diff: 20.56mlTrain batch 12/32 - 109.6ms/batch - loss: 42.37987 - diff: 20.14mlTrain batch 13/32 - 108.4ms/batch - loss: 50.77541 - diff: 20.90mlTrain batch 14/32 - 108.1ms/batch - loss: 50.87735 - diff: 21.06mlTrain batch 15/32 - 92.9ms/batch - loss: 49.26257 - diff: 20.77mlTrain batch 16/32 - 100.3ms/batch - loss: 51.07023 - diff: 21.44mlTrain batch 17/32 - 117.3ms/batch - loss: 49.92090 - diff: 21.22mlTrain batch 18/32 - 109.4ms/batch - loss: 48.32006 - diff: 20.78mlTrain batch 19/32 - 119.5ms/batch - loss: 47.52831 - diff: 20.58mlTrain batch 20/32 - 109.5ms/batch - loss: 46.83388 - diff: 20.54mlTrain batch 21/32 - 109.7ms/batch - loss: 47.13858 - diff: 20.82mlTrain batch 22/32 - 107.8ms/batch - loss: 46.04671 - diff: 20.57mlTrain batch 23/32 - 109.0ms/batch - loss: 44.83771 - diff: 20.28mlTrain batch 24/32 - 100.3ms/batch - loss: 46.31186 - diff: 20.39mlTrain batch 25/32 - 94.8ms/batch - loss: 45.68860 - diff: 20.26mlTrain batch 26/32 - 118.6ms/batch - loss: 45.70246 - diff: 20.42mlTrain batch 27/32 - 120.4ms/batch - loss: 44.92374 - diff: 20.24mlTrain batch 28/32 - 119.2ms/batch - loss: 46.01305 - diff: 20.51mlTrain batch 29/32 - 118.9ms/batch - loss: 45.28368 - diff: 20.38mlTrain batch 30/32 - 117.9ms/batch - loss: 44.61004 - diff: 20.31mlTrain batch 31/32 - 112.0ms/batch - loss: 43.75424 - diff: 20.08mlTrain batch 32/32 - 105.4ms/batch - loss: 44.39253 - diff: 20.08mlTrain batch 32/32 - 11.4s 105.4ms/batch - loss: 44.39253 - diff: 20.08ml
Test 0.7s: val_loss: 52.62549 - diff: 21.52ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 72: current best loss = 52.62549, at epoch 71
Train batch 1/32 - 115.7ms/batch - loss: 51.91324 - diff: 22.22mlTrain batch 2/32 - 111.2ms/batch - loss: 33.15152 - diff: 16.78mlTrain batch 3/32 - 104.3ms/batch - loss: 36.58606 - diff: 18.75mlTrain batch 4/32 - 103.6ms/batch - loss: 32.55176 - diff: 17.35mlTrain batch 5/32 - 104.8ms/batch - loss: 30.11645 - diff: 16.94mlTrain batch 6/32 - 104.5ms/batch - loss: 35.12258 - diff: 17.67mlTrain batch 7/32 - 103.7ms/batch - loss: 34.93785 - diff: 18.00mlTrain batch 8/32 - 102.8ms/batch - loss: 32.52643 - diff: 17.31mlTrain batch 9/32 - 104.1ms/batch - loss: 38.32087 - diff: 18.87mlTrain batch 10/32 - 103.4ms/batch - loss: 38.87072 - diff: 18.83mlTrain batch 11/32 - 111.6ms/batch - loss: 47.64317 - diff: 20.90mlTrain batch 12/32 - 103.4ms/batch - loss: 51.37559 - diff: 21.60mlTrain batch 13/32 - 108.0ms/batch - loss: 49.99522 - diff: 21.41mlTrain batch 14/32 - 108.4ms/batch - loss: 49.82858 - diff: 21.42mlTrain batch 15/32 - 98.6ms/batch - loss: 49.21345 - diff: 21.40mlTrain batch 16/32 - 87.8ms/batch - loss: 47.07237 - diff: 20.82mlTrain batch 17/32 - 125.0ms/batch - loss: 46.01165 - diff: 20.55mlTrain batch 18/32 - 108.9ms/batch - loss: 44.22540 - diff: 20.11mlTrain batch 19/32 - 122.6ms/batch - loss: 42.91359 - diff: 19.80mlTrain batch 20/32 - 113.7ms/batch - loss: 42.31183 - diff: 19.71mlTrain batch 21/32 - 122.0ms/batch - loss: 41.35533 - diff: 19.51mlTrain batch 22/32 - 115.1ms/batch - loss: 43.74796 - diff: 19.71mlTrain batch 23/32 - 121.6ms/batch - loss: 43.60222 - diff: 19.78mlTrain batch 24/32 - 112.9ms/batch - loss: 42.76818 - diff: 19.56mlTrain batch 25/32 - 120.6ms/batch - loss: 42.19376 - diff: 19.46mlTrain batch 26/32 - 113.5ms/batch - loss: 44.80602 - diff: 20.16mlTrain batch 27/32 - 117.7ms/batch - loss: 43.39598 - diff: 19.71mlTrain batch 28/32 - 107.5ms/batch - loss: 42.42748 - diff: 19.45mlTrain batch 29/32 - 107.3ms/batch - loss: 41.82636 - diff: 19.34mlTrain batch 30/32 - 99.0ms/batch - loss: 41.27461 - diff: 19.27mlTrain batch 31/32 - 91.0ms/batch - loss: 41.23544 - diff: 19.29mlTrain batch 32/32 - 81.5ms/batch - loss: 42.40557 - diff: 19.35mlTrain batch 32/32 - 10.9s 81.5ms/batch - loss: 42.40557 - diff: 19.35ml
Test 0.7s: val_loss: 80.37487 - diff: 26.42ml

Epoch 73: current best loss = 52.62549, at epoch 71
Train batch 1/32 - 122.4ms/batch - loss: 16.61477 - diff: 12.03mlTrain batch 2/32 - 108.3ms/batch - loss: 18.95391 - diff: 13.17mlTrain batch 3/32 - 107.2ms/batch - loss: 21.26596 - diff: 14.26mlTrain batch 4/32 - 111.3ms/batch - loss: 27.45662 - diff: 15.46mlTrain batch 5/32 - 105.3ms/batch - loss: 58.36853 - diff: 21.28mlTrain batch 6/32 - 105.4ms/batch - loss: 59.63469 - diff: 22.62mlTrain batch 7/32 - 105.1ms/batch - loss: 53.82145 - diff: 21.31mlTrain batch 8/32 - 103.3ms/batch - loss: 53.82906 - diff: 21.50mlTrain batch 9/32 - 104.0ms/batch - loss: 53.87484 - diff: 21.97mlTrain batch 10/32 - 113.8ms/batch - loss: 52.54568 - diff: 21.91mlTrain batch 11/32 - 112.1ms/batch - loss: 49.67018 - diff: 21.19mlTrain batch 12/32 - 106.9ms/batch - loss: 48.23351 - diff: 21.06mlTrain batch 13/32 - 105.9ms/batch - loss: 46.63381 - diff: 20.73mlTrain batch 14/32 - 103.7ms/batch - loss: 46.84842 - diff: 21.03mlTrain batch 15/32 - 96.1ms/batch - loss: 47.22932 - diff: 21.25mlTrain batch 16/32 - 107.1ms/batch - loss: 44.94319 - diff: 20.49mlTrain batch 17/32 - 107.5ms/batch - loss: 44.60043 - diff: 20.34mlTrain batch 18/32 - 98.3ms/batch - loss: 43.54705 - diff: 19.99mlTrain batch 19/32 - 95.7ms/batch - loss: 43.83741 - diff: 20.15mlTrain batch 20/32 - 118.0ms/batch - loss: 43.39244 - diff: 20.10mlTrain batch 21/32 - 116.4ms/batch - loss: 42.53097 - diff: 19.99mlTrain batch 22/32 - 107.4ms/batch - loss: 41.48898 - diff: 19.70mlTrain batch 23/32 - 106.7ms/batch - loss: 40.29774 - diff: 19.40mlTrain batch 24/32 - 106.9ms/batch - loss: 40.17817 - diff: 19.45mlTrain batch 25/32 - 107.8ms/batch - loss: 39.59572 - diff: 19.28mlTrain batch 26/32 - 115.3ms/batch - loss: 39.09523 - diff: 19.24mlTrain batch 27/32 - 106.9ms/batch - loss: 39.09941 - diff: 19.19mlTrain batch 28/32 - 113.5ms/batch - loss: 38.26232 - diff: 18.96mlTrain batch 29/32 - 106.1ms/batch - loss: 39.53946 - diff: 19.22mlTrain batch 30/32 - 107.8ms/batch - loss: 41.32788 - diff: 19.61mlTrain batch 31/32 - 98.5ms/batch - loss: 41.85720 - diff: 19.77mlTrain batch 32/32 - 81.2ms/batch - loss: 43.90515 - diff: 19.85mlTrain batch 32/32 - 10.7s 81.2ms/batch - loss: 43.90515 - diff: 19.85ml
Test 0.6s: val_loss: 68.09996 - diff: 25.15ml

Epoch 74: current best loss = 52.62549, at epoch 71
Train batch 1/32 - 136.5ms/batch - loss: 24.77663 - diff: 15.09mlTrain batch 2/32 - 115.6ms/batch - loss: 22.36656 - diff: 14.12mlTrain batch 3/32 - 121.3ms/batch - loss: 24.58825 - diff: 15.80mlTrain batch 4/32 - 115.4ms/batch - loss: 22.04235 - diff: 14.96mlTrain batch 5/32 - 123.7ms/batch - loss: 24.38537 - diff: 15.83mlTrain batch 6/32 - 114.3ms/batch - loss: 26.75624 - diff: 16.54mlTrain batch 7/32 - 108.4ms/batch - loss: 34.70046 - diff: 18.70mlTrain batch 8/32 - 118.2ms/batch - loss: 40.14893 - diff: 19.16mlTrain batch 9/32 - 121.9ms/batch - loss: 39.30669 - diff: 19.19mlTrain batch 10/32 - 119.5ms/batch - loss: 46.49228 - diff: 19.68mlTrain batch 11/32 - 114.7ms/batch - loss: 44.18809 - diff: 19.17mlTrain batch 12/32 - 105.2ms/batch - loss: 41.88354 - diff: 18.66mlTrain batch 13/32 - 103.1ms/batch - loss: 39.76392 - diff: 18.16mlTrain batch 14/32 - 110.0ms/batch - loss: 39.60374 - diff: 18.26mlTrain batch 15/32 - 103.8ms/batch - loss: 39.02295 - diff: 18.21mlTrain batch 16/32 - 103.4ms/batch - loss: 39.62260 - diff: 18.11mlTrain batch 17/32 - 103.6ms/batch - loss: 38.95850 - diff: 18.12mlTrain batch 18/32 - 103.0ms/batch - loss: 37.73583 - diff: 17.84mlTrain batch 19/32 - 98.0ms/batch - loss: 38.81789 - diff: 18.34mlTrain batch 20/32 - 88.8ms/batch - loss: 38.76267 - diff: 18.35mlTrain batch 21/32 - 114.7ms/batch - loss: 39.07491 - diff: 18.69mlTrain batch 22/32 - 96.7ms/batch - loss: 38.25632 - diff: 18.53mlTrain batch 23/32 - 106.3ms/batch - loss: 37.60582 - diff: 18.31mlTrain batch 24/32 - 105.1ms/batch - loss: 36.85890 - diff: 18.18mlTrain batch 25/32 - 106.3ms/batch - loss: 36.38728 - diff: 18.03mlTrain batch 26/32 - 105.2ms/batch - loss: 36.29239 - diff: 18.03mlTrain batch 27/32 - 104.4ms/batch - loss: 35.93479 - diff: 18.04mlTrain batch 28/32 - 102.8ms/batch - loss: 35.91579 - diff: 18.11mlTrain batch 29/32 - 102.1ms/batch - loss: 36.47838 - diff: 18.38mlTrain batch 30/32 - 106.4ms/batch - loss: 36.20320 - diff: 18.34mlTrain batch 31/32 - 100.5ms/batch - loss: 35.81915 - diff: 18.24mlTrain batch 32/32 - 80.5ms/batch - loss: 37.33306 - diff: 18.30mlTrain batch 32/32 - 11.1s 80.5ms/batch - loss: 37.33306 - diff: 18.30ml
Test 0.6s: val_loss: 56.24649 - diff: 23.71ml

Epoch 75: current best loss = 52.62549, at epoch 71
Train batch 1/32 - 114.0ms/batch - loss: 18.27487 - diff: 13.15mlTrain batch 2/32 - 97.8ms/batch - loss: 21.27425 - diff: 14.78mlTrain batch 3/32 - 104.4ms/batch - loss: 28.74388 - diff: 17.14mlTrain batch 4/32 - 103.5ms/batch - loss: 30.49375 - diff: 16.94mlTrain batch 5/32 - 103.3ms/batch - loss: 33.82307 - diff: 18.05mlTrain batch 6/32 - 104.1ms/batch - loss: 49.95920 - diff: 21.76mlTrain batch 7/32 - 104.5ms/batch - loss: 48.12787 - diff: 21.80mlTrain batch 8/32 - 104.1ms/batch - loss: 45.55698 - diff: 21.04mlTrain batch 9/32 - 114.6ms/batch - loss: 48.24612 - diff: 21.60mlTrain batch 10/32 - 114.2ms/batch - loss: 45.86378 - diff: 21.06mlTrain batch 11/32 - 103.8ms/batch - loss: 48.14431 - diff: 21.58mlTrain batch 12/32 - 105.0ms/batch - loss: 52.31614 - diff: 22.72mlTrain batch 13/32 - 103.8ms/batch - loss: 51.30452 - diff: 22.55mlTrain batch 14/32 - 104.1ms/batch - loss: 49.51890 - diff: 22.18mlTrain batch 15/32 - 103.2ms/batch - loss: 49.53009 - diff: 22.31mlTrain batch 16/32 - 103.5ms/batch - loss: 47.70381 - diff: 21.77mlTrain batch 17/32 - 104.1ms/batch - loss: 45.55684 - diff: 21.10mlTrain batch 18/32 - 104.1ms/batch - loss: 45.13290 - diff: 21.13mlTrain batch 19/32 - 103.6ms/batch - loss: 43.48322 - diff: 20.53mlTrain batch 20/32 - 103.9ms/batch - loss: 42.92985 - diff: 20.43mlTrain batch 21/32 - 103.3ms/batch - loss: 42.41136 - diff: 20.33mlTrain batch 22/32 - 104.3ms/batch - loss: 41.41849 - diff: 20.06mlTrain batch 23/32 - 115.5ms/batch - loss: 40.37881 - diff: 19.80mlTrain batch 24/32 - 105.2ms/batch - loss: 39.39191 - diff: 19.47mlTrain batch 25/32 - 117.1ms/batch - loss: 38.70488 - diff: 19.27mlTrain batch 26/32 - 117.1ms/batch - loss: 38.63226 - diff: 19.24mlTrain batch 27/32 - 114.5ms/batch - loss: 38.53721 - diff: 19.19mlTrain batch 28/32 - 107.9ms/batch - loss: 37.78843 - diff: 18.99mlTrain batch 29/32 - 107.5ms/batch - loss: 39.24716 - diff: 19.48mlTrain batch 30/32 - 106.9ms/batch - loss: 39.17721 - diff: 19.43mlTrain batch 31/32 - 114.3ms/batch - loss: 39.67776 - diff: 19.58mlTrain batch 32/32 - 111.6ms/batch - loss: 45.69881 - diff: 19.87mlTrain batch 32/32 - 10.9s 111.6ms/batch - loss: 45.69881 - diff: 19.87ml
Test 0.6s: val_loss: 57.91698 - diff: 23.07ml

Epoch 76: current best loss = 52.62549, at epoch 71
Train batch 1/32 - 116.7ms/batch - loss: 26.69545 - diff: 16.25mlTrain batch 2/32 - 104.2ms/batch - loss: 23.17685 - diff: 15.36mlTrain batch 3/32 - 103.0ms/batch - loss: 24.63965 - diff: 15.62mlTrain batch 4/32 - 116.5ms/batch - loss: 24.92032 - diff: 15.64mlTrain batch 5/32 - 115.2ms/batch - loss: 38.05376 - diff: 19.32mlTrain batch 6/32 - 116.8ms/batch - loss: 36.95445 - diff: 18.99mlTrain batch 7/32 - 114.1ms/batch - loss: 34.17487 - diff: 18.22mlTrain batch 8/32 - 114.0ms/batch - loss: 32.96220 - diff: 18.01mlTrain batch 9/32 - 114.4ms/batch - loss: 31.79911 - diff: 17.60mlTrain batch 10/32 - 113.5ms/batch - loss: 32.83240 - diff: 17.95mlTrain batch 11/32 - 113.9ms/batch - loss: 31.39908 - diff: 17.59mlTrain batch 12/32 - 113.4ms/batch - loss: 37.44488 - diff: 18.98mlTrain batch 13/32 - 113.2ms/batch - loss: 36.66354 - diff: 18.57mlTrain batch 14/32 - 113.5ms/batch - loss: 35.69559 - diff: 18.30mlTrain batch 15/32 - 118.8ms/batch - loss: 34.39841 - diff: 18.02mlTrain batch 16/32 - 114.2ms/batch - loss: 35.04387 - diff: 18.31mlTrain batch 17/32 - 113.8ms/batch - loss: 34.11331 - diff: 18.10mlTrain batch 18/32 - 116.3ms/batch - loss: 35.46840 - diff: 18.48mlTrain batch 19/32 - 109.4ms/batch - loss: 35.01846 - diff: 18.23mlTrain batch 20/32 - 111.5ms/batch - loss: 34.87849 - diff: 18.21mlTrain batch 21/32 - 115.5ms/batch - loss: 33.95288 - diff: 17.93mlTrain batch 22/32 - 105.9ms/batch - loss: 33.60817 - diff: 17.80mlTrain batch 23/32 - 110.5ms/batch - loss: 32.68240 - diff: 17.48mlTrain batch 24/32 - 106.8ms/batch - loss: 32.85712 - diff: 17.53mlTrain batch 25/32 - 108.3ms/batch - loss: 33.79229 - diff: 17.75mlTrain batch 26/32 - 91.6ms/batch - loss: 33.62705 - diff: 17.76mlTrain batch 27/32 - 91.8ms/batch - loss: 33.41478 - diff: 17.80mlTrain batch 28/32 - 111.9ms/batch - loss: 33.28904 - diff: 17.82mlTrain batch 29/32 - 106.4ms/batch - loss: 33.16876 - diff: 17.81mlTrain batch 30/32 - 106.9ms/batch - loss: 34.21879 - diff: 18.08mlTrain batch 31/32 - 111.3ms/batch - loss: 34.04456 - diff: 17.96mlTrain batch 32/32 - 96.4ms/batch - loss: 36.80594 - diff: 18.08mlTrain batch 32/32 - 11.6s 96.4ms/batch - loss: 36.80594 - diff: 18.08ml
Test 0.7s: val_loss: 59.91648 - diff: 24.19ml

Epoch 77: current best loss = 52.62549, at epoch 71
Train batch 1/32 - 129.4ms/batch - loss: 27.44737 - diff: 16.51mlTrain batch 2/32 - 112.7ms/batch - loss: 40.11550 - diff: 21.14mlTrain batch 3/32 - 114.2ms/batch - loss: 35.82962 - diff: 19.52mlTrain batch 4/32 - 114.0ms/batch - loss: 40.18932 - diff: 20.66mlTrain batch 5/32 - 114.0ms/batch - loss: 36.70565 - diff: 19.51mlTrain batch 6/32 - 117.6ms/batch - loss: 33.47713 - diff: 18.64mlTrain batch 7/32 - 107.5ms/batch - loss: 35.09923 - diff: 18.57mlTrain batch 8/32 - 107.6ms/batch - loss: 32.65954 - diff: 17.60mlTrain batch 9/32 - 112.2ms/batch - loss: 33.60057 - diff: 17.69mlTrain batch 10/32 - 103.9ms/batch - loss: 35.80208 - diff: 18.32mlTrain batch 11/32 - 102.9ms/batch - loss: 34.17396 - diff: 17.95mlTrain batch 12/32 - 111.6ms/batch - loss: 35.19496 - diff: 18.36mlTrain batch 13/32 - 104.3ms/batch - loss: 34.57242 - diff: 18.29mlTrain batch 14/32 - 104.0ms/batch - loss: 34.69187 - diff: 18.31mlTrain batch 15/32 - 89.1ms/batch - loss: 33.68513 - diff: 18.03mlTrain batch 16/32 - 96.0ms/batch - loss: 33.60115 - diff: 18.12mlTrain batch 17/32 - 91.9ms/batch - loss: 32.95709 - diff: 18.00mlTrain batch 18/32 - 88.4ms/batch - loss: 32.99335 - diff: 18.14mlTrain batch 19/32 - 125.0ms/batch - loss: 32.49903 - diff: 18.06mlTrain batch 20/32 - 113.0ms/batch - loss: 31.97574 - diff: 17.94mlTrain batch 21/32 - 106.8ms/batch - loss: 33.21478 - diff: 18.35mlTrain batch 22/32 - 115.3ms/batch - loss: 32.54003 - diff: 18.14mlTrain batch 23/32 - 108.3ms/batch - loss: 33.01750 - diff: 18.33mlTrain batch 24/32 - 112.9ms/batch - loss: 32.27559 - diff: 18.08mlTrain batch 25/32 - 115.3ms/batch - loss: 32.39864 - diff: 18.14mlTrain batch 26/32 - 107.8ms/batch - loss: 31.53943 - diff: 17.84mlTrain batch 27/32 - 106.4ms/batch - loss: 32.55650 - diff: 17.98mlTrain batch 28/32 - 110.8ms/batch - loss: 35.43461 - diff: 18.51mlTrain batch 29/32 - 106.4ms/batch - loss: 35.85666 - diff: 18.62mlTrain batch 30/32 - 107.1ms/batch - loss: 35.11566 - diff: 18.43mlTrain batch 31/32 - 105.5ms/batch - loss: 34.75461 - diff: 18.34mlTrain batch 32/32 - 78.6ms/batch - loss: 34.99208 - diff: 18.32mlTrain batch 32/32 - 11.6s 78.6ms/batch - loss: 34.99208 - diff: 18.32ml
Test 0.6s: val_loss: 49.73790 - diff: 21.69ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 78: current best loss = 49.73790, at epoch 77
Train batch 1/32 - 120.7ms/batch - loss: 15.71571 - diff: 13.64mlTrain batch 2/32 - 104.2ms/batch - loss: 21.37990 - diff: 15.33mlTrain batch 3/32 - 105.5ms/batch - loss: 23.12438 - diff: 16.38mlTrain batch 4/32 - 104.6ms/batch - loss: 27.03904 - diff: 17.35mlTrain batch 5/32 - 104.7ms/batch - loss: 29.08081 - diff: 17.90mlTrain batch 6/32 - 104.5ms/batch - loss: 29.31070 - diff: 17.73mlTrain batch 7/32 - 105.6ms/batch - loss: 31.89923 - diff: 18.27mlTrain batch 8/32 - 103.6ms/batch - loss: 35.21610 - diff: 19.31mlTrain batch 9/32 - 104.1ms/batch - loss: 39.45790 - diff: 20.63mlTrain batch 10/32 - 108.9ms/batch - loss: 42.22449 - diff: 20.60mlTrain batch 11/32 - 99.6ms/batch - loss: 41.75066 - diff: 20.66mlTrain batch 12/32 - 107.9ms/batch - loss: 42.06842 - diff: 20.75mlTrain batch 13/32 - 109.2ms/batch - loss: 40.30735 - diff: 20.11mlTrain batch 14/32 - 112.2ms/batch - loss: 38.80723 - diff: 19.64mlTrain batch 15/32 - 97.7ms/batch - loss: 37.72469 - diff: 19.49mlTrain batch 16/32 - 107.6ms/batch - loss: 36.41915 - diff: 19.03mlTrain batch 17/32 - 107.4ms/batch - loss: 37.13851 - diff: 19.36mlTrain batch 18/32 - 114.4ms/batch - loss: 35.95945 - diff: 19.01mlTrain batch 19/32 - 115.1ms/batch - loss: 34.91228 - diff: 18.65mlTrain batch 20/32 - 103.5ms/batch - loss: 34.25018 - diff: 18.50mlTrain batch 21/32 - 110.7ms/batch - loss: 35.97957 - diff: 18.88mlTrain batch 22/32 - 103.8ms/batch - loss: 35.11133 - diff: 18.62mlTrain batch 23/32 - 103.8ms/batch - loss: 34.80168 - diff: 18.54mlTrain batch 24/32 - 103.5ms/batch - loss: 34.60221 - diff: 18.54mlTrain batch 25/32 - 103.8ms/batch - loss: 33.70104 - diff: 18.26mlTrain batch 26/32 - 103.3ms/batch - loss: 33.72248 - diff: 18.22mlTrain batch 27/32 - 104.3ms/batch - loss: 33.34743 - diff: 18.13mlTrain batch 28/32 - 103.3ms/batch - loss: 34.48401 - diff: 18.34mlTrain batch 29/32 - 104.9ms/batch - loss: 34.13637 - diff: 18.24mlTrain batch 30/32 - 85.6ms/batch - loss: 33.94596 - diff: 18.26mlTrain batch 31/32 - 99.9ms/batch - loss: 34.70347 - diff: 18.55mlTrain batch 32/32 - 100.4ms/batch - loss: 35.18022 - diff: 18.52mlTrain batch 32/32 - 11.4s 100.4ms/batch - loss: 35.18022 - diff: 18.52ml
Test 0.6s: val_loss: 50.58809 - diff: 20.81ml

Epoch 79: current best loss = 49.73790, at epoch 77
Train batch 1/32 - 118.5ms/batch - loss: 11.71908 - diff: 12.07mlTrain batch 2/32 - 104.4ms/batch - loss: 19.70821 - diff: 14.74mlTrain batch 3/32 - 113.6ms/batch - loss: 24.89670 - diff: 16.45mlTrain batch 4/32 - 105.1ms/batch - loss: 23.76818 - diff: 15.97mlTrain batch 5/32 - 112.6ms/batch - loss: 24.73439 - diff: 16.41mlTrain batch 6/32 - 104.9ms/batch - loss: 25.95328 - diff: 16.77mlTrain batch 7/32 - 112.4ms/batch - loss: 24.59514 - diff: 16.29mlTrain batch 8/32 - 103.8ms/batch - loss: 27.03389 - diff: 16.66mlTrain batch 9/32 - 111.9ms/batch - loss: 26.18893 - diff: 16.47mlTrain batch 10/32 - 104.0ms/batch - loss: 27.51629 - diff: 16.99mlTrain batch 11/32 - 109.4ms/batch - loss: 26.91031 - diff: 16.75mlTrain batch 12/32 - 103.0ms/batch - loss: 26.62304 - diff: 16.51mlTrain batch 13/32 - 111.6ms/batch - loss: 25.58581 - diff: 16.18mlTrain batch 14/32 - 100.7ms/batch - loss: 25.58703 - diff: 16.28mlTrain batch 15/32 - 110.1ms/batch - loss: 25.20936 - diff: 16.12mlTrain batch 16/32 - 92.4ms/batch - loss: 25.39987 - diff: 16.04mlTrain batch 17/32 - 117.4ms/batch - loss: 25.66251 - diff: 16.06mlTrain batch 18/32 - 114.7ms/batch - loss: 26.27627 - diff: 16.32mlTrain batch 19/32 - 108.3ms/batch - loss: 25.92131 - diff: 16.20mlTrain batch 20/32 - 100.7ms/batch - loss: 25.17016 - diff: 15.94mlTrain batch 21/32 - 107.6ms/batch - loss: 25.26735 - diff: 15.88mlTrain batch 22/32 - 107.2ms/batch - loss: 27.16734 - diff: 16.51mlTrain batch 23/32 - 117.4ms/batch - loss: 28.18736 - diff: 16.81mlTrain batch 24/32 - 106.7ms/batch - loss: 27.87153 - diff: 16.71mlTrain batch 25/32 - 113.6ms/batch - loss: 27.52518 - diff: 16.62mlTrain batch 26/32 - 106.1ms/batch - loss: 27.35768 - diff: 16.63mlTrain batch 27/32 - 107.1ms/batch - loss: 27.20286 - diff: 16.62mlTrain batch 28/32 - 106.6ms/batch - loss: 28.85198 - diff: 17.18mlTrain batch 29/32 - 110.8ms/batch - loss: 30.32673 - diff: 17.68mlTrain batch 30/32 - 87.6ms/batch - loss: 29.49911 - diff: 17.34mlTrain batch 31/32 - 112.3ms/batch - loss: 29.08488 - diff: 17.22mlTrain batch 32/32 - 109.8ms/batch - loss: 30.24004 - diff: 17.27mlTrain batch 32/32 - 10.8s 109.8ms/batch - loss: 30.24004 - diff: 17.27ml
Test 0.6s: val_loss: 46.14004 - diff: 21.06ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 80: current best loss = 46.14004, at epoch 79
Train batch 1/32 - 124.8ms/batch - loss: 28.74302 - diff: 18.24mlTrain batch 2/32 - 116.3ms/batch - loss: 29.09107 - diff: 16.67mlTrain batch 3/32 - 106.8ms/batch - loss: 26.06974 - diff: 15.74mlTrain batch 4/32 - 103.3ms/batch - loss: 27.38025 - diff: 16.38mlTrain batch 5/32 - 108.6ms/batch - loss: 29.03288 - diff: 16.87mlTrain batch 6/32 - 107.2ms/batch - loss: 26.12072 - diff: 15.89mlTrain batch 7/32 - 107.9ms/batch - loss: 26.50947 - diff: 16.09mlTrain batch 8/32 - 107.8ms/batch - loss: 24.19833 - diff: 15.20mlTrain batch 9/32 - 104.6ms/batch - loss: 25.24711 - diff: 15.74mlTrain batch 10/32 - 104.1ms/batch - loss: 24.84581 - diff: 15.41mlTrain batch 11/32 - 111.3ms/batch - loss: 28.96020 - diff: 16.76mlTrain batch 12/32 - 103.8ms/batch - loss: 27.65567 - diff: 16.34mlTrain batch 13/32 - 116.8ms/batch - loss: 26.73420 - diff: 16.09mlTrain batch 14/32 - 107.9ms/batch - loss: 32.34506 - diff: 17.71mlTrain batch 15/32 - 118.5ms/batch - loss: 32.56845 - diff: 17.83mlTrain batch 16/32 - 108.8ms/batch - loss: 33.57875 - diff: 18.05mlTrain batch 17/32 - 110.8ms/batch - loss: 32.82042 - diff: 17.91mlTrain batch 18/32 - 107.4ms/batch - loss: 32.87904 - diff: 17.91mlTrain batch 19/32 - 107.9ms/batch - loss: 32.96150 - diff: 17.90mlTrain batch 20/32 - 104.9ms/batch - loss: 33.18583 - diff: 17.94mlTrain batch 21/32 - 111.2ms/batch - loss: 33.85054 - diff: 18.08mlTrain batch 22/32 - 106.1ms/batch - loss: 34.98649 - diff: 18.43mlTrain batch 23/32 - 110.4ms/batch - loss: 33.92316 - diff: 18.08mlTrain batch 24/32 - 104.2ms/batch - loss: 33.20416 - diff: 17.86mlTrain batch 25/32 - 111.3ms/batch - loss: 32.49587 - diff: 17.66mlTrain batch 26/32 - 104.5ms/batch - loss: 32.42358 - diff: 17.60mlTrain batch 27/32 - 110.6ms/batch - loss: 32.74328 - diff: 17.69mlTrain batch 28/32 - 104.5ms/batch - loss: 34.11689 - diff: 18.15mlTrain batch 29/32 - 108.1ms/batch - loss: 37.92408 - diff: 18.75mlTrain batch 30/32 - 105.4ms/batch - loss: 37.84198 - diff: 18.82mlTrain batch 31/32 - 87.4ms/batch - loss: 37.60884 - diff: 18.76mlTrain batch 32/32 - 81.9ms/batch - loss: 40.04254 - diff: 18.88mlTrain batch 32/32 - 11.1s 81.9ms/batch - loss: 40.04254 - diff: 18.88ml
Test 0.6s: val_loss: 54.21257 - diff: 23.32ml

Epoch 81: current best loss = 46.14004, at epoch 79
Train batch 1/32 - 137.2ms/batch - loss: 39.87599 - diff: 16.31mlTrain batch 2/32 - 117.3ms/batch - loss: 88.19061 - diff: 29.27mlTrain batch 3/32 - 119.8ms/batch - loss: 63.69806 - diff: 24.02mlTrain batch 4/32 - 115.8ms/batch - loss: 55.79337 - diff: 21.96mlTrain batch 5/32 - 115.1ms/batch - loss: 51.75606 - diff: 21.48mlTrain batch 6/32 - 114.9ms/batch - loss: 48.78499 - diff: 20.78mlTrain batch 7/32 - 114.4ms/batch - loss: 42.94179 - diff: 19.17mlTrain batch 8/32 - 115.3ms/batch - loss: 43.99426 - diff: 19.74mlTrain batch 9/32 - 115.6ms/batch - loss: 41.71975 - diff: 19.06mlTrain batch 10/32 - 114.8ms/batch - loss: 41.45524 - diff: 19.33mlTrain batch 11/32 - 115.6ms/batch - loss: 42.57095 - diff: 19.59mlTrain batch 12/32 - 114.6ms/batch - loss: 41.74457 - diff: 19.46mlTrain batch 13/32 - 114.7ms/batch - loss: 44.25307 - diff: 20.27mlTrain batch 14/32 - 114.1ms/batch - loss: 42.65638 - diff: 19.85mlTrain batch 15/32 - 117.2ms/batch - loss: 41.66024 - diff: 19.53mlTrain batch 16/32 - 116.3ms/batch - loss: 40.53040 - diff: 19.35mlTrain batch 17/32 - 110.7ms/batch - loss: 39.02684 - diff: 19.00mlTrain batch 18/32 - 107.5ms/batch - loss: 38.03108 - diff: 18.79mlTrain batch 19/32 - 112.4ms/batch - loss: 36.79736 - diff: 18.43mlTrain batch 20/32 - 112.2ms/batch - loss: 35.96615 - diff: 18.09mlTrain batch 21/32 - 112.4ms/batch - loss: 35.51811 - diff: 18.05mlTrain batch 22/32 - 112.2ms/batch - loss: 35.76497 - diff: 18.00mlTrain batch 23/32 - 114.3ms/batch - loss: 36.02982 - diff: 18.09mlTrain batch 24/32 - 113.9ms/batch - loss: 35.35445 - diff: 17.91mlTrain batch 25/32 - 114.5ms/batch - loss: 34.29182 - diff: 17.56mlTrain batch 26/32 - 113.9ms/batch - loss: 34.83814 - diff: 17.83mlTrain batch 27/32 - 121.4ms/batch - loss: 35.26038 - diff: 17.99mlTrain batch 28/32 - 114.0ms/batch - loss: 37.46930 - diff: 18.54mlTrain batch 29/32 - 113.5ms/batch - loss: 37.04555 - diff: 18.49mlTrain batch 30/32 - 113.9ms/batch - loss: 36.84854 - diff: 18.45mlTrain batch 31/32 - 104.9ms/batch - loss: 36.32529 - diff: 18.30mlTrain batch 32/32 - 68.1ms/batch - loss: 36.22575 - diff: 18.22mlTrain batch 32/32 - 11.1s 68.1ms/batch - loss: 36.22575 - diff: 18.22ml
Test 0.6s: val_loss: 54.13494 - diff: 22.78ml

Epoch 82: current best loss = 46.14004, at epoch 79
Train batch 1/32 - 117.3ms/batch - loss: 31.57220 - diff: 20.24mlTrain batch 2/32 - 105.7ms/batch - loss: 25.30021 - diff: 17.09mlTrain batch 3/32 - 103.2ms/batch - loss: 30.72832 - diff: 18.40mlTrain batch 4/32 - 110.7ms/batch - loss: 28.49511 - diff: 16.96mlTrain batch 5/32 - 109.9ms/batch - loss: 27.48490 - diff: 16.87mlTrain batch 6/32 - 110.0ms/batch - loss: 29.04943 - diff: 17.60mlTrain batch 7/32 - 109.8ms/batch - loss: 27.91267 - diff: 17.33mlTrain batch 8/32 - 122.2ms/batch - loss: 30.61163 - diff: 17.97mlTrain batch 9/32 - 106.6ms/batch - loss: 28.87071 - diff: 17.36mlTrain batch 10/32 - 107.9ms/batch - loss: 27.53484 - diff: 16.87mlTrain batch 11/32 - 100.8ms/batch - loss: 26.83739 - diff: 16.67mlTrain batch 12/32 - 107.0ms/batch - loss: 27.59650 - diff: 16.75mlTrain batch 13/32 - 104.2ms/batch - loss: 26.42039 - diff: 16.31mlTrain batch 14/32 - 102.6ms/batch - loss: 26.09069 - diff: 16.02mlTrain batch 15/32 - 109.1ms/batch - loss: 26.52477 - diff: 16.25mlTrain batch 16/32 - 106.8ms/batch - loss: 26.21626 - diff: 16.11mlTrain batch 17/32 - 100.4ms/batch - loss: 25.19729 - diff: 15.72mlTrain batch 18/32 - 108.1ms/batch - loss: 25.34756 - diff: 15.67mlTrain batch 19/32 - 114.0ms/batch - loss: 24.93416 - diff: 15.52mlTrain batch 20/32 - 114.6ms/batch - loss: 25.39823 - diff: 15.65mlTrain batch 21/32 - 120.6ms/batch - loss: 25.86270 - diff: 15.89mlTrain batch 22/32 - 114.9ms/batch - loss: 27.42422 - diff: 16.38mlTrain batch 23/32 - 103.2ms/batch - loss: 26.59858 - diff: 16.06mlTrain batch 24/32 - 90.3ms/batch - loss: 27.72286 - diff: 16.29mlTrain batch 25/32 - 102.9ms/batch - loss: 27.40793 - diff: 16.22mlTrain batch 26/32 - 102.7ms/batch - loss: 26.90529 - diff: 16.10mlTrain batch 27/32 - 105.5ms/batch - loss: 28.53170 - diff: 16.50mlTrain batch 28/32 - 103.0ms/batch - loss: 28.03578 - diff: 16.39mlTrain batch 29/32 - 103.8ms/batch - loss: 27.99496 - diff: 16.39mlTrain batch 30/32 - 112.1ms/batch - loss: 27.55724 - diff: 16.25mlTrain batch 31/32 - 103.3ms/batch - loss: 27.15611 - diff: 16.14mlTrain batch 32/32 - 96.3ms/batch - loss: 28.19632 - diff: 16.17mlTrain batch 32/32 - 11.7s 96.3ms/batch - loss: 28.19632 - diff: 16.17ml
Test 0.6s: val_loss: 49.63794 - diff: 21.33ml

Epoch 83: current best loss = 46.14004, at epoch 79
Train batch 1/32 - 117.4ms/batch - loss: 14.31997 - diff: 13.38mlTrain batch 2/32 - 104.1ms/batch - loss: 15.78840 - diff: 13.71mlTrain batch 3/32 - 110.4ms/batch - loss: 21.23330 - diff: 14.67mlTrain batch 4/32 - 104.5ms/batch - loss: 18.86893 - diff: 13.87mlTrain batch 5/32 - 111.9ms/batch - loss: 25.45688 - diff: 16.33mlTrain batch 6/32 - 103.2ms/batch - loss: 23.71262 - diff: 15.81mlTrain batch 7/32 - 110.9ms/batch - loss: 25.84031 - diff: 16.06mlTrain batch 8/32 - 102.5ms/batch - loss: 32.80364 - diff: 18.16mlTrain batch 9/32 - 104.3ms/batch - loss: 32.95476 - diff: 17.89mlTrain batch 10/32 - 102.7ms/batch - loss: 35.19915 - diff: 18.35mlTrain batch 11/32 - 120.8ms/batch - loss: 33.57584 - diff: 17.91mlTrain batch 12/32 - 112.9ms/batch - loss: 34.17921 - diff: 18.28mlTrain batch 13/32 - 112.9ms/batch - loss: 32.72037 - diff: 17.86mlTrain batch 14/32 - 113.0ms/batch - loss: 35.53616 - diff: 18.77mlTrain batch 15/32 - 101.5ms/batch - loss: 35.05436 - diff: 18.73mlTrain batch 16/32 - 109.3ms/batch - loss: 35.62796 - diff: 18.90mlTrain batch 17/32 - 104.0ms/batch - loss: 34.80932 - diff: 18.72mlTrain batch 18/32 - 104.1ms/batch - loss: 33.57125 - diff: 18.29mlTrain batch 19/32 - 105.9ms/batch - loss: 32.74333 - diff: 18.11mlTrain batch 20/32 - 108.8ms/batch - loss: 32.44061 - diff: 18.07mlTrain batch 21/32 - 115.8ms/batch - loss: 32.71018 - diff: 18.13mlTrain batch 22/32 - 113.2ms/batch - loss: 31.57358 - diff: 17.71mlTrain batch 23/32 - 124.4ms/batch - loss: 32.73991 - diff: 18.17mlTrain batch 24/32 - 118.1ms/batch - loss: 33.24075 - diff: 18.40mlTrain batch 25/32 - 114.6ms/batch - loss: 36.50867 - diff: 19.22mlTrain batch 26/32 - 113.8ms/batch - loss: 37.25542 - diff: 19.53mlTrain batch 27/32 - 113.2ms/batch - loss: 36.39898 - diff: 19.24mlTrain batch 28/32 - 113.5ms/batch - loss: 35.91903 - diff: 19.07mlTrain batch 29/32 - 108.5ms/batch - loss: 35.53203 - diff: 18.92mlTrain batch 30/32 - 109.6ms/batch - loss: 36.33656 - diff: 19.24mlTrain batch 31/32 - 118.3ms/batch - loss: 36.40466 - diff: 19.26mlTrain batch 32/32 - 85.7ms/batch - loss: 38.19158 - diff: 19.32mlTrain batch 32/32 - 10.9s 85.7ms/batch - loss: 38.19158 - diff: 19.32ml
Test 0.5s: val_loss: 48.80252 - diff: 21.50ml

Epoch 84: current best loss = 46.14004, at epoch 79
Train batch 1/32 - 128.6ms/batch - loss: 46.66689 - diff: 22.49mlTrain batch 2/32 - 107.5ms/batch - loss: 34.24548 - diff: 19.27mlTrain batch 3/32 - 113.7ms/batch - loss: 30.82654 - diff: 17.66mlTrain batch 4/32 - 107.1ms/batch - loss: 52.16497 - diff: 23.49mlTrain batch 5/32 - 115.2ms/batch - loss: 64.91839 - diff: 25.94mlTrain batch 6/32 - 106.4ms/batch - loss: 58.69561 - diff: 24.52mlTrain batch 7/32 - 110.0ms/batch - loss: 57.62474 - diff: 24.69mlTrain batch 8/32 - 106.7ms/batch - loss: 56.32818 - diff: 24.00mlTrain batch 9/32 - 92.1ms/batch - loss: 56.67056 - diff: 23.92mlTrain batch 10/32 - 99.6ms/batch - loss: 52.76195 - diff: 22.95mlTrain batch 11/32 - 106.5ms/batch - loss: 52.54675 - diff: 22.88mlTrain batch 12/32 - 96.5ms/batch - loss: 54.61739 - diff: 23.34mlTrain batch 13/32 - 109.1ms/batch - loss: 52.45114 - diff: 22.79mlTrain batch 14/32 - 108.7ms/batch - loss: 52.98777 - diff: 22.99mlTrain batch 15/32 - 108.5ms/batch - loss: 50.35265 - diff: 22.23mlTrain batch 16/32 - 109.2ms/batch - loss: 48.06230 - diff: 21.59mlTrain batch 17/32 - 107.9ms/batch - loss: 48.50817 - diff: 21.71mlTrain batch 18/32 - 108.6ms/batch - loss: 50.83804 - diff: 21.86mlTrain batch 19/32 - 114.7ms/batch - loss: 48.73764 - diff: 21.23mlTrain batch 20/32 - 109.7ms/batch - loss: 49.03515 - diff: 21.46mlTrain batch 21/32 - 109.1ms/batch - loss: 50.13813 - diff: 21.67mlTrain batch 22/32 - 108.8ms/batch - loss: 49.28921 - diff: 21.53mlTrain batch 23/32 - 107.7ms/batch - loss: 48.13949 - diff: 21.33mlTrain batch 24/32 - 108.8ms/batch - loss: 46.41645 - diff: 20.78mlTrain batch 25/32 - 108.5ms/batch - loss: 46.07767 - diff: 20.60mlTrain batch 26/32 - 106.2ms/batch - loss: 46.08707 - diff: 20.71mlTrain batch 27/32 - 104.9ms/batch - loss: 45.42590 - diff: 20.63mlTrain batch 28/32 - 105.1ms/batch - loss: 44.25191 - diff: 20.30mlTrain batch 29/32 - 109.8ms/batch - loss: 43.20381 - diff: 19.96mlTrain batch 30/32 - 109.1ms/batch - loss: 42.53164 - diff: 19.85mlTrain batch 31/32 - 102.1ms/batch - loss: 41.74458 - diff: 19.61mlTrain batch 32/32 - 88.9ms/batch - loss: 42.40317 - diff: 19.60mlTrain batch 32/32 - 10.7s 88.9ms/batch - loss: 42.40317 - diff: 19.60ml
Test 0.6s: val_loss: 90.74020 - diff: 29.64ml

Epoch 85: current best loss = 46.14004, at epoch 79
Train batch 1/32 - 120.7ms/batch - loss: 24.91509 - diff: 17.98mlTrain batch 2/32 - 107.0ms/batch - loss: 25.91244 - diff: 16.76mlTrain batch 3/32 - 115.7ms/batch - loss: 25.66100 - diff: 16.35mlTrain batch 4/32 - 113.7ms/batch - loss: 26.61971 - diff: 16.82mlTrain batch 5/32 - 113.8ms/batch - loss: 24.49402 - diff: 15.87mlTrain batch 6/32 - 113.4ms/batch - loss: 25.94862 - diff: 15.91mlTrain batch 7/32 - 112.8ms/batch - loss: 28.38433 - diff: 16.73mlTrain batch 8/32 - 112.4ms/batch - loss: 30.23212 - diff: 17.24mlTrain batch 9/32 - 111.2ms/batch - loss: 32.46426 - diff: 18.01mlTrain batch 10/32 - 115.0ms/batch - loss: 30.91915 - diff: 17.47mlTrain batch 11/32 - 117.5ms/batch - loss: 38.30948 - diff: 19.34mlTrain batch 12/32 - 123.3ms/batch - loss: 38.65963 - diff: 19.39mlTrain batch 13/32 - 116.9ms/batch - loss: 39.73072 - diff: 19.52mlTrain batch 14/32 - 116.4ms/batch - loss: 38.19751 - diff: 19.15mlTrain batch 15/32 - 116.8ms/batch - loss: 36.38734 - diff: 18.61mlTrain batch 16/32 - 116.6ms/batch - loss: 36.74562 - diff: 18.69mlTrain batch 17/32 - 119.5ms/batch - loss: 35.69562 - diff: 18.42mlTrain batch 18/32 - 122.4ms/batch - loss: 36.47921 - diff: 18.70mlTrain batch 19/32 - 117.3ms/batch - loss: 35.31195 - diff: 18.33mlTrain batch 20/32 - 116.8ms/batch - loss: 37.22944 - diff: 18.98mlTrain batch 21/32 - 117.7ms/batch - loss: 36.50100 - diff: 18.76mlTrain batch 22/32 - 115.7ms/batch - loss: 36.94358 - diff: 18.91mlTrain batch 23/32 - 106.1ms/batch - loss: 40.59530 - diff: 19.81mlTrain batch 24/32 - 108.7ms/batch - loss: 40.85563 - diff: 19.88mlTrain batch 25/32 - 112.6ms/batch - loss: 40.29815 - diff: 19.74mlTrain batch 26/32 - 114.6ms/batch - loss: 41.61189 - diff: 20.20mlTrain batch 27/32 - 113.3ms/batch - loss: 42.78040 - diff: 20.39mlTrain batch 28/32 - 116.9ms/batch - loss: 42.57451 - diff: 20.39mlTrain batch 29/32 - 113.2ms/batch - loss: 42.69170 - diff: 20.39mlTrain batch 30/32 - 112.5ms/batch - loss: 42.28243 - diff: 20.29mlTrain batch 31/32 - 97.8ms/batch - loss: 41.45917 - diff: 20.04mlTrain batch 32/32 - 80.0ms/batch - loss: 42.77250 - diff: 20.06mlTrain batch 32/32 - 12.3s 80.0ms/batch - loss: 42.77250 - diff: 20.06ml
Test 0.6s: val_loss: 59.96888 - diff: 23.16ml

Epoch 86: current best loss = 46.14004, at epoch 79
Train batch 1/32 - 132.6ms/batch - loss: 48.09898 - diff: 20.52mlTrain batch 2/32 - 114.4ms/batch - loss: 39.02973 - diff: 19.09mlTrain batch 3/32 - 115.5ms/batch - loss: 28.28664 - diff: 15.52mlTrain batch 4/32 - 112.9ms/batch - loss: 39.65268 - diff: 18.97mlTrain batch 5/32 - 123.0ms/batch - loss: 34.86589 - diff: 17.78mlTrain batch 6/32 - 111.4ms/batch - loss: 34.26808 - diff: 17.91mlTrain batch 7/32 - 115.3ms/batch - loss: 32.15265 - diff: 17.33mlTrain batch 8/32 - 93.3ms/batch - loss: 30.93951 - diff: 17.06mlTrain batch 9/32 - 95.2ms/batch - loss: 29.98983 - diff: 16.97mlTrain batch 10/32 - 102.7ms/batch - loss: 29.39626 - diff: 16.72mlTrain batch 11/32 - 108.2ms/batch - loss: 29.69329 - diff: 16.97mlTrain batch 12/32 - 120.8ms/batch - loss: 31.96519 - diff: 17.89mlTrain batch 13/32 - 114.0ms/batch - loss: 30.75323 - diff: 17.56mlTrain batch 14/32 - 106.8ms/batch - loss: 30.10516 - diff: 17.41mlTrain batch 15/32 - 106.7ms/batch - loss: 29.54690 - diff: 17.17mlTrain batch 16/32 - 92.3ms/batch - loss: 29.28821 - diff: 17.05mlTrain batch 17/32 - 92.0ms/batch - loss: 29.16257 - diff: 17.07mlTrain batch 18/32 - 93.5ms/batch - loss: 28.75831 - diff: 17.01mlTrain batch 19/32 - 89.4ms/batch - loss: 28.37330 - diff: 16.96mlTrain batch 20/32 - 106.3ms/batch - loss: 29.31121 - diff: 17.30mlTrain batch 21/32 - 107.1ms/batch - loss: 29.07843 - diff: 17.16mlTrain batch 22/32 - 106.6ms/batch - loss: 28.64618 - diff: 17.09mlTrain batch 23/32 - 106.3ms/batch - loss: 28.60168 - diff: 17.04mlTrain batch 24/32 - 106.6ms/batch - loss: 28.47276 - diff: 17.09mlTrain batch 25/32 - 106.7ms/batch - loss: 28.03972 - diff: 16.95mlTrain batch 26/32 - 106.9ms/batch - loss: 28.57841 - diff: 17.19mlTrain batch 27/32 - 106.8ms/batch - loss: 28.53781 - diff: 17.17mlTrain batch 28/32 - 106.3ms/batch - loss: 29.25996 - diff: 17.42mlTrain batch 29/32 - 106.2ms/batch - loss: 29.19088 - diff: 17.38mlTrain batch 30/32 - 106.3ms/batch - loss: 29.36780 - diff: 17.41mlTrain batch 31/32 - 105.8ms/batch - loss: 29.61989 - diff: 17.44mlTrain batch 32/32 - 97.6ms/batch - loss: 30.18328 - diff: 17.46mlTrain batch 32/32 - 11.0s 97.6ms/batch - loss: 30.18328 - diff: 17.46ml
Test 0.6s: val_loss: 55.56119 - diff: 22.99ml

Epoch 87: current best loss = 46.14004, at epoch 79
Train batch 1/32 - 118.0ms/batch - loss: 28.30642 - diff: 17.37mlTrain batch 2/32 - 103.7ms/batch - loss: 35.09809 - diff: 18.73mlTrain batch 3/32 - 103.2ms/batch - loss: 33.75035 - diff: 18.74mlTrain batch 4/32 - 105.0ms/batch - loss: 33.34283 - diff: 18.73mlTrain batch 5/32 - 110.2ms/batch - loss: 29.44492 - diff: 17.19mlTrain batch 6/32 - 103.2ms/batch - loss: 30.22293 - diff: 17.29mlTrain batch 7/32 - 104.4ms/batch - loss: 28.23712 - diff: 16.79mlTrain batch 8/32 - 104.3ms/batch - loss: 27.34044 - diff: 16.65mlTrain batch 9/32 - 103.5ms/batch - loss: 27.79788 - diff: 16.75mlTrain batch 10/32 - 105.8ms/batch - loss: 27.84485 - diff: 16.86mlTrain batch 11/32 - 103.4ms/batch - loss: 28.14168 - diff: 16.73mlTrain batch 12/32 - 104.7ms/batch - loss: 28.78629 - diff: 16.80mlTrain batch 13/32 - 112.3ms/batch - loss: 28.40107 - diff: 16.70mlTrain batch 14/32 - 108.9ms/batch - loss: 29.19739 - diff: 17.03mlTrain batch 15/32 - 114.9ms/batch - loss: 28.17564 - diff: 16.75mlTrain batch 16/32 - 92.6ms/batch - loss: 31.26050 - diff: 17.60mlTrain batch 17/32 - 122.8ms/batch - loss: 30.16882 - diff: 17.28mlTrain batch 18/32 - 115.8ms/batch - loss: 31.97935 - diff: 17.73mlTrain batch 19/32 - 106.1ms/batch - loss: 31.81479 - diff: 17.76mlTrain batch 20/32 - 91.0ms/batch - loss: 35.60258 - diff: 18.68mlTrain batch 21/32 - 113.9ms/batch - loss: 34.81437 - diff: 18.49mlTrain batch 22/32 - 113.3ms/batch - loss: 34.53061 - diff: 18.36mlTrain batch 23/32 - 107.9ms/batch - loss: 35.00917 - diff: 18.55mlTrain batch 24/32 - 112.7ms/batch - loss: 35.97740 - diff: 18.93mlTrain batch 25/32 - 103.7ms/batch - loss: 35.70990 - diff: 18.81mlTrain batch 26/32 - 103.0ms/batch - loss: 35.24447 - diff: 18.64mlTrain batch 27/32 - 112.0ms/batch - loss: 36.21329 - diff: 19.00mlTrain batch 28/32 - 111.6ms/batch - loss: 35.73032 - diff: 18.86mlTrain batch 29/32 - 106.7ms/batch - loss: 35.04747 - diff: 18.66mlTrain batch 30/32 - 108.5ms/batch - loss: 35.06707 - diff: 18.67mlTrain batch 31/32 - 103.0ms/batch - loss: 35.53503 - diff: 18.85mlTrain batch 32/32 - 93.4ms/batch - loss: 38.05298 - diff: 18.97mlTrain batch 32/32 - 11.5s 93.4ms/batch - loss: 38.05298 - diff: 18.97ml
Test 0.6s: val_loss: 49.29980 - diff: 21.30ml

Epoch 88: current best loss = 46.14004, at epoch 79
Train batch 1/32 - 119.4ms/batch - loss: 36.15067 - diff: 20.00mlTrain batch 2/32 - 108.1ms/batch - loss: 28.16711 - diff: 17.88mlTrain batch 3/32 - 113.2ms/batch - loss: 22.89541 - diff: 15.53mlTrain batch 4/32 - 116.1ms/batch - loss: 23.29996 - diff: 15.61mlTrain batch 5/32 - 117.8ms/batch - loss: 26.48195 - diff: 16.40mlTrain batch 6/32 - 106.3ms/batch - loss: 24.27298 - diff: 15.61mlTrain batch 7/32 - 104.2ms/batch - loss: 25.71137 - diff: 16.38mlTrain batch 8/32 - 109.4ms/batch - loss: 29.86627 - diff: 17.47mlTrain batch 9/32 - 106.4ms/batch - loss: 27.12705 - diff: 16.48mlTrain batch 10/32 - 103.9ms/batch - loss: 27.92435 - diff: 16.58mlTrain batch 11/32 - 104.1ms/batch - loss: 28.70870 - diff: 16.93mlTrain batch 12/32 - 109.2ms/batch - loss: 33.37185 - diff: 18.22mlTrain batch 13/32 - 112.5ms/batch - loss: 34.40483 - diff: 18.43mlTrain batch 14/32 - 106.4ms/batch - loss: 33.51965 - diff: 18.01mlTrain batch 15/32 - 112.3ms/batch - loss: 33.40305 - diff: 18.06mlTrain batch 16/32 - 107.9ms/batch - loss: 32.95596 - diff: 17.98mlTrain batch 17/32 - 111.1ms/batch - loss: 33.18810 - diff: 18.01mlTrain batch 18/32 - 106.2ms/batch - loss: 32.69412 - diff: 17.83mlTrain batch 19/32 - 115.9ms/batch - loss: 33.19061 - diff: 17.81mlTrain batch 20/32 - 106.9ms/batch - loss: 33.24044 - diff: 17.94mlTrain batch 21/32 - 118.1ms/batch - loss: 38.90194 - diff: 19.20mlTrain batch 22/32 - 112.7ms/batch - loss: 37.94373 - diff: 18.95mlTrain batch 23/32 - 102.3ms/batch - loss: 37.30163 - diff: 18.81mlTrain batch 24/32 - 107.5ms/batch - loss: 37.41616 - diff: 18.91mlTrain batch 25/32 - 84.1ms/batch - loss: 36.64710 - diff: 18.76mlTrain batch 26/32 - 102.4ms/batch - loss: 37.37535 - diff: 19.08mlTrain batch 27/32 - 107.0ms/batch - loss: 36.73503 - diff: 18.95mlTrain batch 28/32 - 109.4ms/batch - loss: 36.10391 - diff: 18.75mlTrain batch 29/32 - 108.1ms/batch - loss: 35.30450 - diff: 18.53mlTrain batch 30/32 - 99.1ms/batch - loss: 34.92041 - diff: 18.43mlTrain batch 31/32 - 100.8ms/batch - loss: 35.12765 - diff: 18.56mlTrain batch 32/32 - 99.2ms/batch - loss: 36.88946 - diff: 18.62mlTrain batch 32/32 - 11.4s 99.2ms/batch - loss: 36.88946 - diff: 18.62ml
Test 0.6s: val_loss: 69.05996 - diff: 24.21ml

Epoch 89: current best loss = 46.14004, at epoch 79
Train batch 1/32 - 120.6ms/batch - loss: 12.01309 - diff: 12.50mlTrain batch 2/32 - 109.7ms/batch - loss: 35.64162 - diff: 18.90mlTrain batch 3/32 - 108.0ms/batch - loss: 29.47303 - diff: 17.12mlTrain batch 4/32 - 107.7ms/batch - loss: 23.70799 - diff: 14.79mlTrain batch 5/32 - 107.8ms/batch - loss: 29.04218 - diff: 15.92mlTrain batch 6/32 - 108.5ms/batch - loss: 27.00031 - diff: 15.61mlTrain batch 7/32 - 106.9ms/batch - loss: 25.54267 - diff: 15.35mlTrain batch 8/32 - 118.7ms/batch - loss: 28.27444 - diff: 16.19mlTrain batch 9/32 - 108.3ms/batch - loss: 27.14791 - diff: 15.90mlTrain batch 10/32 - 109.0ms/batch - loss: 28.59694 - diff: 16.29mlTrain batch 11/32 - 109.0ms/batch - loss: 32.06928 - diff: 17.49mlTrain batch 12/32 - 108.6ms/batch - loss: 31.04181 - diff: 17.31mlTrain batch 13/32 - 107.9ms/batch - loss: 31.53576 - diff: 17.61mlTrain batch 14/32 - 107.8ms/batch - loss: 31.14769 - diff: 17.42mlTrain batch 15/32 - 106.3ms/batch - loss: 33.71396 - diff: 18.11mlTrain batch 16/32 - 111.6ms/batch - loss: 33.30253 - diff: 18.03mlTrain batch 17/32 - 96.5ms/batch - loss: 32.59724 - diff: 17.79mlTrain batch 18/32 - 107.9ms/batch - loss: 32.45852 - diff: 17.75mlTrain batch 19/32 - 115.1ms/batch - loss: 32.56932 - diff: 17.51mlTrain batch 20/32 - 117.2ms/batch - loss: 32.51481 - diff: 17.39mlTrain batch 21/32 - 108.5ms/batch - loss: 33.71741 - diff: 17.82mlTrain batch 22/32 - 111.9ms/batch - loss: 32.50249 - diff: 17.33mlTrain batch 23/32 - 117.7ms/batch - loss: 31.81459 - diff: 17.15mlTrain batch 24/32 - 111.2ms/batch - loss: 31.38332 - diff: 16.99mlTrain batch 25/32 - 109.3ms/batch - loss: 30.91248 - diff: 16.84mlTrain batch 26/32 - 110.0ms/batch - loss: 30.17395 - diff: 16.58mlTrain batch 27/32 - 100.1ms/batch - loss: 30.09986 - diff: 16.65mlTrain batch 28/32 - 108.5ms/batch - loss: 30.44415 - diff: 16.82mlTrain batch 29/32 - 106.5ms/batch - loss: 29.99641 - diff: 16.71mlTrain batch 30/32 - 107.8ms/batch - loss: 29.49134 - diff: 16.55mlTrain batch 31/32 - 104.8ms/batch - loss: 29.21970 - diff: 16.48mlTrain batch 32/32 - 99.1ms/batch - loss: 31.26500 - diff: 16.59mlTrain batch 32/32 - 11.8s 99.1ms/batch - loss: 31.26500 - diff: 16.59ml
Test 0.6s: val_loss: 56.44389 - diff: 23.07ml

Epoch 90: current best loss = 46.14004, at epoch 79
Train batch 1/32 - 121.0ms/batch - loss: 20.54795 - diff: 14.13mlTrain batch 2/32 - 107.9ms/batch - loss: 20.23142 - diff: 13.82mlTrain batch 3/32 - 107.6ms/batch - loss: 25.01796 - diff: 15.43mlTrain batch 4/32 - 109.7ms/batch - loss: 28.98251 - diff: 17.46mlTrain batch 5/32 - 110.9ms/batch - loss: 26.16803 - diff: 16.41mlTrain batch 6/32 - 115.3ms/batch - loss: 24.43511 - diff: 15.70mlTrain batch 7/32 - 110.7ms/batch - loss: 24.64598 - diff: 15.69mlTrain batch 8/32 - 113.8ms/batch - loss: 25.02697 - diff: 16.18mlTrain batch 9/32 - 113.9ms/batch - loss: 25.14200 - diff: 16.10mlTrain batch 10/32 - 114.1ms/batch - loss: 24.83523 - diff: 16.10mlTrain batch 11/32 - 114.6ms/batch - loss: 23.66679 - diff: 15.53mlTrain batch 12/32 - 116.4ms/batch - loss: 22.55292 - diff: 15.05mlTrain batch 13/32 - 113.3ms/batch - loss: 21.84641 - diff: 14.83mlTrain batch 14/32 - 92.1ms/batch - loss: 21.59679 - diff: 14.73mlTrain batch 15/32 - 101.9ms/batch - loss: 21.42645 - diff: 14.62mlTrain batch 16/32 - 114.8ms/batch - loss: 20.78421 - diff: 14.43mlTrain batch 17/32 - 112.9ms/batch - loss: 22.00501 - diff: 14.86mlTrain batch 18/32 - 113.7ms/batch - loss: 21.41332 - diff: 14.64mlTrain batch 19/32 - 115.0ms/batch - loss: 22.55865 - diff: 15.15mlTrain batch 20/32 - 103.6ms/batch - loss: 22.46853 - diff: 15.15mlTrain batch 21/32 - 110.3ms/batch - loss: 21.85453 - diff: 14.85mlTrain batch 22/32 - 111.9ms/batch - loss: 22.20177 - diff: 15.02mlTrain batch 23/32 - 102.6ms/batch - loss: 21.71862 - diff: 14.87mlTrain batch 24/32 - 104.7ms/batch - loss: 21.40155 - diff: 14.74mlTrain batch 25/32 - 104.6ms/batch - loss: 21.14630 - diff: 14.63mlTrain batch 26/32 - 103.1ms/batch - loss: 21.35373 - diff: 14.77mlTrain batch 27/32 - 111.7ms/batch - loss: 21.64620 - diff: 14.88mlTrain batch 28/32 - 114.4ms/batch - loss: 22.17603 - diff: 15.10mlTrain batch 29/32 - 121.4ms/batch - loss: 22.07521 - diff: 15.06mlTrain batch 30/32 - 113.5ms/batch - loss: 22.86405 - diff: 15.26mlTrain batch 31/32 - 115.7ms/batch - loss: 22.49882 - diff: 15.13mlTrain batch 32/32 - 108.2ms/batch - loss: 23.62738 - diff: 15.19mlTrain batch 32/32 - 11.6s 108.2ms/batch - loss: 23.62738 - diff: 15.19ml
Test 0.7s: val_loss: 49.65825 - diff: 21.45ml
Epoch    91: reducing learning rate of group 0 to 2.5000e-04.

Epoch 91: current best loss = 46.14004, at epoch 79
Train batch 1/32 - 132.8ms/batch - loss: 15.37473 - diff: 12.49mlTrain batch 2/32 - 117.3ms/batch - loss: 16.69338 - diff: 12.91mlTrain batch 3/32 - 116.9ms/batch - loss: 15.92273 - diff: 12.40mlTrain batch 4/32 - 117.1ms/batch - loss: 32.91758 - diff: 15.61mlTrain batch 5/32 - 117.1ms/batch - loss: 31.78882 - diff: 16.22mlTrain batch 6/32 - 116.8ms/batch - loss: 30.61767 - diff: 16.34mlTrain batch 7/32 - 115.2ms/batch - loss: 36.15756 - diff: 18.44mlTrain batch 8/32 - 103.2ms/batch - loss: 35.25909 - diff: 18.35mlTrain batch 9/32 - 108.6ms/batch - loss: 35.13580 - diff: 18.48mlTrain batch 10/32 - 113.3ms/batch - loss: 36.15093 - diff: 19.09mlTrain batch 11/32 - 113.6ms/batch - loss: 35.06119 - diff: 18.87mlTrain batch 12/32 - 115.6ms/batch - loss: 34.09700 - diff: 18.65mlTrain batch 13/32 - 114.3ms/batch - loss: 34.29086 - diff: 18.94mlTrain batch 14/32 - 114.3ms/batch - loss: 35.65325 - diff: 19.15mlTrain batch 15/32 - 114.4ms/batch - loss: 34.50766 - diff: 18.77mlTrain batch 16/32 - 97.0ms/batch - loss: 33.01876 - diff: 18.31mlTrain batch 17/32 - 117.5ms/batch - loss: 34.23922 - diff: 18.71mlTrain batch 18/32 - 117.4ms/batch - loss: 36.64779 - diff: 19.48mlTrain batch 19/32 - 118.6ms/batch - loss: 35.40535 - diff: 19.07mlTrain batch 20/32 - 116.7ms/batch - loss: 38.61260 - diff: 20.01mlTrain batch 21/32 - 112.3ms/batch - loss: 38.22406 - diff: 19.94mlTrain batch 22/32 - 107.5ms/batch - loss: 38.36374 - diff: 19.90mlTrain batch 23/32 - 114.1ms/batch - loss: 37.56782 - diff: 19.62mlTrain batch 24/32 - 109.1ms/batch - loss: 40.39555 - diff: 20.37mlTrain batch 25/32 - 103.7ms/batch - loss: 39.53099 - diff: 20.13mlTrain batch 26/32 - 105.1ms/batch - loss: 39.97762 - diff: 20.35mlTrain batch 27/32 - 103.9ms/batch - loss: 38.84366 - diff: 19.92mlTrain batch 28/32 - 103.7ms/batch - loss: 38.58824 - diff: 19.94mlTrain batch 29/32 - 98.4ms/batch - loss: 37.89412 - diff: 19.78mlTrain batch 30/32 - 87.8ms/batch - loss: 37.75183 - diff: 19.83mlTrain batch 31/32 - 114.5ms/batch - loss: 37.05764 - diff: 19.61mlTrain batch 32/32 - 82.2ms/batch - loss: 37.51846 - diff: 19.59mlTrain batch 32/32 - 10.9s 82.2ms/batch - loss: 37.51846 - diff: 19.59ml
Test 0.6s: val_loss: 48.65567 - diff: 21.09ml

Epoch 92: current best loss = 46.14004, at epoch 79
Train batch 1/32 - 123.5ms/batch - loss: 33.51884 - diff: 17.79mlTrain batch 2/32 - 108.6ms/batch - loss: 21.73252 - diff: 13.74mlTrain batch 3/32 - 112.8ms/batch - loss: 54.09371 - diff: 21.75mlTrain batch 4/32 - 103.3ms/batch - loss: 50.35948 - diff: 21.51mlTrain batch 5/32 - 90.6ms/batch - loss: 42.73724 - diff: 19.32mlTrain batch 6/32 - 104.1ms/batch - loss: 39.33760 - diff: 18.51mlTrain batch 7/32 - 106.4ms/batch - loss: 43.41726 - diff: 19.96mlTrain batch 8/32 - 105.8ms/batch - loss: 39.21257 - diff: 18.72mlTrain batch 9/32 - 105.5ms/batch - loss: 35.98947 - diff: 17.77mlTrain batch 10/32 - 104.4ms/batch - loss: 37.62436 - diff: 18.38mlTrain batch 11/32 - 108.6ms/batch - loss: 36.06515 - diff: 18.19mlTrain batch 12/32 - 104.9ms/batch - loss: 34.41458 - diff: 17.75mlTrain batch 13/32 - 109.5ms/batch - loss: 33.46104 - diff: 17.60mlTrain batch 14/32 - 105.1ms/batch - loss: 32.59209 - diff: 17.32mlTrain batch 15/32 - 106.2ms/batch - loss: 32.76156 - diff: 17.36mlTrain batch 16/32 - 107.1ms/batch - loss: 32.38187 - diff: 17.30mlTrain batch 17/32 - 104.8ms/batch - loss: 31.35215 - diff: 17.00mlTrain batch 18/32 - 104.1ms/batch - loss: 30.31264 - diff: 16.70mlTrain batch 19/32 - 100.4ms/batch - loss: 29.84576 - diff: 16.64mlTrain batch 20/32 - 88.2ms/batch - loss: 30.77567 - diff: 17.01mlTrain batch 21/32 - 84.8ms/batch - loss: 34.33468 - diff: 17.77mlTrain batch 22/32 - 98.3ms/batch - loss: 33.71657 - diff: 17.55mlTrain batch 23/32 - 108.5ms/batch - loss: 32.94585 - diff: 17.34mlTrain batch 24/32 - 113.3ms/batch - loss: 32.60886 - diff: 17.27mlTrain batch 25/32 - 122.0ms/batch - loss: 32.46450 - diff: 17.34mlTrain batch 26/32 - 115.0ms/batch - loss: 34.79644 - diff: 17.94mlTrain batch 27/32 - 118.2ms/batch - loss: 34.30720 - diff: 17.84mlTrain batch 28/32 - 117.1ms/batch - loss: 34.79073 - diff: 18.07mlTrain batch 29/32 - 123.3ms/batch - loss: 34.03431 - diff: 17.86mlTrain batch 30/32 - 117.3ms/batch - loss: 33.40820 - diff: 17.69mlTrain batch 31/32 - 97.7ms/batch - loss: 32.66491 - diff: 17.46mlTrain batch 32/32 - 89.4ms/batch - loss: 35.65739 - diff: 17.62mlTrain batch 32/32 - 10.9s 89.4ms/batch - loss: 35.65739 - diff: 17.62ml
Test 0.6s: val_loss: 53.23884 - diff: 22.43ml

Epoch 93: current best loss = 46.14004, at epoch 79
Train batch 1/32 - 135.4ms/batch - loss: 23.87815 - diff: 14.70mlTrain batch 2/32 - 114.8ms/batch - loss: 23.84807 - diff: 15.64mlTrain batch 3/32 - 114.1ms/batch - loss: 25.90266 - diff: 16.70mlTrain batch 4/32 - 104.0ms/batch - loss: 30.37518 - diff: 18.31mlTrain batch 5/32 - 108.3ms/batch - loss: 28.00356 - diff: 17.76mlTrain batch 6/32 - 114.1ms/batch - loss: 24.34686 - diff: 16.11mlTrain batch 7/32 - 99.9ms/batch - loss: 26.52427 - diff: 16.55mlTrain batch 8/32 - 109.9ms/batch - loss: 25.24633 - diff: 16.15mlTrain batch 9/32 - 109.7ms/batch - loss: 24.83019 - diff: 15.84mlTrain batch 10/32 - 106.8ms/batch - loss: 26.41551 - diff: 16.29mlTrain batch 11/32 - 108.1ms/batch - loss: 25.90949 - diff: 16.10mlTrain batch 12/32 - 104.9ms/batch - loss: 25.71163 - diff: 16.18mlTrain batch 13/32 - 111.0ms/batch - loss: 26.04753 - diff: 16.36mlTrain batch 14/32 - 82.9ms/batch - loss: 25.81473 - diff: 16.35mlTrain batch 15/32 - 103.1ms/batch - loss: 25.33503 - diff: 16.26mlTrain batch 16/32 - 102.0ms/batch - loss: 24.36148 - diff: 15.88mlTrain batch 17/32 - 105.7ms/batch - loss: 24.42535 - diff: 15.93mlTrain batch 18/32 - 118.7ms/batch - loss: 24.17179 - diff: 15.74mlTrain batch 19/32 - 122.4ms/batch - loss: 24.03803 - diff: 15.68mlTrain batch 20/32 - 98.6ms/batch - loss: 23.80370 - diff: 15.51mlTrain batch 21/32 - 107.8ms/batch - loss: 23.61416 - diff: 15.54mlTrain batch 22/32 - 116.4ms/batch - loss: 24.12669 - diff: 15.73mlTrain batch 23/32 - 116.8ms/batch - loss: 23.88757 - diff: 15.66mlTrain batch 24/32 - 113.5ms/batch - loss: 23.48166 - diff: 15.48mlTrain batch 25/32 - 113.6ms/batch - loss: 23.39627 - diff: 15.36mlTrain batch 26/32 - 113.1ms/batch - loss: 23.94032 - diff: 15.62mlTrain batch 27/32 - 113.4ms/batch - loss: 23.69714 - diff: 15.55mlTrain batch 28/32 - 113.7ms/batch - loss: 23.70713 - diff: 15.59mlTrain batch 29/32 - 113.6ms/batch - loss: 23.60791 - diff: 15.52mlTrain batch 30/32 - 107.0ms/batch - loss: 23.21881 - diff: 15.32mlTrain batch 31/32 - 97.8ms/batch - loss: 23.60925 - diff: 15.36mlTrain batch 32/32 - 91.8ms/batch - loss: 23.94444 - diff: 15.34mlTrain batch 32/32 - 11.9s 91.8ms/batch - loss: 23.94444 - diff: 15.34ml
Test 0.6s: val_loss: 50.09349 - diff: 21.04ml

Epoch 94: current best loss = 46.14004, at epoch 79
Train batch 1/32 - 138.7ms/batch - loss: 12.67181 - diff: 11.23mlTrain batch 2/32 - 113.9ms/batch - loss: 10.63388 - diff: 9.98mlTrain batch 3/32 - 113.7ms/batch - loss: 32.94921 - diff: 16.35mlTrain batch 4/32 - 114.1ms/batch - loss: 26.98746 - diff: 14.63mlTrain batch 5/32 - 113.4ms/batch - loss: 23.66905 - diff: 13.81mlTrain batch 6/32 - 113.7ms/batch - loss: 21.13391 - diff: 13.12mlTrain batch 7/32 - 113.1ms/batch - loss: 33.15645 - diff: 16.77mlTrain batch 8/32 - 112.4ms/batch - loss: 30.99708 - diff: 16.38mlTrain batch 9/32 - 106.3ms/batch - loss: 35.67525 - diff: 17.88mlTrain batch 10/32 - 106.2ms/batch - loss: 34.08726 - diff: 17.53mlTrain batch 11/32 - 105.9ms/batch - loss: 32.42869 - diff: 17.20mlTrain batch 12/32 - 105.5ms/batch - loss: 32.38195 - diff: 17.40mlTrain batch 13/32 - 105.7ms/batch - loss: 31.35983 - diff: 17.20mlTrain batch 14/32 - 102.3ms/batch - loss: 29.68109 - diff: 16.56mlTrain batch 15/32 - 103.0ms/batch - loss: 29.58983 - diff: 16.63mlTrain batch 16/32 - 106.6ms/batch - loss: 28.51404 - diff: 16.36mlTrain batch 17/32 - 110.4ms/batch - loss: 27.46808 - diff: 15.97mlTrain batch 18/32 - 106.1ms/batch - loss: 26.86906 - diff: 15.85mlTrain batch 19/32 - 90.7ms/batch - loss: 26.05077 - diff: 15.59mlTrain batch 20/32 - 111.9ms/batch - loss: 26.26678 - diff: 15.78mlTrain batch 21/32 - 109.6ms/batch - loss: 25.94147 - diff: 15.76mlTrain batch 22/32 - 107.7ms/batch - loss: 25.59329 - diff: 15.69mlTrain batch 23/32 - 106.1ms/batch - loss: 25.66653 - diff: 15.72mlTrain batch 24/32 - 92.3ms/batch - loss: 24.98391 - diff: 15.49mlTrain batch 25/32 - 91.9ms/batch - loss: 24.47008 - diff: 15.32mlTrain batch 26/32 - 103.7ms/batch - loss: 24.11627 - diff: 15.20mlTrain batch 27/32 - 103.3ms/batch - loss: 23.96771 - diff: 15.20mlTrain batch 28/32 - 103.1ms/batch - loss: 23.41711 - diff: 14.98mlTrain batch 29/32 - 103.4ms/batch - loss: 23.04145 - diff: 14.87mlTrain batch 30/32 - 107.7ms/batch - loss: 22.77945 - diff: 14.81mlTrain batch 31/32 - 99.5ms/batch - loss: 22.37604 - diff: 14.68mlTrain batch 32/32 - 88.9ms/batch - loss: 22.38396 - diff: 14.62mlTrain batch 32/32 - 11.1s 88.9ms/batch - loss: 22.38396 - diff: 14.62ml
Test 0.6s: val_loss: 52.19222 - diff: 21.58ml

Epoch 95: current best loss = 46.14004, at epoch 79
Train batch 1/32 - 121.4ms/batch - loss: 15.04002 - diff: 10.50mlTrain batch 2/32 - 107.8ms/batch - loss: 21.35875 - diff: 13.19mlTrain batch 3/32 - 108.6ms/batch - loss: 20.52862 - diff: 13.55mlTrain batch 4/32 - 108.9ms/batch - loss: 17.69320 - diff: 12.32mlTrain batch 5/32 - 118.1ms/batch - loss: 17.37264 - diff: 12.35mlTrain batch 6/32 - 108.5ms/batch - loss: 20.29334 - diff: 13.71mlTrain batch 7/32 - 108.3ms/batch - loss: 19.28424 - diff: 13.55mlTrain batch 8/32 - 108.4ms/batch - loss: 25.86760 - diff: 15.74mlTrain batch 9/32 - 107.5ms/batch - loss: 24.76895 - diff: 15.56mlTrain batch 10/32 - 107.2ms/batch - loss: 23.59614 - diff: 15.01mlTrain batch 11/32 - 106.7ms/batch - loss: 25.02979 - diff: 15.42mlTrain batch 12/32 - 106.6ms/batch - loss: 24.57778 - diff: 15.15mlTrain batch 13/32 - 110.5ms/batch - loss: 25.08084 - diff: 15.14mlTrain batch 14/32 - 114.8ms/batch - loss: 25.50346 - diff: 15.43mlTrain batch 15/32 - 121.1ms/batch - loss: 26.07535 - diff: 15.71mlTrain batch 16/32 - 121.7ms/batch - loss: 25.31969 - diff: 15.48mlTrain batch 17/32 - 122.0ms/batch - loss: 24.74301 - diff: 15.29mlTrain batch 18/32 - 122.2ms/batch - loss: 24.76368 - diff: 15.37mlTrain batch 19/32 - 101.2ms/batch - loss: 24.48314 - diff: 15.35mlTrain batch 20/32 - 93.9ms/batch - loss: 23.73629 - diff: 15.10mlTrain batch 21/32 - 113.6ms/batch - loss: 24.14598 - diff: 15.24mlTrain batch 22/32 - 102.2ms/batch - loss: 23.77718 - diff: 15.18mlTrain batch 23/32 - 109.2ms/batch - loss: 23.55955 - diff: 15.03mlTrain batch 24/32 - 108.7ms/batch - loss: 23.15081 - diff: 14.89mlTrain batch 25/32 - 108.9ms/batch - loss: 22.86362 - diff: 14.72mlTrain batch 26/32 - 108.7ms/batch - loss: 23.77015 - diff: 15.05mlTrain batch 27/32 - 110.3ms/batch - loss: 23.32933 - diff: 14.91mlTrain batch 28/32 - 108.2ms/batch - loss: 22.80846 - diff: 14.71mlTrain batch 29/32 - 108.3ms/batch - loss: 22.51146 - diff: 14.65mlTrain batch 30/32 - 107.2ms/batch - loss: 22.18040 - diff: 14.59mlTrain batch 31/32 - 93.5ms/batch - loss: 24.09252 - diff: 15.19mlTrain batch 32/32 - 84.5ms/batch - loss: 24.94870 - diff: 15.20mlTrain batch 32/32 - 11.3s 84.5ms/batch - loss: 24.94870 - diff: 15.20ml
Test 0.5s: val_loss: 44.26552 - diff: 20.07ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 96: current best loss = 44.26552, at epoch 95
Train batch 1/32 - 133.0ms/batch - loss: 48.17121 - diff: 24.77mlTrain batch 2/32 - 114.0ms/batch - loss: 39.17603 - diff: 20.46mlTrain batch 3/32 - 115.0ms/batch - loss: 33.15744 - diff: 18.30mlTrain batch 4/32 - 116.2ms/batch - loss: 29.09664 - diff: 17.05mlTrain batch 5/32 - 117.4ms/batch - loss: 31.51434 - diff: 18.11mlTrain batch 6/32 - 112.7ms/batch - loss: 29.64994 - diff: 17.75mlTrain batch 7/32 - 118.5ms/batch - loss: 28.87159 - diff: 17.68mlTrain batch 8/32 - 112.5ms/batch - loss: 29.91417 - diff: 18.14mlTrain batch 9/32 - 119.8ms/batch - loss: 27.27440 - diff: 17.07mlTrain batch 10/32 - 112.4ms/batch - loss: 28.91263 - diff: 17.85mlTrain batch 11/32 - 114.3ms/batch - loss: 27.70582 - diff: 17.39mlTrain batch 12/32 - 108.0ms/batch - loss: 27.06381 - diff: 16.93mlTrain batch 13/32 - 109.2ms/batch - loss: 25.88129 - diff: 16.44mlTrain batch 14/32 - 107.4ms/batch - loss: 25.71683 - diff: 16.48mlTrain batch 15/32 - 116.2ms/batch - loss: 24.54367 - diff: 15.95mlTrain batch 16/32 - 109.0ms/batch - loss: 25.66384 - diff: 16.28mlTrain batch 17/32 - 109.2ms/batch - loss: 24.57671 - diff: 15.88mlTrain batch 18/32 - 110.0ms/batch - loss: 24.92402 - diff: 16.05mlTrain batch 19/32 - 102.0ms/batch - loss: 24.10192 - diff: 15.72mlTrain batch 20/32 - 91.8ms/batch - loss: 24.36117 - diff: 15.94mlTrain batch 21/32 - 106.5ms/batch - loss: 24.03654 - diff: 15.87mlTrain batch 22/32 - 81.5ms/batch - loss: 23.60347 - diff: 15.71mlTrain batch 23/32 - 106.1ms/batch - loss: 23.54725 - diff: 15.69mlTrain batch 24/32 - 97.5ms/batch - loss: 23.09856 - diff: 15.54mlTrain batch 25/32 - 104.6ms/batch - loss: 22.78788 - diff: 15.46mlTrain batch 26/32 - 104.1ms/batch - loss: 22.42249 - diff: 15.34mlTrain batch 27/32 - 112.3ms/batch - loss: 21.84528 - diff: 15.11mlTrain batch 28/32 - 112.4ms/batch - loss: 21.93522 - diff: 15.15mlTrain batch 29/32 - 111.8ms/batch - loss: 21.58017 - diff: 15.03mlTrain batch 30/32 - 111.5ms/batch - loss: 22.45119 - diff: 15.37mlTrain batch 31/32 - 112.4ms/batch - loss: 22.27651 - diff: 15.34mlTrain batch 32/32 - 82.8ms/batch - loss: 22.45651 - diff: 15.30mlTrain batch 32/32 - 11.1s 82.8ms/batch - loss: 22.45651 - diff: 15.30ml
Test 0.6s: val_loss: 48.50648 - diff: 20.65ml

Epoch 97: current best loss = 44.26552, at epoch 95
Train batch 1/32 - 124.0ms/batch - loss: 47.85385 - diff: 19.07mlTrain batch 2/32 - 110.0ms/batch - loss: 47.81549 - diff: 20.93mlTrain batch 3/32 - 108.6ms/batch - loss: 35.86715 - diff: 17.37mlTrain batch 4/32 - 109.4ms/batch - loss: 36.30417 - diff: 18.19mlTrain batch 5/32 - 118.6ms/batch - loss: 31.61237 - diff: 16.93mlTrain batch 6/32 - 127.9ms/batch - loss: 30.77684 - diff: 16.97mlTrain batch 7/32 - 94.6ms/batch - loss: 28.72448 - diff: 16.58mlTrain batch 8/32 - 107.1ms/batch - loss: 39.34219 - diff: 19.26mlTrain batch 9/32 - 98.0ms/batch - loss: 35.87826 - diff: 18.22mlTrain batch 10/32 - 106.8ms/batch - loss: 36.50560 - diff: 17.87mlTrain batch 11/32 - 116.4ms/batch - loss: 33.89774 - diff: 17.08mlTrain batch 12/32 - 117.7ms/batch - loss: 33.55867 - diff: 17.06mlTrain batch 13/32 - 116.5ms/batch - loss: 32.28125 - diff: 16.87mlTrain batch 14/32 - 116.2ms/batch - loss: 32.37212 - diff: 17.08mlTrain batch 15/32 - 115.8ms/batch - loss: 35.78343 - diff: 18.25mlTrain batch 16/32 - 116.3ms/batch - loss: 38.44310 - diff: 19.04mlTrain batch 17/32 - 106.1ms/batch - loss: 36.85885 - diff: 18.57mlTrain batch 18/32 - 91.5ms/batch - loss: 35.54460 - diff: 18.20mlTrain batch 19/32 - 106.8ms/batch - loss: 34.36169 - diff: 17.84mlTrain batch 20/32 - 102.6ms/batch - loss: 35.80003 - diff: 18.37mlTrain batch 21/32 - 103.7ms/batch - loss: 34.77008 - diff: 18.06mlTrain batch 22/32 - 104.1ms/batch - loss: 33.59379 - diff: 17.67mlTrain batch 23/32 - 104.6ms/batch - loss: 32.69958 - diff: 17.38mlTrain batch 24/32 - 99.8ms/batch - loss: 31.94428 - diff: 17.22mlTrain batch 25/32 - 107.7ms/batch - loss: 31.21768 - diff: 17.01mlTrain batch 26/32 - 112.6ms/batch - loss: 30.53336 - diff: 16.85mlTrain batch 27/32 - 107.1ms/batch - loss: 30.50210 - diff: 16.89mlTrain batch 28/32 - 107.2ms/batch - loss: 29.64268 - diff: 16.56mlTrain batch 29/32 - 111.9ms/batch - loss: 30.04849 - diff: 16.77mlTrain batch 30/32 - 99.0ms/batch - loss: 29.29560 - diff: 16.49mlTrain batch 31/32 - 91.3ms/batch - loss: 29.37034 - diff: 16.59mlTrain batch 32/32 - 73.1ms/batch - loss: 29.35833 - diff: 16.54mlTrain batch 32/32 - 12.1s 73.1ms/batch - loss: 29.35833 - diff: 16.54ml
Test 0.6s: val_loss: 43.87969 - diff: 19.90ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 98: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 123.3ms/batch - loss: 11.07040 - diff: 11.09mlTrain batch 2/32 - 108.5ms/batch - loss: 29.02356 - diff: 17.97mlTrain batch 3/32 - 108.5ms/batch - loss: 23.67470 - diff: 15.91mlTrain batch 4/32 - 107.2ms/batch - loss: 22.94685 - diff: 15.69mlTrain batch 5/32 - 108.0ms/batch - loss: 22.16541 - diff: 15.24mlTrain batch 6/32 - 107.9ms/batch - loss: 20.27275 - diff: 14.45mlTrain batch 7/32 - 108.2ms/batch - loss: 23.04467 - diff: 15.26mlTrain batch 8/32 - 106.8ms/batch - loss: 22.45287 - diff: 15.18mlTrain batch 9/32 - 105.6ms/batch - loss: 24.26480 - diff: 15.73mlTrain batch 10/32 - 115.1ms/batch - loss: 25.88478 - diff: 16.16mlTrain batch 11/32 - 100.2ms/batch - loss: 25.13919 - diff: 15.79mlTrain batch 12/32 - 81.6ms/batch - loss: 25.22442 - diff: 15.75mlTrain batch 13/32 - 104.1ms/batch - loss: 26.13222 - diff: 16.16mlTrain batch 14/32 - 122.6ms/batch - loss: 25.82933 - diff: 15.93mlTrain batch 15/32 - 112.1ms/batch - loss: 24.83841 - diff: 15.61mlTrain batch 16/32 - 113.3ms/batch - loss: 23.92792 - diff: 15.24mlTrain batch 17/32 - 111.5ms/batch - loss: 24.12779 - diff: 15.33mlTrain batch 18/32 - 112.5ms/batch - loss: 25.51222 - diff: 15.83mlTrain batch 19/32 - 90.5ms/batch - loss: 26.75499 - diff: 16.31mlTrain batch 20/32 - 88.9ms/batch - loss: 26.05202 - diff: 16.08mlTrain batch 21/32 - 95.0ms/batch - loss: 26.20212 - diff: 16.20mlTrain batch 22/32 - 104.3ms/batch - loss: 25.72245 - diff: 16.03mlTrain batch 23/32 - 103.9ms/batch - loss: 25.30554 - diff: 15.92mlTrain batch 24/32 - 108.1ms/batch - loss: 25.16417 - diff: 15.95mlTrain batch 25/32 - 103.2ms/batch - loss: 25.17341 - diff: 15.97mlTrain batch 26/32 - 113.5ms/batch - loss: 24.93086 - diff: 15.93mlTrain batch 27/32 - 103.8ms/batch - loss: 24.61590 - diff: 15.78mlTrain batch 28/32 - 114.0ms/batch - loss: 25.04012 - diff: 15.93mlTrain batch 29/32 - 102.9ms/batch - loss: 25.06814 - diff: 15.94mlTrain batch 30/32 - 104.8ms/batch - loss: 24.63199 - diff: 15.78mlTrain batch 31/32 - 96.2ms/batch - loss: 24.26758 - diff: 15.65mlTrain batch 32/32 - 78.1ms/batch - loss: 43.23084 - diff: 16.30mlTrain batch 32/32 - 11.3s 78.1ms/batch - loss: 43.23084 - diff: 16.30ml
Test 0.6s: val_loss: 52.68572 - diff: 21.44ml

Epoch 99: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 102.1ms/batch - loss: 16.98243 - diff: 14.87mlTrain batch 2/32 - 90.6ms/batch - loss: 19.58703 - diff: 15.70mlTrain batch 3/32 - 115.4ms/batch - loss: 29.06257 - diff: 16.51mlTrain batch 4/32 - 114.0ms/batch - loss: 24.44466 - diff: 14.93mlTrain batch 5/32 - 115.2ms/batch - loss: 21.59929 - diff: 13.95mlTrain batch 6/32 - 113.5ms/batch - loss: 20.51971 - diff: 13.67mlTrain batch 7/32 - 114.0ms/batch - loss: 20.74182 - diff: 13.90mlTrain batch 8/32 - 113.9ms/batch - loss: 21.99824 - diff: 13.94mlTrain batch 9/32 - 113.4ms/batch - loss: 21.88485 - diff: 14.13mlTrain batch 10/32 - 114.0ms/batch - loss: 21.51883 - diff: 14.17mlTrain batch 11/32 - 114.0ms/batch - loss: 23.38660 - diff: 14.90mlTrain batch 12/32 - 117.8ms/batch - loss: 23.38797 - diff: 15.13mlTrain batch 13/32 - 113.6ms/batch - loss: 22.71523 - diff: 14.79mlTrain batch 14/32 - 117.3ms/batch - loss: 22.65487 - diff: 14.70mlTrain batch 15/32 - 115.3ms/batch - loss: 22.16901 - diff: 14.34mlTrain batch 16/32 - 114.9ms/batch - loss: 22.53506 - diff: 14.53mlTrain batch 17/32 - 116.0ms/batch - loss: 22.05492 - diff: 14.35mlTrain batch 18/32 - 100.6ms/batch - loss: 29.94039 - diff: 16.14mlTrain batch 19/32 - 113.7ms/batch - loss: 31.68620 - diff: 16.82mlTrain batch 20/32 - 107.7ms/batch - loss: 35.46584 - diff: 17.65mlTrain batch 21/32 - 107.3ms/batch - loss: 35.90286 - diff: 17.98mlTrain batch 22/32 - 105.5ms/batch - loss: 35.95640 - diff: 18.11mlTrain batch 23/32 - 103.9ms/batch - loss: 35.32057 - diff: 18.04mlTrain batch 24/32 - 104.1ms/batch - loss: 34.16796 - diff: 17.66mlTrain batch 25/32 - 104.7ms/batch - loss: 33.42851 - diff: 17.46mlTrain batch 26/32 - 104.8ms/batch - loss: 32.89226 - diff: 17.35mlTrain batch 27/32 - 104.8ms/batch - loss: 32.98424 - diff: 17.48mlTrain batch 28/32 - 104.9ms/batch - loss: 32.14947 - diff: 17.20mlTrain batch 29/32 - 104.5ms/batch - loss: 32.34119 - diff: 17.32mlTrain batch 30/32 - 104.3ms/batch - loss: 31.85720 - diff: 17.19mlTrain batch 31/32 - 104.2ms/batch - loss: 31.84154 - diff: 17.24mlTrain batch 32/32 - 75.5ms/batch - loss: 32.53133 - diff: 17.23mlTrain batch 32/32 - 11.0s 75.5ms/batch - loss: 32.53133 - diff: 17.23ml
Test 0.6s: val_loss: 48.69514 - diff: 20.95ml

Epoch 100: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 122.7ms/batch - loss: 58.84453 - diff: 26.09mlTrain batch 2/32 - 106.8ms/batch - loss: 36.67345 - diff: 19.07mlTrain batch 3/32 - 104.1ms/batch - loss: 27.61569 - diff: 15.66mlTrain batch 4/32 - 109.7ms/batch - loss: 25.28579 - diff: 14.90mlTrain batch 5/32 - 108.2ms/batch - loss: 26.54292 - diff: 15.91mlTrain batch 6/32 - 110.2ms/batch - loss: 28.62580 - diff: 16.78mlTrain batch 7/32 - 107.9ms/batch - loss: 25.37169 - diff: 15.50mlTrain batch 8/32 - 108.8ms/batch - loss: 23.49271 - diff: 14.88mlTrain batch 9/32 - 108.0ms/batch - loss: 22.64660 - diff: 14.69mlTrain batch 10/32 - 108.4ms/batch - loss: 21.68219 - diff: 14.41mlTrain batch 11/32 - 107.6ms/batch - loss: 24.93845 - diff: 15.46mlTrain batch 12/32 - 103.9ms/batch - loss: 24.69762 - diff: 15.40mlTrain batch 13/32 - 107.2ms/batch - loss: 31.94703 - diff: 16.96mlTrain batch 14/32 - 103.6ms/batch - loss: 30.16914 - diff: 16.32mlTrain batch 15/32 - 103.6ms/batch - loss: 29.10486 - diff: 16.06mlTrain batch 16/32 - 104.4ms/batch - loss: 28.49356 - diff: 16.02mlTrain batch 17/32 - 131.6ms/batch - loss: 27.67494 - diff: 15.76mlTrain batch 18/32 - 90.5ms/batch - loss: 27.35083 - diff: 15.77mlTrain batch 19/32 - 94.1ms/batch - loss: 27.49110 - diff: 15.85mlTrain batch 20/32 - 106.0ms/batch - loss: 26.37373 - diff: 15.44mlTrain batch 21/32 - 105.0ms/batch - loss: 26.51671 - diff: 15.54mlTrain batch 22/32 - 106.8ms/batch - loss: 26.15395 - diff: 15.54mlTrain batch 23/32 - 112.3ms/batch - loss: 25.40112 - diff: 15.24mlTrain batch 24/32 - 114.1ms/batch - loss: 26.06672 - diff: 15.51mlTrain batch 25/32 - 113.8ms/batch - loss: 25.62911 - diff: 15.41mlTrain batch 26/32 - 113.7ms/batch - loss: 25.46776 - diff: 15.46mlTrain batch 27/32 - 113.3ms/batch - loss: 25.04648 - diff: 15.34mlTrain batch 28/32 - 108.4ms/batch - loss: 24.60505 - diff: 15.21mlTrain batch 29/32 - 107.4ms/batch - loss: 24.51141 - diff: 15.24mlTrain batch 30/32 - 107.3ms/batch - loss: 24.01730 - diff: 15.06mlTrain batch 31/32 - 100.5ms/batch - loss: 23.63075 - diff: 14.95mlTrain batch 32/32 - 82.9ms/batch - loss: 24.08445 - diff: 14.95mlTrain batch 32/32 - 11.9s 82.9ms/batch - loss: 24.08445 - diff: 14.95ml
Test 0.5s: val_loss: 47.16553 - diff: 20.41ml

Epoch 101: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 119.4ms/batch - loss: 21.47314 - diff: 14.36mlTrain batch 2/32 - 105.0ms/batch - loss: 16.40808 - diff: 12.31mlTrain batch 3/32 - 104.1ms/batch - loss: 15.87848 - diff: 12.34mlTrain batch 4/32 - 109.1ms/batch - loss: 17.70177 - diff: 13.50mlTrain batch 5/32 - 98.1ms/batch - loss: 19.03350 - diff: 13.93mlTrain batch 6/32 - 108.5ms/batch - loss: 17.25349 - diff: 13.21mlTrain batch 7/32 - 115.7ms/batch - loss: 18.88456 - diff: 14.00mlTrain batch 8/32 - 107.4ms/batch - loss: 18.57692 - diff: 13.91mlTrain batch 9/32 - 115.5ms/batch - loss: 18.27257 - diff: 13.94mlTrain batch 10/32 - 106.6ms/batch - loss: 21.14676 - diff: 14.90mlTrain batch 11/32 - 110.2ms/batch - loss: 20.15928 - diff: 14.40mlTrain batch 12/32 - 97.8ms/batch - loss: 20.91649 - diff: 14.73mlTrain batch 13/32 - 108.0ms/batch - loss: 20.91213 - diff: 14.68mlTrain batch 14/32 - 106.4ms/batch - loss: 20.44439 - diff: 14.53mlTrain batch 15/32 - 111.4ms/batch - loss: 19.69841 - diff: 14.26mlTrain batch 16/32 - 107.8ms/batch - loss: 19.82131 - diff: 14.28mlTrain batch 17/32 - 94.7ms/batch - loss: 19.73115 - diff: 14.18mlTrain batch 18/32 - 103.7ms/batch - loss: 20.07520 - diff: 14.25mlTrain batch 19/32 - 111.1ms/batch - loss: 19.64650 - diff: 14.04mlTrain batch 20/32 - 102.4ms/batch - loss: 20.47215 - diff: 14.24mlTrain batch 21/32 - 104.6ms/batch - loss: 20.77461 - diff: 14.45mlTrain batch 22/32 - 102.7ms/batch - loss: 20.89691 - diff: 14.44mlTrain batch 23/32 - 111.4ms/batch - loss: 20.66429 - diff: 14.40mlTrain batch 24/32 - 103.6ms/batch - loss: 21.37151 - diff: 14.70mlTrain batch 25/32 - 108.8ms/batch - loss: 21.45147 - diff: 14.70mlTrain batch 26/32 - 103.6ms/batch - loss: 21.20486 - diff: 14.65mlTrain batch 27/32 - 108.1ms/batch - loss: 20.91857 - diff: 14.58mlTrain batch 28/32 - 103.5ms/batch - loss: 21.10768 - diff: 14.60mlTrain batch 29/32 - 106.0ms/batch - loss: 20.99042 - diff: 14.55mlTrain batch 30/32 - 103.2ms/batch - loss: 20.68511 - diff: 14.44mlTrain batch 31/32 - 80.7ms/batch - loss: 20.87030 - diff: 14.48mlTrain batch 32/32 - 66.4ms/batch - loss: 20.90730 - diff: 14.42mlTrain batch 32/32 - 10.8s 66.4ms/batch - loss: 20.90730 - diff: 14.42ml
Test 0.6s: val_loss: 47.72334 - diff: 20.62ml

Epoch 102: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 139.5ms/batch - loss: 12.11960 - diff: 9.66mlTrain batch 2/32 - 107.6ms/batch - loss: 13.04500 - diff: 10.65mlTrain batch 3/32 - 114.8ms/batch - loss: 19.08868 - diff: 13.81mlTrain batch 4/32 - 102.8ms/batch - loss: 25.35085 - diff: 16.45mlTrain batch 5/32 - 110.6ms/batch - loss: 23.90751 - diff: 15.82mlTrain batch 6/32 - 104.4ms/batch - loss: 21.31372 - diff: 14.80mlTrain batch 7/32 - 110.2ms/batch - loss: 20.26155 - diff: 14.36mlTrain batch 8/32 - 104.0ms/batch - loss: 20.32531 - diff: 14.62mlTrain batch 9/32 - 104.3ms/batch - loss: 22.07541 - diff: 15.20mlTrain batch 10/32 - 102.7ms/batch - loss: 20.60412 - diff: 14.48mlTrain batch 11/32 - 95.5ms/batch - loss: 20.72241 - diff: 14.40mlTrain batch 12/32 - 87.6ms/batch - loss: 20.41195 - diff: 14.45mlTrain batch 13/32 - 123.0ms/batch - loss: 19.56166 - diff: 14.07mlTrain batch 14/32 - 114.9ms/batch - loss: 20.38314 - diff: 14.26mlTrain batch 15/32 - 118.8ms/batch - loss: 20.10967 - diff: 14.09mlTrain batch 16/32 - 118.7ms/batch - loss: 22.68404 - diff: 14.87mlTrain batch 17/32 - 115.9ms/batch - loss: 22.04508 - diff: 14.68mlTrain batch 18/32 - 113.8ms/batch - loss: 21.81262 - diff: 14.46mlTrain batch 19/32 - 106.6ms/batch - loss: 21.15188 - diff: 14.21mlTrain batch 20/32 - 107.0ms/batch - loss: 22.27622 - diff: 14.39mlTrain batch 21/32 - 108.2ms/batch - loss: 21.47846 - diff: 14.07mlTrain batch 22/32 - 106.8ms/batch - loss: 21.27982 - diff: 14.09mlTrain batch 23/32 - 108.2ms/batch - loss: 20.69859 - diff: 13.85mlTrain batch 24/32 - 109.3ms/batch - loss: 20.39008 - diff: 13.73mlTrain batch 25/32 - 107.0ms/batch - loss: 20.49874 - diff: 13.77mlTrain batch 26/32 - 105.7ms/batch - loss: 20.29676 - diff: 13.75mlTrain batch 27/32 - 106.8ms/batch - loss: 19.98317 - diff: 13.65mlTrain batch 28/32 - 106.9ms/batch - loss: 19.68934 - diff: 13.57mlTrain batch 29/32 - 112.7ms/batch - loss: 19.37728 - diff: 13.48mlTrain batch 30/32 - 107.3ms/batch - loss: 20.04147 - diff: 13.76mlTrain batch 31/32 - 102.7ms/batch - loss: 19.89472 - diff: 13.68mlTrain batch 32/32 - 89.9ms/batch - loss: 20.88980 - diff: 13.74mlTrain batch 32/32 - 11.0s 89.9ms/batch - loss: 20.88980 - diff: 13.74ml
Test 0.6s: val_loss: 50.15089 - diff: 21.08ml

Epoch 103: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 121.5ms/batch - loss: 16.30459 - diff: 13.12mlTrain batch 2/32 - 106.5ms/batch - loss: 15.39373 - diff: 13.18mlTrain batch 3/32 - 106.9ms/batch - loss: 13.98408 - diff: 12.30mlTrain batch 4/32 - 107.4ms/batch - loss: 14.85352 - diff: 12.78mlTrain batch 5/32 - 106.8ms/batch - loss: 13.79988 - diff: 12.00mlTrain batch 6/32 - 107.8ms/batch - loss: 26.86666 - diff: 15.93mlTrain batch 7/32 - 107.1ms/batch - loss: 27.54201 - diff: 16.22mlTrain batch 8/32 - 106.6ms/batch - loss: 25.63699 - diff: 15.67mlTrain batch 9/32 - 106.9ms/batch - loss: 23.66305 - diff: 14.97mlTrain batch 10/32 - 106.2ms/batch - loss: 22.55697 - diff: 14.66mlTrain batch 11/32 - 107.0ms/batch - loss: 21.40970 - diff: 14.19mlTrain batch 12/32 - 105.8ms/batch - loss: 21.59196 - diff: 14.40mlTrain batch 13/32 - 99.6ms/batch - loss: 21.22811 - diff: 14.32mlTrain batch 14/32 - 106.3ms/batch - loss: 20.92183 - diff: 14.30mlTrain batch 15/32 - 107.1ms/batch - loss: 20.35371 - diff: 14.14mlTrain batch 16/32 - 107.5ms/batch - loss: 19.74772 - diff: 13.91mlTrain batch 17/32 - 116.7ms/batch - loss: 19.44352 - diff: 13.88mlTrain batch 18/32 - 113.7ms/batch - loss: 19.42782 - diff: 13.91mlTrain batch 19/32 - 115.0ms/batch - loss: 18.85862 - diff: 13.61mlTrain batch 20/32 - 120.6ms/batch - loss: 18.94061 - diff: 13.64mlTrain batch 21/32 - 115.8ms/batch - loss: 18.48406 - diff: 13.47mlTrain batch 22/32 - 115.5ms/batch - loss: 18.88804 - diff: 13.65mlTrain batch 23/32 - 116.5ms/batch - loss: 19.57410 - diff: 13.96mlTrain batch 24/32 - 116.4ms/batch - loss: 19.06813 - diff: 13.73mlTrain batch 25/32 - 115.6ms/batch - loss: 18.90749 - diff: 13.64mlTrain batch 26/32 - 114.3ms/batch - loss: 19.58782 - diff: 13.92mlTrain batch 27/32 - 115.9ms/batch - loss: 19.80031 - diff: 13.94mlTrain batch 28/32 - 115.6ms/batch - loss: 19.42102 - diff: 13.81mlTrain batch 29/32 - 117.9ms/batch - loss: 19.37554 - diff: 13.79mlTrain batch 30/32 - 114.6ms/batch - loss: 19.12720 - diff: 13.71mlTrain batch 31/32 - 112.2ms/batch - loss: 18.77276 - diff: 13.54mlTrain batch 32/32 - 77.2ms/batch - loss: 19.38841 - diff: 13.57mlTrain batch 32/32 - 10.8s 77.2ms/batch - loss: 19.38841 - diff: 13.57ml
Test 0.6s: val_loss: 51.23866 - diff: 20.81ml

Epoch 104: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 129.1ms/batch - loss: 20.32164 - diff: 13.36mlTrain batch 2/32 - 113.5ms/batch - loss: 12.50074 - diff: 10.17mlTrain batch 3/32 - 112.5ms/batch - loss: 20.55980 - diff: 13.45mlTrain batch 4/32 - 114.0ms/batch - loss: 17.77958 - diff: 12.65mlTrain batch 5/32 - 112.5ms/batch - loss: 17.04092 - diff: 12.56mlTrain batch 6/32 - 112.5ms/batch - loss: 15.68160 - diff: 12.03mlTrain batch 7/32 - 112.2ms/batch - loss: 17.05973 - diff: 12.58mlTrain batch 8/32 - 107.2ms/batch - loss: 17.40331 - diff: 12.56mlTrain batch 9/32 - 109.7ms/batch - loss: 16.15525 - diff: 12.02mlTrain batch 10/32 - 107.8ms/batch - loss: 20.12463 - diff: 13.32mlTrain batch 11/32 - 101.6ms/batch - loss: 20.53567 - diff: 13.59mlTrain batch 12/32 - 92.7ms/batch - loss: 20.68019 - diff: 13.64mlTrain batch 13/32 - 82.1ms/batch - loss: 20.10988 - diff: 13.29mlTrain batch 14/32 - 108.6ms/batch - loss: 24.57592 - diff: 14.67mlTrain batch 15/32 - 91.1ms/batch - loss: 25.94647 - diff: 15.30mlTrain batch 16/32 - 106.5ms/batch - loss: 25.04535 - diff: 15.10mlTrain batch 17/32 - 107.8ms/batch - loss: 24.88220 - diff: 15.10mlTrain batch 18/32 - 91.9ms/batch - loss: 24.12946 - diff: 14.87mlTrain batch 19/32 - 100.9ms/batch - loss: 26.13930 - diff: 15.64mlTrain batch 20/32 - 106.4ms/batch - loss: 25.38455 - diff: 15.44mlTrain batch 21/32 - 109.0ms/batch - loss: 26.54853 - diff: 15.96mlTrain batch 22/32 - 106.3ms/batch - loss: 27.05118 - diff: 16.19mlTrain batch 23/32 - 107.2ms/batch - loss: 28.17866 - diff: 16.68mlTrain batch 24/32 - 107.3ms/batch - loss: 27.72531 - diff: 16.57mlTrain batch 25/32 - 106.2ms/batch - loss: 28.29230 - diff: 16.82mlTrain batch 26/32 - 103.0ms/batch - loss: 28.23737 - diff: 16.79mlTrain batch 27/32 - 103.1ms/batch - loss: 27.59545 - diff: 16.55mlTrain batch 28/32 - 114.2ms/batch - loss: 27.02248 - diff: 16.38mlTrain batch 29/32 - 113.8ms/batch - loss: 28.16769 - diff: 16.82mlTrain batch 30/32 - 102.8ms/batch - loss: 27.40955 - diff: 16.51mlTrain batch 31/32 - 97.3ms/batch - loss: 26.92287 - diff: 16.34mlTrain batch 32/32 - 79.0ms/batch - loss: 27.83325 - diff: 16.37mlTrain batch 32/32 - 11.5s 79.0ms/batch - loss: 27.83325 - diff: 16.37ml
Test 0.7s: val_loss: 47.21099 - diff: 20.99ml

Epoch 105: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 142.1ms/batch - loss: 17.97867 - diff: 14.29mlTrain batch 2/32 - 115.3ms/batch - loss: 14.16393 - diff: 12.82mlTrain batch 3/32 - 116.9ms/batch - loss: 16.16014 - diff: 13.51mlTrain batch 4/32 - 115.3ms/batch - loss: 14.99933 - diff: 12.97mlTrain batch 5/32 - 112.3ms/batch - loss: 14.30094 - diff: 12.67mlTrain batch 6/32 - 104.9ms/batch - loss: 17.08703 - diff: 13.69mlTrain batch 7/32 - 107.9ms/batch - loss: 18.05316 - diff: 14.11mlTrain batch 8/32 - 104.0ms/batch - loss: 16.82810 - diff: 13.45mlTrain batch 9/32 - 109.3ms/batch - loss: 17.27882 - diff: 13.69mlTrain batch 10/32 - 104.1ms/batch - loss: 17.53194 - diff: 13.89mlTrain batch 11/32 - 111.2ms/batch - loss: 17.29538 - diff: 13.77mlTrain batch 12/32 - 103.8ms/batch - loss: 18.80270 - diff: 14.38mlTrain batch 13/32 - 111.7ms/batch - loss: 18.22293 - diff: 14.16mlTrain batch 14/32 - 107.5ms/batch - loss: 17.34014 - diff: 13.67mlTrain batch 15/32 - 111.5ms/batch - loss: 17.18198 - diff: 13.55mlTrain batch 16/32 - 104.1ms/batch - loss: 18.05299 - diff: 13.96mlTrain batch 17/32 - 107.9ms/batch - loss: 19.20939 - diff: 14.42mlTrain batch 18/32 - 106.4ms/batch - loss: 19.14257 - diff: 14.41mlTrain batch 19/32 - 117.0ms/batch - loss: 18.96172 - diff: 14.30mlTrain batch 20/32 - 116.3ms/batch - loss: 19.65654 - diff: 14.43mlTrain batch 21/32 - 110.9ms/batch - loss: 19.53048 - diff: 14.44mlTrain batch 22/32 - 91.9ms/batch - loss: 20.05363 - diff: 14.72mlTrain batch 23/32 - 116.4ms/batch - loss: 20.41944 - diff: 14.86mlTrain batch 24/32 - 115.5ms/batch - loss: 20.85067 - diff: 15.05mlTrain batch 25/32 - 110.0ms/batch - loss: 22.19832 - diff: 15.52mlTrain batch 26/32 - 103.6ms/batch - loss: 21.68865 - diff: 15.28mlTrain batch 27/32 - 105.9ms/batch - loss: 21.88954 - diff: 15.36mlTrain batch 28/32 - 117.2ms/batch - loss: 21.53355 - diff: 15.19mlTrain batch 29/32 - 117.7ms/batch - loss: 21.76060 - diff: 15.32mlTrain batch 30/32 - 116.9ms/batch - loss: 21.61988 - diff: 15.26mlTrain batch 31/32 - 99.0ms/batch - loss: 21.43379 - diff: 15.18mlTrain batch 32/32 - 98.7ms/batch - loss: 23.85600 - diff: 15.33mlTrain batch 32/32 - 10.8s 98.7ms/batch - loss: 23.85600 - diff: 15.33ml
Test 0.7s: val_loss: 47.67195 - diff: 21.44ml

Epoch 106: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 121.0ms/batch - loss: 8.31421 - diff: 9.04mlTrain batch 2/32 - 107.9ms/batch - loss: 7.26961 - diff: 8.85mlTrain batch 3/32 - 107.2ms/batch - loss: 10.11067 - diff: 10.06mlTrain batch 4/32 - 95.3ms/batch - loss: 26.14285 - diff: 13.77mlTrain batch 5/32 - 104.6ms/batch - loss: 26.07794 - diff: 14.19mlTrain batch 6/32 - 104.2ms/batch - loss: 23.51494 - diff: 13.57mlTrain batch 7/32 - 104.7ms/batch - loss: 21.27490 - diff: 12.86mlTrain batch 8/32 - 103.3ms/batch - loss: 20.37913 - diff: 12.90mlTrain batch 9/32 - 102.6ms/batch - loss: 21.05201 - diff: 13.31mlTrain batch 10/32 - 103.5ms/batch - loss: 20.26229 - diff: 13.14mlTrain batch 11/32 - 106.8ms/batch - loss: 19.00247 - diff: 12.74mlTrain batch 12/32 - 116.5ms/batch - loss: 18.14638 - diff: 12.47mlTrain batch 13/32 - 109.9ms/batch - loss: 17.59214 - diff: 12.34mlTrain batch 14/32 - 111.8ms/batch - loss: 18.52383 - diff: 12.76mlTrain batch 15/32 - 116.3ms/batch - loss: 19.51829 - diff: 13.07mlTrain batch 16/32 - 118.1ms/batch - loss: 19.39588 - diff: 13.08mlTrain batch 17/32 - 97.6ms/batch - loss: 20.26312 - diff: 13.47mlTrain batch 18/32 - 106.0ms/batch - loss: 19.76920 - diff: 13.32mlTrain batch 19/32 - 128.8ms/batch - loss: 19.17697 - diff: 13.11mlTrain batch 20/32 - 114.4ms/batch - loss: 21.18341 - diff: 13.69mlTrain batch 21/32 - 116.9ms/batch - loss: 20.63810 - diff: 13.54mlTrain batch 22/32 - 114.1ms/batch - loss: 20.96343 - diff: 13.74mlTrain batch 23/32 - 115.4ms/batch - loss: 20.70269 - diff: 13.66mlTrain batch 24/32 - 122.2ms/batch - loss: 20.24342 - diff: 13.47mlTrain batch 25/32 - 114.1ms/batch - loss: 20.84016 - diff: 13.77mlTrain batch 26/32 - 115.0ms/batch - loss: 20.66483 - diff: 13.71mlTrain batch 27/32 - 103.5ms/batch - loss: 20.65371 - diff: 13.71mlTrain batch 28/32 - 99.0ms/batch - loss: 21.11290 - diff: 13.94mlTrain batch 29/32 - 108.9ms/batch - loss: 21.55796 - diff: 14.17mlTrain batch 30/32 - 110.2ms/batch - loss: 21.21895 - diff: 14.08mlTrain batch 31/32 - 90.6ms/batch - loss: 20.85261 - diff: 13.94mlTrain batch 32/32 - 84.0ms/batch - loss: 21.90835 - diff: 14.01mlTrain batch 32/32 - 12.7s 84.0ms/batch - loss: 21.90835 - diff: 14.01ml
Test 0.6s: val_loss: 48.96578 - diff: 20.70ml

Epoch 107: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 122.0ms/batch - loss: 16.38779 - diff: 14.20mlTrain batch 2/32 - 106.8ms/batch - loss: 24.34293 - diff: 16.25mlTrain batch 3/32 - 120.7ms/batch - loss: 21.84285 - diff: 14.08mlTrain batch 4/32 - 120.4ms/batch - loss: 23.99050 - diff: 15.02mlTrain batch 5/32 - 123.7ms/batch - loss: 25.59851 - diff: 15.47mlTrain batch 6/32 - 117.1ms/batch - loss: 23.97467 - diff: 15.15mlTrain batch 7/32 - 109.0ms/batch - loss: 23.08250 - diff: 14.90mlTrain batch 8/32 - 103.7ms/batch - loss: 22.12615 - diff: 14.69mlTrain batch 9/32 - 87.7ms/batch - loss: 22.40030 - diff: 15.10mlTrain batch 10/32 - 104.5ms/batch - loss: 21.41801 - diff: 14.63mlTrain batch 11/32 - 87.9ms/batch - loss: 20.71289 - diff: 14.43mlTrain batch 12/32 - 107.7ms/batch - loss: 21.38125 - diff: 14.45mlTrain batch 13/32 - 103.5ms/batch - loss: 21.23513 - diff: 14.46mlTrain batch 14/32 - 106.1ms/batch - loss: 23.99304 - diff: 15.43mlTrain batch 15/32 - 103.0ms/batch - loss: 23.14054 - diff: 15.11mlTrain batch 16/32 - 104.1ms/batch - loss: 22.82776 - diff: 14.96mlTrain batch 17/32 - 104.8ms/batch - loss: 22.82889 - diff: 14.92mlTrain batch 18/32 - 113.3ms/batch - loss: 22.12059 - diff: 14.69mlTrain batch 19/32 - 107.9ms/batch - loss: 21.41452 - diff: 14.43mlTrain batch 20/32 - 118.3ms/batch - loss: 20.75448 - diff: 14.15mlTrain batch 21/32 - 106.2ms/batch - loss: 20.87228 - diff: 14.26mlTrain batch 22/32 - 108.7ms/batch - loss: 20.81822 - diff: 14.29mlTrain batch 23/32 - 106.3ms/batch - loss: 22.91977 - diff: 14.89mlTrain batch 24/32 - 108.4ms/batch - loss: 22.63455 - diff: 14.80mlTrain batch 25/32 - 106.2ms/batch - loss: 22.29472 - diff: 14.69mlTrain batch 26/32 - 108.1ms/batch - loss: 21.94797 - diff: 14.60mlTrain batch 27/32 - 107.0ms/batch - loss: 22.07566 - diff: 14.64mlTrain batch 28/32 - 113.9ms/batch - loss: 22.26248 - diff: 14.74mlTrain batch 29/32 - 108.4ms/batch - loss: 22.11702 - diff: 14.67mlTrain batch 30/32 - 106.5ms/batch - loss: 21.83184 - diff: 14.58mlTrain batch 31/32 - 92.0ms/batch - loss: 21.71535 - diff: 14.57mlTrain batch 32/32 - 69.3ms/batch - loss: 22.09408 - diff: 14.55mlTrain batch 32/32 - 12.0s 69.3ms/batch - loss: 22.09408 - diff: 14.55ml
Test 0.6s: val_loss: 47.23904 - diff: 20.62ml

Epoch 108: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 127.3ms/batch - loss: 17.25763 - diff: 13.08mlTrain batch 2/32 - 112.5ms/batch - loss: 13.03012 - diff: 11.15mlTrain batch 3/32 - 107.9ms/batch - loss: 28.30230 - diff: 16.77mlTrain batch 4/32 - 108.7ms/batch - loss: 28.75243 - diff: 17.40mlTrain batch 5/32 - 107.7ms/batch - loss: 29.85699 - diff: 17.79mlTrain batch 6/32 - 107.8ms/batch - loss: 25.66382 - diff: 15.84mlTrain batch 7/32 - 107.2ms/batch - loss: 24.15281 - diff: 15.47mlTrain batch 8/32 - 107.0ms/batch - loss: 22.50938 - diff: 14.89mlTrain batch 9/32 - 116.3ms/batch - loss: 23.73152 - diff: 15.43mlTrain batch 10/32 - 107.6ms/batch - loss: 24.52901 - diff: 15.78mlTrain batch 11/32 - 108.5ms/batch - loss: 26.17081 - diff: 16.53mlTrain batch 12/32 - 104.0ms/batch - loss: 24.84689 - diff: 16.00mlTrain batch 13/32 - 104.3ms/batch - loss: 24.75198 - diff: 15.87mlTrain batch 14/32 - 104.3ms/batch - loss: 25.85709 - diff: 16.22mlTrain batch 15/32 - 103.6ms/batch - loss: 25.04683 - diff: 15.92mlTrain batch 16/32 - 103.9ms/batch - loss: 26.43938 - diff: 16.47mlTrain batch 17/32 - 117.1ms/batch - loss: 28.50209 - diff: 17.05mlTrain batch 18/32 - 93.2ms/batch - loss: 27.99506 - diff: 16.88mlTrain batch 19/32 - 108.3ms/batch - loss: 30.01428 - diff: 17.57mlTrain batch 20/32 - 104.7ms/batch - loss: 31.85862 - diff: 18.06mlTrain batch 21/32 - 113.8ms/batch - loss: 30.97267 - diff: 17.80mlTrain batch 22/32 - 113.4ms/batch - loss: 29.91114 - diff: 17.35mlTrain batch 23/32 - 108.4ms/batch - loss: 29.01323 - diff: 17.01mlTrain batch 24/32 - 106.4ms/batch - loss: 28.39039 - diff: 16.77mlTrain batch 25/32 - 111.8ms/batch - loss: 27.65392 - diff: 16.49mlTrain batch 26/32 - 102.6ms/batch - loss: 27.14390 - diff: 16.31mlTrain batch 27/32 - 114.3ms/batch - loss: 26.74772 - diff: 16.24mlTrain batch 28/32 - 114.3ms/batch - loss: 27.00243 - diff: 16.41mlTrain batch 29/32 - 103.8ms/batch - loss: 26.79917 - diff: 16.29mlTrain batch 30/32 - 105.0ms/batch - loss: 27.41511 - diff: 16.46mlTrain batch 31/32 - 104.6ms/batch - loss: 27.00762 - diff: 16.30mlTrain batch 32/32 - 85.2ms/batch - loss: 27.38420 - diff: 16.27mlTrain batch 32/32 - 11.0s 85.2ms/batch - loss: 27.38420 - diff: 16.27ml
Test 0.7s: val_loss: 50.20496 - diff: 21.34ml
Epoch   109: reducing learning rate of group 0 to 1.2500e-04.

Epoch 109: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 128.7ms/batch - loss: 70.96313 - diff: 23.30mlTrain batch 2/32 - 116.0ms/batch - loss: 43.39845 - diff: 17.98mlTrain batch 3/32 - 104.1ms/batch - loss: 34.98499 - diff: 16.90mlTrain batch 4/32 - 104.6ms/batch - loss: 30.99368 - diff: 16.14mlTrain batch 5/32 - 104.1ms/batch - loss: 28.00450 - diff: 15.24mlTrain batch 6/32 - 103.5ms/batch - loss: 26.50962 - diff: 15.10mlTrain batch 7/32 - 104.5ms/batch - loss: 25.75231 - diff: 15.04mlTrain batch 8/32 - 113.4ms/batch - loss: 23.41723 - diff: 14.17mlTrain batch 9/32 - 107.0ms/batch - loss: 21.79641 - diff: 13.70mlTrain batch 10/32 - 99.0ms/batch - loss: 22.35644 - diff: 14.00mlTrain batch 11/32 - 106.7ms/batch - loss: 21.80462 - diff: 13.99mlTrain batch 12/32 - 117.4ms/batch - loss: 20.62501 - diff: 13.55mlTrain batch 13/32 - 113.9ms/batch - loss: 22.05319 - diff: 14.27mlTrain batch 14/32 - 115.4ms/batch - loss: 22.03417 - diff: 14.38mlTrain batch 15/32 - 111.0ms/batch - loss: 21.52358 - diff: 14.15mlTrain batch 16/32 - 110.9ms/batch - loss: 21.79948 - diff: 14.39mlTrain batch 17/32 - 90.4ms/batch - loss: 21.31173 - diff: 14.29mlTrain batch 18/32 - 123.8ms/batch - loss: 20.72356 - diff: 14.05mlTrain batch 19/32 - 118.1ms/batch - loss: 20.51516 - diff: 13.91mlTrain batch 20/32 - 121.2ms/batch - loss: 20.16628 - diff: 13.89mlTrain batch 21/32 - 126.5ms/batch - loss: 19.50557 - diff: 13.60mlTrain batch 22/32 - 124.3ms/batch - loss: 19.43783 - diff: 13.63mlTrain batch 23/32 - 117.6ms/batch - loss: 19.09675 - diff: 13.56mlTrain batch 24/32 - 124.1ms/batch - loss: 21.37601 - diff: 14.32mlTrain batch 25/32 - 117.7ms/batch - loss: 21.29442 - diff: 14.31mlTrain batch 26/32 - 107.8ms/batch - loss: 20.97340 - diff: 14.22mlTrain batch 27/32 - 113.9ms/batch - loss: 20.59070 - diff: 14.07mlTrain batch 28/32 - 115.7ms/batch - loss: 20.33717 - diff: 14.03mlTrain batch 29/32 - 105.6ms/batch - loss: 20.41176 - diff: 14.09mlTrain batch 30/32 - 104.4ms/batch - loss: 19.99385 - diff: 13.88mlTrain batch 31/32 - 102.9ms/batch - loss: 20.77897 - diff: 14.17mlTrain batch 32/32 - 80.4ms/batch - loss: 21.17803 - diff: 14.17mlTrain batch 32/32 - 11.3s 80.4ms/batch - loss: 21.17803 - diff: 14.17ml
Test 0.6s: val_loss: 46.47400 - diff: 20.67ml

Epoch 110: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 122.1ms/batch - loss: 20.52694 - diff: 16.46mlTrain batch 2/32 - 109.7ms/batch - loss: 21.22712 - diff: 16.55mlTrain batch 3/32 - 108.8ms/batch - loss: 17.41528 - diff: 14.65mlTrain batch 4/32 - 108.0ms/batch - loss: 17.96330 - diff: 14.78mlTrain batch 5/32 - 107.9ms/batch - loss: 17.26372 - diff: 14.35mlTrain batch 6/32 - 107.9ms/batch - loss: 29.25124 - diff: 17.83mlTrain batch 7/32 - 110.7ms/batch - loss: 28.11940 - diff: 17.53mlTrain batch 8/32 - 108.7ms/batch - loss: 25.59903 - diff: 16.47mlTrain batch 9/32 - 98.4ms/batch - loss: 23.98579 - diff: 15.79mlTrain batch 10/32 - 103.1ms/batch - loss: 22.25852 - diff: 15.07mlTrain batch 11/32 - 120.0ms/batch - loss: 20.92982 - diff: 14.53mlTrain batch 12/32 - 117.5ms/batch - loss: 23.45328 - diff: 15.43mlTrain batch 13/32 - 117.0ms/batch - loss: 24.70320 - diff: 15.87mlTrain batch 14/32 - 117.3ms/batch - loss: 23.79530 - diff: 15.61mlTrain batch 15/32 - 121.3ms/batch - loss: 22.62862 - diff: 15.05mlTrain batch 16/32 - 124.1ms/batch - loss: 23.50605 - diff: 15.26mlTrain batch 17/32 - 110.1ms/batch - loss: 27.22492 - diff: 16.19mlTrain batch 18/32 - 102.2ms/batch - loss: 28.86445 - diff: 16.87mlTrain batch 19/32 - 115.5ms/batch - loss: 28.07280 - diff: 16.58mlTrain batch 20/32 - 111.9ms/batch - loss: 27.18497 - diff: 16.29mlTrain batch 21/32 - 105.8ms/batch - loss: 26.78143 - diff: 16.25mlTrain batch 22/32 - 103.8ms/batch - loss: 27.50511 - diff: 16.43mlTrain batch 23/32 - 111.2ms/batch - loss: 27.56441 - diff: 16.51mlTrain batch 24/32 - 105.6ms/batch - loss: 26.64082 - diff: 16.16mlTrain batch 25/32 - 114.2ms/batch - loss: 25.99195 - diff: 15.91mlTrain batch 26/32 - 118.1ms/batch - loss: 25.40968 - diff: 15.72mlTrain batch 27/32 - 116.3ms/batch - loss: 25.26742 - diff: 15.75mlTrain batch 28/32 - 119.0ms/batch - loss: 24.89302 - diff: 15.64mlTrain batch 29/32 - 112.5ms/batch - loss: 24.50551 - diff: 15.43mlTrain batch 30/32 - 111.6ms/batch - loss: 25.38159 - diff: 15.77mlTrain batch 31/32 - 112.4ms/batch - loss: 25.86413 - diff: 15.99mlTrain batch 32/32 - 108.6ms/batch - loss: 26.18072 - diff: 15.98mlTrain batch 32/32 - 12.3s 108.6ms/batch - loss: 26.18072 - diff: 15.98ml
Test 0.7s: val_loss: 45.20380 - diff: 20.51ml

Epoch 111: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 103.9ms/batch - loss: 20.70608 - diff: 15.82mlTrain batch 2/32 - 82.5ms/batch - loss: 18.13812 - diff: 13.92mlTrain batch 3/32 - 109.0ms/batch - loss: 21.95362 - diff: 15.71mlTrain batch 4/32 - 104.5ms/batch - loss: 18.52374 - diff: 14.23mlTrain batch 5/32 - 123.6ms/batch - loss: 16.90932 - diff: 13.55mlTrain batch 6/32 - 117.7ms/batch - loss: 18.72972 - diff: 14.07mlTrain batch 7/32 - 119.2ms/batch - loss: 16.63526 - diff: 13.00mlTrain batch 8/32 - 107.9ms/batch - loss: 16.15745 - diff: 12.84mlTrain batch 9/32 - 108.8ms/batch - loss: 17.14831 - diff: 13.31mlTrain batch 10/32 - 104.4ms/batch - loss: 16.93971 - diff: 13.30mlTrain batch 11/32 - 104.8ms/batch - loss: 17.25066 - diff: 13.53mlTrain batch 12/32 - 107.9ms/batch - loss: 16.97251 - diff: 13.38mlTrain batch 13/32 - 112.7ms/batch - loss: 18.36558 - diff: 13.91mlTrain batch 14/32 - 108.7ms/batch - loss: 18.23987 - diff: 13.87mlTrain batch 15/32 - 101.5ms/batch - loss: 17.99323 - diff: 13.76mlTrain batch 16/32 - 107.3ms/batch - loss: 24.51331 - diff: 15.42mlTrain batch 17/32 - 110.1ms/batch - loss: 24.10995 - diff: 15.38mlTrain batch 18/32 - 106.8ms/batch - loss: 24.32383 - diff: 15.51mlTrain batch 19/32 - 114.6ms/batch - loss: 24.69311 - diff: 15.71mlTrain batch 20/32 - 105.2ms/batch - loss: 24.12321 - diff: 15.53mlTrain batch 21/32 - 114.5ms/batch - loss: 23.46982 - diff: 15.29mlTrain batch 22/32 - 113.7ms/batch - loss: 23.89854 - diff: 15.52mlTrain batch 23/32 - 104.2ms/batch - loss: 23.17145 - diff: 15.25mlTrain batch 24/32 - 103.7ms/batch - loss: 23.12401 - diff: 15.15mlTrain batch 25/32 - 103.5ms/batch - loss: 22.48204 - diff: 14.90mlTrain batch 26/32 - 109.5ms/batch - loss: 22.11251 - diff: 14.82mlTrain batch 27/32 - 118.1ms/batch - loss: 21.78453 - diff: 14.71mlTrain batch 28/32 - 125.8ms/batch - loss: 21.45196 - diff: 14.62mlTrain batch 29/32 - 106.5ms/batch - loss: 21.92066 - diff: 14.85mlTrain batch 30/32 - 107.3ms/batch - loss: 22.58402 - diff: 15.14mlTrain batch 31/32 - 101.8ms/batch - loss: 22.41927 - diff: 15.11mlTrain batch 32/32 - 95.2ms/batch - loss: 23.07674 - diff: 15.14mlTrain batch 32/32 - 11.6s 95.2ms/batch - loss: 23.07674 - diff: 15.14ml
Test 0.6s: val_loss: 49.02873 - diff: 20.66ml

Epoch 112: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 121.4ms/batch - loss: 32.54275 - diff: 20.95mlTrain batch 2/32 - 108.6ms/batch - loss: 24.83890 - diff: 16.92mlTrain batch 3/32 - 109.5ms/batch - loss: 50.70913 - diff: 24.14mlTrain batch 4/32 - 108.7ms/batch - loss: 40.68049 - diff: 20.81mlTrain batch 5/32 - 103.4ms/batch - loss: 40.40782 - diff: 21.16mlTrain batch 6/32 - 110.8ms/batch - loss: 37.54759 - diff: 20.11mlTrain batch 7/32 - 108.6ms/batch - loss: 34.64454 - diff: 19.12mlTrain batch 8/32 - 107.2ms/batch - loss: 31.70433 - diff: 18.06mlTrain batch 9/32 - 109.1ms/batch - loss: 30.64042 - diff: 17.74mlTrain batch 10/32 - 104.8ms/batch - loss: 28.63439 - diff: 16.96mlTrain batch 11/32 - 91.4ms/batch - loss: 26.82759 - diff: 16.30mlTrain batch 12/32 - 91.4ms/batch - loss: 28.78575 - diff: 17.04mlTrain batch 13/32 - 116.8ms/batch - loss: 29.64105 - diff: 17.52mlTrain batch 14/32 - 107.4ms/batch - loss: 28.90264 - diff: 17.25mlTrain batch 15/32 - 103.5ms/batch - loss: 27.77383 - diff: 16.91mlTrain batch 16/32 - 103.8ms/batch - loss: 27.70591 - diff: 16.85mlTrain batch 17/32 - 103.4ms/batch - loss: 26.59555 - diff: 16.45mlTrain batch 18/32 - 103.8ms/batch - loss: 25.68960 - diff: 16.12mlTrain batch 19/32 - 104.3ms/batch - loss: 25.10020 - diff: 15.92mlTrain batch 20/32 - 104.1ms/batch - loss: 24.55234 - diff: 15.68mlTrain batch 21/32 - 103.6ms/batch - loss: 24.28215 - diff: 15.56mlTrain batch 22/32 - 103.8ms/batch - loss: 23.68918 - diff: 15.36mlTrain batch 23/32 - 103.6ms/batch - loss: 23.29088 - diff: 15.18mlTrain batch 24/32 - 103.8ms/batch - loss: 23.30031 - diff: 15.25mlTrain batch 25/32 - 104.9ms/batch - loss: 23.13642 - diff: 15.24mlTrain batch 26/32 - 102.7ms/batch - loss: 22.43731 - diff: 14.93mlTrain batch 27/32 - 104.9ms/batch - loss: 21.88209 - diff: 14.73mlTrain batch 28/32 - 103.7ms/batch - loss: 21.68571 - diff: 14.66mlTrain batch 29/32 - 103.6ms/batch - loss: 21.46257 - diff: 14.58mlTrain batch 30/32 - 103.5ms/batch - loss: 21.40108 - diff: 14.61mlTrain batch 31/32 - 106.9ms/batch - loss: 21.83635 - diff: 14.76mlTrain batch 32/32 - 91.2ms/batch - loss: 22.54741 - diff: 14.78mlTrain batch 32/32 - 11.5s 91.2ms/batch - loss: 22.54741 - diff: 14.78ml
Test 0.6s: val_loss: 45.48156 - diff: 20.78ml

Epoch 113: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 138.0ms/batch - loss: 22.37210 - diff: 14.54mlTrain batch 2/32 - 112.5ms/batch - loss: 29.02706 - diff: 17.37mlTrain batch 3/32 - 113.2ms/batch - loss: 31.03891 - diff: 18.14mlTrain batch 4/32 - 113.5ms/batch - loss: 29.38323 - diff: 17.93mlTrain batch 5/32 - 113.0ms/batch - loss: 27.79997 - diff: 17.23mlTrain batch 6/32 - 114.0ms/batch - loss: 29.30488 - diff: 17.81mlTrain batch 7/32 - 113.2ms/batch - loss: 26.13081 - diff: 16.53mlTrain batch 8/32 - 113.9ms/batch - loss: 24.57136 - diff: 15.82mlTrain batch 9/32 - 112.3ms/batch - loss: 22.78740 - diff: 15.05mlTrain batch 10/32 - 102.8ms/batch - loss: 21.61440 - diff: 14.65mlTrain batch 11/32 - 87.6ms/batch - loss: 21.91880 - diff: 14.76mlTrain batch 12/32 - 104.3ms/batch - loss: 23.54400 - diff: 15.51mlTrain batch 13/32 - 93.2ms/batch - loss: 22.67707 - diff: 15.20mlTrain batch 14/32 - 81.8ms/batch - loss: 21.75099 - diff: 14.91mlTrain batch 15/32 - 83.1ms/batch - loss: 21.40247 - diff: 14.77mlTrain batch 16/32 - 112.4ms/batch - loss: 23.89343 - diff: 15.58mlTrain batch 17/32 - 112.6ms/batch - loss: 24.71741 - diff: 15.93mlTrain batch 18/32 - 104.7ms/batch - loss: 24.33931 - diff: 15.82mlTrain batch 19/32 - 103.5ms/batch - loss: 24.28796 - diff: 15.93mlTrain batch 20/32 - 108.2ms/batch - loss: 23.55963 - diff: 15.60mlTrain batch 21/32 - 109.3ms/batch - loss: 23.33223 - diff: 15.57mlTrain batch 22/32 - 113.3ms/batch - loss: 23.16410 - diff: 15.52mlTrain batch 23/32 - 113.6ms/batch - loss: 23.60093 - diff: 15.65mlTrain batch 24/32 - 113.6ms/batch - loss: 23.35587 - diff: 15.60mlTrain batch 25/32 - 113.5ms/batch - loss: 22.87098 - diff: 15.41mlTrain batch 26/32 - 128.2ms/batch - loss: 22.41293 - diff: 15.23mlTrain batch 27/32 - 118.8ms/batch - loss: 22.15770 - diff: 15.05mlTrain batch 28/32 - 119.7ms/batch - loss: 21.57811 - diff: 14.80mlTrain batch 29/32 - 113.3ms/batch - loss: 21.41867 - diff: 14.77mlTrain batch 30/32 - 82.2ms/batch - loss: 21.54334 - diff: 14.84mlTrain batch 31/32 - 80.9ms/batch - loss: 21.26293 - diff: 14.75mlTrain batch 32/32 - 66.5ms/batch - loss: 23.99094 - diff: 14.90mlTrain batch 32/32 - 11.0s 66.5ms/batch - loss: 23.99094 - diff: 14.90ml
Test 0.6s: val_loss: 46.49779 - diff: 20.14ml

Epoch 114: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 115.1ms/batch - loss: 16.41191 - diff: 11.95mlTrain batch 2/32 - 108.1ms/batch - loss: 21.08280 - diff: 14.98mlTrain batch 3/32 - 107.8ms/batch - loss: 16.84495 - diff: 13.06mlTrain batch 4/32 - 107.4ms/batch - loss: 18.05173 - diff: 13.63mlTrain batch 5/32 - 107.3ms/batch - loss: 16.28039 - diff: 12.91mlTrain batch 6/32 - 104.8ms/batch - loss: 16.21181 - diff: 12.86mlTrain batch 7/32 - 104.8ms/batch - loss: 15.79601 - diff: 12.78mlTrain batch 8/32 - 104.8ms/batch - loss: 16.27119 - diff: 13.06mlTrain batch 9/32 - 103.7ms/batch - loss: 15.49461 - diff: 12.67mlTrain batch 10/32 - 106.8ms/batch - loss: 16.80336 - diff: 12.94mlTrain batch 11/32 - 113.0ms/batch - loss: 16.77119 - diff: 13.07mlTrain batch 12/32 - 117.5ms/batch - loss: 16.62947 - diff: 13.06mlTrain batch 13/32 - 109.6ms/batch - loss: 17.79529 - diff: 13.44mlTrain batch 14/32 - 104.0ms/batch - loss: 17.13423 - diff: 13.14mlTrain batch 15/32 - 103.4ms/batch - loss: 17.16703 - diff: 13.17mlTrain batch 16/32 - 104.0ms/batch - loss: 17.15273 - diff: 13.29mlTrain batch 17/32 - 103.8ms/batch - loss: 20.04366 - diff: 14.30mlTrain batch 18/32 - 104.0ms/batch - loss: 20.60193 - diff: 14.48mlTrain batch 19/32 - 107.8ms/batch - loss: 20.18391 - diff: 14.23mlTrain batch 20/32 - 120.9ms/batch - loss: 23.93716 - diff: 15.41mlTrain batch 21/32 - 107.2ms/batch - loss: 23.02353 - diff: 14.98mlTrain batch 22/32 - 122.2ms/batch - loss: 22.79835 - diff: 14.95mlTrain batch 23/32 - 112.7ms/batch - loss: 23.23457 - diff: 15.21mlTrain batch 24/32 - 119.3ms/batch - loss: 24.25272 - diff: 15.51mlTrain batch 25/32 - 113.1ms/batch - loss: 23.57615 - diff: 15.26mlTrain batch 26/32 - 119.8ms/batch - loss: 23.11551 - diff: 15.04mlTrain batch 27/32 - 109.0ms/batch - loss: 23.53490 - diff: 15.26mlTrain batch 28/32 - 115.9ms/batch - loss: 24.62486 - diff: 15.69mlTrain batch 29/32 - 109.0ms/batch - loss: 24.37279 - diff: 15.60mlTrain batch 30/32 - 106.9ms/batch - loss: 23.93331 - diff: 15.43mlTrain batch 31/32 - 89.6ms/batch - loss: 24.30630 - diff: 15.59mlTrain batch 32/32 - 77.0ms/batch - loss: 25.74584 - diff: 15.69mlTrain batch 32/32 - 11.4s 77.0ms/batch - loss: 25.74584 - diff: 15.69ml
Test 0.7s: val_loss: 46.53621 - diff: 20.32ml

Epoch 115: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 133.3ms/batch - loss: 9.63198 - diff: 10.65mlTrain batch 2/32 - 102.0ms/batch - loss: 25.39714 - diff: 17.18mlTrain batch 3/32 - 112.8ms/batch - loss: 19.92300 - diff: 14.70mlTrain batch 4/32 - 113.2ms/batch - loss: 16.48225 - diff: 12.92mlTrain batch 5/32 - 112.4ms/batch - loss: 17.16586 - diff: 13.22mlTrain batch 6/32 - 105.2ms/batch - loss: 15.53367 - diff: 12.55mlTrain batch 7/32 - 104.2ms/batch - loss: 14.96721 - diff: 12.32mlTrain batch 8/32 - 104.1ms/batch - loss: 13.54660 - diff: 11.49mlTrain batch 9/32 - 103.8ms/batch - loss: 14.27531 - diff: 11.80mlTrain batch 10/32 - 104.0ms/batch - loss: 14.18015 - diff: 11.81mlTrain batch 11/32 - 104.7ms/batch - loss: 15.19827 - diff: 12.24mlTrain batch 12/32 - 117.9ms/batch - loss: 15.34902 - diff: 12.34mlTrain batch 13/32 - 112.8ms/batch - loss: 14.60117 - diff: 11.99mlTrain batch 14/32 - 107.9ms/batch - loss: 14.17415 - diff: 11.86mlTrain batch 15/32 - 106.2ms/batch - loss: 14.83406 - diff: 12.20mlTrain batch 16/32 - 108.0ms/batch - loss: 14.16785 - diff: 11.87mlTrain batch 17/32 - 107.4ms/batch - loss: 13.87277 - diff: 11.72mlTrain batch 18/32 - 107.6ms/batch - loss: 13.90035 - diff: 11.74mlTrain batch 19/32 - 106.2ms/batch - loss: 14.00799 - diff: 11.81mlTrain batch 20/32 - 107.0ms/batch - loss: 14.02198 - diff: 11.88mlTrain batch 21/32 - 106.9ms/batch - loss: 17.06114 - diff: 12.87mlTrain batch 22/32 - 107.4ms/batch - loss: 17.37443 - diff: 13.05mlTrain batch 23/32 - 106.6ms/batch - loss: 17.87683 - diff: 13.26mlTrain batch 24/32 - 107.8ms/batch - loss: 17.52901 - diff: 13.11mlTrain batch 25/32 - 108.6ms/batch - loss: 17.04556 - diff: 12.88mlTrain batch 26/32 - 106.9ms/batch - loss: 16.71472 - diff: 12.75mlTrain batch 27/32 - 108.1ms/batch - loss: 16.70248 - diff: 12.77mlTrain batch 28/32 - 106.8ms/batch - loss: 16.37487 - diff: 12.65mlTrain batch 29/32 - 106.3ms/batch - loss: 17.19858 - diff: 12.97mlTrain batch 30/32 - 117.3ms/batch - loss: 17.84777 - diff: 13.15mlTrain batch 31/32 - 119.1ms/batch - loss: 18.23202 - diff: 13.28mlTrain batch 32/32 - 97.7ms/batch - loss: 19.51827 - diff: 13.39mlTrain batch 32/32 - 11.2s 97.7ms/batch - loss: 19.51827 - diff: 13.39ml
Test 0.6s: val_loss: 50.03373 - diff: 21.96ml

Epoch 116: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 135.1ms/batch - loss: 15.83413 - diff: 11.03mlTrain batch 2/32 - 106.6ms/batch - loss: 14.84603 - diff: 11.34mlTrain batch 3/32 - 116.9ms/batch - loss: 20.14530 - diff: 13.64mlTrain batch 4/32 - 113.4ms/batch - loss: 17.60015 - diff: 12.73mlTrain batch 5/32 - 110.8ms/batch - loss: 17.60004 - diff: 13.03mlTrain batch 6/32 - 110.3ms/batch - loss: 16.28044 - diff: 12.50mlTrain batch 7/32 - 107.8ms/batch - loss: 22.55796 - diff: 14.98mlTrain batch 8/32 - 106.7ms/batch - loss: 22.08714 - diff: 14.83mlTrain batch 9/32 - 106.8ms/batch - loss: 20.26591 - diff: 13.95mlTrain batch 10/32 - 96.2ms/batch - loss: 21.76382 - diff: 14.59mlTrain batch 11/32 - 91.1ms/batch - loss: 21.41280 - diff: 14.50mlTrain batch 12/32 - 102.3ms/batch - loss: 21.26567 - diff: 14.54mlTrain batch 13/32 - 109.8ms/batch - loss: 20.48006 - diff: 14.19mlTrain batch 14/32 - 118.2ms/batch - loss: 19.91228 - diff: 13.96mlTrain batch 15/32 - 117.9ms/batch - loss: 20.81835 - diff: 14.30mlTrain batch 16/32 - 120.5ms/batch - loss: 20.71188 - diff: 14.24mlTrain batch 17/32 - 117.9ms/batch - loss: 20.17561 - diff: 14.07mlTrain batch 18/32 - 109.5ms/batch - loss: 20.53479 - diff: 14.21mlTrain batch 19/32 - 91.9ms/batch - loss: 20.63520 - diff: 14.36mlTrain batch 20/32 - 105.5ms/batch - loss: 19.93872 - diff: 14.06mlTrain batch 21/32 - 109.0ms/batch - loss: 20.83237 - diff: 14.41mlTrain batch 22/32 - 104.1ms/batch - loss: 20.14259 - diff: 14.08mlTrain batch 23/32 - 110.9ms/batch - loss: 22.28688 - diff: 14.77mlTrain batch 24/32 - 111.4ms/batch - loss: 21.70837 - diff: 14.55mlTrain batch 25/32 - 104.6ms/batch - loss: 21.28132 - diff: 14.39mlTrain batch 26/32 - 104.4ms/batch - loss: 21.03655 - diff: 14.24mlTrain batch 27/32 - 111.2ms/batch - loss: 20.43593 - diff: 13.99mlTrain batch 28/32 - 104.9ms/batch - loss: 20.11401 - diff: 13.90mlTrain batch 29/32 - 109.8ms/batch - loss: 21.54566 - diff: 14.41mlTrain batch 30/32 - 103.9ms/batch - loss: 21.92149 - diff: 14.52mlTrain batch 31/32 - 105.7ms/batch - loss: 21.88582 - diff: 14.56mlTrain batch 32/32 - 81.9ms/batch - loss: 22.05034 - diff: 14.51mlTrain batch 32/32 - 11.6s 81.9ms/batch - loss: 22.05034 - diff: 14.51ml
Test 0.6s: val_loss: 46.83941 - diff: 20.83ml

Epoch 117: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 133.6ms/batch - loss: 10.82956 - diff: 11.24mlTrain batch 2/32 - 117.9ms/batch - loss: 11.12003 - diff: 10.66mlTrain batch 3/32 - 108.1ms/batch - loss: 15.05049 - diff: 12.48mlTrain batch 4/32 - 108.5ms/batch - loss: 14.68241 - diff: 12.52mlTrain batch 5/32 - 108.6ms/batch - loss: 12.85896 - diff: 11.65mlTrain batch 6/32 - 109.4ms/batch - loss: 13.25501 - diff: 11.89mlTrain batch 7/32 - 88.7ms/batch - loss: 13.46321 - diff: 11.79mlTrain batch 8/32 - 101.6ms/batch - loss: 14.84979 - diff: 12.55mlTrain batch 9/32 - 96.6ms/batch - loss: 24.74208 - diff: 15.44mlTrain batch 10/32 - 105.8ms/batch - loss: 23.53240 - diff: 15.00mlTrain batch 11/32 - 103.8ms/batch - loss: 24.03443 - diff: 15.39mlTrain batch 12/32 - 104.6ms/batch - loss: 24.64901 - diff: 15.71mlTrain batch 13/32 - 115.3ms/batch - loss: 24.95345 - diff: 15.88mlTrain batch 14/32 - 107.1ms/batch - loss: 24.36383 - diff: 15.62mlTrain batch 15/32 - 114.7ms/batch - loss: 27.52167 - diff: 16.44mlTrain batch 16/32 - 102.3ms/batch - loss: 29.64749 - diff: 17.22mlTrain batch 17/32 - 114.1ms/batch - loss: 28.38497 - diff: 16.74mlTrain batch 18/32 - 107.1ms/batch - loss: 27.52206 - diff: 16.49mlTrain batch 19/32 - 109.5ms/batch - loss: 27.30050 - diff: 16.48mlTrain batch 20/32 - 115.6ms/batch - loss: 26.19837 - diff: 16.04mlTrain batch 21/32 - 106.6ms/batch - loss: 25.43849 - diff: 15.75mlTrain batch 22/32 - 106.6ms/batch - loss: 29.55974 - diff: 16.89mlTrain batch 23/32 - 112.2ms/batch - loss: 28.86936 - diff: 16.72mlTrain batch 24/32 - 112.1ms/batch - loss: 30.09847 - diff: 17.24mlTrain batch 25/32 - 112.2ms/batch - loss: 29.29641 - diff: 16.94mlTrain batch 26/32 - 116.2ms/batch - loss: 28.74092 - diff: 16.79mlTrain batch 27/32 - 109.6ms/batch - loss: 28.49859 - diff: 16.75mlTrain batch 28/32 - 108.1ms/batch - loss: 27.66821 - diff: 16.41mlTrain batch 29/32 - 114.8ms/batch - loss: 27.14232 - diff: 16.27mlTrain batch 30/32 - 109.0ms/batch - loss: 27.16369 - diff: 16.22mlTrain batch 31/32 - 92.7ms/batch - loss: 27.11288 - diff: 16.20mlTrain batch 32/32 - 84.1ms/batch - loss: 27.44992 - diff: 16.18mlTrain batch 32/32 - 11.5s 84.1ms/batch - loss: 27.44992 - diff: 16.18ml
Test 0.7s: val_loss: 49.87655 - diff: 21.43ml

Epoch 118: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 122.6ms/batch - loss: 17.91276 - diff: 12.88mlTrain batch 2/32 - 108.5ms/batch - loss: 17.31487 - diff: 13.21mlTrain batch 3/32 - 109.4ms/batch - loss: 15.11097 - diff: 12.17mlTrain batch 4/32 - 110.0ms/batch - loss: 16.65190 - diff: 13.14mlTrain batch 5/32 - 108.3ms/batch - loss: 15.48709 - diff: 12.51mlTrain batch 6/32 - 107.8ms/batch - loss: 15.12621 - diff: 12.37mlTrain batch 7/32 - 118.0ms/batch - loss: 15.16350 - diff: 12.49mlTrain batch 8/32 - 107.4ms/batch - loss: 15.41587 - diff: 12.66mlTrain batch 9/32 - 108.8ms/batch - loss: 19.26118 - diff: 14.11mlTrain batch 10/32 - 107.7ms/batch - loss: 20.24099 - diff: 14.51mlTrain batch 11/32 - 113.8ms/batch - loss: 20.54459 - diff: 14.70mlTrain batch 12/32 - 106.4ms/batch - loss: 19.89744 - diff: 14.42mlTrain batch 13/32 - 117.1ms/batch - loss: 18.83125 - diff: 13.92mlTrain batch 14/32 - 120.7ms/batch - loss: 18.26002 - diff: 13.76mlTrain batch 15/32 - 119.2ms/batch - loss: 17.95993 - diff: 13.59mlTrain batch 16/32 - 118.3ms/batch - loss: 20.14931 - diff: 14.37mlTrain batch 17/32 - 115.8ms/batch - loss: 19.69183 - diff: 14.26mlTrain batch 18/32 - 109.5ms/batch - loss: 20.66590 - diff: 14.61mlTrain batch 19/32 - 109.2ms/batch - loss: 21.13971 - diff: 14.78mlTrain batch 20/32 - 108.5ms/batch - loss: 20.79379 - diff: 14.68mlTrain batch 21/32 - 109.2ms/batch - loss: 20.05855 - diff: 14.34mlTrain batch 22/32 - 108.8ms/batch - loss: 19.82962 - diff: 14.18mlTrain batch 23/32 - 108.9ms/batch - loss: 19.31453 - diff: 13.95mlTrain batch 24/32 - 105.2ms/batch - loss: 19.50015 - diff: 14.00mlTrain batch 25/32 - 106.9ms/batch - loss: 19.23632 - diff: 13.90mlTrain batch 26/32 - 115.6ms/batch - loss: 19.49939 - diff: 13.93mlTrain batch 27/32 - 110.0ms/batch - loss: 19.07246 - diff: 13.75mlTrain batch 28/32 - 107.5ms/batch - loss: 18.89984 - diff: 13.75mlTrain batch 29/32 - 107.2ms/batch - loss: 19.58358 - diff: 14.01mlTrain batch 30/32 - 105.0ms/batch - loss: 19.46176 - diff: 14.01mlTrain batch 31/32 - 104.2ms/batch - loss: 20.05906 - diff: 14.32mlTrain batch 32/32 - 99.7ms/batch - loss: 21.17666 - diff: 14.35mlTrain batch 32/32 - 11.1s 99.7ms/batch - loss: 21.17666 - diff: 14.35ml
Test 0.7s: val_loss: 48.30534 - diff: 21.47ml

Epoch 119: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 128.5ms/batch - loss: 75.66545 - diff: 33.53mlTrain batch 2/32 - 114.1ms/batch - loss: 43.05883 - diff: 22.05mlTrain batch 3/32 - 106.3ms/batch - loss: 31.71747 - diff: 18.27mlTrain batch 4/32 - 107.5ms/batch - loss: 27.05750 - diff: 16.88mlTrain batch 5/32 - 103.5ms/batch - loss: 23.57014 - diff: 15.32mlTrain batch 6/32 - 103.7ms/batch - loss: 25.81293 - diff: 16.33mlTrain batch 7/32 - 101.5ms/batch - loss: 25.02003 - diff: 15.91mlTrain batch 8/32 - 103.8ms/batch - loss: 23.62178 - diff: 15.43mlTrain batch 9/32 - 109.4ms/batch - loss: 22.63752 - diff: 15.05mlTrain batch 10/32 - 109.1ms/batch - loss: 23.90083 - diff: 15.69mlTrain batch 11/32 - 98.1ms/batch - loss: 23.96455 - diff: 15.75mlTrain batch 12/32 - 123.9ms/batch - loss: 22.63981 - diff: 15.16mlTrain batch 13/32 - 96.0ms/batch - loss: 23.30694 - diff: 15.49mlTrain batch 14/32 - 99.4ms/batch - loss: 22.24861 - diff: 14.99mlTrain batch 15/32 - 108.4ms/batch - loss: 22.01536 - diff: 15.02mlTrain batch 16/32 - 109.4ms/batch - loss: 21.43617 - diff: 14.78mlTrain batch 17/32 - 107.6ms/batch - loss: 20.55847 - diff: 14.45mlTrain batch 18/32 - 108.1ms/batch - loss: 20.36974 - diff: 14.43mlTrain batch 19/32 - 107.4ms/batch - loss: 20.39958 - diff: 14.48mlTrain batch 20/32 - 111.9ms/batch - loss: 21.82235 - diff: 14.87mlTrain batch 21/32 - 114.3ms/batch - loss: 21.13926 - diff: 14.60mlTrain batch 22/32 - 121.2ms/batch - loss: 20.63412 - diff: 14.41mlTrain batch 23/32 - 104.3ms/batch - loss: 20.95471 - diff: 14.54mlTrain batch 24/32 - 105.4ms/batch - loss: 20.56105 - diff: 14.38mlTrain batch 25/32 - 104.8ms/batch - loss: 21.91459 - diff: 14.64mlTrain batch 26/32 - 104.5ms/batch - loss: 22.05487 - diff: 14.64mlTrain batch 27/32 - 103.3ms/batch - loss: 21.83964 - diff: 14.54mlTrain batch 28/32 - 104.1ms/batch - loss: 21.80640 - diff: 14.57mlTrain batch 29/32 - 104.4ms/batch - loss: 21.55155 - diff: 14.55mlTrain batch 30/32 - 104.3ms/batch - loss: 23.07101 - diff: 14.97mlTrain batch 31/32 - 112.6ms/batch - loss: 22.52141 - diff: 14.76mlTrain batch 32/32 - 81.1ms/batch - loss: 22.46540 - diff: 14.70mlTrain batch 32/32 - 11.6s 81.1ms/batch - loss: 22.46540 - diff: 14.70ml
Test 0.6s: val_loss: 48.37588 - diff: 20.48ml
Epoch   120: reducing learning rate of group 0 to 6.2500e-05.

Epoch 120: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 130.2ms/batch - loss: 36.73604 - diff: 19.51mlTrain batch 2/32 - 113.8ms/batch - loss: 32.82505 - diff: 18.05mlTrain batch 3/32 - 103.9ms/batch - loss: 27.01284 - diff: 16.19mlTrain batch 4/32 - 106.3ms/batch - loss: 24.90052 - diff: 15.91mlTrain batch 5/32 - 117.5ms/batch - loss: 22.85941 - diff: 15.34mlTrain batch 6/32 - 107.5ms/batch - loss: 20.12844 - diff: 14.26mlTrain batch 7/32 - 109.1ms/batch - loss: 23.43384 - diff: 14.66mlTrain batch 8/32 - 119.4ms/batch - loss: 21.95638 - diff: 14.13mlTrain batch 9/32 - 117.5ms/batch - loss: 21.54068 - diff: 14.29mlTrain batch 10/32 - 119.2ms/batch - loss: 20.74269 - diff: 14.01mlTrain batch 11/32 - 107.7ms/batch - loss: 20.91702 - diff: 14.20mlTrain batch 12/32 - 114.1ms/batch - loss: 21.36149 - diff: 14.35mlTrain batch 13/32 - 104.1ms/batch - loss: 20.10783 - diff: 13.76mlTrain batch 14/32 - 91.7ms/batch - loss: 19.59264 - diff: 13.62mlTrain batch 15/32 - 105.9ms/batch - loss: 19.29787 - diff: 13.57mlTrain batch 16/32 - 108.0ms/batch - loss: 18.90542 - diff: 13.46mlTrain batch 17/32 - 107.5ms/batch - loss: 18.76620 - diff: 13.50mlTrain batch 18/32 - 109.4ms/batch - loss: 18.15567 - diff: 13.27mlTrain batch 19/32 - 106.9ms/batch - loss: 17.64326 - diff: 13.07mlTrain batch 20/32 - 107.6ms/batch - loss: 18.23026 - diff: 13.25mlTrain batch 21/32 - 109.1ms/batch - loss: 17.69726 - diff: 12.99mlTrain batch 22/32 - 107.4ms/batch - loss: 17.45508 - diff: 12.92mlTrain batch 23/32 - 108.5ms/batch - loss: 16.95942 - diff: 12.75mlTrain batch 24/32 - 105.1ms/batch - loss: 16.50529 - diff: 12.55mlTrain batch 25/32 - 105.6ms/batch - loss: 16.27776 - diff: 12.44mlTrain batch 26/32 - 106.5ms/batch - loss: 15.79824 - diff: 12.20mlTrain batch 27/32 - 108.6ms/batch - loss: 15.80277 - diff: 12.22mlTrain batch 28/32 - 106.0ms/batch - loss: 15.75298 - diff: 12.21mlTrain batch 29/32 - 115.4ms/batch - loss: 15.79364 - diff: 12.27mlTrain batch 30/32 - 115.3ms/batch - loss: 15.80014 - diff: 12.25mlTrain batch 31/32 - 103.6ms/batch - loss: 15.73101 - diff: 12.25mlTrain batch 32/32 - 96.7ms/batch - loss: 15.72770 - diff: 12.20mlTrain batch 32/32 - 11.0s 96.7ms/batch - loss: 15.72770 - diff: 12.20ml
Test 0.7s: val_loss: 44.32142 - diff: 20.00ml

Epoch 121: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 137.3ms/batch - loss: 9.53798 - diff: 9.73mlTrain batch 2/32 - 113.8ms/batch - loss: 27.40324 - diff: 16.89mlTrain batch 3/32 - 113.8ms/batch - loss: 21.30152 - diff: 14.47mlTrain batch 4/32 - 109.5ms/batch - loss: 17.91798 - diff: 13.22mlTrain batch 5/32 - 109.8ms/batch - loss: 19.09219 - diff: 14.01mlTrain batch 6/32 - 110.0ms/batch - loss: 16.69164 - diff: 12.70mlTrain batch 7/32 - 109.4ms/batch - loss: 18.66011 - diff: 13.69mlTrain batch 8/32 - 108.6ms/batch - loss: 18.72798 - diff: 13.86mlTrain batch 9/32 - 107.7ms/batch - loss: 21.04963 - diff: 14.54mlTrain batch 10/32 - 107.5ms/batch - loss: 19.96435 - diff: 14.15mlTrain batch 11/32 - 100.0ms/batch - loss: 20.57779 - diff: 14.59mlTrain batch 12/32 - 109.6ms/batch - loss: 19.36052 - diff: 13.93mlTrain batch 13/32 - 108.2ms/batch - loss: 18.57083 - diff: 13.67mlTrain batch 14/32 - 103.0ms/batch - loss: 17.75410 - diff: 13.19mlTrain batch 15/32 - 103.8ms/batch - loss: 17.78005 - diff: 13.14mlTrain batch 16/32 - 104.7ms/batch - loss: 17.29605 - diff: 12.89mlTrain batch 17/32 - 104.3ms/batch - loss: 17.34016 - diff: 12.95mlTrain batch 18/32 - 119.4ms/batch - loss: 18.43905 - diff: 13.49mlTrain batch 19/32 - 115.1ms/batch - loss: 19.56280 - diff: 13.97mlTrain batch 20/32 - 106.9ms/batch - loss: 21.13390 - diff: 14.60mlTrain batch 21/32 - 99.3ms/batch - loss: 20.99765 - diff: 14.56mlTrain batch 22/32 - 107.2ms/batch - loss: 20.39305 - diff: 14.27mlTrain batch 23/32 - 107.1ms/batch - loss: 19.98329 - diff: 14.12mlTrain batch 24/32 - 106.9ms/batch - loss: 21.86721 - diff: 14.74mlTrain batch 25/32 - 107.1ms/batch - loss: 21.99628 - diff: 14.84mlTrain batch 26/32 - 106.8ms/batch - loss: 21.36030 - diff: 14.54mlTrain batch 27/32 - 106.8ms/batch - loss: 21.09640 - diff: 14.35mlTrain batch 28/32 - 108.6ms/batch - loss: 21.66958 - diff: 14.66mlTrain batch 29/32 - 107.1ms/batch - loss: 21.40807 - diff: 14.61mlTrain batch 30/32 - 108.1ms/batch - loss: 20.97166 - diff: 14.43mlTrain batch 31/32 - 107.8ms/batch - loss: 20.72625 - diff: 14.34mlTrain batch 32/32 - 91.3ms/batch - loss: 20.66169 - diff: 14.27mlTrain batch 32/32 - 11.0s 91.3ms/batch - loss: 20.66169 - diff: 14.27ml
Test 0.6s: val_loss: 46.58850 - diff: 20.21ml

Epoch 122: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 130.1ms/batch - loss: 6.93817 - diff: 8.53mlTrain batch 2/32 - 111.4ms/batch - loss: 10.55157 - diff: 10.50mlTrain batch 3/32 - 107.0ms/batch - loss: 13.14432 - diff: 10.91mlTrain batch 4/32 - 104.1ms/batch - loss: 18.27556 - diff: 13.70mlTrain batch 5/32 - 107.2ms/batch - loss: 16.49207 - diff: 12.90mlTrain batch 6/32 - 103.7ms/batch - loss: 15.34349 - diff: 12.33mlTrain batch 7/32 - 102.8ms/batch - loss: 16.63964 - diff: 12.64mlTrain batch 8/32 - 107.5ms/batch - loss: 16.72682 - diff: 12.82mlTrain batch 9/32 - 115.7ms/batch - loss: 16.02731 - diff: 12.34mlTrain batch 10/32 - 110.5ms/batch - loss: 16.27274 - diff: 12.59mlTrain batch 11/32 - 118.8ms/batch - loss: 16.48312 - diff: 12.74mlTrain batch 12/32 - 119.3ms/batch - loss: 15.96960 - diff: 12.61mlTrain batch 13/32 - 121.1ms/batch - loss: 15.17002 - diff: 12.23mlTrain batch 14/32 - 120.4ms/batch - loss: 15.95358 - diff: 12.47mlTrain batch 15/32 - 115.4ms/batch - loss: 16.23577 - diff: 12.67mlTrain batch 16/32 - 115.0ms/batch - loss: 15.67130 - diff: 12.43mlTrain batch 17/32 - 115.1ms/batch - loss: 15.14184 - diff: 12.17mlTrain batch 18/32 - 117.5ms/batch - loss: 15.36769 - diff: 12.30mlTrain batch 19/32 - 115.3ms/batch - loss: 16.35815 - diff: 12.70mlTrain batch 20/32 - 115.9ms/batch - loss: 16.18801 - diff: 12.65mlTrain batch 21/32 - 115.6ms/batch - loss: 15.93595 - diff: 12.52mlTrain batch 22/32 - 116.0ms/batch - loss: 16.74072 - diff: 12.84mlTrain batch 23/32 - 115.3ms/batch - loss: 18.09165 - diff: 13.36mlTrain batch 24/32 - 117.0ms/batch - loss: 18.06203 - diff: 13.30mlTrain batch 25/32 - 114.5ms/batch - loss: 17.67252 - diff: 13.13mlTrain batch 26/32 - 115.3ms/batch - loss: 17.40075 - diff: 13.05mlTrain batch 27/32 - 114.6ms/batch - loss: 17.28124 - diff: 13.03mlTrain batch 28/32 - 115.5ms/batch - loss: 17.10384 - diff: 12.94mlTrain batch 29/32 - 115.8ms/batch - loss: 17.24241 - diff: 13.00mlTrain batch 30/32 - 115.4ms/batch - loss: 17.28012 - diff: 13.04mlTrain batch 31/32 - 97.7ms/batch - loss: 17.00511 - diff: 12.91mlTrain batch 32/32 - 66.0ms/batch - loss: 18.35723 - diff: 12.99mlTrain batch 32/32 - 11.2s 66.0ms/batch - loss: 18.35723 - diff: 12.99ml
Test 0.7s: val_loss: 47.69233 - diff: 21.41ml

Epoch 123: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 120.5ms/batch - loss: 15.39824 - diff: 12.70mlTrain batch 2/32 - 104.5ms/batch - loss: 13.44473 - diff: 12.29mlTrain batch 3/32 - 106.3ms/batch - loss: 17.10174 - diff: 13.42mlTrain batch 4/32 - 104.9ms/batch - loss: 14.83022 - diff: 12.26mlTrain batch 5/32 - 105.6ms/batch - loss: 14.84141 - diff: 12.33mlTrain batch 6/32 - 104.9ms/batch - loss: 16.04856 - diff: 12.81mlTrain batch 7/32 - 104.9ms/batch - loss: 16.62839 - diff: 12.91mlTrain batch 8/32 - 104.5ms/batch - loss: 16.42181 - diff: 12.73mlTrain batch 9/32 - 104.5ms/batch - loss: 16.16375 - diff: 12.64mlTrain batch 10/32 - 104.5ms/batch - loss: 14.90702 - diff: 11.92mlTrain batch 11/32 - 104.8ms/batch - loss: 14.49681 - diff: 11.77mlTrain batch 12/32 - 101.6ms/batch - loss: 16.74950 - diff: 12.77mlTrain batch 13/32 - 109.4ms/batch - loss: 16.92884 - diff: 12.91mlTrain batch 14/32 - 102.0ms/batch - loss: 16.51936 - diff: 12.78mlTrain batch 15/32 - 104.0ms/batch - loss: 16.49498 - diff: 12.74mlTrain batch 16/32 - 103.8ms/batch - loss: 16.97935 - diff: 13.09mlTrain batch 17/32 - 104.6ms/batch - loss: 17.72445 - diff: 13.39mlTrain batch 18/32 - 105.0ms/batch - loss: 17.24424 - diff: 13.16mlTrain batch 19/32 - 104.2ms/batch - loss: 17.06153 - diff: 13.06mlTrain batch 20/32 - 104.9ms/batch - loss: 17.20622 - diff: 13.12mlTrain batch 21/32 - 105.2ms/batch - loss: 17.00107 - diff: 13.07mlTrain batch 22/32 - 103.7ms/batch - loss: 16.73183 - diff: 12.95mlTrain batch 23/32 - 105.2ms/batch - loss: 16.63426 - diff: 12.95mlTrain batch 24/32 - 104.8ms/batch - loss: 16.44571 - diff: 12.94mlTrain batch 25/32 - 103.1ms/batch - loss: 16.74517 - diff: 13.10mlTrain batch 26/32 - 104.3ms/batch - loss: 16.45592 - diff: 12.99mlTrain batch 27/32 - 99.1ms/batch - loss: 16.46989 - diff: 13.03mlTrain batch 28/32 - 101.2ms/batch - loss: 16.05003 - diff: 12.83mlTrain batch 29/32 - 102.3ms/batch - loss: 15.84666 - diff: 12.75mlTrain batch 30/32 - 87.6ms/batch - loss: 15.90422 - diff: 12.75mlTrain batch 31/32 - 93.0ms/batch - loss: 15.84445 - diff: 12.73mlTrain batch 32/32 - 79.6ms/batch - loss: 29.96462 - diff: 13.23mlTrain batch 32/32 - 10.9s 79.6ms/batch - loss: 29.96462 - diff: 13.23ml
Test 0.7s: val_loss: 49.27878 - diff: 20.64ml

Epoch 124: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 126.8ms/batch - loss: 19.17534 - diff: 12.55mlTrain batch 2/32 - 105.1ms/batch - loss: 38.17643 - diff: 20.32mlTrain batch 3/32 - 111.4ms/batch - loss: 39.20157 - diff: 20.74mlTrain batch 4/32 - 104.0ms/batch - loss: 35.85380 - diff: 19.56mlTrain batch 5/32 - 112.0ms/batch - loss: 34.10065 - diff: 18.94mlTrain batch 6/32 - 107.4ms/batch - loss: 30.23439 - diff: 17.43mlTrain batch 7/32 - 108.3ms/batch - loss: 29.32450 - diff: 17.27mlTrain batch 8/32 - 104.9ms/batch - loss: 28.15804 - diff: 16.97mlTrain batch 9/32 - 104.6ms/batch - loss: 26.56141 - diff: 16.43mlTrain batch 10/32 - 104.3ms/batch - loss: 24.92933 - diff: 15.90mlTrain batch 11/32 - 103.8ms/batch - loss: 24.83188 - diff: 16.06mlTrain batch 12/32 - 115.0ms/batch - loss: 25.50253 - diff: 16.28mlTrain batch 13/32 - 114.8ms/batch - loss: 24.66108 - diff: 15.82mlTrain batch 14/32 - 114.5ms/batch - loss: 23.63467 - diff: 15.40mlTrain batch 15/32 - 107.8ms/batch - loss: 24.28788 - diff: 15.55mlTrain batch 16/32 - 94.5ms/batch - loss: 23.94620 - diff: 15.46mlTrain batch 17/32 - 105.2ms/batch - loss: 23.13578 - diff: 15.17mlTrain batch 18/32 - 104.3ms/batch - loss: 22.31049 - diff: 14.86mlTrain batch 19/32 - 107.8ms/batch - loss: 21.89691 - diff: 14.71mlTrain batch 20/32 - 109.4ms/batch - loss: 21.54911 - diff: 14.58mlTrain batch 21/32 - 114.0ms/batch - loss: 21.89698 - diff: 14.67mlTrain batch 22/32 - 113.4ms/batch - loss: 22.64052 - diff: 14.93mlTrain batch 23/32 - 115.2ms/batch - loss: 22.49166 - diff: 14.94mlTrain batch 24/32 - 101.7ms/batch - loss: 22.81759 - diff: 15.13mlTrain batch 25/32 - 118.0ms/batch - loss: 24.48276 - diff: 15.73mlTrain batch 26/32 - 114.5ms/batch - loss: 23.84274 - diff: 15.47mlTrain batch 27/32 - 116.3ms/batch - loss: 23.84535 - diff: 15.49mlTrain batch 28/32 - 115.1ms/batch - loss: 23.47344 - diff: 15.31mlTrain batch 29/32 - 111.1ms/batch - loss: 23.16282 - diff: 15.24mlTrain batch 30/32 - 101.9ms/batch - loss: 22.68909 - diff: 15.06mlTrain batch 31/32 - 87.1ms/batch - loss: 22.20458 - diff: 14.85mlTrain batch 32/32 - 75.9ms/batch - loss: 27.29434 - diff: 15.10mlTrain batch 32/32 - 11.0s 75.9ms/batch - loss: 27.29434 - diff: 15.10ml
Test 0.7s: val_loss: 49.80674 - diff: 20.62ml

Epoch 125: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 105.5ms/batch - loss: 15.90604 - diff: 13.03mlTrain batch 2/32 - 98.1ms/batch - loss: 11.16191 - diff: 10.41mlTrain batch 3/32 - 108.2ms/batch - loss: 11.46536 - diff: 10.82mlTrain batch 4/32 - 105.1ms/batch - loss: 15.04610 - diff: 11.82mlTrain batch 5/32 - 104.4ms/batch - loss: 13.67743 - diff: 11.39mlTrain batch 6/32 - 107.3ms/batch - loss: 13.31181 - diff: 11.31mlTrain batch 7/32 - 107.6ms/batch - loss: 13.12379 - diff: 11.44mlTrain batch 8/32 - 100.7ms/batch - loss: 13.95700 - diff: 11.88mlTrain batch 9/32 - 108.7ms/batch - loss: 23.23478 - diff: 14.46mlTrain batch 10/32 - 109.2ms/batch - loss: 25.97764 - diff: 15.63mlTrain batch 11/32 - 119.4ms/batch - loss: 23.96084 - diff: 14.77mlTrain batch 12/32 - 117.7ms/batch - loss: 22.98251 - diff: 14.52mlTrain batch 13/32 - 118.5ms/batch - loss: 22.02191 - diff: 14.28mlTrain batch 14/32 - 117.6ms/batch - loss: 20.96873 - diff: 13.94mlTrain batch 15/32 - 107.9ms/batch - loss: 20.52614 - diff: 13.88mlTrain batch 16/32 - 119.0ms/batch - loss: 19.70270 - diff: 13.53mlTrain batch 17/32 - 116.2ms/batch - loss: 19.68681 - diff: 13.58mlTrain batch 18/32 - 118.6ms/batch - loss: 20.05294 - diff: 13.72mlTrain batch 19/32 - 119.1ms/batch - loss: 19.88688 - diff: 13.62mlTrain batch 20/32 - 113.1ms/batch - loss: 21.18022 - diff: 14.21mlTrain batch 21/32 - 100.3ms/batch - loss: 20.81035 - diff: 14.09mlTrain batch 22/32 - 116.9ms/batch - loss: 20.39734 - diff: 13.87mlTrain batch 23/32 - 117.4ms/batch - loss: 19.68693 - diff: 13.58mlTrain batch 24/32 - 116.2ms/batch - loss: 19.13529 - diff: 13.36mlTrain batch 25/32 - 105.3ms/batch - loss: 20.43540 - diff: 13.82mlTrain batch 26/32 - 106.3ms/batch - loss: 19.97723 - diff: 13.62mlTrain batch 27/32 - 112.6ms/batch - loss: 19.41525 - diff: 13.38mlTrain batch 28/32 - 112.5ms/batch - loss: 19.65197 - diff: 13.47mlTrain batch 29/32 - 118.0ms/batch - loss: 19.47390 - diff: 13.45mlTrain batch 30/32 - 116.1ms/batch - loss: 19.17368 - diff: 13.36mlTrain batch 31/32 - 92.2ms/batch - loss: 18.96663 - diff: 13.33mlTrain batch 32/32 - 95.8ms/batch - loss: 19.09901 - diff: 13.31mlTrain batch 32/32 - 11.9s 95.8ms/batch - loss: 19.09901 - diff: 13.31ml
Test 0.7s: val_loss: 44.05399 - diff: 20.19ml

Epoch 126: current best loss = 43.87969, at epoch 97
Train batch 1/32 - 133.7ms/batch - loss: 23.49881 - diff: 15.99mlTrain batch 2/32 - 106.1ms/batch - loss: 25.87598 - diff: 17.37mlTrain batch 3/32 - 127.0ms/batch - loss: 20.70151 - diff: 15.25mlTrain batch 4/32 - 112.6ms/batch - loss: 22.32765 - diff: 15.29mlTrain batch 5/32 - 115.5ms/batch - loss: 22.17004 - diff: 15.43mlTrain batch 6/32 - 115.4ms/batch - loss: 19.84877 - diff: 14.54mlTrain batch 7/32 - 104.2ms/batch - loss: 22.85856 - diff: 15.76mlTrain batch 8/32 - 104.1ms/batch - loss: 21.25172 - diff: 15.01mlTrain batch 9/32 - 114.9ms/batch - loss: 20.59667 - diff: 14.58mlTrain batch 10/32 - 118.9ms/batch - loss: 20.48203 - diff: 14.70mlTrain batch 11/32 - 114.8ms/batch - loss: 19.14195 - diff: 14.01mlTrain batch 12/32 - 112.4ms/batch - loss: 19.60172 - diff: 14.31mlTrain batch 13/32 - 110.7ms/batch - loss: 19.66987 - diff: 14.48mlTrain batch 14/32 - 100.2ms/batch - loss: 18.74798 - diff: 14.03mlTrain batch 15/32 - 118.0ms/batch - loss: 18.49023 - diff: 13.84mlTrain batch 16/32 - 118.6ms/batch - loss: 18.26485 - diff: 13.72mlTrain batch 17/32 - 110.3ms/batch - loss: 19.39084 - diff: 14.26mlTrain batch 18/32 - 107.3ms/batch - loss: 18.74250 - diff: 13.97mlTrain batch 19/32 - 106.6ms/batch - loss: 18.48354 - diff: 13.87mlTrain batch 20/32 - 119.7ms/batch - loss: 18.21190 - diff: 13.76mlTrain batch 21/32 - 118.8ms/batch - loss: 18.01908 - diff: 13.74mlTrain batch 22/32 - 121.7ms/batch - loss: 17.65627 - diff: 13.59mlTrain batch 23/32 - 113.3ms/batch - loss: 17.18042 - diff: 13.32mlTrain batch 24/32 - 122.4ms/batch - loss: 16.86403 - diff: 13.14mlTrain batch 25/32 - 113.2ms/batch - loss: 16.92843 - diff: 13.19mlTrain batch 26/32 - 121.4ms/batch - loss: 16.95068 - diff: 13.21mlTrain batch 27/32 - 114.3ms/batch - loss: 16.83123 - diff: 13.13mlTrain batch 28/32 - 114.3ms/batch - loss: 16.57218 - diff: 13.00mlTrain batch 29/32 - 117.8ms/batch - loss: 16.56575 - diff: 13.02mlTrain batch 30/32 - 117.7ms/batch - loss: 16.42303 - diff: 12.99mlTrain batch 31/32 - 114.6ms/batch - loss: 16.23087 - diff: 12.93mlTrain batch 32/32 - 111.5ms/batch - loss: 16.63175 - diff: 12.94mlTrain batch 32/32 - 12.1s 111.5ms/batch - loss: 16.63175 - diff: 12.94ml
Test 0.6s: val_loss: 42.78891 - diff: 19.83ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 127: current best loss = 42.78891, at epoch 126
Train batch 1/32 - 120.6ms/batch - loss: 18.74598 - diff: 13.95mlTrain batch 2/32 - 115.6ms/batch - loss: 16.81867 - diff: 13.48mlTrain batch 3/32 - 114.1ms/batch - loss: 15.33244 - diff: 12.73mlTrain batch 4/32 - 106.5ms/batch - loss: 19.89378 - diff: 14.28mlTrain batch 5/32 - 118.6ms/batch - loss: 17.57001 - diff: 13.23mlTrain batch 6/32 - 121.9ms/batch - loss: 15.58365 - diff: 12.38mlTrain batch 7/32 - 117.0ms/batch - loss: 14.77350 - diff: 11.94mlTrain batch 8/32 - 104.9ms/batch - loss: 14.59711 - diff: 12.11mlTrain batch 9/32 - 103.8ms/batch - loss: 15.34781 - diff: 12.47mlTrain batch 10/32 - 104.7ms/batch - loss: 14.51534 - diff: 11.99mlTrain batch 11/32 - 103.7ms/batch - loss: 15.56694 - diff: 12.56mlTrain batch 12/32 - 92.1ms/batch - loss: 14.79331 - diff: 12.14mlTrain batch 13/32 - 101.6ms/batch - loss: 15.15552 - diff: 12.31mlTrain batch 14/32 - 95.0ms/batch - loss: 15.16833 - diff: 12.34mlTrain batch 15/32 - 100.6ms/batch - loss: 15.54607 - diff: 12.54mlTrain batch 16/32 - 123.1ms/batch - loss: 15.28723 - diff: 12.46mlTrain batch 17/32 - 110.1ms/batch - loss: 17.08723 - diff: 13.12mlTrain batch 18/32 - 104.4ms/batch - loss: 16.89728 - diff: 13.09mlTrain batch 19/32 - 107.5ms/batch - loss: 17.57485 - diff: 13.27mlTrain batch 20/32 - 104.1ms/batch - loss: 17.31984 - diff: 13.23mlTrain batch 21/32 - 110.8ms/batch - loss: 17.52553 - diff: 13.43mlTrain batch 22/32 - 104.3ms/batch - loss: 17.71130 - diff: 13.47mlTrain batch 23/32 - 106.9ms/batch - loss: 17.49764 - diff: 13.39mlTrain batch 24/32 - 104.4ms/batch - loss: 20.60262 - diff: 14.33mlTrain batch 25/32 - 107.9ms/batch - loss: 20.07696 - diff: 14.11mlTrain batch 26/32 - 104.4ms/batch - loss: 19.55976 - diff: 13.90mlTrain batch 27/32 - 113.5ms/batch - loss: 19.16268 - diff: 13.75mlTrain batch 28/32 - 104.8ms/batch - loss: 18.95148 - diff: 13.66mlTrain batch 29/32 - 109.4ms/batch - loss: 18.92088 - diff: 13.64mlTrain batch 30/32 - 110.9ms/batch - loss: 19.59387 - diff: 13.88mlTrain batch 31/32 - 96.7ms/batch - loss: 19.28307 - diff: 13.80mlTrain batch 32/32 - 95.0ms/batch - loss: 21.71316 - diff: 13.97mlTrain batch 32/32 - 11.3s 95.0ms/batch - loss: 21.71316 - diff: 13.97ml
Test 0.6s: val_loss: 47.13507 - diff: 20.32ml

Epoch 128: current best loss = 42.78891, at epoch 126
Train batch 1/32 - 138.6ms/batch - loss: 7.13678 - diff: 8.67mlTrain batch 2/32 - 118.2ms/batch - loss: 8.90306 - diff: 9.69mlTrain batch 3/32 - 115.8ms/batch - loss: 10.38743 - diff: 10.37mlTrain batch 4/32 - 109.4ms/batch - loss: 11.81294 - diff: 10.92mlTrain batch 5/32 - 107.7ms/batch - loss: 10.86738 - diff: 10.39mlTrain batch 6/32 - 119.9ms/batch - loss: 14.53081 - diff: 12.13mlTrain batch 7/32 - 115.5ms/batch - loss: 14.71886 - diff: 12.40mlTrain batch 8/32 - 114.8ms/batch - loss: 14.80172 - diff: 12.50mlTrain batch 9/32 - 114.4ms/batch - loss: 15.39488 - diff: 12.49mlTrain batch 10/32 - 118.3ms/batch - loss: 14.76597 - diff: 12.17mlTrain batch 11/32 - 108.7ms/batch - loss: 17.33371 - diff: 13.22mlTrain batch 12/32 - 125.6ms/batch - loss: 18.65928 - diff: 13.89mlTrain batch 13/32 - 119.4ms/batch - loss: 17.82645 - diff: 13.45mlTrain batch 14/32 - 114.0ms/batch - loss: 17.97596 - diff: 13.60mlTrain batch 15/32 - 113.6ms/batch - loss: 18.76089 - diff: 14.02mlTrain batch 16/32 - 108.5ms/batch - loss: 18.94866 - diff: 13.80mlTrain batch 17/32 - 107.3ms/batch - loss: 18.32050 - diff: 13.52mlTrain batch 18/32 - 118.8ms/batch - loss: 17.86567 - diff: 13.34mlTrain batch 19/32 - 117.0ms/batch - loss: 17.81454 - diff: 13.35mlTrain batch 20/32 - 107.7ms/batch - loss: 17.91680 - diff: 13.44mlTrain batch 21/32 - 107.8ms/batch - loss: 17.62436 - diff: 13.28mlTrain batch 22/32 - 105.1ms/batch - loss: 18.45107 - diff: 13.61mlTrain batch 23/32 - 104.3ms/batch - loss: 18.12396 - diff: 13.49mlTrain batch 24/32 - 104.3ms/batch - loss: 17.79980 - diff: 13.34mlTrain batch 25/32 - 103.8ms/batch - loss: 17.60739 - diff: 13.33mlTrain batch 26/32 - 97.5ms/batch - loss: 18.47762 - diff: 13.69mlTrain batch 27/32 - 107.6ms/batch - loss: 18.18199 - diff: 13.55mlTrain batch 28/32 - 103.9ms/batch - loss: 17.96260 - diff: 13.50mlTrain batch 29/32 - 104.0ms/batch - loss: 17.63167 - diff: 13.35mlTrain batch 30/32 - 96.9ms/batch - loss: 19.81044 - diff: 13.98mlTrain batch 31/32 - 101.4ms/batch - loss: 19.80883 - diff: 14.00mlTrain batch 32/32 - 96.8ms/batch - loss: 20.61971 - diff: 14.06mlTrain batch 32/32 - 11.3s 96.8ms/batch - loss: 20.61971 - diff: 14.06ml
Test 0.7s: val_loss: 46.92196 - diff: 20.10ml

Epoch 129: current best loss = 42.78891, at epoch 126
Train batch 1/32 - 124.6ms/batch - loss: 33.98318 - diff: 18.73mlTrain batch 2/32 - 108.9ms/batch - loss: 21.13438 - diff: 14.12mlTrain batch 3/32 - 119.7ms/batch - loss: 19.70702 - diff: 13.81mlTrain batch 4/32 - 116.0ms/batch - loss: 25.55844 - diff: 16.22mlTrain batch 5/32 - 115.0ms/batch - loss: 25.90076 - diff: 15.85mlTrain batch 6/32 - 112.4ms/batch - loss: 24.06841 - diff: 15.31mlTrain batch 7/32 - 104.4ms/batch - loss: 22.31945 - diff: 14.49mlTrain batch 8/32 - 107.5ms/batch - loss: 28.82591 - diff: 16.70mlTrain batch 9/32 - 120.6ms/batch - loss: 26.60444 - diff: 15.86mlTrain batch 10/32 - 116.2ms/batch - loss: 25.42576 - diff: 15.48mlTrain batch 11/32 - 115.9ms/batch - loss: 23.77535 - diff: 14.89mlTrain batch 12/32 - 105.7ms/batch - loss: 22.63253 - diff: 14.49mlTrain batch 13/32 - 108.1ms/batch - loss: 21.75567 - diff: 14.23mlTrain batch 14/32 - 107.4ms/batch - loss: 21.35296 - diff: 14.15mlTrain batch 15/32 - 103.6ms/batch - loss: 20.54763 - diff: 13.92mlTrain batch 16/32 - 103.9ms/batch - loss: 19.73728 - diff: 13.57mlTrain batch 17/32 - 104.5ms/batch - loss: 19.12063 - diff: 13.34mlTrain batch 18/32 - 102.6ms/batch - loss: 19.18046 - diff: 13.49mlTrain batch 19/32 - 104.8ms/batch - loss: 18.77757 - diff: 13.31mlTrain batch 20/32 - 104.4ms/batch - loss: 18.50876 - diff: 13.29mlTrain batch 21/32 - 103.1ms/batch - loss: 17.94736 - diff: 12.96mlTrain batch 22/32 - 105.4ms/batch - loss: 21.45639 - diff: 14.08mlTrain batch 23/32 - 104.3ms/batch - loss: 21.20303 - diff: 14.03mlTrain batch 24/32 - 98.0ms/batch - loss: 21.31333 - diff: 14.13mlTrain batch 25/32 - 102.8ms/batch - loss: 20.92564 - diff: 14.01mlTrain batch 26/32 - 104.0ms/batch - loss: 20.73089 - diff: 13.95mlTrain batch 27/32 - 105.4ms/batch - loss: 22.17501 - diff: 14.18mlTrain batch 28/32 - 110.3ms/batch - loss: 21.69699 - diff: 14.01mlTrain batch 29/32 - 106.4ms/batch - loss: 21.24042 - diff: 13.84mlTrain batch 30/32 - 110.1ms/batch - loss: 20.68614 - diff: 13.58mlTrain batch 31/32 - 107.7ms/batch - loss: 21.17975 - diff: 13.82mlTrain batch 32/32 - 100.5ms/batch - loss: 21.55469 - diff: 13.81mlTrain batch 32/32 - 11.8s 100.5ms/batch - loss: 21.55469 - diff: 13.81ml
Test 0.7s: val_loss: 47.11713 - diff: 20.89ml

Epoch 130: current best loss = 42.78891, at epoch 126
Train batch 1/32 - 117.6ms/batch - loss: 10.57313 - diff: 10.21mlTrain batch 2/32 - 116.6ms/batch - loss: 6.81380 - diff: 7.72mlTrain batch 3/32 - 111.8ms/batch - loss: 8.49363 - diff: 8.70mlTrain batch 4/32 - 105.3ms/batch - loss: 14.77522 - diff: 11.59mlTrain batch 5/32 - 115.9ms/batch - loss: 15.12842 - diff: 11.68mlTrain batch 6/32 - 115.4ms/batch - loss: 15.64216 - diff: 11.96mlTrain batch 7/32 - 118.5ms/batch - loss: 15.56248 - diff: 12.13mlTrain batch 8/32 - 107.7ms/batch - loss: 14.19453 - diff: 11.44mlTrain batch 9/32 - 96.1ms/batch - loss: 13.53836 - diff: 11.11mlTrain batch 10/32 - 107.2ms/batch - loss: 13.67752 - diff: 11.23mlTrain batch 11/32 - 117.4ms/batch - loss: 14.29793 - diff: 11.57mlTrain batch 12/32 - 113.6ms/batch - loss: 13.90142 - diff: 11.35mlTrain batch 13/32 - 106.6ms/batch - loss: 13.56945 - diff: 11.27mlTrain batch 14/32 - 106.7ms/batch - loss: 13.57269 - diff: 11.35mlTrain batch 15/32 - 105.6ms/batch - loss: 14.54004 - diff: 11.92mlTrain batch 16/32 - 105.4ms/batch - loss: 13.83339 - diff: 11.56mlTrain batch 17/32 - 106.0ms/batch - loss: 13.41334 - diff: 11.33mlTrain batch 18/32 - 109.9ms/batch - loss: 14.99139 - diff: 12.07mlTrain batch 19/32 - 109.6ms/batch - loss: 14.83651 - diff: 12.07mlTrain batch 20/32 - 115.8ms/batch - loss: 15.01240 - diff: 12.06mlTrain batch 21/32 - 107.3ms/batch - loss: 15.64700 - diff: 12.34mlTrain batch 22/32 - 108.1ms/batch - loss: 15.44812 - diff: 12.24mlTrain batch 23/32 - 107.2ms/batch - loss: 14.96520 - diff: 11.97mlTrain batch 24/32 - 113.8ms/batch - loss: 14.85095 - diff: 11.91mlTrain batch 25/32 - 107.7ms/batch - loss: 16.83016 - diff: 12.65mlTrain batch 26/32 - 111.0ms/batch - loss: 17.10849 - diff: 12.80mlTrain batch 27/32 - 107.8ms/batch - loss: 18.72371 - diff: 13.43mlTrain batch 28/32 - 109.8ms/batch - loss: 18.59970 - diff: 13.40mlTrain batch 29/32 - 107.6ms/batch - loss: 18.22710 - diff: 13.25mlTrain batch 30/32 - 103.8ms/batch - loss: 18.27111 - diff: 13.33mlTrain batch 31/32 - 102.9ms/batch - loss: 18.67113 - diff: 13.58mlTrain batch 32/32 - 112.2ms/batch - loss: 35.85125 - diff: 14.21mlTrain batch 32/32 - 11.1s 112.2ms/batch - loss: 35.85125 - diff: 14.21ml
Test 0.7s: val_loss: 49.33650 - diff: 20.70ml

Epoch 131: current best loss = 42.78891, at epoch 126
Train batch 1/32 - 130.1ms/batch - loss: 8.21639 - diff: 9.31mlTrain batch 2/32 - 107.2ms/batch - loss: 9.03689 - diff: 9.79mlTrain batch 3/32 - 103.5ms/batch - loss: 10.00942 - diff: 10.48mlTrain batch 4/32 - 105.0ms/batch - loss: 9.50853 - diff: 9.94mlTrain batch 5/32 - 103.2ms/batch - loss: 10.28881 - diff: 10.27mlTrain batch 6/32 - 104.8ms/batch - loss: 15.26614 - diff: 12.17mlTrain batch 7/32 - 106.0ms/batch - loss: 14.18786 - diff: 11.64mlTrain batch 8/32 - 107.8ms/batch - loss: 13.80645 - diff: 11.51mlTrain batch 9/32 - 109.2ms/batch - loss: 30.94452 - diff: 15.30mlTrain batch 10/32 - 104.5ms/batch - loss: 28.38756 - diff: 14.53mlTrain batch 11/32 - 106.3ms/batch - loss: 26.73020 - diff: 14.22mlTrain batch 12/32 - 107.4ms/batch - loss: 31.39129 - diff: 15.76mlTrain batch 13/32 - 105.8ms/batch - loss: 30.42736 - diff: 15.72mlTrain batch 14/32 - 104.4ms/batch - loss: 29.32411 - diff: 15.51mlTrain batch 15/32 - 102.8ms/batch - loss: 29.08161 - diff: 15.63mlTrain batch 16/32 - 105.3ms/batch - loss: 28.03566 - diff: 15.45mlTrain batch 17/32 - 103.0ms/batch - loss: 27.87467 - diff: 15.56mlTrain batch 18/32 - 103.9ms/batch - loss: 26.65811 - diff: 15.13mlTrain batch 19/32 - 103.3ms/batch - loss: 26.68279 - diff: 15.06mlTrain batch 20/32 - 105.7ms/batch - loss: 25.85345 - diff: 14.84mlTrain batch 21/32 - 106.7ms/batch - loss: 25.03292 - diff: 14.60mlTrain batch 22/32 - 109.5ms/batch - loss: 24.57705 - diff: 14.48mlTrain batch 23/32 - 115.7ms/batch - loss: 24.06140 - diff: 14.34mlTrain batch 24/32 - 107.0ms/batch - loss: 23.58965 - diff: 14.25mlTrain batch 25/32 - 109.2ms/batch - loss: 22.88389 - diff: 14.04mlTrain batch 26/32 - 101.0ms/batch - loss: 22.55147 - diff: 13.94mlTrain batch 27/32 - 97.2ms/batch - loss: 22.08795 - diff: 13.76mlTrain batch 28/32 - 106.6ms/batch - loss: 22.11372 - diff: 13.85mlTrain batch 29/32 - 118.0ms/batch - loss: 22.34793 - diff: 13.97mlTrain batch 30/32 - 116.6ms/batch - loss: 23.18573 - diff: 14.40mlTrain batch 31/32 - 108.0ms/batch - loss: 23.20022 - diff: 14.49mlTrain batch 32/32 - 67.4ms/batch - loss: 23.15791 - diff: 14.43mlTrain batch 32/32 - 11.6s 67.4ms/batch - loss: 23.15791 - diff: 14.43ml
Test 0.6s: val_loss: 48.04710 - diff: 21.14ml

Epoch 132: current best loss = 42.78891, at epoch 126
Train batch 1/32 - 131.2ms/batch - loss: 15.87560 - diff: 12.44mlTrain batch 2/32 - 111.8ms/batch - loss: 12.16627 - diff: 10.80mlTrain batch 3/32 - 121.3ms/batch - loss: 10.46125 - diff: 10.32mlTrain batch 4/32 - 115.0ms/batch - loss: 9.77543 - diff: 10.13mlTrain batch 5/32 - 117.2ms/batch - loss: 10.26619 - diff: 10.47mlTrain batch 6/32 - 122.6ms/batch - loss: 12.28612 - diff: 11.43mlTrain batch 7/32 - 115.8ms/batch - loss: 11.42339 - diff: 10.96mlTrain batch 8/32 - 91.3ms/batch - loss: 11.12004 - diff: 10.90mlTrain batch 9/32 - 93.9ms/batch - loss: 10.61734 - diff: 10.56mlTrain batch 10/32 - 93.5ms/batch - loss: 12.32669 - diff: 11.34mlTrain batch 11/32 - 91.6ms/batch - loss: 12.14321 - diff: 11.18mlTrain batch 12/32 - 92.9ms/batch - loss: 12.54668 - diff: 11.37mlTrain batch 13/32 - 102.7ms/batch - loss: 12.63912 - diff: 11.48mlTrain batch 14/32 - 105.2ms/batch - loss: 13.08998 - diff: 11.68mlTrain batch 15/32 - 112.3ms/batch - loss: 12.70936 - diff: 11.52mlTrain batch 16/32 - 111.9ms/batch - loss: 12.91297 - diff: 11.73mlTrain batch 17/32 - 112.0ms/batch - loss: 14.61356 - diff: 12.30mlTrain batch 18/32 - 111.9ms/batch - loss: 13.95494 - diff: 11.93mlTrain batch 19/32 - 112.9ms/batch - loss: 13.61308 - diff: 11.73mlTrain batch 20/32 - 112.3ms/batch - loss: 14.36490 - diff: 12.09mlTrain batch 21/32 - 119.5ms/batch - loss: 14.30677 - diff: 12.07mlTrain batch 22/32 - 112.2ms/batch - loss: 14.16320 - diff: 12.03mlTrain batch 23/32 - 112.2ms/batch - loss: 17.35504 - diff: 12.80mlTrain batch 24/32 - 113.0ms/batch - loss: 17.19774 - diff: 12.77mlTrain batch 25/32 - 112.7ms/batch - loss: 16.81471 - diff: 12.60mlTrain batch 26/32 - 121.5ms/batch - loss: 16.44473 - diff: 12.47mlTrain batch 27/32 - 115.9ms/batch - loss: 16.28063 - diff: 12.36mlTrain batch 28/32 - 107.6ms/batch - loss: 16.04072 - diff: 12.29mlTrain batch 29/32 - 106.6ms/batch - loss: 18.26449 - diff: 13.02mlTrain batch 30/32 - 108.9ms/batch - loss: 18.34858 - diff: 13.13mlTrain batch 31/32 - 100.6ms/batch - loss: 18.71046 - diff: 13.33mlTrain batch 32/32 - 84.0ms/batch - loss: 21.75385 - diff: 13.52mlTrain batch 32/32 - 11.6s 84.0ms/batch - loss: 21.75385 - diff: 13.52ml
Test 0.7s: val_loss: 47.92157 - diff: 20.03ml

Epoch 133: current best loss = 42.78891, at epoch 126
Train batch 1/32 - 127.4ms/batch - loss: 8.45791 - diff: 9.42mlTrain batch 2/32 - 105.0ms/batch - loss: 7.38400 - diff: 9.22mlTrain batch 3/32 - 107.6ms/batch - loss: 9.69569 - diff: 9.93mlTrain batch 4/32 - 114.2ms/batch - loss: 8.51860 - diff: 9.32mlTrain batch 5/32 - 113.1ms/batch - loss: 10.48162 - diff: 10.53mlTrain batch 6/32 - 110.0ms/batch - loss: 11.80630 - diff: 11.11mlTrain batch 7/32 - 103.0ms/batch - loss: 25.54078 - diff: 15.25mlTrain batch 8/32 - 110.7ms/batch - loss: 25.06364 - diff: 15.25mlTrain batch 9/32 - 109.3ms/batch - loss: 23.21237 - diff: 14.47mlTrain batch 10/32 - 104.8ms/batch - loss: 26.80327 - diff: 15.88mlTrain batch 11/32 - 100.9ms/batch - loss: 25.92648 - diff: 15.71mlTrain batch 12/32 - 112.8ms/batch - loss: 25.26117 - diff: 15.62mlTrain batch 13/32 - 113.0ms/batch - loss: 28.84241 - diff: 16.80mlTrain batch 14/32 - 113.0ms/batch - loss: 27.93603 - diff: 16.45mlTrain batch 15/32 - 112.8ms/batch - loss: 26.81694 - diff: 16.05mlTrain batch 16/32 - 105.0ms/batch - loss: 25.90377 - diff: 15.66mlTrain batch 17/32 - 102.8ms/batch - loss: 25.59095 - diff: 15.72mlTrain batch 18/32 - 104.8ms/batch - loss: 25.40424 - diff: 15.70mlTrain batch 19/32 - 103.9ms/batch - loss: 26.31222 - diff: 16.13mlTrain batch 20/32 - 103.5ms/batch - loss: 25.39005 - diff: 15.81mlTrain batch 21/32 - 103.2ms/batch - loss: 24.89893 - diff: 15.57mlTrain batch 22/32 - 103.5ms/batch - loss: 24.49742 - diff: 15.50mlTrain batch 23/32 - 104.7ms/batch - loss: 24.12625 - diff: 15.26mlTrain batch 24/32 - 99.9ms/batch - loss: 25.19176 - diff: 15.68mlTrain batch 25/32 - 106.5ms/batch - loss: 24.58630 - diff: 15.47mlTrain batch 26/32 - 106.8ms/batch - loss: 26.15251 - diff: 16.01mlTrain batch 27/32 - 105.9ms/batch - loss: 25.56863 - diff: 15.83mlTrain batch 28/32 - 105.6ms/batch - loss: 25.21098 - diff: 15.77mlTrain batch 29/32 - 105.5ms/batch - loss: 25.15327 - diff: 15.82mlTrain batch 30/32 - 105.9ms/batch - loss: 24.72235 - diff: 15.70mlTrain batch 31/32 - 108.6ms/batch - loss: 24.31399 - diff: 15.51mlTrain batch 32/32 - 89.0ms/batch - loss: 24.38907 - diff: 15.46mlTrain batch 32/32 - 10.9s 89.0ms/batch - loss: 24.38907 - diff: 15.46ml
Test 0.7s: val_loss: 46.58949 - diff: 20.72ml

Epoch 134: current best loss = 42.78891, at epoch 126
Train batch 1/32 - 131.2ms/batch - loss: 8.51146 - diff: 9.34mlTrain batch 2/32 - 109.2ms/batch - loss: 7.91323 - diff: 9.16mlTrain batch 3/32 - 105.9ms/batch - loss: 9.43798 - diff: 9.95mlTrain batch 4/32 - 104.0ms/batch - loss: 9.62372 - diff: 10.05mlTrain batch 5/32 - 114.7ms/batch - loss: 9.09095 - diff: 9.60mlTrain batch 6/32 - 103.4ms/batch - loss: 10.05454 - diff: 10.08mlTrain batch 7/32 - 117.8ms/batch - loss: 16.28714 - diff: 12.28mlTrain batch 8/32 - 106.9ms/batch - loss: 17.29472 - diff: 12.66mlTrain batch 9/32 - 117.3ms/batch - loss: 21.32141 - diff: 14.38mlTrain batch 10/32 - 107.7ms/batch - loss: 19.66547 - diff: 13.70mlTrain batch 11/32 - 116.3ms/batch - loss: 19.11820 - diff: 13.48mlTrain batch 12/32 - 105.3ms/batch - loss: 18.04177 - diff: 13.11mlTrain batch 13/32 - 95.1ms/batch - loss: 17.10057 - diff: 12.72mlTrain batch 14/32 - 92.9ms/batch - loss: 16.27267 - diff: 12.34mlTrain batch 15/32 - 105.6ms/batch - loss: 16.90576 - diff: 12.55mlTrain batch 16/32 - 103.1ms/batch - loss: 18.49865 - diff: 13.05mlTrain batch 17/32 - 115.4ms/batch - loss: 18.08690 - diff: 12.96mlTrain batch 18/32 - 114.5ms/batch - loss: 17.45509 - diff: 12.69mlTrain batch 19/32 - 109.8ms/batch - loss: 17.81991 - diff: 12.99mlTrain batch 20/32 - 105.9ms/batch - loss: 17.35119 - diff: 12.82mlTrain batch 21/32 - 109.5ms/batch - loss: 16.73282 - diff: 12.51mlTrain batch 22/32 - 106.5ms/batch - loss: 16.72944 - diff: 12.52mlTrain batch 23/32 - 108.0ms/batch - loss: 16.27522 - diff: 12.33mlTrain batch 24/32 - 105.9ms/batch - loss: 15.94181 - diff: 12.23mlTrain batch 25/32 - 107.5ms/batch - loss: 15.77410 - diff: 12.22mlTrain batch 26/32 - 106.0ms/batch - loss: 15.76837 - diff: 12.25mlTrain batch 27/32 - 108.4ms/batch - loss: 15.66653 - diff: 12.19mlTrain batch 28/32 - 106.2ms/batch - loss: 15.51813 - diff: 12.15mlTrain batch 29/32 - 109.5ms/batch - loss: 15.73627 - diff: 12.21mlTrain batch 30/32 - 108.2ms/batch - loss: 15.63705 - diff: 12.17mlTrain batch 31/32 - 92.2ms/batch - loss: 15.59526 - diff: 12.19mlTrain batch 32/32 - 83.0ms/batch - loss: 17.07883 - diff: 12.29mlTrain batch 32/32 - 10.8s 83.0ms/batch - loss: 17.07883 - diff: 12.29ml
Test 0.6s: val_loss: 48.48399 - diff: 21.46ml

Epoch 135: current best loss = 42.78891, at epoch 126
Train batch 1/32 - 119.1ms/batch - loss: 43.69090 - diff: 22.91mlTrain batch 2/32 - 105.5ms/batch - loss: 27.34376 - diff: 16.74mlTrain batch 3/32 - 104.9ms/batch - loss: 22.28547 - diff: 14.80mlTrain batch 4/32 - 104.6ms/batch - loss: 19.94222 - diff: 14.25mlTrain batch 5/32 - 115.1ms/batch - loss: 18.13528 - diff: 13.34mlTrain batch 6/32 - 114.3ms/batch - loss: 16.13579 - diff: 12.48mlTrain batch 7/32 - 114.6ms/batch - loss: 15.07792 - diff: 12.12mlTrain batch 8/32 - 115.7ms/batch - loss: 16.05207 - diff: 12.62mlTrain batch 9/32 - 114.7ms/batch - loss: 15.09818 - diff: 12.25mlTrain batch 10/32 - 114.8ms/batch - loss: 14.48184 - diff: 11.94mlTrain batch 11/32 - 115.1ms/batch - loss: 16.81811 - diff: 12.92mlTrain batch 12/32 - 115.1ms/batch - loss: 16.95048 - diff: 12.93mlTrain batch 13/32 - 104.4ms/batch - loss: 18.34038 - diff: 13.65mlTrain batch 14/32 - 104.1ms/batch - loss: 18.00625 - diff: 13.52mlTrain batch 15/32 - 104.2ms/batch - loss: 17.92901 - diff: 13.55mlTrain batch 16/32 - 124.5ms/batch - loss: 16.97769 - diff: 13.04mlTrain batch 17/32 - 113.7ms/batch - loss: 16.62912 - diff: 12.94mlTrain batch 18/32 - 116.9ms/batch - loss: 16.30575 - diff: 12.81mlTrain batch 19/32 - 119.9ms/batch - loss: 15.65953 - diff: 12.46mlTrain batch 20/32 - 112.8ms/batch - loss: 15.37615 - diff: 12.34mlTrain batch 21/32 - 114.3ms/batch - loss: 15.31043 - diff: 12.27mlTrain batch 22/32 - 112.1ms/batch - loss: 15.45635 - diff: 12.35mlTrain batch 23/32 - 112.0ms/batch - loss: 15.14894 - diff: 12.24mlTrain batch 24/32 - 112.0ms/batch - loss: 14.94629 - diff: 12.16mlTrain batch 25/32 - 103.6ms/batch - loss: 16.03516 - diff: 12.44mlTrain batch 26/32 - 104.4ms/batch - loss: 15.91583 - diff: 12.43mlTrain batch 27/32 - 99.0ms/batch - loss: 15.77558 - diff: 12.39mlTrain batch 28/32 - 108.4ms/batch - loss: 15.47646 - diff: 12.23mlTrain batch 29/32 - 98.8ms/batch - loss: 15.57254 - diff: 12.32mlTrain batch 30/32 - 107.1ms/batch - loss: 15.58078 - diff: 12.35mlTrain batch 31/32 - 92.8ms/batch - loss: 16.25168 - diff: 12.67mlTrain batch 32/32 - 67.7ms/batch - loss: 16.67486 - diff: 12.68mlTrain batch 32/32 - 11.2s 67.7ms/batch - loss: 16.67486 - diff: 12.68ml
Test 0.6s: val_loss: 45.69799 - diff: 20.64ml

Epoch 136: current best loss = 42.78891, at epoch 126
Train batch 1/32 - 131.6ms/batch - loss: 14.80587 - diff: 12.60mlTrain batch 2/32 - 108.7ms/batch - loss: 21.03765 - diff: 15.59mlTrain batch 3/32 - 116.0ms/batch - loss: 25.30419 - diff: 17.55mlTrain batch 4/32 - 107.7ms/batch - loss: 29.95703 - diff: 19.24mlTrain batch 5/32 - 116.9ms/batch - loss: 25.43879 - diff: 17.08mlTrain batch 6/32 - 116.1ms/batch - loss: 21.67231 - diff: 15.16mlTrain batch 7/32 - 106.8ms/batch - loss: 20.29303 - diff: 14.75mlTrain batch 8/32 - 107.2ms/batch - loss: 23.45101 - diff: 15.83mlTrain batch 9/32 - 108.0ms/batch - loss: 21.75117 - diff: 15.08mlTrain batch 10/32 - 108.6ms/batch - loss: 23.54715 - diff: 15.75mlTrain batch 11/32 - 106.8ms/batch - loss: 22.34203 - diff: 15.18mlTrain batch 12/32 - 106.9ms/batch - loss: 26.34488 - diff: 16.30mlTrain batch 13/32 - 109.2ms/batch - loss: 25.25879 - diff: 15.86mlTrain batch 14/32 - 102.9ms/batch - loss: 24.12749 - diff: 15.45mlTrain batch 15/32 - 95.1ms/batch - loss: 23.22888 - diff: 15.10mlTrain batch 16/32 - 116.6ms/batch - loss: 23.31012 - diff: 15.31mlTrain batch 17/32 - 117.7ms/batch - loss: 22.42345 - diff: 14.93mlTrain batch 18/32 - 118.5ms/batch - loss: 23.15430 - diff: 15.11mlTrain batch 19/32 - 114.4ms/batch - loss: 22.33313 - diff: 14.75mlTrain batch 20/32 - 109.9ms/batch - loss: 21.82004 - diff: 14.55mlTrain batch 21/32 - 104.1ms/batch - loss: 21.14241 - diff: 14.29mlTrain batch 22/32 - 110.6ms/batch - loss: 21.07612 - diff: 14.29mlTrain batch 23/32 - 102.1ms/batch - loss: 20.55568 - diff: 14.09mlTrain batch 24/32 - 103.6ms/batch - loss: 20.29191 - diff: 14.04mlTrain batch 25/32 - 112.3ms/batch - loss: 19.91199 - diff: 13.90mlTrain batch 26/32 - 112.0ms/batch - loss: 19.56064 - diff: 13.76mlTrain batch 27/32 - 112.1ms/batch - loss: 19.03297 - diff: 13.52mlTrain batch 28/32 - 111.9ms/batch - loss: 18.67547 - diff: 13.42mlTrain batch 29/32 - 109.9ms/batch - loss: 18.31814 - diff: 13.26mlTrain batch 30/32 - 102.2ms/batch - loss: 18.24075 - diff: 13.30mlTrain batch 31/32 - 106.4ms/batch - loss: 18.27025 - diff: 13.38mlTrain batch 32/32 - 99.6ms/batch - loss: 28.38543 - diff: 13.85mlTrain batch 32/32 - 11.6s 99.6ms/batch - loss: 28.38543 - diff: 13.85ml
Test 0.6s: val_loss: 56.12815 - diff: 23.55ml

Epoch 137: current best loss = 42.78891, at epoch 126
Train batch 1/32 - 144.5ms/batch - loss: 7.39795 - diff: 8.56mlTrain batch 2/32 - 116.9ms/batch - loss: 12.31224 - diff: 10.93mlTrain batch 3/32 - 117.3ms/batch - loss: 11.90203 - diff: 10.76mlTrain batch 4/32 - 116.7ms/batch - loss: 14.19924 - diff: 12.36mlTrain batch 5/32 - 108.1ms/batch - loss: 12.60522 - diff: 11.58mlTrain batch 6/32 - 110.1ms/batch - loss: 13.32040 - diff: 11.91mlTrain batch 7/32 - 107.9ms/batch - loss: 12.80368 - diff: 11.50mlTrain batch 8/32 - 106.5ms/batch - loss: 12.83940 - diff: 11.71mlTrain batch 9/32 - 107.0ms/batch - loss: 20.76951 - diff: 13.88mlTrain batch 10/32 - 106.7ms/batch - loss: 23.95889 - diff: 15.23mlTrain batch 11/32 - 106.9ms/batch - loss: 30.72939 - diff: 17.35mlTrain batch 12/32 - 106.5ms/batch - loss: 28.66978 - diff: 16.56mlTrain batch 13/32 - 93.7ms/batch - loss: 27.75950 - diff: 16.42mlTrain batch 14/32 - 103.6ms/batch - loss: 26.46929 - diff: 15.99mlTrain batch 15/32 - 117.7ms/batch - loss: 25.69520 - diff: 15.80mlTrain batch 16/32 - 116.3ms/batch - loss: 25.34679 - diff: 15.71mlTrain batch 17/32 - 107.3ms/batch - loss: 24.72472 - diff: 15.54mlTrain batch 18/32 - 106.5ms/batch - loss: 25.02370 - diff: 15.63mlTrain batch 19/32 - 113.9ms/batch - loss: 24.04419 - diff: 15.24mlTrain batch 20/32 - 108.0ms/batch - loss: 24.44401 - diff: 15.25mlTrain batch 21/32 - 105.0ms/batch - loss: 24.07131 - diff: 15.06mlTrain batch 22/32 - 99.9ms/batch - loss: 25.72199 - diff: 15.73mlTrain batch 23/32 - 108.7ms/batch - loss: 25.85999 - diff: 15.81mlTrain batch 24/32 - 102.5ms/batch - loss: 24.99311 - diff: 15.45mlTrain batch 25/32 - 108.3ms/batch - loss: 25.21422 - diff: 15.60mlTrain batch 26/32 - 107.1ms/batch - loss: 25.17058 - diff: 15.64mlTrain batch 27/32 - 109.1ms/batch - loss: 25.56124 - diff: 15.75mlTrain batch 28/32 - 106.7ms/batch - loss: 25.47327 - diff: 15.71mlTrain batch 29/32 - 110.2ms/batch - loss: 24.93560 - diff: 15.55mlTrain batch 30/32 - 107.9ms/batch - loss: 24.33638 - diff: 15.30mlTrain batch 31/32 - 107.4ms/batch - loss: 24.20549 - diff: 15.26mlTrain batch 32/32 - 107.6ms/batch - loss: 28.25712 - diff: 15.49mlTrain batch 32/32 - 11.2s 107.6ms/batch - loss: 28.25712 - diff: 15.49ml
Test 0.7s: val_loss: 47.01349 - diff: 20.64ml
Epoch   138: reducing learning rate of group 0 to 3.1250e-05.

Epoch 138: current best loss = 42.78891, at epoch 126
Train batch 1/32 - 119.3ms/batch - loss: 6.55633 - diff: 8.24mlTrain batch 2/32 - 104.2ms/batch - loss: 14.71581 - diff: 12.62mlTrain batch 3/32 - 104.3ms/batch - loss: 16.42771 - diff: 13.51mlTrain batch 4/32 - 104.7ms/batch - loss: 17.20537 - diff: 13.70mlTrain batch 5/32 - 88.7ms/batch - loss: 15.69808 - diff: 12.60mlTrain batch 6/32 - 104.0ms/batch - loss: 14.50097 - diff: 12.09mlTrain batch 7/32 - 106.1ms/batch - loss: 13.57644 - diff: 11.75mlTrain batch 8/32 - 107.1ms/batch - loss: 19.78457 - diff: 14.14mlTrain batch 9/32 - 107.6ms/batch - loss: 18.79600 - diff: 13.68mlTrain batch 10/32 - 107.9ms/batch - loss: 17.59257 - diff: 13.15mlTrain batch 11/32 - 107.3ms/batch - loss: 16.73423 - diff: 12.84mlTrain batch 12/32 - 107.4ms/batch - loss: 16.21226 - diff: 12.62mlTrain batch 13/32 - 114.7ms/batch - loss: 15.46679 - diff: 12.26mlTrain batch 14/32 - 107.6ms/batch - loss: 14.80671 - diff: 11.96mlTrain batch 15/32 - 117.1ms/batch - loss: 14.68627 - diff: 11.93mlTrain batch 16/32 - 107.1ms/batch - loss: 14.24209 - diff: 11.79mlTrain batch 17/32 - 110.8ms/batch - loss: 14.66446 - diff: 12.06mlTrain batch 18/32 - 113.8ms/batch - loss: 15.72107 - diff: 12.57mlTrain batch 19/32 - 104.5ms/batch - loss: 15.99874 - diff: 12.75mlTrain batch 20/32 - 102.6ms/batch - loss: 15.98262 - diff: 12.82mlTrain batch 21/32 - 103.6ms/batch - loss: 15.83781 - diff: 12.77mlTrain batch 22/32 - 103.9ms/batch - loss: 17.68329 - diff: 13.40mlTrain batch 23/32 - 104.1ms/batch - loss: 17.51520 - diff: 13.32mlTrain batch 24/32 - 104.2ms/batch - loss: 17.93900 - diff: 13.57mlTrain batch 25/32 - 104.4ms/batch - loss: 17.52934 - diff: 13.40mlTrain batch 26/32 - 103.5ms/batch - loss: 17.81911 - diff: 13.47mlTrain batch 27/32 - 104.1ms/batch - loss: 18.06549 - diff: 13.61mlTrain batch 28/32 - 104.0ms/batch - loss: 18.08398 - diff: 13.69mlTrain batch 29/32 - 105.0ms/batch - loss: 18.41372 - diff: 13.81mlTrain batch 30/32 - 106.6ms/batch - loss: 18.37962 - diff: 13.77mlTrain batch 31/32 - 102.2ms/batch - loss: 18.52858 - diff: 13.81mlTrain batch 32/32 - 82.8ms/batch - loss: 21.84397 - diff: 14.03mlTrain batch 32/32 - 10.8s 82.8ms/batch - loss: 21.84397 - diff: 14.03ml
Test 0.6s: val_loss: 48.17176 - diff: 20.74ml

Epoch 139: current best loss = 42.78891, at epoch 126
Train batch 1/32 - 123.2ms/batch - loss: 16.37829 - diff: 12.47mlTrain batch 2/32 - 114.5ms/batch - loss: 11.28595 - diff: 10.36mlTrain batch 3/32 - 119.0ms/batch - loss: 10.07528 - diff: 9.58mlTrain batch 4/32 - 118.8ms/batch - loss: 8.80852 - diff: 9.01mlTrain batch 5/32 - 118.4ms/batch - loss: 11.31154 - diff: 10.37mlTrain batch 6/32 - 118.4ms/batch - loss: 13.80054 - diff: 11.31mlTrain batch 7/32 - 122.8ms/batch - loss: 12.54852 - diff: 10.71mlTrain batch 8/32 - 100.0ms/batch - loss: 12.49280 - diff: 10.67mlTrain batch 9/32 - 109.7ms/batch - loss: 11.76811 - diff: 10.34mlTrain batch 10/32 - 116.2ms/batch - loss: 12.43265 - diff: 10.73mlTrain batch 11/32 - 99.3ms/batch - loss: 12.62954 - diff: 11.03mlTrain batch 12/32 - 108.7ms/batch - loss: 13.32377 - diff: 11.47mlTrain batch 13/32 - 88.9ms/batch - loss: 14.14187 - diff: 11.83mlTrain batch 14/32 - 90.9ms/batch - loss: 14.85240 - diff: 12.11mlTrain batch 15/32 - 104.6ms/batch - loss: 14.49517 - diff: 12.00mlTrain batch 16/32 - 96.1ms/batch - loss: 14.81544 - diff: 12.21mlTrain batch 17/32 - 104.3ms/batch - loss: 15.50500 - diff: 12.58mlTrain batch 18/32 - 107.8ms/batch - loss: 18.94934 - diff: 13.66mlTrain batch 19/32 - 102.2ms/batch - loss: 19.72134 - diff: 13.99mlTrain batch 20/32 - 89.8ms/batch - loss: 19.29486 - diff: 13.87mlTrain batch 21/32 - 114.6ms/batch - loss: 18.94617 - diff: 13.75mlTrain batch 22/32 - 115.0ms/batch - loss: 18.39801 - diff: 13.52mlTrain batch 23/32 - 114.7ms/batch - loss: 18.64127 - diff: 13.63mlTrain batch 24/32 - 116.2ms/batch - loss: 18.25569 - diff: 13.46mlTrain batch 25/32 - 115.1ms/batch - loss: 18.58391 - diff: 13.64mlTrain batch 26/32 - 116.4ms/batch - loss: 20.07464 - diff: 14.15mlTrain batch 27/32 - 114.7ms/batch - loss: 19.76978 - diff: 14.04mlTrain batch 28/32 - 121.1ms/batch - loss: 20.05689 - diff: 14.15mlTrain batch 29/32 - 117.0ms/batch - loss: 19.71871 - diff: 13.99mlTrain batch 30/32 - 117.8ms/batch - loss: 21.53472 - diff: 14.62mlTrain batch 31/32 - 115.6ms/batch - loss: 21.06553 - diff: 14.42mlTrain batch 32/32 - 84.0ms/batch - loss: 23.78799 - diff: 14.60mlTrain batch 32/32 - 11.6s 84.0ms/batch - loss: 23.78799 - diff: 14.60ml
Test 0.7s: val_loss: 46.49200 - diff: 20.42ml

Epoch 140: current best loss = 42.78891, at epoch 126
Train batch 1/32 - 121.8ms/batch - loss: 13.42268 - diff: 9.28mlTrain batch 2/32 - 108.5ms/batch - loss: 14.15652 - diff: 10.55mlTrain batch 3/32 - 108.4ms/batch - loss: 11.82078 - diff: 9.90mlTrain batch 4/32 - 108.0ms/batch - loss: 11.59746 - diff: 10.11mlTrain batch 5/32 - 114.9ms/batch - loss: 14.40618 - diff: 11.60mlTrain batch 6/32 - 103.9ms/batch - loss: 13.20067 - diff: 11.13mlTrain batch 7/32 - 112.2ms/batch - loss: 12.83754 - diff: 10.96mlTrain batch 8/32 - 104.1ms/batch - loss: 13.45267 - diff: 11.09mlTrain batch 9/32 - 114.2ms/batch - loss: 16.13751 - diff: 12.24mlTrain batch 10/32 - 103.9ms/batch - loss: 15.41616 - diff: 11.90mlTrain batch 11/32 - 106.9ms/batch - loss: 14.79953 - diff: 11.72mlTrain batch 12/32 - 103.9ms/batch - loss: 14.50975 - diff: 11.55mlTrain batch 13/32 - 100.7ms/batch - loss: 14.08702 - diff: 11.28mlTrain batch 14/32 - 88.3ms/batch - loss: 13.37087 - diff: 10.87mlTrain batch 15/32 - 114.3ms/batch - loss: 12.95535 - diff: 10.73mlTrain batch 16/32 - 112.2ms/batch - loss: 12.84582 - diff: 10.80mlTrain batch 17/32 - 124.3ms/batch - loss: 12.66221 - diff: 10.82mlTrain batch 18/32 - 117.5ms/batch - loss: 13.46101 - diff: 11.23mlTrain batch 19/32 - 123.1ms/batch - loss: 14.33162 - diff: 11.72mlTrain batch 20/32 - 116.0ms/batch - loss: 14.69508 - diff: 11.90mlTrain batch 21/32 - 123.6ms/batch - loss: 14.35253 - diff: 11.66mlTrain batch 22/32 - 115.7ms/batch - loss: 14.21953 - diff: 11.64mlTrain batch 23/32 - 119.8ms/batch - loss: 14.02066 - diff: 11.53mlTrain batch 24/32 - 115.0ms/batch - loss: 14.93133 - diff: 11.96mlTrain batch 25/32 - 116.7ms/batch - loss: 14.79896 - diff: 11.95mlTrain batch 26/32 - 115.5ms/batch - loss: 14.50913 - diff: 11.87mlTrain batch 27/32 - 123.2ms/batch - loss: 14.22983 - diff: 11.75mlTrain batch 28/32 - 115.0ms/batch - loss: 14.17133 - diff: 11.74mlTrain batch 29/32 - 112.1ms/batch - loss: 14.84286 - diff: 12.03mlTrain batch 30/32 - 103.7ms/batch - loss: 15.04086 - diff: 12.12mlTrain batch 31/32 - 91.5ms/batch - loss: 15.11587 - diff: 12.20mlTrain batch 32/32 - 83.4ms/batch - loss: 15.57029 - diff: 12.20mlTrain batch 32/32 - 10.9s 83.4ms/batch - loss: 15.57029 - diff: 12.20ml
Test 0.7s: val_loss: 46.09370 - diff: 20.24ml

Epoch 141: current best loss = 42.78891, at epoch 126
Train batch 1/32 - 138.8ms/batch - loss: 47.66554 - diff: 25.79mlTrain batch 2/32 - 116.8ms/batch - loss: 47.32245 - diff: 25.31mlTrain batch 3/32 - 123.5ms/batch - loss: 33.73406 - diff: 19.73mlTrain batch 4/32 - 116.6ms/batch - loss: 38.59857 - diff: 21.75mlTrain batch 5/32 - 123.2ms/batch - loss: 38.82897 - diff: 21.73mlTrain batch 6/32 - 116.6ms/batch - loss: 38.64742 - diff: 20.80mlTrain batch 7/32 - 123.1ms/batch - loss: 35.06925 - diff: 19.54mlTrain batch 8/32 - 111.9ms/batch - loss: 31.42603 - diff: 18.10mlTrain batch 9/32 - 111.3ms/batch - loss: 30.96585 - diff: 18.10mlTrain batch 10/32 - 105.3ms/batch - loss: 28.26249 - diff: 16.94mlTrain batch 11/32 - 111.9ms/batch - loss: 27.09868 - diff: 16.58mlTrain batch 12/32 - 105.7ms/batch - loss: 26.62536 - diff: 16.42mlTrain batch 13/32 - 114.3ms/batch - loss: 25.78981 - diff: 16.15mlTrain batch 14/32 - 107.6ms/batch - loss: 25.58686 - diff: 16.19mlTrain batch 15/32 - 108.7ms/batch - loss: 26.42924 - diff: 16.58mlTrain batch 16/32 - 82.4ms/batch - loss: 27.56128 - diff: 16.96mlTrain batch 17/32 - 100.6ms/batch - loss: 26.36221 - diff: 16.52mlTrain batch 18/32 - 116.8ms/batch - loss: 25.94713 - diff: 16.40mlTrain batch 19/32 - 115.7ms/batch - loss: 25.26443 - diff: 16.12mlTrain batch 20/32 - 114.3ms/batch - loss: 24.79761 - diff: 15.98mlTrain batch 21/32 - 117.1ms/batch - loss: 23.91184 - diff: 15.61mlTrain batch 22/32 - 117.2ms/batch - loss: 24.07211 - diff: 15.75mlTrain batch 23/32 - 118.9ms/batch - loss: 23.73245 - diff: 15.70mlTrain batch 24/32 - 117.8ms/batch - loss: 23.11613 - diff: 15.48mlTrain batch 25/32 - 117.0ms/batch - loss: 22.62669 - diff: 15.25mlTrain batch 26/32 - 106.9ms/batch - loss: 23.16994 - diff: 15.49mlTrain batch 27/32 - 105.6ms/batch - loss: 24.16808 - diff: 15.86mlTrain batch 28/32 - 113.0ms/batch - loss: 23.92580 - diff: 15.81mlTrain batch 29/32 - 107.9ms/batch - loss: 23.32789 - diff: 15.53mlTrain batch 30/32 - 107.1ms/batch - loss: 23.19440 - diff: 15.49mlTrain batch 31/32 - 100.2ms/batch - loss: 22.77303 - diff: 15.32mlTrain batch 32/32 - 80.9ms/batch - loss: 22.86440 - diff: 15.28mlTrain batch 32/32 - 11.0s 80.9ms/batch - loss: 22.86440 - diff: 15.28ml
Test 0.6s: val_loss: 45.56509 - diff: 20.25ml

Epoch 142: current best loss = 42.78891, at epoch 126
Train batch 1/32 - 122.2ms/batch - loss: 9.08336 - diff: 10.41mlTrain batch 2/32 - 118.9ms/batch - loss: 27.93540 - diff: 17.41mlTrain batch 3/32 - 108.3ms/batch - loss: 21.17172 - diff: 14.64mlTrain batch 4/32 - 110.0ms/batch - loss: 17.66805 - diff: 13.09mlTrain batch 5/32 - 114.8ms/batch - loss: 16.44570 - diff: 12.80mlTrain batch 6/32 - 116.0ms/batch - loss: 14.33489 - diff: 11.72mlTrain batch 7/32 - 112.2ms/batch - loss: 13.57061 - diff: 11.49mlTrain batch 8/32 - 113.1ms/batch - loss: 13.12391 - diff: 11.26mlTrain batch 9/32 - 111.4ms/batch - loss: 12.74941 - diff: 11.09mlTrain batch 10/32 - 113.5ms/batch - loss: 14.95561 - diff: 11.92mlTrain batch 11/32 - 106.3ms/batch - loss: 16.99144 - diff: 12.61mlTrain batch 12/32 - 109.5ms/batch - loss: 17.04181 - diff: 12.73mlTrain batch 13/32 - 107.3ms/batch - loss: 18.37932 - diff: 13.44mlTrain batch 14/32 - 107.6ms/batch - loss: 17.64087 - diff: 13.20mlTrain batch 15/32 - 106.5ms/batch - loss: 17.15755 - diff: 13.02mlTrain batch 16/32 - 91.8ms/batch - loss: 16.51907 - diff: 12.74mlTrain batch 17/32 - 91.1ms/batch - loss: 17.92370 - diff: 13.31mlTrain batch 18/32 - 114.0ms/batch - loss: 17.47363 - diff: 13.10mlTrain batch 19/32 - 95.1ms/batch - loss: 17.12769 - diff: 12.93mlTrain batch 20/32 - 116.8ms/batch - loss: 16.84743 - diff: 12.90mlTrain batch 21/32 - 112.6ms/batch - loss: 17.81619 - diff: 13.35mlTrain batch 22/32 - 103.6ms/batch - loss: 18.26690 - diff: 13.61mlTrain batch 23/32 - 103.3ms/batch - loss: 18.27602 - diff: 13.51mlTrain batch 24/32 - 103.5ms/batch - loss: 19.45035 - diff: 13.85mlTrain batch 25/32 - 103.6ms/batch - loss: 18.95363 - diff: 13.64mlTrain batch 26/32 - 103.0ms/batch - loss: 18.98069 - diff: 13.63mlTrain batch 27/32 - 104.2ms/batch - loss: 18.80556 - diff: 13.59mlTrain batch 28/32 - 103.0ms/batch - loss: 19.11276 - diff: 13.65mlTrain batch 29/32 - 97.9ms/batch - loss: 18.78702 - diff: 13.49mlTrain batch 30/32 - 102.1ms/batch - loss: 18.57520 - diff: 13.31mlTrain batch 31/32 - 87.1ms/batch - loss: 18.18344 - diff: 13.15mlTrain batch 32/32 - 69.4ms/batch - loss: 19.41809 - diff: 13.21mlTrain batch 32/32 - 11.2s 69.4ms/batch - loss: 19.41809 - diff: 13.21ml
Test 0.6s: val_loss: 47.54594 - diff: 20.33ml

Epoch 143: current best loss = 42.78891, at epoch 126
Train batch 1/32 - 120.3ms/batch - loss: 26.47495 - diff: 16.50mlTrain batch 2/32 - 103.8ms/batch - loss: 16.97216 - diff: 12.53mlTrain batch 3/32 - 105.0ms/batch - loss: 13.96759 - diff: 11.61mlTrain batch 4/32 - 103.4ms/batch - loss: 14.11788 - diff: 11.82mlTrain batch 5/32 - 104.4ms/batch - loss: 15.27010 - diff: 11.89mlTrain batch 6/32 - 103.0ms/batch - loss: 16.32880 - diff: 12.59mlTrain batch 7/32 - 105.0ms/batch - loss: 18.94297 - diff: 13.51mlTrain batch 8/32 - 105.5ms/batch - loss: 17.51070 - diff: 12.92mlTrain batch 9/32 - 106.8ms/batch - loss: 16.67462 - diff: 12.67mlTrain batch 10/32 - 107.9ms/batch - loss: 16.02104 - diff: 12.46mlTrain batch 11/32 - 106.8ms/batch - loss: 16.47054 - diff: 12.78mlTrain batch 12/32 - 110.2ms/batch - loss: 16.29974 - diff: 12.79mlTrain batch 13/32 - 110.9ms/batch - loss: 15.73579 - diff: 12.52mlTrain batch 14/32 - 108.0ms/batch - loss: 15.64619 - diff: 12.56mlTrain batch 15/32 - 107.2ms/batch - loss: 15.04098 - diff: 12.27mlTrain batch 16/32 - 108.7ms/batch - loss: 15.72903 - diff: 12.63mlTrain batch 17/32 - 99.6ms/batch - loss: 15.47113 - diff: 12.55mlTrain batch 18/32 - 101.6ms/batch - loss: 15.03646 - diff: 12.32mlTrain batch 19/32 - 83.8ms/batch - loss: 14.56799 - diff: 12.05mlTrain batch 20/32 - 106.6ms/batch - loss: 14.95342 - diff: 12.17mlTrain batch 21/32 - 111.8ms/batch - loss: 15.28342 - diff: 12.40mlTrain batch 22/32 - 119.8ms/batch - loss: 15.02092 - diff: 12.25mlTrain batch 23/32 - 112.1ms/batch - loss: 15.20776 - diff: 12.35mlTrain batch 24/32 - 111.5ms/batch - loss: 15.68911 - diff: 12.56mlTrain batch 25/32 - 102.6ms/batch - loss: 16.11171 - diff: 12.77mlTrain batch 26/32 - 105.1ms/batch - loss: 15.74831 - diff: 12.60mlTrain batch 27/32 - 103.1ms/batch - loss: 19.23415 - diff: 13.34mlTrain batch 28/32 - 104.9ms/batch - loss: 19.24899 - diff: 13.42mlTrain batch 29/32 - 102.6ms/batch - loss: 18.81280 - diff: 13.21mlTrain batch 30/32 - 108.8ms/batch - loss: 18.54119 - diff: 13.11mlTrain batch 31/32 - 90.8ms/batch - loss: 18.53831 - diff: 13.15mlTrain batch 32/32 - 91.0ms/batch - loss: 19.09864 - diff: 13.17mlTrain batch 32/32 - 11.1s 91.0ms/batch - loss: 19.09864 - diff: 13.17ml
Test 0.7s: val_loss: 46.31335 - diff: 20.42ml

Epoch 144: current best loss = 42.78891, at epoch 126
Train batch 1/32 - 117.8ms/batch - loss: 6.76747 - diff: 8.90mlTrain batch 2/32 - 104.4ms/batch - loss: 6.38127 - diff: 8.21mlTrain batch 3/32 - 104.3ms/batch - loss: 6.31687 - diff: 7.82mlTrain batch 4/32 - 104.5ms/batch - loss: 11.75313 - diff: 10.31mlTrain batch 5/32 - 103.6ms/batch - loss: 17.22392 - diff: 12.66mlTrain batch 6/32 - 104.3ms/batch - loss: 15.92006 - diff: 12.34mlTrain batch 7/32 - 112.4ms/batch - loss: 15.82514 - diff: 12.66mlTrain batch 8/32 - 113.6ms/batch - loss: 14.66009 - diff: 12.05mlTrain batch 9/32 - 113.4ms/batch - loss: 15.41865 - diff: 12.33mlTrain batch 10/32 - 112.4ms/batch - loss: 24.43980 - diff: 14.93mlTrain batch 11/32 - 112.2ms/batch - loss: 23.18350 - diff: 14.48mlTrain batch 12/32 - 111.8ms/batch - loss: 22.18043 - diff: 14.17mlTrain batch 13/32 - 105.1ms/batch - loss: 21.10274 - diff: 13.76mlTrain batch 14/32 - 103.4ms/batch - loss: 20.11640 - diff: 13.38mlTrain batch 15/32 - 107.9ms/batch - loss: 22.55313 - diff: 14.31mlTrain batch 16/32 - 105.2ms/batch - loss: 21.73495 - diff: 14.04mlTrain batch 17/32 - 103.9ms/batch - loss: 21.03739 - diff: 13.82mlTrain batch 18/32 - 104.5ms/batch - loss: 24.00285 - diff: 14.77mlTrain batch 19/32 - 96.6ms/batch - loss: 23.71117 - diff: 14.74mlTrain batch 20/32 - 105.2ms/batch - loss: 23.04334 - diff: 14.52mlTrain batch 21/32 - 94.8ms/batch - loss: 22.66104 - diff: 14.43mlTrain batch 22/32 - 89.2ms/batch - loss: 26.66806 - diff: 15.61mlTrain batch 23/32 - 109.1ms/batch - loss: 25.92797 - diff: 15.38mlTrain batch 24/32 - 106.6ms/batch - loss: 25.39178 - diff: 15.23mlTrain batch 25/32 - 107.9ms/batch - loss: 25.12758 - diff: 15.19mlTrain batch 26/32 - 107.4ms/batch - loss: 24.67602 - diff: 15.08mlTrain batch 27/32 - 108.9ms/batch - loss: 23.90280 - diff: 14.71mlTrain batch 28/32 - 107.6ms/batch - loss: 23.32158 - diff: 14.48mlTrain batch 29/32 - 108.7ms/batch - loss: 22.84594 - diff: 14.34mlTrain batch 30/32 - 109.7ms/batch - loss: 22.39208 - diff: 14.16mlTrain batch 31/32 - 107.1ms/batch - loss: 22.80041 - diff: 14.35mlTrain batch 32/32 - 83.1ms/batch - loss: 23.26399 - diff: 14.37mlTrain batch 32/32 - 10.8s 83.1ms/batch - loss: 23.26399 - diff: 14.37ml
Test 0.6s: val_loss: 44.79278 - diff: 20.08ml

Epoch 145: current best loss = 42.78891, at epoch 126
Train batch 1/32 - 124.8ms/batch - loss: 4.18411 - diff: 6.14mlTrain batch 2/32 - 108.4ms/batch - loss: 4.89525 - diff: 6.68mlTrain batch 3/32 - 112.6ms/batch - loss: 9.67666 - diff: 9.48mlTrain batch 4/32 - 109.3ms/batch - loss: 12.20906 - diff: 10.41mlTrain batch 5/32 - 114.1ms/batch - loss: 11.85774 - diff: 10.38mlTrain batch 6/32 - 121.1ms/batch - loss: 13.89966 - diff: 11.72mlTrain batch 7/32 - 107.9ms/batch - loss: 13.69395 - diff: 11.46mlTrain batch 8/32 - 108.0ms/batch - loss: 16.52112 - diff: 12.88mlTrain batch 9/32 - 107.1ms/batch - loss: 16.89044 - diff: 13.14mlTrain batch 10/32 - 108.0ms/batch - loss: 17.78717 - diff: 13.54mlTrain batch 11/32 - 103.9ms/batch - loss: 17.14520 - diff: 13.22mlTrain batch 12/32 - 108.1ms/batch - loss: 18.17702 - diff: 13.69mlTrain batch 13/32 - 116.9ms/batch - loss: 18.27081 - diff: 13.78mlTrain batch 14/32 - 110.6ms/batch - loss: 17.53909 - diff: 13.45mlTrain batch 15/32 - 106.1ms/batch - loss: 16.83938 - diff: 13.10mlTrain batch 16/32 - 101.0ms/batch - loss: 17.34048 - diff: 13.29mlTrain batch 17/32 - 103.9ms/batch - loss: 16.97736 - diff: 13.08mlTrain batch 18/32 - 107.2ms/batch - loss: 16.68091 - diff: 12.97mlTrain batch 19/32 - 101.7ms/batch - loss: 16.07757 - diff: 12.71mlTrain batch 20/32 - 104.1ms/batch - loss: 15.76678 - diff: 12.62mlTrain batch 21/32 - 106.5ms/batch - loss: 15.59221 - diff: 12.44mlTrain batch 22/32 - 112.9ms/batch - loss: 15.38946 - diff: 12.33mlTrain batch 23/32 - 113.5ms/batch - loss: 15.54027 - diff: 12.43mlTrain batch 24/32 - 114.1ms/batch - loss: 15.15829 - diff: 12.21mlTrain batch 25/32 - 114.0ms/batch - loss: 14.91843 - diff: 12.10mlTrain batch 26/32 - 114.2ms/batch - loss: 14.95129 - diff: 12.15mlTrain batch 27/32 - 114.6ms/batch - loss: 14.90390 - diff: 12.16mlTrain batch 28/32 - 114.6ms/batch - loss: 14.53759 - diff: 11.98mlTrain batch 29/32 - 108.9ms/batch - loss: 14.58698 - diff: 12.01mlTrain batch 30/32 - 115.0ms/batch - loss: 14.27254 - diff: 11.85mlTrain batch 31/32 - 116.7ms/batch - loss: 14.60935 - diff: 12.04mlTrain batch 32/32 - 85.9ms/batch - loss: 15.35666 - diff: 12.10mlTrain batch 32/32 - 11.8s 85.9ms/batch - loss: 15.35666 - diff: 12.10ml
Test 0.7s: val_loss: 48.12350 - diff: 20.28ml

Epoch 146: current best loss = 42.78891, at epoch 126
Train batch 1/32 - 138.1ms/batch - loss: 19.09118 - diff: 13.05mlTrain batch 2/32 - 113.3ms/batch - loss: 13.28059 - diff: 11.27mlTrain batch 3/32 - 113.7ms/batch - loss: 14.18598 - diff: 12.21mlTrain batch 4/32 - 112.8ms/batch - loss: 12.77321 - diff: 11.65mlTrain batch 5/32 - 113.5ms/batch - loss: 13.31006 - diff: 11.82mlTrain batch 6/32 - 112.7ms/batch - loss: 12.67104 - diff: 11.50mlTrain batch 7/32 - 104.4ms/batch - loss: 12.54404 - diff: 11.33mlTrain batch 8/32 - 103.5ms/batch - loss: 12.09417 - diff: 11.21mlTrain batch 9/32 - 104.9ms/batch - loss: 11.18003 - diff: 10.66mlTrain batch 10/32 - 102.9ms/batch - loss: 11.73804 - diff: 10.98mlTrain batch 11/32 - 112.4ms/batch - loss: 11.47173 - diff: 10.81mlTrain batch 12/32 - 102.8ms/batch - loss: 11.81019 - diff: 11.08mlTrain batch 13/32 - 113.0ms/batch - loss: 11.68853 - diff: 10.97mlTrain batch 14/32 - 112.5ms/batch - loss: 12.16195 - diff: 11.01mlTrain batch 15/32 - 112.1ms/batch - loss: 12.13795 - diff: 10.99mlTrain batch 16/32 - 112.6ms/batch - loss: 12.05867 - diff: 10.96mlTrain batch 17/32 - 104.2ms/batch - loss: 12.12895 - diff: 10.98mlTrain batch 18/32 - 82.5ms/batch - loss: 14.05067 - diff: 11.70mlTrain batch 19/32 - 93.3ms/batch - loss: 13.77402 - diff: 11.61mlTrain batch 20/32 - 89.1ms/batch - loss: 13.41707 - diff: 11.47mlTrain batch 21/32 - 112.8ms/batch - loss: 13.11842 - diff: 11.35mlTrain batch 22/32 - 112.6ms/batch - loss: 12.83531 - diff: 11.22mlTrain batch 23/32 - 114.2ms/batch - loss: 13.11392 - diff: 11.30mlTrain batch 24/32 - 113.9ms/batch - loss: 13.19471 - diff: 11.37mlTrain batch 25/32 - 111.8ms/batch - loss: 13.30414 - diff: 11.43mlTrain batch 26/32 - 116.0ms/batch - loss: 13.20689 - diff: 11.33mlTrain batch 27/32 - 116.1ms/batch - loss: 14.36904 - diff: 11.80mlTrain batch 28/32 - 92.7ms/batch - loss: 14.39006 - diff: 11.83mlTrain batch 29/32 - 116.1ms/batch - loss: 14.25879 - diff: 11.80mlTrain batch 30/32 - 123.2ms/batch - loss: 14.75862 - diff: 11.98mlTrain batch 31/32 - 115.4ms/batch - loss: 14.96587 - diff: 12.13mlTrain batch 32/32 - 86.5ms/batch - loss: 16.79249 - diff: 12.24mlTrain batch 32/32 - 10.8s 86.5ms/batch - loss: 16.79249 - diff: 12.24ml
Test 0.6s: val_loss: 46.12491 - diff: 20.68ml

Epoch 147: current best loss = 42.78891, at epoch 126
Train batch 1/32 - 141.2ms/batch - loss: 3.03259 - diff: 5.33mlTrain batch 2/32 - 115.8ms/batch - loss: 35.09673 - diff: 17.99mlTrain batch 3/32 - 123.6ms/batch - loss: 25.16170 - diff: 14.61mlTrain batch 4/32 - 115.7ms/batch - loss: 22.15669 - diff: 13.94mlTrain batch 5/32 - 122.5ms/batch - loss: 21.66763 - diff: 14.05mlTrain batch 6/32 - 115.5ms/batch - loss: 21.67187 - diff: 14.42mlTrain batch 7/32 - 121.8ms/batch - loss: 19.92935 - diff: 13.80mlTrain batch 8/32 - 114.6ms/batch - loss: 18.48772 - diff: 13.28mlTrain batch 9/32 - 117.0ms/batch - loss: 18.21501 - diff: 13.08mlTrain batch 10/32 - 114.3ms/batch - loss: 17.40896 - diff: 12.64mlTrain batch 11/32 - 106.0ms/batch - loss: 16.61316 - diff: 12.38mlTrain batch 12/32 - 103.6ms/batch - loss: 16.08803 - diff: 12.26mlTrain batch 13/32 - 112.2ms/batch - loss: 15.63455 - diff: 12.06mlTrain batch 14/32 - 105.4ms/batch - loss: 14.94152 - diff: 11.80mlTrain batch 15/32 - 104.0ms/batch - loss: 14.46361 - diff: 11.67mlTrain batch 16/32 - 105.7ms/batch - loss: 15.43965 - diff: 12.19mlTrain batch 17/32 - 103.7ms/batch - loss: 16.14967 - diff: 12.44mlTrain batch 18/32 - 104.4ms/batch - loss: 16.02925 - diff: 12.38mlTrain batch 19/32 - 114.6ms/batch - loss: 15.61484 - diff: 12.18mlTrain batch 20/32 - 112.3ms/batch - loss: 15.43205 - diff: 12.16mlTrain batch 21/32 - 110.7ms/batch - loss: 16.33638 - diff: 12.55mlTrain batch 22/32 - 107.5ms/batch - loss: 17.32112 - diff: 13.00mlTrain batch 23/32 - 115.7ms/batch - loss: 16.97219 - diff: 12.90mlTrain batch 24/32 - 108.0ms/batch - loss: 18.15829 - diff: 13.42mlTrain batch 25/32 - 125.0ms/batch - loss: 18.30523 - diff: 13.54mlTrain batch 26/32 - 117.2ms/batch - loss: 18.02445 - diff: 13.46mlTrain batch 27/32 - 124.8ms/batch - loss: 17.76402 - diff: 13.32mlTrain batch 28/32 - 118.0ms/batch - loss: 17.74147 - diff: 13.30mlTrain batch 29/32 - 108.9ms/batch - loss: 17.38793 - diff: 13.12mlTrain batch 30/32 - 117.5ms/batch - loss: 17.23058 - diff: 13.06mlTrain batch 31/32 - 94.7ms/batch - loss: 16.94718 - diff: 12.91mlTrain batch 32/32 - 80.0ms/batch - loss: 17.47629 - diff: 12.94mlTrain batch 32/32 - 10.7s 80.0ms/batch - loss: 17.47629 - diff: 12.94ml
Test 0.6s: val_loss: 47.35512 - diff: 20.06ml

Epoch 148: current best loss = 42.78891, at epoch 126
Train batch 1/32 - 113.2ms/batch - loss: 6.18868 - diff: 8.68mlTrain batch 2/32 - 81.9ms/batch - loss: 9.13226 - diff: 10.39mlTrain batch 3/32 - 114.8ms/batch - loss: 12.95769 - diff: 11.95mlTrain batch 4/32 - 105.2ms/batch - loss: 11.13926 - diff: 10.94mlTrain batch 5/32 - 111.3ms/batch - loss: 10.81797 - diff: 11.01mlTrain batch 6/32 - 101.8ms/batch - loss: 9.94580 - diff: 10.43mlTrain batch 7/32 - 102.0ms/batch - loss: 12.87569 - diff: 11.09mlTrain batch 8/32 - 105.2ms/batch - loss: 13.43249 - diff: 11.37mlTrain batch 9/32 - 94.6ms/batch - loss: 12.74632 - diff: 11.06mlTrain batch 10/32 - 91.4ms/batch - loss: 13.27492 - diff: 11.28mlTrain batch 11/32 - 92.6ms/batch - loss: 13.50001 - diff: 11.42mlTrain batch 12/32 - 90.9ms/batch - loss: 12.82105 - diff: 11.06mlTrain batch 13/32 - 97.5ms/batch - loss: 12.06276 - diff: 10.61mlTrain batch 14/32 - 94.3ms/batch - loss: 11.62691 - diff: 10.36mlTrain batch 15/32 - 113.2ms/batch - loss: 11.40254 - diff: 10.15mlTrain batch 16/32 - 92.0ms/batch - loss: 11.36781 - diff: 10.22mlTrain batch 17/32 - 125.1ms/batch - loss: 10.93560 - diff: 9.99mlTrain batch 18/32 - 115.9ms/batch - loss: 10.63909 - diff: 9.81mlTrain batch 19/32 - 113.5ms/batch - loss: 10.39163 - diff: 9.71mlTrain batch 20/32 - 97.6ms/batch - loss: 10.82126 - diff: 9.93mlTrain batch 21/32 - 95.4ms/batch - loss: 11.11599 - diff: 10.05mlTrain batch 22/32 - 91.0ms/batch - loss: 11.18124 - diff: 10.10mlTrain batch 23/32 - 115.2ms/batch - loss: 11.24153 - diff: 10.10mlTrain batch 24/32 - 90.9ms/batch - loss: 11.02975 - diff: 10.04mlTrain batch 25/32 - 125.5ms/batch - loss: 10.96597 - diff: 10.01mlTrain batch 26/32 - 102.4ms/batch - loss: 10.86770 - diff: 9.97mlTrain batch 27/32 - 97.8ms/batch - loss: 11.70577 - diff: 10.36mlTrain batch 28/32 - 90.7ms/batch - loss: 11.63875 - diff: 10.38mlTrain batch 29/32 - 101.0ms/batch - loss: 11.70215 - diff: 10.35mlTrain batch 30/32 - 95.5ms/batch - loss: 11.55446 - diff: 10.30mlTrain batch 31/32 - 107.8ms/batch - loss: 11.77947 - diff: 10.45mlTrain batch 32/32 - 74.9ms/batch - loss: 11.94946 - diff: 10.44mlTrain batch 32/32 - 10.5s 74.9ms/batch - loss: 11.94946 - diff: 10.44ml
Test 0.5s: val_loss: 45.27415 - diff: 20.40ml
Epoch   149: reducing learning rate of group 0 to 1.5625e-05.

Epoch 149: current best loss = 42.78891, at epoch 126
Train batch 1/32 - 133.0ms/batch - loss: 12.10029 - diff: 12.35mlTrain batch 2/32 - 111.4ms/batch - loss: 12.03533 - diff: 11.54mlTrain batch 3/32 - 93.5ms/batch - loss: 19.01864 - diff: 14.80mlTrain batch 4/32 - 106.4ms/batch - loss: 17.20565 - diff: 14.11mlTrain batch 5/32 - 91.2ms/batch - loss: 16.24009 - diff: 13.36mlTrain batch 6/32 - 91.1ms/batch - loss: 15.77231 - diff: 13.05mlTrain batch 7/32 - 91.5ms/batch - loss: 14.51231 - diff: 12.41mlTrain batch 8/32 - 91.2ms/batch - loss: 13.75481 - diff: 11.92mlTrain batch 9/32 - 116.1ms/batch - loss: 18.52816 - diff: 13.82mlTrain batch 10/32 - 115.8ms/batch - loss: 18.95605 - diff: 13.86mlTrain batch 11/32 - 116.3ms/batch - loss: 20.06052 - diff: 14.32mlTrain batch 12/32 - 98.2ms/batch - loss: 19.20916 - diff: 14.02mlTrain batch 13/32 - 116.3ms/batch - loss: 22.69951 - diff: 15.19mlTrain batch 14/32 - 103.3ms/batch - loss: 22.38527 - diff: 15.02mlTrain batch 15/32 - 91.2ms/batch - loss: 23.29449 - diff: 15.25mlTrain batch 16/32 - 100.9ms/batch - loss: 22.60522 - diff: 14.98mlTrain batch 17/32 - 91.7ms/batch - loss: 22.13252 - diff: 14.90mlTrain batch 18/32 - 93.0ms/batch - loss: 21.63753 - diff: 14.70mlTrain batch 19/32 - 93.4ms/batch - loss: 20.68633 - diff: 14.25mlTrain batch 20/32 - 91.1ms/batch - loss: 20.35792 - diff: 14.16mlTrain batch 21/32 - 92.7ms/batch - loss: 19.74061 - diff: 13.86mlTrain batch 22/32 - 91.5ms/batch - loss: 19.25772 - diff: 13.63mlTrain batch 23/32 - 92.1ms/batch - loss: 19.11340 - diff: 13.53mlTrain batch 24/32 - 91.0ms/batch - loss: 18.83286 - diff: 13.45mlTrain batch 25/32 - 92.8ms/batch - loss: 18.91687 - diff: 13.58mlTrain batch 26/32 - 91.0ms/batch - loss: 19.09423 - diff: 13.71mlTrain batch 27/32 - 93.2ms/batch - loss: 18.89281 - diff: 13.67mlTrain batch 28/32 - 91.0ms/batch - loss: 19.23570 - diff: 13.82mlTrain batch 29/32 - 91.4ms/batch - loss: 18.87027 - diff: 13.68mlTrain batch 30/32 - 91.3ms/batch - loss: 18.96412 - diff: 13.73mlTrain batch 31/32 - 90.2ms/batch - loss: 18.75368 - diff: 13.65mlTrain batch 32/32 - 80.7ms/batch - loss: 19.54915 - diff: 13.71mlTrain batch 32/32 - 10.5s 80.7ms/batch - loss: 19.54915 - diff: 13.71ml
Test 0.6s: val_loss: 47.60006 - diff: 20.64ml

Traceback (most recent call last):
  File "train.py", line 266, in <module>
    plot_results(train_losses, test_losses, title=f"Loss {target_label}", save_as=exp_name + "_loss")
  File "/home/allochi/tfm/workspace/lib/utils.py", line 393, in plot_results
    plt.plot(train_values, "r", label="train")
  File "/home/allochi/.conda/envs/DL_env/lib/python3.8/site-packages/matplotlib/pyplot.py", line 2824, in plot
    return gca().plot(
  File "/home/allochi/.conda/envs/DL_env/lib/python3.8/site-packages/matplotlib/pyplot.py", line 2352, in gca
    return gcf().gca(**kwargs)
  File "/home/allochi/.conda/envs/DL_env/lib/python3.8/site-packages/matplotlib/pyplot.py", line 731, in gcf
    return figure()
  File "/home/allochi/.conda/envs/DL_env/lib/python3.8/site-packages/matplotlib/pyplot.py", line 671, in figure
    figManager = new_figure_manager(num, figsize=figsize,
  File "/home/allochi/.conda/envs/DL_env/lib/python3.8/site-packages/matplotlib/pyplot.py", line 299, in new_figure_manager
    return _backend_mod.new_figure_manager(*args, **kwargs)
  File "/home/allochi/.conda/envs/DL_env/lib/python3.8/site-packages/matplotlib/backend_bases.py", line 3494, in new_figure_manager
    return cls.new_figure_manager_given_figure(num, fig)
  File "/home/allochi/.conda/envs/DL_env/lib/python3.8/site-packages/matplotlib/backends/_backend_tk.py", line 868, in new_figure_manager_given_figure
    window = tk.Tk(className="matplotlib")
  File "/home/allochi/.conda/envs/DL_env/lib/python3.8/tkinter/__init__.py", line 2261, in __init__
    self.tk = _tkinter.create(screenName, baseName, className, interactive, wantobjects, useTk, sync, use)
_tkinter.TclError: couldn't connect to display "localhost:10.0"
