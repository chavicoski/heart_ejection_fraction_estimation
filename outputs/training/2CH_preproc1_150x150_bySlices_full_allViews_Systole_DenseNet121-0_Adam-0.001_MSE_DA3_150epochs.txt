nohup: ignoring input
Running experiment 2CH_preproc1_150x150_bySlices_dataset_allViews_Systole_DenseNet121_0_Adam-0.001_MSE_DA3
Going to train with the GPU in the slot 0 -> device model: GeForce RTX 2080
Model architecture:
 DenseNet121_0(
  (first_conv): Sequential(
    (0): Conv2d(30, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (pretrained_block): Sequential(
    (0): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (1): _Transition(
      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (2): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (3): _Transition(
      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (4): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer13): _DenseLayer(
        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer14): _DenseLayer(
        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer15): _DenseLayer(
        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer16): _DenseLayer(
        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer17): _DenseLayer(
        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer18): _DenseLayer(
        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer19): _DenseLayer(
        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer20): _DenseLayer(
        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer21): _DenseLayer(
        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer22): _DenseLayer(
        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer23): _DenseLayer(
        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer24): _DenseLayer(
        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (5): _Transition(
      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (6): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer13): _DenseLayer(
        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer14): _DenseLayer(
        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer15): _DenseLayer(
        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer16): _DenseLayer(
        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (reduction_block): Sequential(
    (0): AdaptiveAvgPool2d(output_size=(1, 1))
    (1): Flatten()
  )
  (dense_block): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.4, inplace=False)
    (3): Linear(in_features=512, out_features=1, bias=True)
  )
) 


############################
# TRAIN PHASE:  150 epochs #
############################

Epoch 0: 
Train batch 1/32 - 95.0ms/batch - loss: 417.48523 - diff: 76.76mlTrain batch 2/32 - 52.6ms/batch - loss: 381.91524 - diff: 71.23mlTrain batch 3/32 - 61.0ms/batch - loss: 403.48215 - diff: 72.13mlTrain batch 4/32 - 52.4ms/batch - loss: 402.25571 - diff: 71.66mlTrain batch 5/32 - 70.9ms/batch - loss: 358.20301 - diff: 67.43mlTrain batch 6/32 - 64.9ms/batch - loss: 359.29159 - diff: 67.36mlTrain batch 7/32 - 52.2ms/batch - loss: 366.91232 - diff: 67.92mlTrain batch 8/32 - 65.3ms/batch - loss: 349.38551 - diff: 65.41mlTrain batch 9/32 - 56.2ms/batch - loss: 369.89466 - diff: 65.46mlTrain batch 10/32 - 53.4ms/batch - loss: 364.22857 - diff: 64.59mlTrain batch 11/32 - 64.0ms/batch - loss: 365.38243 - diff: 64.86mlTrain batch 12/32 - 51.0ms/batch - loss: 361.40137 - diff: 63.45mlTrain batch 13/32 - 49.8ms/batch - loss: 363.13132 - diff: 63.05mlTrain batch 14/32 - 55.2ms/batch - loss: 369.98623 - diff: 63.51mlTrain batch 15/32 - 51.1ms/batch - loss: 365.68149 - diff: 63.03mlTrain batch 16/32 - 65.0ms/batch - loss: 398.36614 - diff: 62.61mlTrain batch 17/32 - 61.8ms/batch - loss: 390.82726 - diff: 62.31mlTrain batch 18/32 - 55.0ms/batch - loss: 376.80945 - diff: 61.13mlTrain batch 19/32 - 52.3ms/batch - loss: 365.92675 - diff: 59.95mlTrain batch 20/32 - 52.3ms/batch - loss: 355.86838 - diff: 59.03mlTrain batch 21/32 - 52.4ms/batch - loss: 343.91404 - diff: 57.85mlTrain batch 22/32 - 52.4ms/batch - loss: 330.74037 - diff: 56.35mlTrain batch 23/32 - 51.1ms/batch - loss: 318.93002 - diff: 54.88mlTrain batch 24/32 - 54.8ms/batch - loss: 310.27003 - diff: 53.79mlTrain batch 25/32 - 51.1ms/batch - loss: 300.13541 - diff: 52.62mlTrain batch 26/32 - 53.4ms/batch - loss: 290.65617 - diff: 51.52mlTrain batch 27/32 - 50.5ms/batch - loss: 288.16334 - diff: 51.13mlTrain batch 28/32 - 55.5ms/batch - loss: 283.78243 - diff: 50.55mlTrain batch 29/32 - 51.1ms/batch - loss: 279.70675 - diff: 50.00mlTrain batch 30/32 - 56.0ms/batch - loss: 277.42502 - diff: 49.74mlTrain batch 31/32 - 52.3ms/batch - loss: 274.91334 - diff: 49.44mlTrain batch 32/32 - 38.0ms/batch - loss: 277.40165 - diff: 49.33mlTrain batch 32/32 - 10.2s 38.0ms/batch - loss: 277.40165 - diff: 49.33ml
Test 0.5s: val_loss: 131.98974 - diff: 32.62ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Systole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 1: current best loss = 131.98974, at epoch 0
Train batch 1/32 - 62.4ms/batch - loss: 186.89952 - diff: 44.51mlTrain batch 2/32 - 49.2ms/batch - loss: 117.13734 - diff: 33.71mlTrain batch 3/32 - 56.0ms/batch - loss: 102.46242 - diff: 31.68mlTrain batch 4/32 - 48.8ms/batch - loss: 130.92243 - diff: 34.32mlTrain batch 5/32 - 52.5ms/batch - loss: 141.00038 - diff: 36.05mlTrain batch 6/32 - 52.5ms/batch - loss: 141.42846 - diff: 36.35mlTrain batch 7/32 - 51.4ms/batch - loss: 132.31299 - diff: 35.35mlTrain batch 8/32 - 51.3ms/batch - loss: 146.37100 - diff: 35.82mlTrain batch 9/32 - 54.3ms/batch - loss: 138.33031 - diff: 34.90mlTrain batch 10/32 - 48.7ms/batch - loss: 131.83559 - diff: 34.13mlTrain batch 11/32 - 49.5ms/batch - loss: 147.94024 - diff: 35.11mlTrain batch 12/32 - 48.7ms/batch - loss: 150.35747 - diff: 35.20mlTrain batch 13/32 - 51.8ms/batch - loss: 145.93871 - diff: 35.11mlTrain batch 14/32 - 49.1ms/batch - loss: 145.11007 - diff: 35.33mlTrain batch 15/32 - 51.5ms/batch - loss: 139.66254 - diff: 34.58mlTrain batch 16/32 - 48.9ms/batch - loss: 134.47904 - diff: 34.08mlTrain batch 17/32 - 52.8ms/batch - loss: 146.25415 - diff: 35.18mlTrain batch 18/32 - 52.6ms/batch - loss: 145.44685 - diff: 35.16mlTrain batch 19/32 - 57.4ms/batch - loss: 140.50154 - diff: 34.48mlTrain batch 20/32 - 57.3ms/batch - loss: 141.16760 - diff: 34.59mlTrain batch 21/32 - 81.4ms/batch - loss: 136.79491 - diff: 34.03mlTrain batch 22/32 - 64.0ms/batch - loss: 135.56705 - diff: 34.03mlTrain batch 23/32 - 66.3ms/batch - loss: 130.85283 - diff: 33.34mlTrain batch 24/32 - 66.3ms/batch - loss: 129.31502 - diff: 33.37mlTrain batch 25/32 - 64.4ms/batch - loss: 126.70161 - diff: 32.97mlTrain batch 26/32 - 65.6ms/batch - loss: 151.53040 - diff: 33.71mlTrain batch 27/32 - 63.3ms/batch - loss: 148.84256 - diff: 33.50mlTrain batch 28/32 - 66.1ms/batch - loss: 145.26310 - diff: 33.06mlTrain batch 29/32 - 63.0ms/batch - loss: 142.31457 - diff: 32.84mlTrain batch 30/32 - 62.8ms/batch - loss: 142.51319 - diff: 32.85mlTrain batch 31/32 - 62.7ms/batch - loss: 139.84817 - diff: 32.45mlTrain batch 32/32 - 53.9ms/batch - loss: 143.20208 - diff: 32.47mlTrain batch 32/32 - 10.5s 53.9ms/batch - loss: 143.20208 - diff: 32.47ml
Test 0.6s: val_loss: 145.53308 - diff: 33.59ml

Epoch 2: current best loss = 131.98974, at epoch 0
Train batch 1/32 - 73.2ms/batch - loss: 40.42113 - diff: 19.37mlTrain batch 2/32 - 59.4ms/batch - loss: 44.94633 - diff: 20.65mlTrain batch 3/32 - 59.4ms/batch - loss: 56.15628 - diff: 23.76mlTrain batch 4/32 - 58.4ms/batch - loss: 68.01381 - diff: 24.94mlTrain batch 5/32 - 57.5ms/batch - loss: 69.65498 - diff: 25.33mlTrain batch 6/32 - 61.4ms/batch - loss: 66.72295 - diff: 24.90mlTrain batch 7/32 - 61.5ms/batch - loss: 86.13764 - diff: 27.62mlTrain batch 8/32 - 59.4ms/batch - loss: 104.16870 - diff: 28.65mlTrain batch 9/32 - 59.3ms/batch - loss: 119.24097 - diff: 30.42mlTrain batch 10/32 - 59.1ms/batch - loss: 114.82873 - diff: 29.85mlTrain batch 11/32 - 61.0ms/batch - loss: 114.38721 - diff: 29.73mlTrain batch 12/32 - 60.1ms/batch - loss: 107.45032 - diff: 28.83mlTrain batch 13/32 - 60.2ms/batch - loss: 101.79511 - diff: 28.08mlTrain batch 14/32 - 59.9ms/batch - loss: 108.73052 - diff: 29.10mlTrain batch 15/32 - 60.0ms/batch - loss: 108.57378 - diff: 29.48mlTrain batch 16/32 - 58.8ms/batch - loss: 107.33053 - diff: 29.33mlTrain batch 17/32 - 59.3ms/batch - loss: 119.00138 - diff: 30.16mlTrain batch 18/32 - 59.4ms/batch - loss: 114.61430 - diff: 29.47mlTrain batch 19/32 - 59.1ms/batch - loss: 116.45253 - diff: 29.69mlTrain batch 20/32 - 60.3ms/batch - loss: 151.72537 - diff: 31.24mlTrain batch 21/32 - 60.4ms/batch - loss: 149.83781 - diff: 31.01mlTrain batch 22/32 - 60.8ms/batch - loss: 145.09703 - diff: 30.51mlTrain batch 23/32 - 61.2ms/batch - loss: 149.48476 - diff: 30.98mlTrain batch 24/32 - 60.6ms/batch - loss: 148.42583 - diff: 31.25mlTrain batch 25/32 - 66.7ms/batch - loss: 145.79204 - diff: 31.18mlTrain batch 26/32 - 59.9ms/batch - loss: 143.35632 - diff: 31.06mlTrain batch 27/32 - 66.5ms/batch - loss: 140.91383 - diff: 30.94mlTrain batch 28/32 - 59.7ms/batch - loss: 137.91709 - diff: 30.74mlTrain batch 29/32 - 61.3ms/batch - loss: 138.52882 - diff: 31.13mlTrain batch 30/32 - 62.5ms/batch - loss: 135.99856 - diff: 31.00mlTrain batch 31/32 - 55.8ms/batch - loss: 133.66034 - diff: 30.88mlTrain batch 32/32 - 44.2ms/batch - loss: 133.11103 - diff: 30.75mlTrain batch 32/32 - 10.6s 44.2ms/batch - loss: 133.11103 - diff: 30.75ml
Test 0.5s: val_loss: 190.83910 - diff: 40.50ml

Epoch 3: current best loss = 131.98974, at epoch 0
Train batch 1/32 - 73.4ms/batch - loss: 62.02383 - diff: 25.43mlTrain batch 2/32 - 65.7ms/batch - loss: 58.45864 - diff: 24.01mlTrain batch 3/32 - 65.4ms/batch - loss: 54.73314 - diff: 23.48mlTrain batch 4/32 - 66.8ms/batch - loss: 60.96461 - diff: 24.85mlTrain batch 5/32 - 61.5ms/batch - loss: 56.39113 - diff: 24.16mlTrain batch 6/32 - 60.3ms/batch - loss: 239.15435 - diff: 31.97mlTrain batch 7/32 - 60.4ms/batch - loss: 224.82383 - diff: 31.91mlTrain batch 8/32 - 60.2ms/batch - loss: 203.41831 - diff: 30.48mlTrain batch 9/32 - 59.6ms/batch - loss: 211.73123 - diff: 31.96mlTrain batch 10/32 - 55.0ms/batch - loss: 194.83531 - diff: 30.87mlTrain batch 11/32 - 64.1ms/batch - loss: 183.53982 - diff: 30.44mlTrain batch 12/32 - 59.7ms/batch - loss: 171.83383 - diff: 29.71mlTrain batch 13/32 - 58.2ms/batch - loss: 171.88025 - diff: 30.50mlTrain batch 14/32 - 64.1ms/batch - loss: 164.87170 - diff: 30.29mlTrain batch 15/32 - 58.3ms/batch - loss: 157.53151 - diff: 30.04mlTrain batch 16/32 - 63.7ms/batch - loss: 153.11843 - diff: 29.91mlTrain batch 17/32 - 57.5ms/batch - loss: 148.98197 - diff: 29.71mlTrain batch 18/32 - 64.5ms/batch - loss: 145.72740 - diff: 29.70mlTrain batch 19/32 - 58.2ms/batch - loss: 141.12232 - diff: 29.38mlTrain batch 20/32 - 60.0ms/batch - loss: 138.32538 - diff: 29.29mlTrain batch 21/32 - 59.0ms/batch - loss: 134.98676 - diff: 29.13mlTrain batch 22/32 - 64.6ms/batch - loss: 132.22892 - diff: 29.17mlTrain batch 23/32 - 57.5ms/batch - loss: 142.37042 - diff: 30.36mlTrain batch 24/32 - 65.0ms/batch - loss: 139.29578 - diff: 30.07mlTrain batch 25/32 - 57.2ms/batch - loss: 137.62394 - diff: 29.92mlTrain batch 26/32 - 57.8ms/batch - loss: 133.97399 - diff: 29.65mlTrain batch 27/32 - 57.1ms/batch - loss: 131.52531 - diff: 29.56mlTrain batch 28/32 - 57.3ms/batch - loss: 129.89579 - diff: 29.63mlTrain batch 29/32 - 49.7ms/batch - loss: 131.51212 - diff: 29.82mlTrain batch 30/32 - 53.4ms/batch - loss: 128.25088 - diff: 29.40mlTrain batch 31/32 - 55.3ms/batch - loss: 125.16014 - diff: 29.06mlTrain batch 32/32 - 43.0ms/batch - loss: 132.80050 - diff: 29.23mlTrain batch 32/32 - 11.2s 43.0ms/batch - loss: 132.80050 - diff: 29.23ml
Test 0.6s: val_loss: 114.93559 - diff: 28.55ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Systole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 4: current best loss = 114.93559, at epoch 3
Train batch 1/32 - 67.8ms/batch - loss: 37.04575 - diff: 19.45mlTrain batch 2/32 - 60.8ms/batch - loss: 77.47331 - diff: 27.54mlTrain batch 3/32 - 65.6ms/batch - loss: 105.53121 - diff: 30.27mlTrain batch 4/32 - 65.2ms/batch - loss: 98.06880 - diff: 29.95mlTrain batch 5/32 - 62.8ms/batch - loss: 105.89009 - diff: 30.19mlTrain batch 6/32 - 62.1ms/batch - loss: 108.96994 - diff: 30.63mlTrain batch 7/32 - 64.1ms/batch - loss: 107.61969 - diff: 31.25mlTrain batch 8/32 - 64.4ms/batch - loss: 101.22927 - diff: 30.43mlTrain batch 9/32 - 62.9ms/batch - loss: 97.51523 - diff: 30.23mlTrain batch 10/32 - 63.4ms/batch - loss: 94.08563 - diff: 29.90mlTrain batch 11/32 - 57.9ms/batch - loss: 88.20658 - diff: 28.67mlTrain batch 12/32 - 57.4ms/batch - loss: 83.82789 - diff: 27.96mlTrain batch 13/32 - 57.2ms/batch - loss: 81.25247 - diff: 27.59mlTrain batch 14/32 - 57.2ms/batch - loss: 79.58331 - diff: 27.33mlTrain batch 15/32 - 59.9ms/batch - loss: 78.67311 - diff: 27.23mlTrain batch 16/32 - 59.5ms/batch - loss: 78.22522 - diff: 27.25mlTrain batch 17/32 - 60.9ms/batch - loss: 80.27247 - diff: 27.58mlTrain batch 18/32 - 60.4ms/batch - loss: 78.77552 - diff: 27.23mlTrain batch 19/32 - 59.6ms/batch - loss: 79.62722 - diff: 27.03mlTrain batch 20/32 - 59.9ms/batch - loss: 112.33596 - diff: 28.30mlTrain batch 21/32 - 60.4ms/batch - loss: 109.78973 - diff: 27.90mlTrain batch 22/32 - 57.6ms/batch - loss: 112.00290 - diff: 28.06mlTrain batch 23/32 - 57.3ms/batch - loss: 114.02741 - diff: 28.59mlTrain batch 24/32 - 56.5ms/batch - loss: 113.45459 - diff: 28.50mlTrain batch 25/32 - 57.5ms/batch - loss: 110.77747 - diff: 28.29mlTrain batch 26/32 - 56.8ms/batch - loss: 107.66259 - diff: 27.89mlTrain batch 27/32 - 57.8ms/batch - loss: 116.16025 - diff: 28.40mlTrain batch 28/32 - 56.5ms/batch - loss: 114.28722 - diff: 28.33mlTrain batch 29/32 - 51.7ms/batch - loss: 112.73080 - diff: 28.25mlTrain batch 30/32 - 48.7ms/batch - loss: 111.69920 - diff: 28.31mlTrain batch 31/32 - 60.5ms/batch - loss: 110.13014 - diff: 28.25mlTrain batch 32/32 - 50.3ms/batch - loss: 110.16561 - diff: 28.17mlTrain batch 32/32 - 10.6s 50.3ms/batch - loss: 110.16561 - diff: 28.17ml
Test 0.5s: val_loss: 102.46167 - diff: 26.22ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Systole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 5: current best loss = 102.46167, at epoch 4
Train batch 1/32 - 65.6ms/batch - loss: 38.60018 - diff: 19.27mlTrain batch 2/32 - 57.7ms/batch - loss: 45.87337 - diff: 21.56mlTrain batch 3/32 - 57.8ms/batch - loss: 46.06992 - diff: 21.83mlTrain batch 4/32 - 57.9ms/batch - loss: 54.54058 - diff: 23.50mlTrain batch 5/32 - 64.6ms/batch - loss: 54.02547 - diff: 23.14mlTrain batch 6/32 - 64.3ms/batch - loss: 57.93296 - diff: 23.46mlTrain batch 7/32 - 62.5ms/batch - loss: 76.14242 - diff: 25.09mlTrain batch 8/32 - 63.0ms/batch - loss: 82.42023 - diff: 26.26mlTrain batch 9/32 - 61.8ms/batch - loss: 83.11451 - diff: 26.47mlTrain batch 10/32 - 61.5ms/batch - loss: 81.01865 - diff: 26.23mlTrain batch 11/32 - 62.7ms/batch - loss: 83.79545 - diff: 26.78mlTrain batch 12/32 - 61.8ms/batch - loss: 145.72178 - diff: 28.53mlTrain batch 13/32 - 61.5ms/batch - loss: 139.58198 - diff: 28.31mlTrain batch 14/32 - 61.8ms/batch - loss: 134.18971 - diff: 27.98mlTrain batch 15/32 - 65.3ms/batch - loss: 132.36799 - diff: 28.29mlTrain batch 16/32 - 65.7ms/batch - loss: 138.83383 - diff: 28.30mlTrain batch 17/32 - 65.5ms/batch - loss: 135.89255 - diff: 27.92mlTrain batch 18/32 - 65.7ms/batch - loss: 131.05234 - diff: 27.62mlTrain batch 19/32 - 66.5ms/batch - loss: 131.41873 - diff: 28.06mlTrain batch 20/32 - 65.9ms/batch - loss: 130.19243 - diff: 28.27mlTrain batch 21/32 - 66.4ms/batch - loss: 131.11328 - diff: 28.54mlTrain batch 22/32 - 65.6ms/batch - loss: 127.08472 - diff: 28.28mlTrain batch 23/32 - 67.1ms/batch - loss: 127.52863 - diff: 28.45mlTrain batch 24/32 - 65.9ms/batch - loss: 125.81637 - diff: 28.39mlTrain batch 25/32 - 66.4ms/batch - loss: 123.83381 - diff: 28.38mlTrain batch 26/32 - 61.6ms/batch - loss: 123.39271 - diff: 28.39mlTrain batch 27/32 - 65.1ms/batch - loss: 122.54689 - diff: 28.39mlTrain batch 28/32 - 60.3ms/batch - loss: 119.77350 - diff: 28.22mlTrain batch 29/32 - 63.9ms/batch - loss: 119.15337 - diff: 28.38mlTrain batch 30/32 - 59.6ms/batch - loss: 118.49074 - diff: 28.46mlTrain batch 31/32 - 60.9ms/batch - loss: 116.61485 - diff: 28.38mlTrain batch 32/32 - 44.8ms/batch - loss: 116.07270 - diff: 28.22mlTrain batch 32/32 - 10.4s 44.8ms/batch - loss: 116.07270 - diff: 28.22ml
Test 0.6s: val_loss: 93.99569 - diff: 29.21ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Systole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 6: current best loss = 93.99569, at epoch 5
Train batch 1/32 - 68.7ms/batch - loss: 32.07266 - diff: 18.45mlTrain batch 2/32 - 61.4ms/batch - loss: 42.73849 - diff: 20.84mlTrain batch 3/32 - 67.2ms/batch - loss: 60.71412 - diff: 23.15mlTrain batch 4/32 - 66.5ms/batch - loss: 79.33475 - diff: 26.69mlTrain batch 5/32 - 64.4ms/batch - loss: 86.45124 - diff: 27.33mlTrain batch 6/32 - 59.9ms/batch - loss: 81.56016 - diff: 27.12mlTrain batch 7/32 - 56.2ms/batch - loss: 160.24826 - diff: 29.23mlTrain batch 8/32 - 59.5ms/batch - loss: 145.45344 - diff: 28.42mlTrain batch 9/32 - 66.7ms/batch - loss: 134.69188 - diff: 27.92mlTrain batch 10/32 - 65.3ms/batch - loss: 126.58876 - diff: 27.65mlTrain batch 11/32 - 53.9ms/batch - loss: 119.66687 - diff: 27.18mlTrain batch 12/32 - 61.2ms/batch - loss: 117.63812 - diff: 27.36mlTrain batch 13/32 - 62.7ms/batch - loss: 115.82969 - diff: 27.46mlTrain batch 14/32 - 61.1ms/batch - loss: 116.25822 - diff: 28.12mlTrain batch 15/32 - 63.3ms/batch - loss: 114.12757 - diff: 28.16mlTrain batch 16/32 - 61.4ms/batch - loss: 110.62606 - diff: 27.69mlTrain batch 17/32 - 61.7ms/batch - loss: 113.73559 - diff: 28.05mlTrain batch 18/32 - 61.2ms/batch - loss: 110.26591 - diff: 27.70mlTrain batch 19/32 - 58.6ms/batch - loss: 112.54643 - diff: 28.13mlTrain batch 20/32 - 59.1ms/batch - loss: 116.07942 - diff: 28.09mlTrain batch 21/32 - 59.0ms/batch - loss: 111.82978 - diff: 27.62mlTrain batch 22/32 - 58.5ms/batch - loss: 108.80425 - diff: 27.40mlTrain batch 23/32 - 59.3ms/batch - loss: 111.02547 - diff: 27.69mlTrain batch 24/32 - 59.5ms/batch - loss: 108.22131 - diff: 27.45mlTrain batch 25/32 - 59.1ms/batch - loss: 114.00041 - diff: 27.92mlTrain batch 26/32 - 59.1ms/batch - loss: 110.63302 - diff: 27.51mlTrain batch 27/32 - 59.4ms/batch - loss: 112.34403 - diff: 27.88mlTrain batch 28/32 - 59.0ms/batch - loss: 109.64555 - diff: 27.61mlTrain batch 29/32 - 52.3ms/batch - loss: 106.99448 - diff: 27.37mlTrain batch 30/32 - 58.6ms/batch - loss: 105.44968 - diff: 27.34mlTrain batch 31/32 - 56.5ms/batch - loss: 104.76849 - diff: 27.39mlTrain batch 32/32 - 49.1ms/batch - loss: 117.59355 - diff: 27.66mlTrain batch 32/32 - 10.6s 49.1ms/batch - loss: 117.59355 - diff: 27.66ml
Test 0.5s: val_loss: 84.41515 - diff: 26.49ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Systole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 7: current best loss = 84.41515, at epoch 6
Train batch 1/32 - 68.1ms/batch - loss: 99.31477 - diff: 28.82mlTrain batch 2/32 - 60.3ms/batch - loss: 102.10551 - diff: 29.32mlTrain batch 3/32 - 66.2ms/batch - loss: 116.62696 - diff: 31.37mlTrain batch 4/32 - 53.2ms/batch - loss: 97.03319 - diff: 28.69mlTrain batch 5/32 - 67.5ms/batch - loss: 100.92006 - diff: 29.35mlTrain batch 6/32 - 61.1ms/batch - loss: 92.79572 - diff: 27.85mlTrain batch 7/32 - 65.2ms/batch - loss: 183.90773 - diff: 30.73mlTrain batch 8/32 - 64.7ms/batch - loss: 165.24178 - diff: 29.25mlTrain batch 9/32 - 65.0ms/batch - loss: 159.41810 - diff: 29.39mlTrain batch 10/32 - 64.6ms/batch - loss: 149.45390 - diff: 29.02mlTrain batch 11/32 - 65.0ms/batch - loss: 140.00205 - diff: 28.31mlTrain batch 12/32 - 64.6ms/batch - loss: 134.00358 - diff: 27.78mlTrain batch 13/32 - 57.7ms/batch - loss: 128.06793 - diff: 27.50mlTrain batch 14/32 - 57.4ms/batch - loss: 123.13695 - diff: 27.52mlTrain batch 15/32 - 58.9ms/batch - loss: 117.32986 - diff: 26.94mlTrain batch 16/32 - 59.3ms/batch - loss: 115.39793 - diff: 27.23mlTrain batch 17/32 - 57.4ms/batch - loss: 111.42821 - diff: 27.07mlTrain batch 18/32 - 57.7ms/batch - loss: 112.84208 - diff: 27.44mlTrain batch 19/32 - 57.4ms/batch - loss: 111.04054 - diff: 27.40mlTrain batch 20/32 - 57.4ms/batch - loss: 107.72928 - diff: 27.04mlTrain batch 21/32 - 57.1ms/batch - loss: 104.88992 - diff: 26.66mlTrain batch 22/32 - 57.5ms/batch - loss: 109.62480 - diff: 27.22mlTrain batch 23/32 - 57.9ms/batch - loss: 108.82363 - diff: 27.27mlTrain batch 24/32 - 57.5ms/batch - loss: 106.17902 - diff: 27.12mlTrain batch 25/32 - 58.5ms/batch - loss: 105.62101 - diff: 27.22mlTrain batch 26/32 - 58.8ms/batch - loss: 104.10191 - diff: 27.21mlTrain batch 27/32 - 57.8ms/batch - loss: 103.01798 - diff: 27.20mlTrain batch 28/32 - 57.3ms/batch - loss: 100.66597 - diff: 26.86mlTrain batch 29/32 - 58.0ms/batch - loss: 107.44784 - diff: 27.28mlTrain batch 30/32 - 58.0ms/batch - loss: 106.03285 - diff: 27.22mlTrain batch 31/32 - 55.8ms/batch - loss: 105.40498 - diff: 27.35mlTrain batch 32/32 - 45.3ms/batch - loss: 105.52910 - diff: 27.29mlTrain batch 32/32 - 10.6s 45.3ms/batch - loss: 105.52910 - diff: 27.29ml
Test 0.5s: val_loss: 99.49372 - diff: 25.11ml

Epoch 8: current best loss = 84.41515, at epoch 6
Train batch 1/32 - 73.4ms/batch - loss: 60.13242 - diff: 22.75mlTrain batch 2/32 - 63.9ms/batch - loss: 78.17224 - diff: 28.74mlTrain batch 3/32 - 64.6ms/batch - loss: 74.45795 - diff: 27.77mlTrain batch 4/32 - 63.6ms/batch - loss: 83.48966 - diff: 28.78mlTrain batch 5/32 - 58.6ms/batch - loss: 79.06408 - diff: 28.51mlTrain batch 6/32 - 65.1ms/batch - loss: 82.66874 - diff: 28.71mlTrain batch 7/32 - 61.2ms/batch - loss: 74.51613 - diff: 26.80mlTrain batch 8/32 - 61.0ms/batch - loss: 70.58923 - diff: 25.99mlTrain batch 9/32 - 60.7ms/batch - loss: 91.11894 - diff: 26.94mlTrain batch 10/32 - 59.6ms/batch - loss: 153.50646 - diff: 29.50mlTrain batch 11/32 - 60.1ms/batch - loss: 142.61831 - diff: 28.32mlTrain batch 12/32 - 59.9ms/batch - loss: 142.39071 - diff: 28.83mlTrain batch 13/32 - 60.5ms/batch - loss: 135.81584 - diff: 28.26mlTrain batch 14/32 - 59.8ms/batch - loss: 136.13761 - diff: 28.66mlTrain batch 15/32 - 59.9ms/batch - loss: 129.99327 - diff: 28.15mlTrain batch 16/32 - 59.8ms/batch - loss: 123.76300 - diff: 27.55mlTrain batch 17/32 - 61.0ms/batch - loss: 120.36338 - diff: 27.16mlTrain batch 18/32 - 61.5ms/batch - loss: 115.05235 - diff: 26.62mlTrain batch 19/32 - 58.9ms/batch - loss: 117.15648 - diff: 26.74mlTrain batch 20/32 - 58.0ms/batch - loss: 113.04768 - diff: 26.45mlTrain batch 21/32 - 57.3ms/batch - loss: 109.81857 - diff: 26.30mlTrain batch 22/32 - 58.1ms/batch - loss: 106.02200 - diff: 25.82mlTrain batch 23/32 - 62.1ms/batch - loss: 105.86230 - diff: 26.02mlTrain batch 24/32 - 61.4ms/batch - loss: 109.27489 - diff: 26.51mlTrain batch 25/32 - 62.2ms/batch - loss: 107.51418 - diff: 26.47mlTrain batch 26/32 - 61.4ms/batch - loss: 105.10117 - diff: 26.39mlTrain batch 27/32 - 63.6ms/batch - loss: 104.99579 - diff: 26.48mlTrain batch 28/32 - 63.6ms/batch - loss: 104.22669 - diff: 26.58mlTrain batch 29/32 - 60.2ms/batch - loss: 103.29727 - diff: 26.50mlTrain batch 30/32 - 58.3ms/batch - loss: 103.70854 - diff: 26.54mlTrain batch 31/32 - 62.4ms/batch - loss: 101.85629 - diff: 26.40mlTrain batch 32/32 - 44.1ms/batch - loss: 102.64822 - diff: 26.39mlTrain batch 32/32 - 10.6s 44.1ms/batch - loss: 102.64822 - diff: 26.39ml
Test 0.5s: val_loss: 87.93120 - diff: 24.30ml

Epoch 9: current best loss = 84.41515, at epoch 6
Train batch 1/32 - 68.7ms/batch - loss: 69.40848 - diff: 24.59mlTrain batch 2/32 - 61.3ms/batch - loss: 57.22205 - diff: 22.43mlTrain batch 3/32 - 60.9ms/batch - loss: 59.93561 - diff: 23.73mlTrain batch 4/32 - 60.9ms/batch - loss: 56.55438 - diff: 23.70mlTrain batch 5/32 - 61.9ms/batch - loss: 54.19775 - diff: 23.30mlTrain batch 6/32 - 62.4ms/batch - loss: 67.49686 - diff: 24.58mlTrain batch 7/32 - 65.3ms/batch - loss: 62.46432 - diff: 23.87mlTrain batch 8/32 - 63.9ms/batch - loss: 63.92274 - diff: 24.30mlTrain batch 9/32 - 54.6ms/batch - loss: 63.21125 - diff: 24.51mlTrain batch 10/32 - 58.7ms/batch - loss: 61.26074 - diff: 23.99mlTrain batch 11/32 - 52.3ms/batch - loss: 63.42367 - diff: 24.17mlTrain batch 12/32 - 59.6ms/batch - loss: 114.24874 - diff: 26.39mlTrain batch 13/32 - 58.9ms/batch - loss: 116.99324 - diff: 27.26mlTrain batch 14/32 - 61.4ms/batch - loss: 110.63058 - diff: 26.53mlTrain batch 15/32 - 59.8ms/batch - loss: 110.74386 - diff: 26.52mlTrain batch 16/32 - 59.7ms/batch - loss: 106.56218 - diff: 26.33mlTrain batch 17/32 - 59.4ms/batch - loss: 101.94057 - diff: 25.82mlTrain batch 18/32 - 59.8ms/batch - loss: 101.77628 - diff: 26.01mlTrain batch 19/32 - 63.7ms/batch - loss: 98.77011 - diff: 25.72mlTrain batch 20/32 - 64.4ms/batch - loss: 96.90129 - diff: 25.68mlTrain batch 21/32 - 58.0ms/batch - loss: 97.90266 - diff: 25.94mlTrain batch 22/32 - 58.9ms/batch - loss: 95.62696 - diff: 25.76mlTrain batch 23/32 - 59.0ms/batch - loss: 95.03569 - diff: 25.98mlTrain batch 24/32 - 68.3ms/batch - loss: 97.72711 - diff: 26.33mlTrain batch 25/32 - 61.6ms/batch - loss: 95.04744 - diff: 26.04mlTrain batch 26/32 - 64.2ms/batch - loss: 96.72381 - diff: 26.30mlTrain batch 27/32 - 58.0ms/batch - loss: 94.29883 - diff: 25.87mlTrain batch 28/32 - 58.4ms/batch - loss: 93.47281 - diff: 25.86mlTrain batch 29/32 - 57.5ms/batch - loss: 99.66338 - diff: 26.29mlTrain batch 30/32 - 58.8ms/batch - loss: 99.96363 - diff: 26.38mlTrain batch 31/32 - 56.5ms/batch - loss: 98.27569 - diff: 26.31mlTrain batch 32/32 - 43.6ms/batch - loss: 99.06846 - diff: 26.30mlTrain batch 32/32 - 10.5s 43.6ms/batch - loss: 99.06846 - diff: 26.30ml
Test 0.5s: val_loss: 117.28225 - diff: 26.74ml

Epoch 10: current best loss = 84.41515, at epoch 6
Train batch 1/32 - 75.8ms/batch - loss: 31.07900 - diff: 17.61mlTrain batch 2/32 - 66.9ms/batch - loss: 43.34982 - diff: 20.85mlTrain batch 3/32 - 72.8ms/batch - loss: 53.12092 - diff: 21.49mlTrain batch 4/32 - 66.8ms/batch - loss: 67.29280 - diff: 24.84mlTrain batch 5/32 - 64.3ms/batch - loss: 61.57994 - diff: 23.67mlTrain batch 6/32 - 57.8ms/batch - loss: 60.69295 - diff: 23.25mlTrain batch 7/32 - 61.4ms/batch - loss: 62.50737 - diff: 24.01mlTrain batch 8/32 - 60.1ms/batch - loss: 60.88949 - diff: 24.23mlTrain batch 9/32 - 61.2ms/batch - loss: 60.63069 - diff: 24.21mlTrain batch 10/32 - 60.8ms/batch - loss: 57.52229 - diff: 23.58mlTrain batch 11/32 - 54.4ms/batch - loss: 57.47182 - diff: 22.99mlTrain batch 12/32 - 60.4ms/batch - loss: 63.93658 - diff: 23.89mlTrain batch 13/32 - 70.0ms/batch - loss: 61.67725 - diff: 23.45mlTrain batch 14/32 - 63.9ms/batch - loss: 58.34237 - diff: 22.62mlTrain batch 15/32 - 63.4ms/batch - loss: 59.08101 - diff: 22.91mlTrain batch 16/32 - 64.0ms/batch - loss: 59.79179 - diff: 23.26mlTrain batch 17/32 - 70.5ms/batch - loss: 73.54272 - diff: 24.15mlTrain batch 18/32 - 62.8ms/batch - loss: 107.10474 - diff: 25.55mlTrain batch 19/32 - 64.0ms/batch - loss: 107.58276 - diff: 26.01mlTrain batch 20/32 - 62.9ms/batch - loss: 105.62827 - diff: 25.88mlTrain batch 21/32 - 62.7ms/batch - loss: 102.58800 - diff: 25.71mlTrain batch 22/32 - 63.0ms/batch - loss: 101.45063 - diff: 25.79mlTrain batch 23/32 - 62.4ms/batch - loss: 98.92212 - diff: 25.55mlTrain batch 24/32 - 65.2ms/batch - loss: 97.60025 - diff: 25.50mlTrain batch 25/32 - 64.4ms/batch - loss: 99.83774 - diff: 25.97mlTrain batch 26/32 - 65.5ms/batch - loss: 97.86287 - diff: 25.90mlTrain batch 27/32 - 71.7ms/batch - loss: 95.98433 - diff: 25.74mlTrain batch 28/32 - 65.1ms/batch - loss: 94.58866 - diff: 25.67mlTrain batch 29/32 - 72.3ms/batch - loss: 93.11388 - diff: 25.68mlTrain batch 30/32 - 67.1ms/batch - loss: 93.05577 - diff: 25.86mlTrain batch 31/32 - 60.8ms/batch - loss: 92.00517 - diff: 25.79mlTrain batch 32/32 - 36.8ms/batch - loss: 93.07777 - diff: 25.76mlTrain batch 32/32 - 10.6s 36.8ms/batch - loss: 93.07777 - diff: 25.76ml
Test 0.6s: val_loss: 81.95172 - diff: 24.59ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Systole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 11: current best loss = 81.95172, at epoch 10
Train batch 1/32 - 73.6ms/batch - loss: 233.85271 - diff: 36.63mlTrain batch 2/32 - 57.9ms/batch - loss: 143.57161 - diff: 30.60mlTrain batch 3/32 - 64.4ms/batch - loss: 271.61673 - diff: 33.33mlTrain batch 4/32 - 57.8ms/batch - loss: 215.77045 - diff: 30.73mlTrain batch 5/32 - 64.9ms/batch - loss: 182.89701 - diff: 29.34mlTrain batch 6/32 - 57.0ms/batch - loss: 163.11291 - diff: 28.71mlTrain batch 7/32 - 66.6ms/batch - loss: 154.92140 - diff: 29.38mlTrain batch 8/32 - 57.9ms/batch - loss: 140.29824 - diff: 28.21mlTrain batch 9/32 - 66.1ms/batch - loss: 133.47539 - diff: 28.18mlTrain batch 10/32 - 58.0ms/batch - loss: 127.56206 - diff: 28.28mlTrain batch 11/32 - 64.9ms/batch - loss: 124.43710 - diff: 28.61mlTrain batch 12/32 - 57.2ms/batch - loss: 116.56620 - diff: 27.70mlTrain batch 13/32 - 61.5ms/batch - loss: 110.32410 - diff: 27.01mlTrain batch 14/32 - 58.2ms/batch - loss: 108.00995 - diff: 26.85mlTrain batch 15/32 - 62.8ms/batch - loss: 106.23470 - diff: 27.01mlTrain batch 16/32 - 60.2ms/batch - loss: 103.21244 - diff: 26.62mlTrain batch 17/32 - 62.5ms/batch - loss: 100.43774 - diff: 26.62mlTrain batch 18/32 - 62.3ms/batch - loss: 97.98466 - diff: 26.51mlTrain batch 19/32 - 65.1ms/batch - loss: 96.43120 - diff: 26.48mlTrain batch 20/32 - 58.3ms/batch - loss: 96.09761 - diff: 26.55mlTrain batch 21/32 - 61.6ms/batch - loss: 94.57625 - diff: 26.57mlTrain batch 22/32 - 61.0ms/batch - loss: 93.52150 - diff: 26.46mlTrain batch 23/32 - 63.3ms/batch - loss: 96.92842 - diff: 26.93mlTrain batch 24/32 - 60.6ms/batch - loss: 99.76981 - diff: 27.36mlTrain batch 25/32 - 63.1ms/batch - loss: 98.30299 - diff: 27.17mlTrain batch 26/32 - 52.5ms/batch - loss: 96.64307 - diff: 26.99mlTrain batch 27/32 - 61.2ms/batch - loss: 98.94427 - diff: 27.05mlTrain batch 28/32 - 60.1ms/batch - loss: 96.59455 - diff: 26.83mlTrain batch 29/32 - 62.1ms/batch - loss: 95.79286 - diff: 26.83mlTrain batch 30/32 - 61.3ms/batch - loss: 93.91323 - diff: 26.65mlTrain batch 31/32 - 66.3ms/batch - loss: 93.57823 - diff: 26.48mlTrain batch 32/32 - 58.7ms/batch - loss: 93.62732 - diff: 26.41mlTrain batch 32/32 - 10.5s 58.7ms/batch - loss: 93.62732 - diff: 26.41ml
Test 0.6s: val_loss: 92.46424 - diff: 25.88ml

Epoch 12: current best loss = 81.95172, at epoch 10
Train batch 1/32 - 68.8ms/batch - loss: 69.39898 - diff: 25.90mlTrain batch 2/32 - 57.1ms/batch - loss: 104.81768 - diff: 31.24mlTrain batch 3/32 - 59.7ms/batch - loss: 326.72092 - diff: 40.58mlTrain batch 4/32 - 63.8ms/batch - loss: 257.16820 - diff: 36.30mlTrain batch 5/32 - 66.8ms/batch - loss: 218.11291 - diff: 33.96mlTrain batch 6/32 - 63.8ms/batch - loss: 189.09986 - diff: 32.02mlTrain batch 7/32 - 59.1ms/batch - loss: 172.40250 - diff: 31.37mlTrain batch 8/32 - 58.8ms/batch - loss: 159.91159 - diff: 31.22mlTrain batch 9/32 - 60.5ms/batch - loss: 162.99675 - diff: 32.12mlTrain batch 10/32 - 62.4ms/batch - loss: 180.11284 - diff: 33.43mlTrain batch 11/32 - 65.0ms/batch - loss: 167.10395 - diff: 32.21mlTrain batch 12/32 - 61.2ms/batch - loss: 159.40836 - diff: 31.66mlTrain batch 13/32 - 60.0ms/batch - loss: 151.40902 - diff: 31.00mlTrain batch 14/32 - 61.7ms/batch - loss: 143.45944 - diff: 30.37mlTrain batch 15/32 - 63.9ms/batch - loss: 136.77894 - diff: 29.70mlTrain batch 16/32 - 53.9ms/batch - loss: 131.61835 - diff: 29.26mlTrain batch 17/32 - 61.5ms/batch - loss: 127.16040 - diff: 28.89mlTrain batch 18/32 - 57.1ms/batch - loss: 122.27479 - diff: 28.38mlTrain batch 19/32 - 62.6ms/batch - loss: 118.57362 - diff: 28.16mlTrain batch 20/32 - 61.8ms/batch - loss: 114.99773 - diff: 27.82mlTrain batch 21/32 - 61.5ms/batch - loss: 113.20839 - diff: 27.62mlTrain batch 22/32 - 61.0ms/batch - loss: 110.57414 - diff: 27.45mlTrain batch 23/32 - 61.5ms/batch - loss: 107.22216 - diff: 27.13mlTrain batch 24/32 - 61.6ms/batch - loss: 105.08163 - diff: 27.02mlTrain batch 25/32 - 61.4ms/batch - loss: 102.02046 - diff: 26.59mlTrain batch 26/32 - 61.3ms/batch - loss: 100.46214 - diff: 26.50mlTrain batch 27/32 - 60.4ms/batch - loss: 101.65366 - diff: 26.81mlTrain batch 28/32 - 61.5ms/batch - loss: 99.93837 - diff: 26.77mlTrain batch 29/32 - 61.6ms/batch - loss: 101.90216 - diff: 27.05mlTrain batch 30/32 - 60.5ms/batch - loss: 99.57870 - diff: 26.78mlTrain batch 31/32 - 52.9ms/batch - loss: 98.73531 - diff: 26.80mlTrain batch 32/32 - 44.3ms/batch - loss: 99.20156 - diff: 26.76mlTrain batch 32/32 - 10.4s 44.3ms/batch - loss: 99.20156 - diff: 26.76ml
Test 0.5s: val_loss: 92.33172 - diff: 23.68ml

Epoch 13: current best loss = 81.95172, at epoch 10
Train batch 1/32 - 71.2ms/batch - loss: 33.36002 - diff: 20.52mlTrain batch 2/32 - 60.5ms/batch - loss: 26.96763 - diff: 15.92mlTrain batch 3/32 - 60.1ms/batch - loss: 38.32897 - diff: 18.02mlTrain batch 4/32 - 57.4ms/batch - loss: 35.77593 - diff: 18.06mlTrain batch 5/32 - 60.4ms/batch - loss: 38.72956 - diff: 18.87mlTrain batch 6/32 - 57.9ms/batch - loss: 39.29071 - diff: 18.96mlTrain batch 7/32 - 59.7ms/batch - loss: 74.51549 - diff: 23.21mlTrain batch 8/32 - 58.0ms/batch - loss: 75.85826 - diff: 23.81mlTrain batch 9/32 - 59.7ms/batch - loss: 72.74710 - diff: 23.66mlTrain batch 10/32 - 58.0ms/batch - loss: 68.20359 - diff: 22.86mlTrain batch 11/32 - 60.3ms/batch - loss: 68.05366 - diff: 23.20mlTrain batch 12/32 - 59.0ms/batch - loss: 76.30827 - diff: 23.93mlTrain batch 13/32 - 59.4ms/batch - loss: 75.65666 - diff: 24.08mlTrain batch 14/32 - 58.0ms/batch - loss: 75.19137 - diff: 23.91mlTrain batch 15/32 - 59.8ms/batch - loss: 75.10566 - diff: 24.15mlTrain batch 16/32 - 57.7ms/batch - loss: 74.01087 - diff: 24.15mlTrain batch 17/32 - 50.8ms/batch - loss: 73.00932 - diff: 24.26mlTrain batch 18/32 - 49.8ms/batch - loss: 73.60212 - diff: 24.48mlTrain batch 19/32 - 70.0ms/batch - loss: 73.67875 - diff: 24.58mlTrain batch 20/32 - 63.9ms/batch - loss: 72.76190 - diff: 24.54mlTrain batch 21/32 - 63.9ms/batch - loss: 70.88715 - diff: 24.27mlTrain batch 22/32 - 62.4ms/batch - loss: 71.76369 - diff: 24.43mlTrain batch 23/32 - 62.6ms/batch - loss: 75.95755 - diff: 24.75mlTrain batch 24/32 - 61.5ms/batch - loss: 74.71035 - diff: 24.54mlTrain batch 25/32 - 62.6ms/batch - loss: 94.27917 - diff: 25.13mlTrain batch 26/32 - 65.8ms/batch - loss: 94.66541 - diff: 25.43mlTrain batch 27/32 - 60.3ms/batch - loss: 93.76814 - diff: 25.50mlTrain batch 28/32 - 59.8ms/batch - loss: 94.46916 - diff: 25.82mlTrain batch 29/32 - 59.5ms/batch - loss: 94.63391 - diff: 26.03mlTrain batch 30/32 - 60.1ms/batch - loss: 93.23360 - diff: 25.90mlTrain batch 31/32 - 61.5ms/batch - loss: 92.15391 - diff: 25.79mlTrain batch 32/32 - 44.7ms/batch - loss: 92.80316 - diff: 25.77mlTrain batch 32/32 - 10.8s 44.7ms/batch - loss: 92.80316 - diff: 25.77ml
Test 0.6s: val_loss: 79.16707 - diff: 23.63ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Systole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 14: current best loss = 79.16707, at epoch 13
Train batch 1/32 - 69.6ms/batch - loss: 65.73916 - diff: 25.70mlTrain batch 2/32 - 61.7ms/batch - loss: 59.30071 - diff: 24.50mlTrain batch 3/32 - 59.8ms/batch - loss: 54.02702 - diff: 23.47mlTrain batch 4/32 - 66.9ms/batch - loss: 187.25848 - diff: 27.29mlTrain batch 5/32 - 67.0ms/batch - loss: 164.72768 - diff: 26.36mlTrain batch 6/32 - 58.7ms/batch - loss: 143.16853 - diff: 25.22mlTrain batch 7/32 - 62.7ms/batch - loss: 128.70354 - diff: 24.74mlTrain batch 8/32 - 62.4ms/batch - loss: 138.76020 - diff: 26.57mlTrain batch 9/32 - 62.0ms/batch - loss: 129.40411 - diff: 26.10mlTrain batch 10/32 - 62.1ms/batch - loss: 120.40332 - diff: 25.53mlTrain batch 11/32 - 61.4ms/batch - loss: 117.09233 - diff: 25.82mlTrain batch 12/32 - 65.1ms/batch - loss: 112.32131 - diff: 25.76mlTrain batch 13/32 - 64.2ms/batch - loss: 107.84668 - diff: 25.56mlTrain batch 14/32 - 66.4ms/batch - loss: 105.19506 - diff: 25.59mlTrain batch 15/32 - 65.3ms/batch - loss: 114.55161 - diff: 26.14mlTrain batch 16/32 - 63.3ms/batch - loss: 111.70143 - diff: 26.20mlTrain batch 17/32 - 63.0ms/batch - loss: 109.88325 - diff: 26.42mlTrain batch 18/32 - 62.0ms/batch - loss: 106.23001 - diff: 26.11mlTrain batch 19/32 - 61.9ms/batch - loss: 103.85947 - diff: 26.13mlTrain batch 20/32 - 62.5ms/batch - loss: 100.26336 - diff: 25.64mlTrain batch 21/32 - 62.4ms/batch - loss: 98.89945 - diff: 25.47mlTrain batch 22/32 - 57.3ms/batch - loss: 98.91336 - diff: 25.44mlTrain batch 23/32 - 58.1ms/batch - loss: 98.99223 - diff: 25.67mlTrain batch 24/32 - 59.0ms/batch - loss: 97.62060 - diff: 25.62mlTrain batch 25/32 - 58.6ms/batch - loss: 95.83578 - diff: 25.52mlTrain batch 26/32 - 58.3ms/batch - loss: 93.67340 - diff: 25.34mlTrain batch 27/32 - 57.9ms/batch - loss: 95.62488 - diff: 25.74mlTrain batch 28/32 - 60.3ms/batch - loss: 93.19196 - diff: 25.49mlTrain batch 29/32 - 59.7ms/batch - loss: 93.08172 - diff: 25.66mlTrain batch 30/32 - 61.2ms/batch - loss: 91.28508 - diff: 25.50mlTrain batch 31/32 - 59.3ms/batch - loss: 89.91475 - diff: 25.36mlTrain batch 32/32 - 52.8ms/batch - loss: 89.95678 - diff: 25.30mlTrain batch 32/32 - 11.1s 52.8ms/batch - loss: 89.95678 - diff: 25.30ml
Test 0.5s: val_loss: 69.80195 - diff: 22.38ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Systole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 15: current best loss = 69.80195, at epoch 14
Train batch 1/32 - 72.8ms/batch - loss: 117.26714 - diff: 28.86mlTrain batch 2/32 - 62.2ms/batch - loss: 70.80207 - diff: 21.84mlTrain batch 3/32 - 61.8ms/batch - loss: 64.20323 - diff: 21.54mlTrain batch 4/32 - 62.2ms/batch - loss: 61.14368 - diff: 22.78mlTrain batch 5/32 - 62.1ms/batch - loss: 72.13538 - diff: 24.11mlTrain batch 6/32 - 63.8ms/batch - loss: 66.97478 - diff: 23.81mlTrain batch 7/32 - 62.6ms/batch - loss: 65.20735 - diff: 23.68mlTrain batch 8/32 - 62.5ms/batch - loss: 65.02455 - diff: 23.32mlTrain batch 9/32 - 62.5ms/batch - loss: 67.54765 - diff: 24.22mlTrain batch 10/32 - 62.9ms/batch - loss: 66.63757 - diff: 24.54mlTrain batch 11/32 - 63.5ms/batch - loss: 115.56715 - diff: 26.66mlTrain batch 12/32 - 64.1ms/batch - loss: 110.83970 - diff: 26.38mlTrain batch 13/32 - 63.2ms/batch - loss: 105.70942 - diff: 26.10mlTrain batch 14/32 - 63.8ms/batch - loss: 102.94970 - diff: 25.87mlTrain batch 15/32 - 63.8ms/batch - loss: 98.87053 - diff: 25.65mlTrain batch 16/32 - 63.9ms/batch - loss: 96.86045 - diff: 25.55mlTrain batch 17/32 - 64.6ms/batch - loss: 92.48402 - diff: 25.00mlTrain batch 18/32 - 64.4ms/batch - loss: 91.97690 - diff: 25.12mlTrain batch 19/32 - 62.4ms/batch - loss: 89.59992 - diff: 24.97mlTrain batch 20/32 - 61.6ms/batch - loss: 90.92517 - diff: 25.20mlTrain batch 21/32 - 63.4ms/batch - loss: 97.84336 - diff: 25.45mlTrain batch 22/32 - 63.1ms/batch - loss: 94.97419 - diff: 25.21mlTrain batch 23/32 - 64.8ms/batch - loss: 93.34254 - diff: 25.16mlTrain batch 24/32 - 64.0ms/batch - loss: 93.27491 - diff: 25.30mlTrain batch 25/32 - 64.1ms/batch - loss: 91.17635 - diff: 25.14mlTrain batch 26/32 - 63.6ms/batch - loss: 89.34625 - diff: 25.03mlTrain batch 27/32 - 63.8ms/batch - loss: 87.36383 - diff: 24.84mlTrain batch 28/32 - 63.0ms/batch - loss: 86.29440 - diff: 24.77mlTrain batch 29/32 - 62.9ms/batch - loss: 89.75004 - diff: 25.08mlTrain batch 30/32 - 48.8ms/batch - loss: 89.02017 - diff: 25.08mlTrain batch 31/32 - 51.4ms/batch - loss: 88.83296 - diff: 25.17mlTrain batch 32/32 - 44.2ms/batch - loss: 88.76535 - diff: 25.08mlTrain batch 32/32 - 11.1s 44.2ms/batch - loss: 88.76535 - diff: 25.08ml
Test 0.5s: val_loss: 75.40362 - diff: 22.20ml

Epoch 16: current best loss = 69.80195, at epoch 14
Train batch 1/32 - 67.7ms/batch - loss: 44.45035 - diff: 22.12mlTrain batch 2/32 - 59.4ms/batch - loss: 64.78442 - diff: 25.36mlTrain batch 3/32 - 57.5ms/batch - loss: 54.96148 - diff: 22.89mlTrain batch 4/32 - 58.6ms/batch - loss: 49.30015 - diff: 21.44mlTrain batch 5/32 - 59.5ms/batch - loss: 56.05715 - diff: 23.28mlTrain batch 6/32 - 58.2ms/batch - loss: 59.98700 - diff: 24.24mlTrain batch 7/32 - 58.6ms/batch - loss: 56.96941 - diff: 23.72mlTrain batch 8/32 - 67.6ms/batch - loss: 79.16975 - diff: 25.55mlTrain batch 9/32 - 58.6ms/batch - loss: 76.37308 - diff: 25.36mlTrain batch 10/32 - 58.8ms/batch - loss: 79.98632 - diff: 25.59mlTrain batch 11/32 - 57.8ms/batch - loss: 77.63658 - diff: 25.59mlTrain batch 12/32 - 66.1ms/batch - loss: 74.48180 - diff: 25.21mlTrain batch 13/32 - 65.5ms/batch - loss: 75.17657 - diff: 25.45mlTrain batch 14/32 - 65.1ms/batch - loss: 109.89755 - diff: 27.12mlTrain batch 15/32 - 64.0ms/batch - loss: 107.73941 - diff: 27.27mlTrain batch 16/32 - 54.1ms/batch - loss: 102.65804 - diff: 26.62mlTrain batch 17/32 - 57.5ms/batch - loss: 99.19797 - diff: 26.35mlTrain batch 18/32 - 61.0ms/batch - loss: 100.15107 - diff: 26.64mlTrain batch 19/32 - 66.0ms/batch - loss: 98.31107 - diff: 26.45mlTrain batch 20/32 - 59.9ms/batch - loss: 96.66873 - diff: 26.28mlTrain batch 21/32 - 60.9ms/batch - loss: 95.05444 - diff: 26.24mlTrain batch 22/32 - 59.3ms/batch - loss: 93.47118 - diff: 26.14mlTrain batch 23/32 - 60.0ms/batch - loss: 93.73185 - diff: 26.04mlTrain batch 24/32 - 60.0ms/batch - loss: 92.13422 - diff: 25.75mlTrain batch 25/32 - 59.4ms/batch - loss: 89.94404 - diff: 25.46mlTrain batch 26/32 - 59.8ms/batch - loss: 88.79837 - diff: 25.49mlTrain batch 27/32 - 59.1ms/batch - loss: 86.78143 - diff: 25.23mlTrain batch 28/32 - 60.2ms/batch - loss: 86.00083 - diff: 25.33mlTrain batch 29/32 - 60.0ms/batch - loss: 85.46907 - diff: 25.32mlTrain batch 30/32 - 67.3ms/batch - loss: 84.83929 - diff: 25.20mlTrain batch 31/32 - 60.6ms/batch - loss: 83.34795 - diff: 25.09mlTrain batch 32/32 - 44.6ms/batch - loss: 84.07218 - diff: 25.08mlTrain batch 32/32 - 11.3s 44.6ms/batch - loss: 84.07218 - diff: 25.08ml
Test 0.5s: val_loss: 68.72073 - diff: 22.07ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Systole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 17: current best loss = 68.72073, at epoch 16
Train batch 1/32 - 55.0ms/batch - loss: 280.66098 - diff: 34.93mlTrain batch 2/32 - 47.9ms/batch - loss: 151.24327 - diff: 25.05mlTrain batch 3/32 - 63.5ms/batch - loss: 298.81869 - diff: 33.15mlTrain batch 4/32 - 57.6ms/batch - loss: 232.68956 - diff: 29.65mlTrain batch 5/32 - 64.6ms/batch - loss: 193.51384 - diff: 27.39mlTrain batch 6/32 - 64.2ms/batch - loss: 183.52302 - diff: 27.30mlTrain batch 7/32 - 58.7ms/batch - loss: 164.12810 - diff: 26.52mlTrain batch 8/32 - 59.7ms/batch - loss: 151.77886 - diff: 26.50mlTrain batch 9/32 - 59.2ms/batch - loss: 138.50167 - diff: 25.64mlTrain batch 10/32 - 59.8ms/batch - loss: 128.76697 - diff: 25.25mlTrain batch 11/32 - 58.9ms/batch - loss: 119.85668 - diff: 24.49mlTrain batch 12/32 - 59.1ms/batch - loss: 113.64185 - diff: 24.36mlTrain batch 13/32 - 58.8ms/batch - loss: 110.11103 - diff: 24.24mlTrain batch 14/32 - 58.3ms/batch - loss: 106.29766 - diff: 24.25mlTrain batch 15/32 - 59.3ms/batch - loss: 101.79778 - diff: 24.02mlTrain batch 16/32 - 57.5ms/batch - loss: 97.31863 - diff: 23.57mlTrain batch 17/32 - 58.6ms/batch - loss: 93.19701 - diff: 23.13mlTrain batch 18/32 - 56.8ms/batch - loss: 90.92133 - diff: 22.97mlTrain batch 19/32 - 58.1ms/batch - loss: 90.09423 - diff: 23.21mlTrain batch 20/32 - 67.8ms/batch - loss: 87.52560 - diff: 23.06mlTrain batch 21/32 - 59.6ms/batch - loss: 87.59532 - diff: 23.47mlTrain batch 22/32 - 68.5ms/batch - loss: 87.08232 - diff: 23.54mlTrain batch 23/32 - 60.6ms/batch - loss: 88.09239 - diff: 23.77mlTrain batch 24/32 - 71.2ms/batch - loss: 85.89440 - diff: 23.54mlTrain batch 25/32 - 61.4ms/batch - loss: 86.74080 - diff: 23.82mlTrain batch 26/32 - 62.8ms/batch - loss: 85.72102 - diff: 23.87mlTrain batch 27/32 - 61.5ms/batch - loss: 85.11473 - diff: 23.81mlTrain batch 28/32 - 60.1ms/batch - loss: 83.86488 - diff: 23.84mlTrain batch 29/32 - 60.3ms/batch - loss: 82.59526 - diff: 23.76mlTrain batch 30/32 - 60.7ms/batch - loss: 84.22085 - diff: 24.18mlTrain batch 31/32 - 47.9ms/batch - loss: 83.33887 - diff: 24.20mlTrain batch 32/32 - 35.1ms/batch - loss: 84.68643 - diff: 24.20mlTrain batch 32/32 - 10.5s 35.1ms/batch - loss: 84.68643 - diff: 24.20ml
Test 0.5s: val_loss: 71.12474 - diff: 22.89ml

Epoch 18: current best loss = 68.72073, at epoch 16
Train batch 1/32 - 71.8ms/batch - loss: 32.16687 - diff: 16.52mlTrain batch 2/32 - 55.2ms/batch - loss: 86.93649 - diff: 26.67mlTrain batch 3/32 - 65.4ms/batch - loss: 141.88168 - diff: 29.49mlTrain batch 4/32 - 64.6ms/batch - loss: 117.39481 - diff: 27.23mlTrain batch 5/32 - 65.5ms/batch - loss: 106.78754 - diff: 27.41mlTrain batch 6/32 - 65.4ms/batch - loss: 95.17502 - diff: 26.23mlTrain batch 7/32 - 60.8ms/batch - loss: 91.21127 - diff: 26.24mlTrain batch 8/32 - 61.8ms/batch - loss: 92.19911 - diff: 27.27mlTrain batch 9/32 - 66.2ms/batch - loss: 90.96735 - diff: 27.66mlTrain batch 10/32 - 66.7ms/batch - loss: 92.04593 - diff: 28.03mlTrain batch 11/32 - 63.5ms/batch - loss: 88.82435 - diff: 27.81mlTrain batch 12/32 - 55.1ms/batch - loss: 86.20443 - diff: 27.46mlTrain batch 13/32 - 64.0ms/batch - loss: 87.78388 - diff: 27.02mlTrain batch 14/32 - 63.6ms/batch - loss: 84.16563 - diff: 26.54mlTrain batch 15/32 - 63.4ms/batch - loss: 82.02830 - diff: 26.40mlTrain batch 16/32 - 63.0ms/batch - loss: 117.08706 - diff: 27.76mlTrain batch 17/32 - 63.6ms/batch - loss: 113.03541 - diff: 27.50mlTrain batch 18/32 - 62.7ms/batch - loss: 108.89320 - diff: 27.16mlTrain batch 19/32 - 59.4ms/batch - loss: 105.82684 - diff: 26.96mlTrain batch 20/32 - 59.2ms/batch - loss: 103.60946 - diff: 26.97mlTrain batch 21/32 - 62.7ms/batch - loss: 100.95079 - diff: 26.84mlTrain batch 22/32 - 59.3ms/batch - loss: 98.35840 - diff: 26.59mlTrain batch 23/32 - 61.6ms/batch - loss: 95.20353 - diff: 26.11mlTrain batch 24/32 - 66.4ms/batch - loss: 96.40958 - diff: 26.44mlTrain batch 25/32 - 66.7ms/batch - loss: 94.43935 - diff: 26.31mlTrain batch 26/32 - 66.2ms/batch - loss: 92.82705 - diff: 26.16mlTrain batch 27/32 - 66.2ms/batch - loss: 96.14318 - diff: 26.70mlTrain batch 28/32 - 73.3ms/batch - loss: 94.84144 - diff: 26.62mlTrain batch 29/32 - 67.4ms/batch - loss: 92.49463 - diff: 26.33mlTrain batch 30/32 - 65.2ms/batch - loss: 91.17789 - diff: 26.17mlTrain batch 31/32 - 63.2ms/batch - loss: 89.33910 - diff: 25.93mlTrain batch 32/32 - 53.6ms/batch - loss: 89.50231 - diff: 25.87mlTrain batch 32/32 - 10.9s 53.6ms/batch - loss: 89.50231 - diff: 25.87ml
Test 0.5s: val_loss: 79.24295 - diff: 25.44ml

Epoch 19: current best loss = 68.72073, at epoch 16
Train batch 1/32 - 82.5ms/batch - loss: 37.21220 - diff: 19.90mlTrain batch 2/32 - 58.0ms/batch - loss: 50.09494 - diff: 23.09mlTrain batch 3/32 - 57.4ms/batch - loss: 53.26003 - diff: 23.89mlTrain batch 4/32 - 57.6ms/batch - loss: 49.97856 - diff: 22.98mlTrain batch 5/32 - 64.2ms/batch - loss: 47.31542 - diff: 22.41mlTrain batch 6/32 - 57.3ms/batch - loss: 54.88061 - diff: 23.38mlTrain batch 7/32 - 58.5ms/batch - loss: 65.87229 - diff: 25.41mlTrain batch 8/32 - 57.1ms/batch - loss: 62.48138 - diff: 24.88mlTrain batch 9/32 - 58.3ms/batch - loss: 59.35395 - diff: 24.29mlTrain batch 10/32 - 57.1ms/batch - loss: 58.91281 - diff: 24.41mlTrain batch 11/32 - 58.8ms/batch - loss: 60.16181 - diff: 24.15mlTrain batch 12/32 - 58.7ms/batch - loss: 61.36120 - diff: 24.34mlTrain batch 13/32 - 57.3ms/batch - loss: 62.40376 - diff: 24.36mlTrain batch 14/32 - 57.9ms/batch - loss: 64.10562 - diff: 24.70mlTrain batch 15/32 - 57.8ms/batch - loss: 63.60873 - diff: 24.46mlTrain batch 16/32 - 57.7ms/batch - loss: 64.58606 - diff: 24.46mlTrain batch 17/32 - 47.9ms/batch - loss: 72.57435 - diff: 25.21mlTrain batch 18/32 - 48.1ms/batch - loss: 70.30550 - diff: 24.77mlTrain batch 19/32 - 63.7ms/batch - loss: 68.13819 - diff: 24.37mlTrain batch 20/32 - 52.5ms/batch - loss: 67.99057 - diff: 24.29mlTrain batch 21/32 - 58.7ms/batch - loss: 67.94156 - diff: 24.25mlTrain batch 22/32 - 58.3ms/batch - loss: 66.13782 - diff: 23.99mlTrain batch 23/32 - 60.4ms/batch - loss: 74.19142 - diff: 24.41mlTrain batch 24/32 - 60.1ms/batch - loss: 73.17068 - diff: 24.37mlTrain batch 25/32 - 66.0ms/batch - loss: 71.87287 - diff: 24.23mlTrain batch 26/32 - 66.0ms/batch - loss: 71.78155 - diff: 24.08mlTrain batch 27/32 - 68.2ms/batch - loss: 71.67413 - diff: 24.24mlTrain batch 28/32 - 67.7ms/batch - loss: 90.45353 - diff: 24.88mlTrain batch 29/32 - 66.8ms/batch - loss: 90.33070 - diff: 25.01mlTrain batch 30/32 - 67.4ms/batch - loss: 88.79500 - diff: 24.92mlTrain batch 31/32 - 65.9ms/batch - loss: 87.62187 - diff: 24.87mlTrain batch 32/32 - 60.7ms/batch - loss: 88.08059 - diff: 24.83mlTrain batch 32/32 - 10.8s 60.7ms/batch - loss: 88.08059 - diff: 24.83ml
Test 0.6s: val_loss: 116.48465 - diff: 30.53ml

Epoch 20: current best loss = 68.72073, at epoch 16
Train batch 1/32 - 70.6ms/batch - loss: 20.47556 - diff: 14.74mlTrain batch 2/32 - 60.1ms/batch - loss: 28.89890 - diff: 17.00mlTrain batch 3/32 - 60.6ms/batch - loss: 48.85662 - diff: 20.95mlTrain batch 4/32 - 60.2ms/batch - loss: 44.78006 - diff: 20.52mlTrain batch 5/32 - 60.3ms/batch - loss: 58.30167 - diff: 22.16mlTrain batch 6/32 - 60.4ms/batch - loss: 55.56209 - diff: 22.13mlTrain batch 7/32 - 65.5ms/batch - loss: 129.99187 - diff: 26.10mlTrain batch 8/32 - 61.1ms/batch - loss: 118.26268 - diff: 25.21mlTrain batch 9/32 - 65.8ms/batch - loss: 115.35702 - diff: 25.90mlTrain batch 10/32 - 65.8ms/batch - loss: 126.34377 - diff: 27.04mlTrain batch 11/32 - 60.7ms/batch - loss: 119.75257 - diff: 26.77mlTrain batch 12/32 - 60.5ms/batch - loss: 116.81626 - diff: 26.82mlTrain batch 13/32 - 60.6ms/batch - loss: 111.66786 - diff: 26.62mlTrain batch 14/32 - 60.7ms/batch - loss: 109.11765 - diff: 26.76mlTrain batch 15/32 - 60.0ms/batch - loss: 103.82485 - diff: 26.04mlTrain batch 16/32 - 60.4ms/batch - loss: 100.95212 - diff: 25.95mlTrain batch 17/32 - 61.0ms/batch - loss: 96.24907 - diff: 25.31mlTrain batch 18/32 - 60.3ms/batch - loss: 93.96225 - diff: 25.21mlTrain batch 19/32 - 65.6ms/batch - loss: 90.44738 - diff: 24.87mlTrain batch 20/32 - 65.5ms/batch - loss: 90.32795 - diff: 25.02mlTrain batch 21/32 - 63.6ms/batch - loss: 87.56665 - diff: 24.68mlTrain batch 22/32 - 63.5ms/batch - loss: 85.99237 - diff: 24.66mlTrain batch 23/32 - 58.7ms/batch - loss: 86.14305 - diff: 24.76mlTrain batch 24/32 - 58.5ms/batch - loss: 84.74747 - diff: 24.67mlTrain batch 25/32 - 58.3ms/batch - loss: 86.63243 - diff: 25.01mlTrain batch 26/32 - 57.1ms/batch - loss: 90.06694 - diff: 25.33mlTrain batch 27/32 - 58.7ms/batch - loss: 88.54161 - diff: 25.27mlTrain batch 28/32 - 57.7ms/batch - loss: 88.40137 - diff: 25.39mlTrain batch 29/32 - 58.0ms/batch - loss: 88.27239 - diff: 25.32mlTrain batch 30/32 - 57.3ms/batch - loss: 88.98313 - diff: 25.46mlTrain batch 31/32 - 58.0ms/batch - loss: 88.15077 - diff: 25.39mlTrain batch 32/32 - 48.4ms/batch - loss: 87.99304 - diff: 25.32mlTrain batch 32/32 - 11.6s 48.4ms/batch - loss: 87.99304 - diff: 25.32ml
Test 0.6s: val_loss: 73.97722 - diff: 23.13ml

Epoch 21: current best loss = 68.72073, at epoch 16
Train batch 1/32 - 65.2ms/batch - loss: 56.16359 - diff: 22.62mlTrain batch 2/32 - 63.5ms/batch - loss: 59.33045 - diff: 21.53mlTrain batch 3/32 - 58.8ms/batch - loss: 57.39458 - diff: 22.58mlTrain batch 4/32 - 57.6ms/batch - loss: 55.80277 - diff: 22.82mlTrain batch 5/32 - 57.6ms/batch - loss: 65.46760 - diff: 23.84mlTrain batch 6/32 - 57.6ms/batch - loss: 71.50274 - diff: 24.79mlTrain batch 7/32 - 58.0ms/batch - loss: 64.03337 - diff: 23.13mlTrain batch 8/32 - 57.8ms/batch - loss: 64.60584 - diff: 23.65mlTrain batch 9/32 - 58.0ms/batch - loss: 63.15399 - diff: 23.33mlTrain batch 10/32 - 57.2ms/batch - loss: 60.43474 - diff: 23.00mlTrain batch 11/32 - 57.7ms/batch - loss: 68.14862 - diff: 23.96mlTrain batch 12/32 - 58.0ms/batch - loss: 65.13038 - diff: 23.56mlTrain batch 13/32 - 59.5ms/batch - loss: 74.97757 - diff: 24.69mlTrain batch 14/32 - 58.0ms/batch - loss: 115.87944 - diff: 26.73mlTrain batch 15/32 - 55.9ms/batch - loss: 113.77958 - diff: 27.12mlTrain batch 16/32 - 50.8ms/batch - loss: 109.13252 - diff: 26.77mlTrain batch 17/32 - 69.7ms/batch - loss: 107.24403 - diff: 26.86mlTrain batch 18/32 - 64.4ms/batch - loss: 105.78749 - diff: 26.84mlTrain batch 19/32 - 61.2ms/batch - loss: 103.57789 - diff: 26.87mlTrain batch 20/32 - 61.4ms/batch - loss: 104.42006 - diff: 27.18mlTrain batch 21/32 - 66.2ms/batch - loss: 104.21492 - diff: 27.33mlTrain batch 22/32 - 63.9ms/batch - loss: 102.56841 - diff: 27.27mlTrain batch 23/32 - 58.4ms/batch - loss: 100.76503 - diff: 27.34mlTrain batch 24/32 - 59.7ms/batch - loss: 97.70501 - diff: 26.95mlTrain batch 25/32 - 62.6ms/batch - loss: 96.95999 - diff: 26.90mlTrain batch 26/32 - 58.5ms/batch - loss: 94.38577 - diff: 26.61mlTrain batch 27/32 - 63.4ms/batch - loss: 92.11723 - diff: 26.38mlTrain batch 28/32 - 63.5ms/batch - loss: 97.64348 - diff: 26.67mlTrain batch 29/32 - 63.5ms/batch - loss: 95.88121 - diff: 26.54mlTrain batch 30/32 - 63.7ms/batch - loss: 96.19857 - diff: 26.63mlTrain batch 31/32 - 60.8ms/batch - loss: 93.79333 - diff: 26.27mlTrain batch 32/32 - 58.2ms/batch - loss: 93.93849 - diff: 26.22mlTrain batch 32/32 - 11.7s 58.2ms/batch - loss: 93.93849 - diff: 26.22ml
Test 0.6s: val_loss: 143.14088 - diff: 39.53ml

Epoch 22: current best loss = 68.72073, at epoch 16
Train batch 1/32 - 73.8ms/batch - loss: 99.56631 - diff: 30.64mlTrain batch 2/32 - 64.3ms/batch - loss: 70.71892 - diff: 26.18mlTrain batch 3/32 - 57.7ms/batch - loss: 58.44090 - diff: 23.01mlTrain batch 4/32 - 57.3ms/batch - loss: 58.92367 - diff: 23.00mlTrain batch 5/32 - 58.8ms/batch - loss: 67.28586 - diff: 24.11mlTrain batch 6/32 - 63.7ms/batch - loss: 60.27277 - diff: 23.01mlTrain batch 7/32 - 58.8ms/batch - loss: 148.70398 - diff: 27.49mlTrain batch 8/32 - 62.1ms/batch - loss: 139.07885 - diff: 26.80mlTrain batch 9/32 - 62.0ms/batch - loss: 126.87189 - diff: 25.87mlTrain batch 10/32 - 61.2ms/batch - loss: 123.56384 - diff: 26.03mlTrain batch 11/32 - 60.1ms/batch - loss: 125.08750 - diff: 26.75mlTrain batch 12/32 - 66.9ms/batch - loss: 117.14227 - diff: 25.84mlTrain batch 13/32 - 65.2ms/batch - loss: 110.67867 - diff: 25.33mlTrain batch 14/32 - 68.9ms/batch - loss: 106.81885 - diff: 25.18mlTrain batch 15/32 - 68.0ms/batch - loss: 103.61735 - diff: 24.97mlTrain batch 16/32 - 66.0ms/batch - loss: 99.03246 - diff: 24.34mlTrain batch 17/32 - 65.6ms/batch - loss: 96.70891 - diff: 24.41mlTrain batch 18/32 - 52.9ms/batch - loss: 96.42028 - diff: 24.80mlTrain batch 19/32 - 60.3ms/batch - loss: 102.57537 - diff: 25.25mlTrain batch 20/32 - 58.8ms/batch - loss: 99.54221 - diff: 24.92mlTrain batch 21/32 - 57.4ms/batch - loss: 98.13927 - diff: 25.15mlTrain batch 22/32 - 57.7ms/batch - loss: 95.81579 - diff: 25.03mlTrain batch 23/32 - 59.2ms/batch - loss: 94.87527 - diff: 25.16mlTrain batch 24/32 - 58.3ms/batch - loss: 92.02040 - diff: 24.82mlTrain batch 25/32 - 58.0ms/batch - loss: 91.64451 - diff: 24.88mlTrain batch 26/32 - 59.0ms/batch - loss: 89.87697 - diff: 24.78mlTrain batch 27/32 - 58.9ms/batch - loss: 88.76121 - diff: 24.74mlTrain batch 28/32 - 58.6ms/batch - loss: 87.47018 - diff: 24.75mlTrain batch 29/32 - 58.5ms/batch - loss: 85.56971 - diff: 24.48mlTrain batch 30/32 - 61.2ms/batch - loss: 83.33750 - diff: 24.15mlTrain batch 31/32 - 59.9ms/batch - loss: 81.23927 - diff: 23.85mlTrain batch 32/32 - 46.0ms/batch - loss: 81.64585 - diff: 23.81mlTrain batch 32/32 - 11.4s 46.0ms/batch - loss: 81.64585 - diff: 23.81ml
Test 0.6s: val_loss: 73.90108 - diff: 23.33ml

Epoch 23: current best loss = 68.72073, at epoch 16
Train batch 1/32 - 86.1ms/batch - loss: 46.90557 - diff: 20.53mlTrain batch 2/32 - 66.4ms/batch - loss: 48.38837 - diff: 21.58mlTrain batch 3/32 - 66.8ms/batch - loss: 225.01923 - diff: 30.55mlTrain batch 4/32 - 66.4ms/batch - loss: 185.07023 - diff: 28.87mlTrain batch 5/32 - 66.3ms/batch - loss: 155.83444 - diff: 27.00mlTrain batch 6/32 - 60.3ms/batch - loss: 140.91514 - diff: 26.62mlTrain batch 7/32 - 61.1ms/batch - loss: 133.58513 - diff: 26.52mlTrain batch 8/32 - 61.5ms/batch - loss: 128.05431 - diff: 26.72mlTrain batch 9/32 - 61.7ms/batch - loss: 117.71388 - diff: 26.00mlTrain batch 10/32 - 61.0ms/batch - loss: 128.74421 - diff: 26.57mlTrain batch 11/32 - 60.2ms/batch - loss: 123.08341 - diff: 26.65mlTrain batch 12/32 - 60.8ms/batch - loss: 125.09026 - diff: 27.92mlTrain batch 13/32 - 59.8ms/batch - loss: 121.66964 - diff: 27.87mlTrain batch 14/32 - 64.8ms/batch - loss: 115.99822 - diff: 27.57mlTrain batch 15/32 - 67.0ms/batch - loss: 112.21863 - diff: 27.31mlTrain batch 16/32 - 61.8ms/batch - loss: 109.02522 - diff: 27.08mlTrain batch 17/32 - 61.9ms/batch - loss: 105.43337 - diff: 26.81mlTrain batch 18/32 - 59.3ms/batch - loss: 102.81084 - diff: 26.84mlTrain batch 19/32 - 61.1ms/batch - loss: 99.68138 - diff: 26.51mlTrain batch 20/32 - 61.4ms/batch - loss: 97.49313 - diff: 26.37mlTrain batch 21/32 - 61.1ms/batch - loss: 95.89278 - diff: 26.24mlTrain batch 22/32 - 61.0ms/batch - loss: 93.23534 - diff: 26.04mlTrain batch 23/32 - 61.0ms/batch - loss: 91.70756 - diff: 25.97mlTrain batch 24/32 - 60.6ms/batch - loss: 91.16138 - diff: 25.92mlTrain batch 25/32 - 60.8ms/batch - loss: 91.28570 - diff: 26.05mlTrain batch 26/32 - 61.9ms/batch - loss: 88.76846 - diff: 25.60mlTrain batch 27/32 - 62.1ms/batch - loss: 87.62020 - diff: 25.44mlTrain batch 28/32 - 62.3ms/batch - loss: 88.11376 - diff: 25.50mlTrain batch 29/32 - 61.9ms/batch - loss: 87.04616 - diff: 25.48mlTrain batch 30/32 - 65.3ms/batch - loss: 85.84874 - diff: 25.41mlTrain batch 31/32 - 64.7ms/batch - loss: 84.49610 - diff: 25.26mlTrain batch 32/32 - 53.4ms/batch - loss: 84.10197 - diff: 25.14mlTrain batch 32/32 - 10.9s 53.4ms/batch - loss: 84.10197 - diff: 25.14ml
Test 0.6s: val_loss: 70.36892 - diff: 22.67ml

Epoch 24: current best loss = 68.72073, at epoch 16
Train batch 1/32 - 69.6ms/batch - loss: 42.24821 - diff: 20.82mlTrain batch 2/32 - 58.4ms/batch - loss: 137.26083 - diff: 29.29mlTrain batch 3/32 - 58.1ms/batch - loss: 118.40319 - diff: 30.25mlTrain batch 4/32 - 62.7ms/batch - loss: 117.76474 - diff: 30.78mlTrain batch 5/32 - 64.3ms/batch - loss: 101.21424 - diff: 28.11mlTrain batch 6/32 - 57.8ms/batch - loss: 89.65066 - diff: 26.44mlTrain batch 7/32 - 64.0ms/batch - loss: 85.81681 - diff: 26.40mlTrain batch 8/32 - 57.6ms/batch - loss: 83.62759 - diff: 26.19mlTrain batch 9/32 - 67.3ms/batch - loss: 77.59722 - diff: 25.37mlTrain batch 10/32 - 58.3ms/batch - loss: 76.28034 - diff: 25.18mlTrain batch 11/32 - 70.1ms/batch - loss: 73.51271 - diff: 24.79mlTrain batch 12/32 - 64.0ms/batch - loss: 70.43307 - diff: 24.37mlTrain batch 13/32 - 63.1ms/batch - loss: 72.88942 - diff: 24.63mlTrain batch 14/32 - 54.0ms/batch - loss: 72.50878 - diff: 24.50mlTrain batch 15/32 - 58.7ms/batch - loss: 74.44863 - diff: 25.04mlTrain batch 16/32 - 58.5ms/batch - loss: 74.30607 - diff: 25.15mlTrain batch 17/32 - 50.6ms/batch - loss: 73.16818 - diff: 25.16mlTrain batch 18/32 - 48.1ms/batch - loss: 71.72353 - diff: 24.91mlTrain batch 19/32 - 58.4ms/batch - loss: 70.36227 - diff: 24.70mlTrain batch 20/32 - 58.7ms/batch - loss: 71.36339 - diff: 24.90mlTrain batch 21/32 - 61.2ms/batch - loss: 73.71992 - diff: 25.32mlTrain batch 22/32 - 60.5ms/batch - loss: 72.42029 - diff: 25.13mlTrain batch 23/32 - 65.8ms/batch - loss: 70.65229 - diff: 24.78mlTrain batch 24/32 - 66.4ms/batch - loss: 69.51631 - diff: 24.68mlTrain batch 25/32 - 66.1ms/batch - loss: 68.53742 - diff: 24.46mlTrain batch 26/32 - 65.6ms/batch - loss: 66.72896 - diff: 24.11mlTrain batch 27/32 - 65.9ms/batch - loss: 66.43307 - diff: 24.15mlTrain batch 28/32 - 65.5ms/batch - loss: 64.93213 - diff: 23.82mlTrain batch 29/32 - 69.6ms/batch - loss: 65.03773 - diff: 23.82mlTrain batch 30/32 - 63.4ms/batch - loss: 64.04910 - diff: 23.63mlTrain batch 31/32 - 53.1ms/batch - loss: 81.07589 - diff: 24.28mlTrain batch 32/32 - 42.0ms/batch - loss: 81.54236 - diff: 24.24mlTrain batch 32/32 - 10.8s 42.0ms/batch - loss: 81.54236 - diff: 24.24ml
Test 0.5s: val_loss: 63.74068 - diff: 21.50ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Systole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 25: current best loss = 63.74068, at epoch 24
Train batch 1/32 - 68.5ms/batch - loss: 40.16802 - diff: 20.28mlTrain batch 2/32 - 50.5ms/batch - loss: 38.31417 - diff: 19.52mlTrain batch 3/32 - 63.3ms/batch - loss: 57.44103 - diff: 22.80mlTrain batch 4/32 - 57.4ms/batch - loss: 52.79550 - diff: 21.86mlTrain batch 5/32 - 61.1ms/batch - loss: 49.63364 - diff: 20.90mlTrain batch 6/32 - 61.6ms/batch - loss: 49.26259 - diff: 20.87mlTrain batch 7/32 - 63.9ms/batch - loss: 60.53300 - diff: 22.07mlTrain batch 8/32 - 61.6ms/batch - loss: 67.03979 - diff: 23.39mlTrain batch 9/32 - 64.5ms/batch - loss: 64.99327 - diff: 23.02mlTrain batch 10/32 - 62.4ms/batch - loss: 62.38677 - diff: 22.61mlTrain batch 11/32 - 63.2ms/batch - loss: 61.56761 - diff: 22.48mlTrain batch 12/32 - 64.7ms/batch - loss: 60.79550 - diff: 22.59mlTrain batch 13/32 - 61.6ms/batch - loss: 59.06214 - diff: 22.36mlTrain batch 14/32 - 63.5ms/batch - loss: 59.35703 - diff: 22.51mlTrain batch 15/32 - 60.5ms/batch - loss: 60.26448 - diff: 22.75mlTrain batch 16/32 - 71.6ms/batch - loss: 60.48073 - diff: 22.90mlTrain batch 17/32 - 60.3ms/batch - loss: 59.37527 - diff: 22.79mlTrain batch 18/32 - 67.2ms/batch - loss: 59.65740 - diff: 22.79mlTrain batch 19/32 - 57.7ms/batch - loss: 60.79129 - diff: 23.08mlTrain batch 20/32 - 66.7ms/batch - loss: 85.46068 - diff: 23.74mlTrain batch 21/32 - 58.7ms/batch - loss: 83.77370 - diff: 23.73mlTrain batch 22/32 - 64.4ms/batch - loss: 82.57212 - diff: 23.74mlTrain batch 23/32 - 63.1ms/batch - loss: 80.28938 - diff: 23.45mlTrain batch 24/32 - 63.5ms/batch - loss: 79.57733 - diff: 23.49mlTrain batch 25/32 - 63.4ms/batch - loss: 78.76241 - diff: 23.55mlTrain batch 26/32 - 63.3ms/batch - loss: 86.53337 - diff: 24.28mlTrain batch 27/32 - 63.7ms/batch - loss: 85.13332 - diff: 24.22mlTrain batch 28/32 - 63.5ms/batch - loss: 84.11801 - diff: 24.15mlTrain batch 29/32 - 63.4ms/batch - loss: 85.10600 - diff: 24.35mlTrain batch 30/32 - 64.5ms/batch - loss: 83.30429 - diff: 24.11mlTrain batch 31/32 - 56.4ms/batch - loss: 82.18934 - diff: 24.03mlTrain batch 32/32 - 42.2ms/batch - loss: 83.41236 - diff: 24.00mlTrain batch 32/32 - 10.6s 42.2ms/batch - loss: 83.41236 - diff: 24.00ml
Test 0.5s: val_loss: 71.02218 - diff: 22.66ml

Epoch 26: current best loss = 63.74068, at epoch 24
Train batch 1/32 - 71.1ms/batch - loss: 19.81001 - diff: 13.22mlTrain batch 2/32 - 60.8ms/batch - loss: 281.33164 - diff: 31.96mlTrain batch 3/32 - 61.9ms/batch - loss: 203.29341 - diff: 29.38mlTrain batch 4/32 - 61.8ms/batch - loss: 165.55632 - diff: 27.29mlTrain batch 5/32 - 67.9ms/batch - loss: 141.65541 - diff: 26.11mlTrain batch 6/32 - 61.5ms/batch - loss: 127.57782 - diff: 25.48mlTrain batch 7/32 - 64.4ms/batch - loss: 113.92874 - diff: 24.51mlTrain batch 8/32 - 61.2ms/batch - loss: 108.55293 - diff: 24.96mlTrain batch 9/32 - 62.3ms/batch - loss: 107.24162 - diff: 25.42mlTrain batch 10/32 - 61.4ms/batch - loss: 100.34344 - diff: 24.98mlTrain batch 11/32 - 61.5ms/batch - loss: 94.89689 - diff: 24.65mlTrain batch 12/32 - 60.7ms/batch - loss: 93.89858 - diff: 24.76mlTrain batch 13/32 - 62.3ms/batch - loss: 92.24805 - diff: 24.65mlTrain batch 14/32 - 52.3ms/batch - loss: 90.13922 - diff: 24.87mlTrain batch 15/32 - 71.7ms/batch - loss: 88.75415 - diff: 25.01mlTrain batch 16/32 - 63.0ms/batch - loss: 85.03718 - diff: 24.42mlTrain batch 17/32 - 67.7ms/batch - loss: 83.10446 - diff: 24.46mlTrain batch 18/32 - 57.4ms/batch - loss: 83.04962 - diff: 24.62mlTrain batch 19/32 - 59.7ms/batch - loss: 82.50908 - diff: 24.71mlTrain batch 20/32 - 55.4ms/batch - loss: 81.85447 - diff: 24.58mlTrain batch 21/32 - 58.3ms/batch - loss: 79.37512 - diff: 24.29mlTrain batch 22/32 - 56.8ms/batch - loss: 77.41013 - diff: 24.12mlTrain batch 23/32 - 57.1ms/batch - loss: 86.63426 - diff: 25.07mlTrain batch 24/32 - 58.4ms/batch - loss: 85.27426 - diff: 25.05mlTrain batch 25/32 - 59.2ms/batch - loss: 87.45728 - diff: 25.46mlTrain batch 26/32 - 60.4ms/batch - loss: 85.27102 - diff: 25.21mlTrain batch 27/32 - 53.3ms/batch - loss: 84.02386 - diff: 25.11mlTrain batch 28/32 - 60.4ms/batch - loss: 82.68738 - diff: 25.01mlTrain batch 29/32 - 61.7ms/batch - loss: 80.98077 - diff: 24.87mlTrain batch 30/32 - 60.9ms/batch - loss: 80.15649 - diff: 24.81mlTrain batch 31/32 - 52.2ms/batch - loss: 80.27339 - diff: 24.85mlTrain batch 32/32 - 44.5ms/batch - loss: 80.89537 - diff: 24.80mlTrain batch 32/32 - 12.1s 44.5ms/batch - loss: 80.89537 - diff: 24.80ml
Test 0.6s: val_loss: 107.24111 - diff: 32.10ml

Epoch 27: current best loss = 63.74068, at epoch 24
Train batch 1/32 - 71.7ms/batch - loss: 50.25969 - diff: 20.32mlTrain batch 2/32 - 61.0ms/batch - loss: 37.09194 - diff: 17.79mlTrain batch 3/32 - 60.6ms/batch - loss: 61.28366 - diff: 22.76mlTrain batch 4/32 - 60.7ms/batch - loss: 56.63254 - diff: 22.54mlTrain batch 5/32 - 61.4ms/batch - loss: 59.07024 - diff: 23.31mlTrain batch 6/32 - 60.8ms/batch - loss: 62.47759 - diff: 23.14mlTrain batch 7/32 - 61.5ms/batch - loss: 59.65417 - diff: 22.94mlTrain batch 8/32 - 61.4ms/batch - loss: 57.29509 - diff: 22.40mlTrain batch 9/32 - 59.7ms/batch - loss: 54.92028 - diff: 22.11mlTrain batch 10/32 - 54.3ms/batch - loss: 54.33219 - diff: 22.09mlTrain batch 11/32 - 67.1ms/batch - loss: 52.74241 - diff: 21.95mlTrain batch 12/32 - 67.5ms/batch - loss: 52.41370 - diff: 22.17mlTrain batch 13/32 - 61.3ms/batch - loss: 57.69225 - diff: 22.98mlTrain batch 14/32 - 61.5ms/batch - loss: 58.70194 - diff: 23.41mlTrain batch 15/32 - 62.4ms/batch - loss: 56.75239 - diff: 23.03mlTrain batch 16/32 - 61.0ms/batch - loss: 55.99782 - diff: 22.90mlTrain batch 17/32 - 61.7ms/batch - loss: 56.97520 - diff: 23.04mlTrain batch 18/32 - 62.5ms/batch - loss: 59.11202 - diff: 23.12mlTrain batch 19/32 - 67.1ms/batch - loss: 60.12081 - diff: 23.38mlTrain batch 20/32 - 66.8ms/batch - loss: 59.43697 - diff: 23.20mlTrain batch 21/32 - 64.6ms/batch - loss: 67.37497 - diff: 23.47mlTrain batch 22/32 - 64.4ms/batch - loss: 67.47056 - diff: 23.73mlTrain batch 23/32 - 64.6ms/batch - loss: 66.22561 - diff: 23.63mlTrain batch 24/32 - 64.4ms/batch - loss: 64.27299 - diff: 23.27mlTrain batch 25/32 - 64.4ms/batch - loss: 64.83890 - diff: 23.45mlTrain batch 26/32 - 64.7ms/batch - loss: 63.77024 - diff: 23.37mlTrain batch 27/32 - 64.3ms/batch - loss: 62.75651 - diff: 23.17mlTrain batch 28/32 - 64.5ms/batch - loss: 88.05281 - diff: 24.15mlTrain batch 29/32 - 64.9ms/batch - loss: 85.77582 - diff: 23.82mlTrain batch 30/32 - 64.6ms/batch - loss: 84.51857 - diff: 23.78mlTrain batch 31/32 - 64.7ms/batch - loss: 84.40451 - diff: 23.84mlTrain batch 32/32 - 41.5ms/batch - loss: 84.72398 - diff: 23.79mlTrain batch 32/32 - 11.0s 41.5ms/batch - loss: 84.72398 - diff: 23.79ml
Test 0.5s: val_loss: 69.24205 - diff: 21.95ml

Epoch 28: current best loss = 63.74068, at epoch 24
Train batch 1/32 - 68.9ms/batch - loss: 41.36208 - diff: 18.44mlTrain batch 2/32 - 58.4ms/batch - loss: 67.36489 - diff: 21.75mlTrain batch 3/32 - 58.4ms/batch - loss: 68.12627 - diff: 22.83mlTrain batch 4/32 - 58.2ms/batch - loss: 62.30860 - diff: 22.54mlTrain batch 5/32 - 59.4ms/batch - loss: 62.75229 - diff: 23.32mlTrain batch 6/32 - 59.8ms/batch - loss: 60.01468 - diff: 23.00mlTrain batch 7/32 - 59.4ms/batch - loss: 55.60567 - diff: 21.80mlTrain batch 8/32 - 63.3ms/batch - loss: 56.42740 - diff: 22.52mlTrain batch 9/32 - 62.3ms/batch - loss: 54.70701 - diff: 21.95mlTrain batch 10/32 - 63.0ms/batch - loss: 73.49572 - diff: 23.83mlTrain batch 11/32 - 63.1ms/batch - loss: 69.43982 - diff: 23.10mlTrain batch 12/32 - 61.9ms/batch - loss: 68.07405 - diff: 23.09mlTrain batch 13/32 - 64.8ms/batch - loss: 72.16217 - diff: 23.75mlTrain batch 14/32 - 56.4ms/batch - loss: 71.97057 - diff: 24.07mlTrain batch 15/32 - 60.3ms/batch - loss: 71.67508 - diff: 24.17mlTrain batch 16/32 - 60.5ms/batch - loss: 69.46175 - diff: 23.91mlTrain batch 17/32 - 61.7ms/batch - loss: 88.70279 - diff: 24.57mlTrain batch 18/32 - 59.9ms/batch - loss: 86.28691 - diff: 24.22mlTrain batch 19/32 - 59.7ms/batch - loss: 83.51895 - diff: 24.05mlTrain batch 20/32 - 58.2ms/batch - loss: 81.39276 - diff: 23.83mlTrain batch 21/32 - 59.9ms/batch - loss: 79.18187 - diff: 23.54mlTrain batch 22/32 - 57.7ms/batch - loss: 79.16217 - diff: 23.84mlTrain batch 23/32 - 61.0ms/batch - loss: 78.54255 - diff: 23.95mlTrain batch 24/32 - 66.1ms/batch - loss: 77.02757 - diff: 23.85mlTrain batch 25/32 - 62.6ms/batch - loss: 75.33788 - diff: 23.64mlTrain batch 26/32 - 62.5ms/batch - loss: 77.13935 - diff: 24.04mlTrain batch 27/32 - 61.7ms/batch - loss: 75.74770 - diff: 23.92mlTrain batch 28/32 - 61.5ms/batch - loss: 75.04088 - diff: 23.99mlTrain batch 29/32 - 60.9ms/batch - loss: 74.55195 - diff: 23.98mlTrain batch 30/32 - 60.8ms/batch - loss: 72.86567 - diff: 23.64mlTrain batch 31/32 - 60.5ms/batch - loss: 72.61015 - diff: 23.60mlTrain batch 32/32 - 56.9ms/batch - loss: 74.07818 - diff: 23.61mlTrain batch 32/32 - 11.6s 56.9ms/batch - loss: 74.07818 - diff: 23.61ml
Test 0.5s: val_loss: 88.68777 - diff: 26.08ml

Epoch 29: current best loss = 63.74068, at epoch 24
Train batch 1/32 - 72.3ms/batch - loss: 44.47477 - diff: 22.63mlTrain batch 2/32 - 64.0ms/batch - loss: 62.71140 - diff: 24.39mlTrain batch 3/32 - 66.3ms/batch - loss: 62.54186 - diff: 23.72mlTrain batch 4/32 - 56.4ms/batch - loss: 63.51321 - diff: 24.41mlTrain batch 5/32 - 64.4ms/batch - loss: 90.25687 - diff: 25.31mlTrain batch 6/32 - 62.9ms/batch - loss: 79.64687 - diff: 23.71mlTrain batch 7/32 - 63.0ms/batch - loss: 76.48915 - diff: 23.64mlTrain batch 8/32 - 62.8ms/batch - loss: 68.71255 - diff: 21.90mlTrain batch 9/32 - 64.5ms/batch - loss: 65.49618 - diff: 21.86mlTrain batch 10/32 - 62.9ms/batch - loss: 70.85504 - diff: 22.78mlTrain batch 11/32 - 63.1ms/batch - loss: 114.06135 - diff: 24.54mlTrain batch 12/32 - 63.3ms/batch - loss: 106.94367 - diff: 23.93mlTrain batch 13/32 - 61.0ms/batch - loss: 103.99625 - diff: 24.04mlTrain batch 14/32 - 63.4ms/batch - loss: 100.15101 - diff: 23.79mlTrain batch 15/32 - 58.4ms/batch - loss: 95.70266 - diff: 23.18mlTrain batch 16/32 - 57.7ms/batch - loss: 105.62024 - diff: 25.08mlTrain batch 17/32 - 63.5ms/batch - loss: 105.38838 - diff: 25.08mlTrain batch 18/32 - 62.9ms/batch - loss: 101.23150 - diff: 24.76mlTrain batch 19/32 - 63.9ms/batch - loss: 100.05127 - diff: 24.96mlTrain batch 20/32 - 62.6ms/batch - loss: 97.48212 - diff: 24.95mlTrain batch 21/32 - 59.4ms/batch - loss: 95.24264 - diff: 24.88mlTrain batch 22/32 - 58.7ms/batch - loss: 95.03427 - diff: 24.98mlTrain batch 23/32 - 67.1ms/batch - loss: 93.16569 - diff: 24.89mlTrain batch 24/32 - 66.4ms/batch - loss: 93.09551 - diff: 24.96mlTrain batch 25/32 - 65.0ms/batch - loss: 93.52438 - diff: 25.18mlTrain batch 26/32 - 54.8ms/batch - loss: 91.23881 - diff: 24.96mlTrain batch 27/32 - 73.5ms/batch - loss: 89.72615 - diff: 24.83mlTrain batch 28/32 - 66.2ms/batch - loss: 88.08820 - diff: 24.76mlTrain batch 29/32 - 73.7ms/batch - loss: 90.87025 - diff: 25.26mlTrain batch 30/32 - 65.3ms/batch - loss: 89.04721 - diff: 25.04mlTrain batch 31/32 - 61.4ms/batch - loss: 89.36013 - diff: 25.20mlTrain batch 32/32 - 44.6ms/batch - loss: 90.20670 - diff: 25.20mlTrain batch 32/32 - 10.6s 44.6ms/batch - loss: 90.20670 - diff: 25.20ml
Test 0.5s: val_loss: 89.76919 - diff: 27.78ml

Epoch 30: current best loss = 63.74068, at epoch 24
Train batch 1/32 - 72.0ms/batch - loss: 76.09940 - diff: 26.51mlTrain batch 2/32 - 62.4ms/batch - loss: 274.83800 - diff: 32.40mlTrain batch 3/32 - 61.1ms/batch - loss: 213.88747 - diff: 30.18mlTrain batch 4/32 - 61.1ms/batch - loss: 184.95504 - diff: 30.37mlTrain batch 5/32 - 61.9ms/batch - loss: 169.71292 - diff: 30.53mlTrain batch 6/32 - 62.1ms/batch - loss: 151.15313 - diff: 29.62mlTrain batch 7/32 - 62.1ms/batch - loss: 135.16707 - diff: 28.35mlTrain batch 8/32 - 62.0ms/batch - loss: 128.02434 - diff: 27.94mlTrain batch 9/32 - 61.5ms/batch - loss: 127.47885 - diff: 28.79mlTrain batch 10/32 - 62.2ms/batch - loss: 120.90619 - diff: 28.57mlTrain batch 11/32 - 62.1ms/batch - loss: 113.16149 - diff: 27.65mlTrain batch 12/32 - 62.3ms/batch - loss: 112.65672 - diff: 27.91mlTrain batch 13/32 - 62.4ms/batch - loss: 108.24289 - diff: 27.48mlTrain batch 14/32 - 54.6ms/batch - loss: 104.81310 - diff: 27.20mlTrain batch 15/32 - 61.6ms/batch - loss: 103.24824 - diff: 27.24mlTrain batch 16/32 - 61.4ms/batch - loss: 99.12846 - diff: 26.77mlTrain batch 17/32 - 60.9ms/batch - loss: 98.28188 - diff: 26.62mlTrain batch 18/32 - 60.9ms/batch - loss: 94.60271 - diff: 26.32mlTrain batch 19/32 - 61.0ms/batch - loss: 90.61340 - diff: 25.75mlTrain batch 20/32 - 60.6ms/batch - loss: 89.18150 - diff: 25.81mlTrain batch 21/32 - 60.4ms/batch - loss: 86.72869 - diff: 25.42mlTrain batch 22/32 - 61.1ms/batch - loss: 84.13844 - diff: 25.08mlTrain batch 23/32 - 60.3ms/batch - loss: 84.08565 - diff: 24.99mlTrain batch 24/32 - 60.5ms/batch - loss: 87.71581 - diff: 25.12mlTrain batch 25/32 - 60.3ms/batch - loss: 84.92609 - diff: 24.70mlTrain batch 26/32 - 60.5ms/batch - loss: 85.86007 - diff: 24.97mlTrain batch 27/32 - 60.4ms/batch - loss: 85.43757 - diff: 25.01mlTrain batch 28/32 - 69.7ms/batch - loss: 83.91826 - diff: 24.79mlTrain batch 29/32 - 60.8ms/batch - loss: 83.20861 - diff: 24.71mlTrain batch 30/32 - 61.1ms/batch - loss: 82.58207 - diff: 24.61mlTrain batch 31/32 - 60.0ms/batch - loss: 82.26055 - diff: 24.59mlTrain batch 32/32 - 58.2ms/batch - loss: 85.26009 - diff: 24.66mlTrain batch 32/32 - 10.9s 58.2ms/batch - loss: 85.26009 - diff: 24.66ml
Test 0.6s: val_loss: 72.35706 - diff: 23.73ml

Epoch 31: current best loss = 63.74068, at epoch 24
Train batch 1/32 - 76.5ms/batch - loss: 48.89560 - diff: 23.12mlTrain batch 2/32 - 61.3ms/batch - loss: 42.25985 - diff: 20.02mlTrain batch 3/32 - 64.0ms/batch - loss: 34.99176 - diff: 18.18mlTrain batch 4/32 - 60.6ms/batch - loss: 41.97422 - diff: 19.71mlTrain batch 5/32 - 62.6ms/batch - loss: 44.99124 - diff: 20.87mlTrain batch 6/32 - 60.8ms/batch - loss: 40.20288 - diff: 19.53mlTrain batch 7/32 - 61.8ms/batch - loss: 40.42762 - diff: 19.42mlTrain batch 8/32 - 58.2ms/batch - loss: 39.50025 - diff: 19.35mlTrain batch 9/32 - 64.3ms/batch - loss: 47.49876 - diff: 20.96mlTrain batch 10/32 - 66.0ms/batch - loss: 47.37132 - diff: 20.62mlTrain batch 11/32 - 66.5ms/batch - loss: 50.07393 - diff: 21.02mlTrain batch 12/32 - 59.8ms/batch - loss: 49.09532 - diff: 21.03mlTrain batch 13/32 - 58.9ms/batch - loss: 47.28397 - diff: 20.73mlTrain batch 14/32 - 62.8ms/batch - loss: 47.56326 - diff: 20.52mlTrain batch 15/32 - 52.6ms/batch - loss: 57.63373 - diff: 21.63mlTrain batch 16/32 - 61.9ms/batch - loss: 57.58026 - diff: 21.86mlTrain batch 17/32 - 61.4ms/batch - loss: 57.42484 - diff: 21.94mlTrain batch 18/32 - 60.9ms/batch - loss: 57.89021 - diff: 22.16mlTrain batch 19/32 - 61.0ms/batch - loss: 56.05493 - diff: 21.85mlTrain batch 20/32 - 61.0ms/batch - loss: 54.63859 - diff: 21.61mlTrain batch 21/32 - 60.4ms/batch - loss: 56.96862 - diff: 21.93mlTrain batch 22/32 - 60.6ms/batch - loss: 73.27528 - diff: 22.70mlTrain batch 23/32 - 58.9ms/batch - loss: 71.63904 - diff: 22.56mlTrain batch 24/32 - 57.9ms/batch - loss: 71.21437 - diff: 22.70mlTrain batch 25/32 - 58.1ms/batch - loss: 69.48625 - diff: 22.42mlTrain batch 26/32 - 58.9ms/batch - loss: 69.35787 - diff: 22.37mlTrain batch 27/32 - 58.3ms/batch - loss: 68.69146 - diff: 22.38mlTrain batch 28/32 - 58.3ms/batch - loss: 71.53868 - diff: 22.97mlTrain batch 29/32 - 57.3ms/batch - loss: 70.58836 - diff: 22.90mlTrain batch 30/32 - 52.2ms/batch - loss: 69.74977 - diff: 22.80mlTrain batch 31/32 - 49.5ms/batch - loss: 69.61100 - diff: 22.95mlTrain batch 32/32 - 54.0ms/batch - loss: 70.24649 - diff: 22.92mlTrain batch 32/32 - 10.6s 54.0ms/batch - loss: 70.24649 - diff: 22.92ml
Test 0.6s: val_loss: 68.72458 - diff: 24.08ml

Epoch 32: current best loss = 63.74068, at epoch 24
Train batch 1/32 - 69.5ms/batch - loss: 71.82800 - diff: 22.73mlTrain batch 2/32 - 58.3ms/batch - loss: 53.75949 - diff: 20.42mlTrain batch 3/32 - 58.6ms/batch - loss: 54.88974 - diff: 21.80mlTrain batch 4/32 - 59.7ms/batch - loss: 56.41352 - diff: 22.59mlTrain batch 5/32 - 59.6ms/batch - loss: 56.02750 - diff: 22.71mlTrain batch 6/32 - 59.3ms/batch - loss: 53.27075 - diff: 22.31mlTrain batch 7/32 - 59.1ms/batch - loss: 56.24435 - diff: 22.70mlTrain batch 8/32 - 59.3ms/batch - loss: 59.52577 - diff: 22.36mlTrain batch 9/32 - 60.1ms/batch - loss: 58.19387 - diff: 22.51mlTrain batch 10/32 - 58.6ms/batch - loss: 55.68563 - diff: 22.15mlTrain batch 11/32 - 59.3ms/batch - loss: 54.52948 - diff: 21.92mlTrain batch 12/32 - 51.2ms/batch - loss: 53.36641 - diff: 21.75mlTrain batch 13/32 - 60.5ms/batch - loss: 86.24685 - diff: 23.13mlTrain batch 14/32 - 61.6ms/batch - loss: 82.08247 - diff: 22.75mlTrain batch 15/32 - 64.4ms/batch - loss: 79.19129 - diff: 22.59mlTrain batch 16/32 - 57.9ms/batch - loss: 78.87132 - diff: 22.78mlTrain batch 17/32 - 58.6ms/batch - loss: 76.58414 - diff: 22.70mlTrain batch 18/32 - 61.1ms/batch - loss: 73.36276 - diff: 22.21mlTrain batch 19/32 - 58.1ms/batch - loss: 72.22192 - diff: 22.27mlTrain batch 20/32 - 58.5ms/batch - loss: 71.61831 - diff: 22.50mlTrain batch 21/32 - 65.4ms/batch - loss: 73.38153 - diff: 22.71mlTrain batch 22/32 - 65.7ms/batch - loss: 73.01748 - diff: 22.80mlTrain batch 23/32 - 65.6ms/batch - loss: 71.28712 - diff: 22.66mlTrain batch 24/32 - 52.4ms/batch - loss: 72.34295 - diff: 22.84mlTrain batch 25/32 - 58.7ms/batch - loss: 72.31289 - diff: 22.84mlTrain batch 26/32 - 58.5ms/batch - loss: 71.17662 - diff: 22.77mlTrain batch 27/32 - 59.1ms/batch - loss: 70.05282 - diff: 22.71mlTrain batch 28/32 - 59.5ms/batch - loss: 68.99865 - diff: 22.47mlTrain batch 29/32 - 65.3ms/batch - loss: 68.44117 - diff: 22.43mlTrain batch 30/32 - 58.4ms/batch - loss: 73.47451 - diff: 22.90mlTrain batch 31/32 - 59.3ms/batch - loss: 74.23835 - diff: 23.13mlTrain batch 32/32 - 45.0ms/batch - loss: 75.87709 - diff: 23.17mlTrain batch 32/32 - 11.4s 45.0ms/batch - loss: 75.87709 - diff: 23.17ml
Test 0.5s: val_loss: 94.47324 - diff: 28.32ml

Epoch 33: current best loss = 63.74068, at epoch 24
Train batch 1/32 - 72.1ms/batch - loss: 73.69348 - diff: 28.58mlTrain batch 2/32 - 67.4ms/batch - loss: 86.79665 - diff: 27.76mlTrain batch 3/32 - 63.3ms/batch - loss: 69.98899 - diff: 24.30mlTrain batch 4/32 - 58.3ms/batch - loss: 62.48742 - diff: 23.31mlTrain batch 5/32 - 60.0ms/batch - loss: 57.84349 - diff: 22.18mlTrain batch 6/32 - 61.2ms/batch - loss: 56.47721 - diff: 22.32mlTrain batch 7/32 - 59.3ms/batch - loss: 52.89911 - diff: 21.74mlTrain batch 8/32 - 61.6ms/batch - loss: 113.91143 - diff: 24.16mlTrain batch 9/32 - 62.6ms/batch - loss: 114.38247 - diff: 24.56mlTrain batch 10/32 - 61.9ms/batch - loss: 104.43249 - diff: 23.35mlTrain batch 11/32 - 61.8ms/batch - loss: 97.12270 - diff: 22.50mlTrain batch 12/32 - 61.3ms/batch - loss: 93.85186 - diff: 22.91mlTrain batch 13/32 - 62.0ms/batch - loss: 91.14317 - diff: 22.92mlTrain batch 14/32 - 64.2ms/batch - loss: 88.73597 - diff: 22.92mlTrain batch 15/32 - 64.6ms/batch - loss: 86.51411 - diff: 22.85mlTrain batch 16/32 - 61.1ms/batch - loss: 82.83579 - diff: 22.42mlTrain batch 17/32 - 60.1ms/batch - loss: 80.47613 - diff: 22.33mlTrain batch 18/32 - 67.4ms/batch - loss: 77.83046 - diff: 22.09mlTrain batch 19/32 - 62.6ms/batch - loss: 79.39580 - diff: 22.57mlTrain batch 20/32 - 65.5ms/batch - loss: 75.97458 - diff: 22.01mlTrain batch 21/32 - 64.2ms/batch - loss: 75.73911 - diff: 22.16mlTrain batch 22/32 - 59.5ms/batch - loss: 74.10629 - diff: 22.04mlTrain batch 23/32 - 57.8ms/batch - loss: 73.16554 - diff: 22.04mlTrain batch 24/32 - 58.0ms/batch - loss: 73.51303 - diff: 22.21mlTrain batch 25/32 - 58.3ms/batch - loss: 73.44194 - diff: 22.40mlTrain batch 26/32 - 64.6ms/batch - loss: 72.62738 - diff: 22.35mlTrain batch 27/32 - 64.5ms/batch - loss: 70.50151 - diff: 21.98mlTrain batch 28/32 - 64.2ms/batch - loss: 68.79207 - diff: 21.68mlTrain batch 29/32 - 64.0ms/batch - loss: 68.28334 - diff: 21.80mlTrain batch 30/32 - 60.8ms/batch - loss: 67.21794 - diff: 21.75mlTrain batch 31/32 - 59.9ms/batch - loss: 74.39530 - diff: 22.43mlTrain batch 32/32 - 45.5ms/batch - loss: 74.60864 - diff: 22.37mlTrain batch 32/32 - 11.0s 45.5ms/batch - loss: 74.60864 - diff: 22.37ml
Test 0.5s: val_loss: 65.60181 - diff: 22.34ml

Epoch 34: current best loss = 63.74068, at epoch 24
Train batch 1/32 - 80.2ms/batch - loss: 51.90648 - diff: 19.52mlTrain batch 2/32 - 66.5ms/batch - loss: 47.10796 - diff: 19.22mlTrain batch 3/32 - 67.7ms/batch - loss: 66.14502 - diff: 23.76mlTrain batch 4/32 - 67.7ms/batch - loss: 65.78690 - diff: 24.42mlTrain batch 5/32 - 67.2ms/batch - loss: 57.43114 - diff: 22.77mlTrain batch 6/32 - 67.2ms/batch - loss: 51.50995 - diff: 21.56mlTrain batch 7/32 - 67.0ms/batch - loss: 48.54648 - diff: 21.00mlTrain batch 8/32 - 68.6ms/batch - loss: 102.97127 - diff: 24.21mlTrain batch 9/32 - 68.7ms/batch - loss: 95.96946 - diff: 23.77mlTrain batch 10/32 - 68.4ms/batch - loss: 88.41938 - diff: 22.82mlTrain batch 11/32 - 68.0ms/batch - loss: 84.78179 - diff: 22.84mlTrain batch 12/32 - 69.0ms/batch - loss: 80.83519 - diff: 22.23mlTrain batch 13/32 - 69.8ms/batch - loss: 78.24714 - diff: 22.23mlTrain batch 14/32 - 67.8ms/batch - loss: 87.47384 - diff: 23.38mlTrain batch 15/32 - 62.5ms/batch - loss: 85.12210 - diff: 23.20mlTrain batch 16/32 - 61.5ms/batch - loss: 81.96363 - diff: 22.97mlTrain batch 17/32 - 61.8ms/batch - loss: 79.59480 - diff: 22.81mlTrain batch 18/32 - 62.3ms/batch - loss: 77.96932 - diff: 22.63mlTrain batch 19/32 - 61.5ms/batch - loss: 76.57681 - diff: 22.58mlTrain batch 20/32 - 62.2ms/batch - loss: 75.54365 - diff: 22.64mlTrain batch 21/32 - 61.8ms/batch - loss: 73.65118 - diff: 22.42mlTrain batch 22/32 - 62.4ms/batch - loss: 72.35793 - diff: 22.39mlTrain batch 23/32 - 67.7ms/batch - loss: 71.12600 - diff: 22.18mlTrain batch 24/32 - 63.5ms/batch - loss: 69.80948 - diff: 22.00mlTrain batch 25/32 - 63.4ms/batch - loss: 70.65725 - diff: 22.19mlTrain batch 26/32 - 63.9ms/batch - loss: 72.04601 - diff: 22.45mlTrain batch 27/32 - 63.1ms/batch - loss: 72.39469 - diff: 22.46mlTrain batch 28/32 - 64.0ms/batch - loss: 71.64541 - diff: 22.34mlTrain batch 29/32 - 63.4ms/batch - loss: 70.90173 - diff: 22.35mlTrain batch 30/32 - 64.2ms/batch - loss: 69.77218 - diff: 22.22mlTrain batch 31/32 - 56.6ms/batch - loss: 70.33839 - diff: 22.46mlTrain batch 32/32 - 35.5ms/batch - loss: 70.16317 - diff: 22.38mlTrain batch 32/32 - 10.4s 35.5ms/batch - loss: 70.16317 - diff: 22.38ml
Test 0.6s: val_loss: 100.94875 - diff: 27.94ml

Epoch 35: current best loss = 63.74068, at epoch 24
Train batch 1/32 - 79.8ms/batch - loss: 32.95825 - diff: 16.55mlTrain batch 2/32 - 66.5ms/batch - loss: 32.74219 - diff: 18.07mlTrain batch 3/32 - 54.2ms/batch - loss: 42.90216 - diff: 20.52mlTrain batch 4/32 - 48.1ms/batch - loss: 40.85389 - diff: 20.08mlTrain batch 5/32 - 61.0ms/batch - loss: 40.62651 - diff: 19.77mlTrain batch 6/32 - 60.6ms/batch - loss: 41.47741 - diff: 20.66mlTrain batch 7/32 - 60.7ms/batch - loss: 39.96059 - diff: 19.97mlTrain batch 8/32 - 61.3ms/batch - loss: 44.10459 - diff: 20.78mlTrain batch 9/32 - 61.0ms/batch - loss: 79.13223 - diff: 22.08mlTrain batch 10/32 - 60.6ms/batch - loss: 74.24372 - diff: 21.66mlTrain batch 11/32 - 60.4ms/batch - loss: 70.29012 - diff: 21.30mlTrain batch 12/32 - 68.9ms/batch - loss: 71.06434 - diff: 21.82mlTrain batch 13/32 - 61.7ms/batch - loss: 81.53006 - diff: 22.63mlTrain batch 14/32 - 61.1ms/batch - loss: 81.50365 - diff: 23.25mlTrain batch 15/32 - 64.4ms/batch - loss: 80.00557 - diff: 23.38mlTrain batch 16/32 - 69.8ms/batch - loss: 77.34663 - diff: 23.18mlTrain batch 17/32 - 62.0ms/batch - loss: 78.60849 - diff: 23.49mlTrain batch 18/32 - 61.6ms/batch - loss: 76.67526 - diff: 23.32mlTrain batch 19/32 - 67.4ms/batch - loss: 76.61101 - diff: 23.57mlTrain batch 20/32 - 66.9ms/batch - loss: 74.99345 - diff: 23.57mlTrain batch 21/32 - 68.6ms/batch - loss: 74.21936 - diff: 23.58mlTrain batch 22/32 - 66.6ms/batch - loss: 72.46005 - diff: 23.25mlTrain batch 23/32 - 68.8ms/batch - loss: 70.76791 - diff: 23.13mlTrain batch 24/32 - 61.4ms/batch - loss: 71.97738 - diff: 23.42mlTrain batch 25/32 - 63.8ms/batch - loss: 70.54996 - diff: 23.27mlTrain batch 26/32 - 67.4ms/batch - loss: 70.17200 - diff: 23.32mlTrain batch 27/32 - 61.6ms/batch - loss: 69.96570 - diff: 23.39mlTrain batch 28/32 - 59.3ms/batch - loss: 71.95740 - diff: 23.77mlTrain batch 29/32 - 52.3ms/batch - loss: 70.17124 - diff: 23.39mlTrain batch 30/32 - 50.4ms/batch - loss: 68.98059 - diff: 23.23mlTrain batch 31/32 - 63.6ms/batch - loss: 70.36161 - diff: 23.59mlTrain batch 32/32 - 46.3ms/batch - loss: 70.86124 - diff: 23.55mlTrain batch 32/32 - 11.5s 46.3ms/batch - loss: 70.86124 - diff: 23.55ml
Test 0.6s: val_loss: 74.60764 - diff: 23.76ml
Epoch    36: reducing learning rate of group 0 to 5.0000e-04.

Epoch 36: current best loss = 63.74068, at epoch 24
Train batch 1/32 - 71.1ms/batch - loss: 62.70767 - diff: 20.64mlTrain batch 2/32 - 61.3ms/batch - loss: 90.10038 - diff: 24.60mlTrain batch 3/32 - 60.9ms/batch - loss: 75.77400 - diff: 22.98mlTrain batch 4/32 - 62.5ms/batch - loss: 74.54718 - diff: 23.73mlTrain batch 5/32 - 62.1ms/batch - loss: 86.25538 - diff: 26.30mlTrain batch 6/32 - 68.3ms/batch - loss: 78.93873 - diff: 25.74mlTrain batch 7/32 - 61.9ms/batch - loss: 79.93318 - diff: 25.99mlTrain batch 8/32 - 62.0ms/batch - loss: 74.93749 - diff: 25.19mlTrain batch 9/32 - 62.1ms/batch - loss: 76.66197 - diff: 25.05mlTrain batch 10/32 - 60.5ms/batch - loss: 74.13234 - diff: 24.66mlTrain batch 11/32 - 59.8ms/batch - loss: 69.68640 - diff: 23.81mlTrain batch 12/32 - 57.0ms/batch - loss: 69.05855 - diff: 24.06mlTrain batch 13/32 - 57.2ms/batch - loss: 67.97290 - diff: 24.03mlTrain batch 14/32 - 65.8ms/batch - loss: 66.11195 - diff: 23.60mlTrain batch 15/32 - 67.2ms/batch - loss: 65.44379 - diff: 23.50mlTrain batch 16/32 - 60.7ms/batch - loss: 64.23174 - diff: 23.35mlTrain batch 17/32 - 62.4ms/batch - loss: 63.31957 - diff: 23.12mlTrain batch 18/32 - 65.2ms/batch - loss: 62.14604 - diff: 22.84mlTrain batch 19/32 - 66.3ms/batch - loss: 60.38624 - diff: 22.54mlTrain batch 20/32 - 64.8ms/batch - loss: 60.50196 - diff: 22.79mlTrain batch 21/32 - 64.5ms/batch - loss: 69.20107 - diff: 23.22mlTrain batch 22/32 - 60.5ms/batch - loss: 68.50320 - diff: 23.33mlTrain batch 23/32 - 60.7ms/batch - loss: 68.23207 - diff: 23.46mlTrain batch 24/32 - 65.7ms/batch - loss: 67.93694 - diff: 23.43mlTrain batch 25/32 - 64.4ms/batch - loss: 67.58367 - diff: 23.53mlTrain batch 26/32 - 60.1ms/batch - loss: 67.42268 - diff: 23.52mlTrain batch 27/32 - 60.6ms/batch - loss: 74.93767 - diff: 23.81mlTrain batch 28/32 - 60.4ms/batch - loss: 76.22013 - diff: 24.03mlTrain batch 29/32 - 60.8ms/batch - loss: 75.57467 - diff: 23.98mlTrain batch 30/32 - 60.9ms/batch - loss: 74.09435 - diff: 23.82mlTrain batch 31/32 - 59.8ms/batch - loss: 73.15267 - diff: 23.78mlTrain batch 32/32 - 50.3ms/batch - loss: 74.38182 - diff: 23.83mlTrain batch 32/32 - 11.5s 50.3ms/batch - loss: 74.38182 - diff: 23.83ml
Test 0.6s: val_loss: 76.85917 - diff: 24.22ml

Epoch 37: current best loss = 63.74068, at epoch 24
Train batch 1/32 - 72.3ms/batch - loss: 318.06943 - diff: 35.33mlTrain batch 2/32 - 60.7ms/batch - loss: 224.01283 - diff: 35.24mlTrain batch 3/32 - 60.9ms/batch - loss: 173.88567 - diff: 32.02mlTrain batch 4/32 - 60.0ms/batch - loss: 166.65671 - diff: 32.03mlTrain batch 5/32 - 60.5ms/batch - loss: 154.84669 - diff: 31.12mlTrain batch 6/32 - 60.6ms/batch - loss: 137.48344 - diff: 29.62mlTrain batch 7/32 - 61.0ms/batch - loss: 125.25053 - diff: 28.85mlTrain batch 8/32 - 60.0ms/batch - loss: 113.74147 - diff: 27.59mlTrain batch 9/32 - 60.8ms/batch - loss: 106.71394 - diff: 26.70mlTrain batch 10/32 - 60.6ms/batch - loss: 100.32514 - diff: 26.24mlTrain batch 11/32 - 61.2ms/batch - loss: 94.82285 - diff: 25.80mlTrain batch 12/32 - 61.0ms/batch - loss: 105.14124 - diff: 27.01mlTrain batch 13/32 - 61.9ms/batch - loss: 107.17095 - diff: 27.17mlTrain batch 14/32 - 60.8ms/batch - loss: 103.41445 - diff: 26.58mlTrain batch 15/32 - 61.6ms/batch - loss: 99.04204 - diff: 26.05mlTrain batch 16/32 - 63.6ms/batch - loss: 94.11377 - diff: 25.36mlTrain batch 17/32 - 61.7ms/batch - loss: 89.50452 - diff: 24.65mlTrain batch 18/32 - 53.0ms/batch - loss: 86.54736 - diff: 24.30mlTrain batch 19/32 - 60.3ms/batch - loss: 83.96955 - diff: 24.22mlTrain batch 20/32 - 60.7ms/batch - loss: 85.32130 - diff: 24.52mlTrain batch 21/32 - 60.8ms/batch - loss: 83.26343 - diff: 24.45mlTrain batch 22/32 - 61.2ms/batch - loss: 81.74802 - diff: 24.39mlTrain batch 23/32 - 53.6ms/batch - loss: 80.13962 - diff: 24.32mlTrain batch 24/32 - 56.5ms/batch - loss: 79.70456 - diff: 24.43mlTrain batch 25/32 - 64.7ms/batch - loss: 78.52824 - diff: 24.38mlTrain batch 26/32 - 61.9ms/batch - loss: 77.60744 - diff: 24.33mlTrain batch 27/32 - 62.4ms/batch - loss: 78.26894 - diff: 24.41mlTrain batch 28/32 - 61.7ms/batch - loss: 77.00347 - diff: 24.30mlTrain batch 29/32 - 60.9ms/batch - loss: 76.40386 - diff: 24.24mlTrain batch 30/32 - 60.7ms/batch - loss: 76.66915 - diff: 24.37mlTrain batch 31/32 - 51.6ms/batch - loss: 75.67973 - diff: 24.31mlTrain batch 32/32 - 50.8ms/batch - loss: 75.80377 - diff: 24.26mlTrain batch 32/32 - 10.6s 50.8ms/batch - loss: 75.80377 - diff: 24.26ml
Test 0.6s: val_loss: 96.66526 - diff: 28.98ml

Epoch 38: current best loss = 63.74068, at epoch 24
Train batch 1/32 - 72.5ms/batch - loss: 70.99091 - diff: 25.19mlTrain batch 2/32 - 61.0ms/batch - loss: 59.05515 - diff: 24.05mlTrain batch 3/32 - 61.3ms/batch - loss: 61.93320 - diff: 23.85mlTrain batch 4/32 - 61.6ms/batch - loss: 58.15122 - diff: 23.59mlTrain batch 5/32 - 62.6ms/batch - loss: 57.28360 - diff: 23.72mlTrain batch 6/32 - 61.5ms/batch - loss: 60.30481 - diff: 24.12mlTrain batch 7/32 - 56.6ms/batch - loss: 58.85746 - diff: 23.74mlTrain batch 8/32 - 65.8ms/batch - loss: 69.37592 - diff: 23.84mlTrain batch 9/32 - 67.8ms/batch - loss: 65.93189 - diff: 23.56mlTrain batch 10/32 - 70.5ms/batch - loss: 61.83039 - diff: 22.80mlTrain batch 11/32 - 56.1ms/batch - loss: 59.79001 - diff: 22.60mlTrain batch 12/32 - 57.8ms/batch - loss: 58.12057 - diff: 22.51mlTrain batch 13/32 - 74.6ms/batch - loss: 61.12780 - diff: 23.13mlTrain batch 14/32 - 64.7ms/batch - loss: 60.71361 - diff: 23.09mlTrain batch 15/32 - 64.8ms/batch - loss: 62.20393 - diff: 23.27mlTrain batch 16/32 - 63.7ms/batch - loss: 59.99580 - diff: 22.91mlTrain batch 17/32 - 59.1ms/batch - loss: 58.64588 - diff: 22.67mlTrain batch 18/32 - 57.3ms/batch - loss: 58.98120 - diff: 22.99mlTrain batch 19/32 - 58.4ms/batch - loss: 57.95455 - diff: 22.69mlTrain batch 20/32 - 58.9ms/batch - loss: 77.17978 - diff: 23.66mlTrain batch 21/32 - 58.5ms/batch - loss: 75.82737 - diff: 23.61mlTrain batch 22/32 - 57.9ms/batch - loss: 73.72636 - diff: 23.30mlTrain batch 23/32 - 60.2ms/batch - loss: 72.64112 - diff: 23.29mlTrain batch 24/32 - 60.4ms/batch - loss: 75.21061 - diff: 23.67mlTrain batch 25/32 - 60.2ms/batch - loss: 74.27315 - diff: 23.69mlTrain batch 26/32 - 60.0ms/batch - loss: 72.67128 - diff: 23.52mlTrain batch 27/32 - 60.4ms/batch - loss: 72.10574 - diff: 23.52mlTrain batch 28/32 - 60.9ms/batch - loss: 72.04985 - diff: 23.51mlTrain batch 29/32 - 65.8ms/batch - loss: 70.79174 - diff: 23.39mlTrain batch 30/32 - 60.4ms/batch - loss: 69.33827 - diff: 23.16mlTrain batch 31/32 - 62.2ms/batch - loss: 68.27258 - diff: 23.04mlTrain batch 32/32 - 52.6ms/batch - loss: 71.33956 - diff: 23.14mlTrain batch 32/32 - 12.0s 52.6ms/batch - loss: 71.33956 - diff: 23.14ml
Test 0.6s: val_loss: 64.82728 - diff: 21.66ml

Epoch 39: current best loss = 63.74068, at epoch 24
Train batch 1/32 - 84.1ms/batch - loss: 32.44313 - diff: 17.38mlTrain batch 2/32 - 54.8ms/batch - loss: 28.87194 - diff: 15.81mlTrain batch 3/32 - 65.7ms/batch - loss: 29.53477 - diff: 16.39mlTrain batch 4/32 - 63.2ms/batch - loss: 30.62823 - diff: 17.18mlTrain batch 5/32 - 62.8ms/batch - loss: 37.66866 - diff: 18.73mlTrain batch 6/32 - 63.3ms/batch - loss: 52.08387 - diff: 21.17mlTrain batch 7/32 - 63.2ms/batch - loss: 62.80176 - diff: 22.65mlTrain batch 8/32 - 64.1ms/batch - loss: 60.69352 - diff: 22.42mlTrain batch 9/32 - 61.9ms/batch - loss: 59.39752 - diff: 22.50mlTrain batch 10/32 - 65.9ms/batch - loss: 59.67991 - diff: 22.54mlTrain batch 11/32 - 67.7ms/batch - loss: 59.90423 - diff: 22.72mlTrain batch 12/32 - 72.7ms/batch - loss: 59.28047 - diff: 22.36mlTrain batch 13/32 - 62.6ms/batch - loss: 63.26371 - diff: 22.49mlTrain batch 14/32 - 61.8ms/batch - loss: 62.77726 - diff: 22.68mlTrain batch 15/32 - 65.2ms/batch - loss: 62.98219 - diff: 22.81mlTrain batch 16/32 - 63.9ms/batch - loss: 62.89649 - diff: 22.75mlTrain batch 17/32 - 64.7ms/batch - loss: 65.50711 - diff: 23.20mlTrain batch 18/32 - 58.5ms/batch - loss: 64.04993 - diff: 23.01mlTrain batch 19/32 - 59.5ms/batch - loss: 62.84072 - diff: 22.93mlTrain batch 20/32 - 58.5ms/batch - loss: 62.57513 - diff: 22.85mlTrain batch 21/32 - 62.1ms/batch - loss: 84.30342 - diff: 24.03mlTrain batch 22/32 - 62.1ms/batch - loss: 83.65555 - diff: 24.12mlTrain batch 23/32 - 58.9ms/batch - loss: 84.48627 - diff: 24.50mlTrain batch 24/32 - 58.5ms/batch - loss: 82.37829 - diff: 24.10mlTrain batch 25/32 - 58.8ms/batch - loss: 80.66508 - diff: 23.97mlTrain batch 26/32 - 59.3ms/batch - loss: 79.46711 - diff: 23.87mlTrain batch 27/32 - 59.7ms/batch - loss: 79.08776 - diff: 23.91mlTrain batch 28/32 - 59.4ms/batch - loss: 85.10901 - diff: 24.27mlTrain batch 29/32 - 59.6ms/batch - loss: 83.06935 - diff: 23.90mlTrain batch 30/32 - 59.0ms/batch - loss: 82.11664 - diff: 23.80mlTrain batch 31/32 - 61.4ms/batch - loss: 80.84681 - diff: 23.73mlTrain batch 32/32 - 53.4ms/batch - loss: 80.94053 - diff: 23.67mlTrain batch 32/32 - 10.6s 53.4ms/batch - loss: 80.94053 - diff: 23.67ml
Test 0.5s: val_loss: 72.60577 - diff: 23.75ml

Epoch 40: current best loss = 63.74068, at epoch 24
Train batch 1/32 - 75.7ms/batch - loss: 44.86207 - diff: 21.03mlTrain batch 2/32 - 64.6ms/batch - loss: 41.04504 - diff: 20.67mlTrain batch 3/32 - 65.5ms/batch - loss: 172.75745 - diff: 29.15mlTrain batch 4/32 - 66.5ms/batch - loss: 148.53407 - diff: 27.24mlTrain batch 5/32 - 63.5ms/batch - loss: 131.28311 - diff: 26.88mlTrain batch 6/32 - 67.4ms/batch - loss: 121.16029 - diff: 26.94mlTrain batch 7/32 - 72.7ms/batch - loss: 110.55984 - diff: 25.88mlTrain batch 8/32 - 64.5ms/batch - loss: 103.64234 - diff: 25.23mlTrain batch 9/32 - 65.0ms/batch - loss: 99.11695 - diff: 25.06mlTrain batch 10/32 - 64.1ms/batch - loss: 97.79296 - diff: 25.56mlTrain batch 11/32 - 65.6ms/batch - loss: 92.28960 - diff: 25.15mlTrain batch 12/32 - 63.0ms/batch - loss: 87.56708 - diff: 24.69mlTrain batch 13/32 - 70.1ms/batch - loss: 82.51472 - diff: 23.92mlTrain batch 14/32 - 62.0ms/batch - loss: 78.03975 - diff: 23.23mlTrain batch 15/32 - 66.4ms/batch - loss: 76.91563 - diff: 23.42mlTrain batch 16/32 - 66.0ms/batch - loss: 73.80415 - diff: 23.00mlTrain batch 17/32 - 65.2ms/batch - loss: 73.78294 - diff: 23.18mlTrain batch 18/32 - 63.9ms/batch - loss: 73.34474 - diff: 23.17mlTrain batch 19/32 - 64.4ms/batch - loss: 71.02173 - diff: 22.90mlTrain batch 20/32 - 65.4ms/batch - loss: 69.23986 - diff: 22.79mlTrain batch 21/32 - 64.3ms/batch - loss: 70.18037 - diff: 23.13mlTrain batch 22/32 - 64.6ms/batch - loss: 71.21639 - diff: 23.46mlTrain batch 23/32 - 64.3ms/batch - loss: 73.94362 - diff: 23.88mlTrain batch 24/32 - 64.4ms/batch - loss: 73.05839 - diff: 23.95mlTrain batch 25/32 - 65.8ms/batch - loss: 71.38606 - diff: 23.62mlTrain batch 26/32 - 65.6ms/batch - loss: 70.91003 - diff: 23.72mlTrain batch 27/32 - 66.2ms/batch - loss: 75.69010 - diff: 23.96mlTrain batch 28/32 - 64.9ms/batch - loss: 74.03489 - diff: 23.67mlTrain batch 29/32 - 65.8ms/batch - loss: 75.93954 - diff: 24.01mlTrain batch 30/32 - 64.8ms/batch - loss: 78.56362 - diff: 24.48mlTrain batch 31/32 - 67.2ms/batch - loss: 77.21826 - diff: 24.25mlTrain batch 32/32 - 35.1ms/batch - loss: 77.83962 - diff: 24.23mlTrain batch 32/32 - 10.5s 35.1ms/batch - loss: 77.83962 - diff: 24.23ml
Test 0.6s: val_loss: 59.74411 - diff: 21.12ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Systole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 41: current best loss = 59.74411, at epoch 40
Train batch 1/32 - 73.5ms/batch - loss: 69.03840 - diff: 27.14mlTrain batch 2/32 - 61.1ms/batch - loss: 60.60864 - diff: 25.23mlTrain batch 3/32 - 61.4ms/batch - loss: 54.96031 - diff: 23.88mlTrain batch 4/32 - 61.6ms/batch - loss: 48.16666 - diff: 22.15mlTrain batch 5/32 - 62.5ms/batch - loss: 43.79774 - diff: 21.34mlTrain batch 6/32 - 62.0ms/batch - loss: 47.55266 - diff: 21.26mlTrain batch 7/32 - 62.5ms/batch - loss: 44.98317 - diff: 20.71mlTrain batch 8/32 - 59.3ms/batch - loss: 47.61182 - diff: 21.24mlTrain batch 9/32 - 58.5ms/batch - loss: 47.55693 - diff: 21.08mlTrain batch 10/32 - 60.0ms/batch - loss: 45.12495 - diff: 20.62mlTrain batch 11/32 - 58.5ms/batch - loss: 46.27475 - diff: 20.69mlTrain batch 12/32 - 64.5ms/batch - loss: 46.90572 - diff: 21.21mlTrain batch 13/32 - 64.2ms/batch - loss: 47.23531 - diff: 21.46mlTrain batch 14/32 - 58.1ms/batch - loss: 47.65883 - diff: 21.78mlTrain batch 15/32 - 58.0ms/batch - loss: 73.73969 - diff: 22.97mlTrain batch 16/32 - 58.3ms/batch - loss: 70.77303 - diff: 22.56mlTrain batch 17/32 - 57.9ms/batch - loss: 70.45936 - diff: 22.71mlTrain batch 18/32 - 58.6ms/batch - loss: 69.30449 - diff: 22.80mlTrain batch 19/32 - 57.9ms/batch - loss: 67.14943 - diff: 22.44mlTrain batch 20/32 - 58.4ms/batch - loss: 64.72980 - diff: 22.05mlTrain batch 21/32 - 57.7ms/batch - loss: 64.77081 - diff: 22.05mlTrain batch 22/32 - 59.4ms/batch - loss: 62.98356 - diff: 21.69mlTrain batch 23/32 - 58.6ms/batch - loss: 61.34202 - diff: 21.34mlTrain batch 24/32 - 58.3ms/batch - loss: 67.62807 - diff: 21.93mlTrain batch 25/32 - 61.6ms/batch - loss: 67.21823 - diff: 22.05mlTrain batch 26/32 - 60.6ms/batch - loss: 66.87435 - diff: 22.22mlTrain batch 27/32 - 64.8ms/batch - loss: 65.91479 - diff: 22.16mlTrain batch 28/32 - 60.9ms/batch - loss: 65.41588 - diff: 22.18mlTrain batch 29/32 - 62.8ms/batch - loss: 64.39625 - diff: 22.07mlTrain batch 30/32 - 53.1ms/batch - loss: 64.30824 - diff: 22.12mlTrain batch 31/32 - 52.7ms/batch - loss: 64.96218 - diff: 22.34mlTrain batch 32/32 - 38.0ms/batch - loss: 66.03937 - diff: 22.33mlTrain batch 32/32 - 10.8s 38.0ms/batch - loss: 66.03937 - diff: 22.33ml
Test 0.6s: val_loss: 62.67137 - diff: 22.39ml

Epoch 42: current best loss = 59.74411, at epoch 40
Train batch 1/32 - 77.9ms/batch - loss: 21.21102 - diff: 15.02mlTrain batch 2/32 - 65.9ms/batch - loss: 50.74226 - diff: 20.20mlTrain batch 3/32 - 59.1ms/batch - loss: 46.84801 - diff: 20.63mlTrain batch 4/32 - 62.2ms/batch - loss: 41.40393 - diff: 19.72mlTrain batch 5/32 - 58.4ms/batch - loss: 45.54834 - diff: 20.71mlTrain batch 6/32 - 60.1ms/batch - loss: 46.97531 - diff: 20.99mlTrain batch 7/32 - 58.7ms/batch - loss: 44.09784 - diff: 20.41mlTrain batch 8/32 - 59.1ms/batch - loss: 94.79891 - diff: 22.27mlTrain batch 9/32 - 59.0ms/batch - loss: 86.99932 - diff: 21.62mlTrain batch 10/32 - 58.2ms/batch - loss: 80.98931 - diff: 21.11mlTrain batch 11/32 - 63.4ms/batch - loss: 80.17337 - diff: 21.17mlTrain batch 12/32 - 62.9ms/batch - loss: 85.84415 - diff: 22.19mlTrain batch 13/32 - 66.1ms/batch - loss: 84.05715 - diff: 22.24mlTrain batch 14/32 - 66.6ms/batch - loss: 81.64608 - diff: 22.03mlTrain batch 15/32 - 66.1ms/batch - loss: 79.57083 - diff: 22.10mlTrain batch 16/32 - 65.6ms/batch - loss: 75.86487 - diff: 21.72mlTrain batch 17/32 - 66.3ms/batch - loss: 73.71367 - diff: 21.68mlTrain batch 18/32 - 65.9ms/batch - loss: 81.43516 - diff: 22.46mlTrain batch 19/32 - 65.9ms/batch - loss: 82.05202 - diff: 22.81mlTrain batch 20/32 - 66.4ms/batch - loss: 87.65032 - diff: 23.96mlTrain batch 21/32 - 62.3ms/batch - loss: 85.77144 - diff: 23.85mlTrain batch 22/32 - 61.0ms/batch - loss: 83.04450 - diff: 23.53mlTrain batch 23/32 - 58.9ms/batch - loss: 81.39931 - diff: 23.37mlTrain batch 24/32 - 57.8ms/batch - loss: 80.19820 - diff: 23.36mlTrain batch 25/32 - 59.0ms/batch - loss: 78.54007 - diff: 23.14mlTrain batch 26/32 - 58.4ms/batch - loss: 77.40288 - diff: 23.21mlTrain batch 27/32 - 58.6ms/batch - loss: 77.26820 - diff: 23.33mlTrain batch 28/32 - 59.6ms/batch - loss: 76.62052 - diff: 23.47mlTrain batch 29/32 - 59.3ms/batch - loss: 75.87147 - diff: 23.41mlTrain batch 30/32 - 74.4ms/batch - loss: 74.60630 - diff: 23.30mlTrain batch 31/32 - 65.2ms/batch - loss: 74.43631 - diff: 23.33mlTrain batch 32/32 - 56.5ms/batch - loss: 74.52595 - diff: 23.27mlTrain batch 32/32 - 11.3s 56.5ms/batch - loss: 74.52595 - diff: 23.27ml
Test 0.6s: val_loss: 67.37904 - diff: 21.49ml

Epoch 43: current best loss = 59.74411, at epoch 40
Train batch 1/32 - 69.3ms/batch - loss: 32.74989 - diff: 16.28mlTrain batch 2/32 - 58.6ms/batch - loss: 30.38206 - diff: 16.77mlTrain batch 3/32 - 58.8ms/batch - loss: 42.47676 - diff: 18.33mlTrain batch 4/32 - 59.1ms/batch - loss: 43.35574 - diff: 19.08mlTrain batch 5/32 - 58.6ms/batch - loss: 42.91461 - diff: 19.49mlTrain batch 6/32 - 59.6ms/batch - loss: 45.26407 - diff: 20.44mlTrain batch 7/32 - 59.1ms/batch - loss: 43.61739 - diff: 20.21mlTrain batch 8/32 - 58.9ms/batch - loss: 47.80652 - diff: 21.32mlTrain batch 9/32 - 59.9ms/batch - loss: 46.28255 - diff: 21.08mlTrain batch 10/32 - 58.7ms/batch - loss: 49.88857 - diff: 21.65mlTrain batch 11/32 - 59.3ms/batch - loss: 47.67360 - diff: 21.04mlTrain batch 12/32 - 58.0ms/batch - loss: 46.68653 - diff: 20.80mlTrain batch 13/32 - 56.2ms/batch - loss: 44.46468 - diff: 20.24mlTrain batch 14/32 - 63.7ms/batch - loss: 70.33052 - diff: 22.00mlTrain batch 15/32 - 68.7ms/batch - loss: 73.82280 - diff: 22.60mlTrain batch 16/32 - 61.1ms/batch - loss: 74.88795 - diff: 22.85mlTrain batch 17/32 - 70.8ms/batch - loss: 72.05039 - diff: 22.44mlTrain batch 18/32 - 58.1ms/batch - loss: 69.77251 - diff: 22.24mlTrain batch 19/32 - 70.2ms/batch - loss: 68.06941 - diff: 22.01mlTrain batch 20/32 - 63.8ms/batch - loss: 66.95110 - diff: 21.95mlTrain batch 21/32 - 65.0ms/batch - loss: 67.99145 - diff: 21.85mlTrain batch 22/32 - 64.5ms/batch - loss: 66.91904 - diff: 21.87mlTrain batch 23/32 - 63.6ms/batch - loss: 65.89203 - diff: 21.81mlTrain batch 24/32 - 63.3ms/batch - loss: 63.86516 - diff: 21.49mlTrain batch 25/32 - 64.3ms/batch - loss: 64.66848 - diff: 21.70mlTrain batch 26/32 - 63.8ms/batch - loss: 66.69188 - diff: 22.08mlTrain batch 27/32 - 57.8ms/batch - loss: 65.71736 - diff: 22.00mlTrain batch 28/32 - 57.7ms/batch - loss: 65.03099 - diff: 21.88mlTrain batch 29/32 - 57.7ms/batch - loss: 65.14747 - diff: 22.06mlTrain batch 30/32 - 57.4ms/batch - loss: 64.83768 - diff: 22.12mlTrain batch 31/32 - 54.9ms/batch - loss: 64.84950 - diff: 22.23mlTrain batch 32/32 - 35.8ms/batch - loss: 66.57246 - diff: 22.28mlTrain batch 32/32 - 10.5s 35.8ms/batch - loss: 66.57246 - diff: 22.28ml
Test 0.6s: val_loss: 64.72647 - diff: 21.70ml

Epoch 44: current best loss = 59.74411, at epoch 40
Train batch 1/32 - 76.7ms/batch - loss: 38.17949 - diff: 16.95mlTrain batch 2/32 - 65.1ms/batch - loss: 38.94063 - diff: 17.51mlTrain batch 3/32 - 63.8ms/batch - loss: 57.00343 - diff: 22.20mlTrain batch 4/32 - 63.2ms/batch - loss: 64.02612 - diff: 22.95mlTrain batch 5/32 - 64.4ms/batch - loss: 59.12051 - diff: 22.43mlTrain batch 6/32 - 65.3ms/batch - loss: 56.38077 - diff: 22.16mlTrain batch 7/32 - 65.0ms/batch - loss: 56.32480 - diff: 22.63mlTrain batch 8/32 - 65.9ms/batch - loss: 56.91824 - diff: 22.88mlTrain batch 9/32 - 64.9ms/batch - loss: 95.49838 - diff: 24.82mlTrain batch 10/32 - 68.4ms/batch - loss: 89.05887 - diff: 24.01mlTrain batch 11/32 - 63.0ms/batch - loss: 91.59530 - diff: 24.96mlTrain batch 12/32 - 61.8ms/batch - loss: 88.95743 - diff: 24.85mlTrain batch 13/32 - 62.7ms/batch - loss: 85.54305 - diff: 24.51mlTrain batch 14/32 - 62.1ms/batch - loss: 82.64661 - diff: 24.26mlTrain batch 15/32 - 60.9ms/batch - loss: 78.96269 - diff: 23.73mlTrain batch 16/32 - 61.3ms/batch - loss: 76.66621 - diff: 23.62mlTrain batch 17/32 - 61.5ms/batch - loss: 75.45358 - diff: 23.59mlTrain batch 18/32 - 61.5ms/batch - loss: 73.20413 - diff: 23.28mlTrain batch 19/32 - 60.3ms/batch - loss: 81.79307 - diff: 24.00mlTrain batch 20/32 - 62.9ms/batch - loss: 78.96315 - diff: 23.58mlTrain batch 21/32 - 62.1ms/batch - loss: 78.84412 - diff: 23.80mlTrain batch 22/32 - 62.5ms/batch - loss: 77.88435 - diff: 23.86mlTrain batch 23/32 - 61.0ms/batch - loss: 77.88763 - diff: 23.78mlTrain batch 24/32 - 61.5ms/batch - loss: 76.19827 - diff: 23.33mlTrain batch 25/32 - 58.7ms/batch - loss: 75.22276 - diff: 23.30mlTrain batch 26/32 - 59.2ms/batch - loss: 72.76516 - diff: 22.80mlTrain batch 27/32 - 50.0ms/batch - loss: 70.73558 - diff: 22.47mlTrain batch 28/32 - 63.1ms/batch - loss: 70.38477 - diff: 22.56mlTrain batch 29/32 - 63.6ms/batch - loss: 69.19273 - diff: 22.38mlTrain batch 30/32 - 60.0ms/batch - loss: 70.94900 - diff: 22.59mlTrain batch 31/32 - 68.7ms/batch - loss: 70.19410 - diff: 22.49mlTrain batch 32/32 - 50.8ms/batch - loss: 72.57519 - diff: 22.51mlTrain batch 32/32 - 10.7s 50.8ms/batch - loss: 72.57519 - diff: 22.51ml
Test 0.6s: val_loss: 74.91869 - diff: 21.03ml

Epoch 45: current best loss = 59.74411, at epoch 40
Going to unfreeze the pretrained weights
Train batch 1/32 - 148.3ms/batch - loss: 24.93501 - diff: 15.11mlTrain batch 2/32 - 127.8ms/batch - loss: 57.22878 - diff: 19.46mlTrain batch 3/32 - 122.0ms/batch - loss: 54.68325 - diff: 20.65mlTrain batch 4/32 - 113.0ms/batch - loss: 72.77588 - diff: 23.59mlTrain batch 5/32 - 121.9ms/batch - loss: 67.58213 - diff: 23.69mlTrain batch 6/32 - 112.8ms/batch - loss: 71.99770 - diff: 25.03mlTrain batch 7/32 - 113.6ms/batch - loss: 78.79599 - diff: 26.89mlTrain batch 8/32 - 113.1ms/batch - loss: 85.66384 - diff: 27.73mlTrain batch 9/32 - 112.3ms/batch - loss: 84.88702 - diff: 27.66mlTrain batch 10/32 - 112.6ms/batch - loss: 83.56337 - diff: 27.47mlTrain batch 11/32 - 112.1ms/batch - loss: 86.08110 - diff: 27.90mlTrain batch 12/32 - 112.3ms/batch - loss: 89.29412 - diff: 28.36mlTrain batch 13/32 - 112.8ms/batch - loss: 93.67454 - diff: 28.73mlTrain batch 14/32 - 112.8ms/batch - loss: 89.21787 - diff: 27.97mlTrain batch 15/32 - 117.3ms/batch - loss: 90.56905 - diff: 27.91mlTrain batch 16/32 - 111.0ms/batch - loss: 86.99533 - diff: 27.40mlTrain batch 17/32 - 113.3ms/batch - loss: 85.77045 - diff: 27.33mlTrain batch 18/32 - 113.7ms/batch - loss: 88.26171 - diff: 27.99mlTrain batch 19/32 - 102.5ms/batch - loss: 100.10452 - diff: 28.30mlTrain batch 20/32 - 102.7ms/batch - loss: 100.75538 - diff: 28.17mlTrain batch 21/32 - 106.2ms/batch - loss: 100.99510 - diff: 28.59mlTrain batch 22/32 - 103.2ms/batch - loss: 102.38775 - diff: 28.78mlTrain batch 23/32 - 102.2ms/batch - loss: 119.00859 - diff: 29.58mlTrain batch 24/32 - 97.0ms/batch - loss: 119.00330 - diff: 29.70mlTrain batch 25/32 - 105.2ms/batch - loss: 116.35128 - diff: 29.45mlTrain batch 26/32 - 106.7ms/batch - loss: 115.02298 - diff: 29.49mlTrain batch 27/32 - 104.6ms/batch - loss: 114.82753 - diff: 29.61mlTrain batch 28/32 - 107.0ms/batch - loss: 113.24276 - diff: 29.52mlTrain batch 29/32 - 101.4ms/batch - loss: 114.14816 - diff: 29.74mlTrain batch 30/32 - 91.5ms/batch - loss: 113.63129 - diff: 29.87mlTrain batch 31/32 - 108.3ms/batch - loss: 112.86921 - diff: 30.02mlTrain batch 32/32 - 86.0ms/batch - loss: 114.95051 - diff: 30.07mlTrain batch 32/32 - 10.7s 86.0ms/batch - loss: 114.95051 - diff: 30.07ml
Test 0.6s: val_loss: 148.79349 - diff: 38.42ml

Epoch 46: current best loss = 59.74411, at epoch 40
Train batch 1/32 - 127.5ms/batch - loss: 69.14624 - diff: 25.75mlTrain batch 2/32 - 106.7ms/batch - loss: 65.80859 - diff: 25.18mlTrain batch 3/32 - 107.0ms/batch - loss: 60.48193 - diff: 23.47mlTrain batch 4/32 - 105.3ms/batch - loss: 205.53293 - diff: 29.76mlTrain batch 5/32 - 106.2ms/batch - loss: 189.90533 - diff: 29.90mlTrain batch 6/32 - 107.1ms/batch - loss: 175.17448 - diff: 30.56mlTrain batch 7/32 - 102.7ms/batch - loss: 161.20691 - diff: 29.53mlTrain batch 8/32 - 105.3ms/batch - loss: 147.57862 - diff: 29.08mlTrain batch 9/32 - 105.5ms/batch - loss: 136.37036 - diff: 28.22mlTrain batch 10/32 - 105.1ms/batch - loss: 128.06939 - diff: 27.73mlTrain batch 11/32 - 114.9ms/batch - loss: 125.10573 - diff: 27.83mlTrain batch 12/32 - 114.2ms/batch - loss: 119.65489 - diff: 27.43mlTrain batch 13/32 - 97.8ms/batch - loss: 114.62319 - diff: 27.16mlTrain batch 14/32 - 100.3ms/batch - loss: 109.80659 - diff: 26.78mlTrain batch 15/32 - 114.7ms/batch - loss: 106.37982 - diff: 26.42mlTrain batch 16/32 - 104.5ms/batch - loss: 103.67428 - diff: 26.21mlTrain batch 17/32 - 108.7ms/batch - loss: 99.87783 - diff: 25.70mlTrain batch 18/32 - 110.9ms/batch - loss: 98.85811 - diff: 25.67mlTrain batch 19/32 - 111.9ms/batch - loss: 95.91984 - diff: 25.43mlTrain batch 20/32 - 111.4ms/batch - loss: 91.61681 - diff: 24.71mlTrain batch 21/32 - 98.5ms/batch - loss: 88.77711 - diff: 24.44mlTrain batch 22/32 - 87.6ms/batch - loss: 90.77425 - diff: 24.85mlTrain batch 23/32 - 103.0ms/batch - loss: 90.66180 - diff: 24.91mlTrain batch 24/32 - 113.6ms/batch - loss: 95.37634 - diff: 25.59mlTrain batch 25/32 - 115.3ms/batch - loss: 97.67216 - diff: 26.06mlTrain batch 26/32 - 111.9ms/batch - loss: 95.52928 - diff: 25.89mlTrain batch 27/32 - 118.7ms/batch - loss: 95.15939 - diff: 26.00mlTrain batch 28/32 - 111.0ms/batch - loss: 95.53032 - diff: 26.25mlTrain batch 29/32 - 112.9ms/batch - loss: 93.76901 - diff: 26.12mlTrain batch 30/32 - 107.1ms/batch - loss: 91.81579 - diff: 25.94mlTrain batch 31/32 - 90.2ms/batch - loss: 90.67164 - diff: 25.80mlTrain batch 32/32 - 71.3ms/batch - loss: 91.71387 - diff: 25.82mlTrain batch 32/32 - 11.3s 71.3ms/batch - loss: 91.71387 - diff: 25.82ml
Test 0.6s: val_loss: 126.79031 - diff: 32.97ml

Epoch 47: current best loss = 59.74411, at epoch 40
Train batch 1/32 - 107.3ms/batch - loss: 33.00601 - diff: 17.71mlTrain batch 2/32 - 87.8ms/batch - loss: 52.32711 - diff: 23.50mlTrain batch 3/32 - 111.4ms/batch - loss: 44.30552 - diff: 21.28mlTrain batch 4/32 - 103.2ms/batch - loss: 77.20962 - diff: 25.68mlTrain batch 5/32 - 104.5ms/batch - loss: 203.24172 - diff: 30.96mlTrain batch 6/32 - 103.1ms/batch - loss: 174.13880 - diff: 28.74mlTrain batch 7/32 - 104.3ms/batch - loss: 154.80445 - diff: 27.53mlTrain batch 8/32 - 102.8ms/batch - loss: 143.57436 - diff: 26.93mlTrain batch 9/32 - 104.5ms/batch - loss: 135.14220 - diff: 27.03mlTrain batch 10/32 - 105.4ms/batch - loss: 126.94869 - diff: 26.51mlTrain batch 11/32 - 105.0ms/batch - loss: 118.64612 - diff: 25.81mlTrain batch 12/32 - 104.8ms/batch - loss: 114.79738 - diff: 26.10mlTrain batch 13/32 - 108.3ms/batch - loss: 109.81720 - diff: 25.78mlTrain batch 14/32 - 108.7ms/batch - loss: 107.65348 - diff: 25.87mlTrain batch 15/32 - 107.1ms/batch - loss: 107.71315 - diff: 26.46mlTrain batch 16/32 - 111.1ms/batch - loss: 105.92810 - diff: 26.41mlTrain batch 17/32 - 94.7ms/batch - loss: 104.39386 - diff: 26.27mlTrain batch 18/32 - 87.8ms/batch - loss: 103.16157 - diff: 26.30mlTrain batch 19/32 - 88.5ms/batch - loss: 99.54317 - diff: 25.86mlTrain batch 20/32 - 105.1ms/batch - loss: 97.79353 - diff: 25.80mlTrain batch 21/32 - 101.6ms/batch - loss: 96.28508 - diff: 25.91mlTrain batch 22/32 - 109.6ms/batch - loss: 97.48688 - diff: 26.13mlTrain batch 23/32 - 101.9ms/batch - loss: 95.24388 - diff: 25.96mlTrain batch 24/32 - 102.5ms/batch - loss: 92.68359 - diff: 25.63mlTrain batch 25/32 - 102.5ms/batch - loss: 91.14875 - diff: 25.58mlTrain batch 26/32 - 106.8ms/batch - loss: 88.88991 - diff: 25.28mlTrain batch 27/32 - 104.9ms/batch - loss: 86.32227 - diff: 24.83mlTrain batch 28/32 - 107.4ms/batch - loss: 83.94803 - diff: 24.52mlTrain batch 29/32 - 105.7ms/batch - loss: 82.20979 - diff: 24.27mlTrain batch 30/32 - 96.2ms/batch - loss: 87.29579 - diff: 24.49mlTrain batch 31/32 - 106.5ms/batch - loss: 86.59271 - diff: 24.60mlTrain batch 32/32 - 90.3ms/batch - loss: 92.51034 - diff: 24.84mlTrain batch 32/32 - 10.7s 90.3ms/batch - loss: 92.51034 - diff: 24.84ml
Test 0.6s: val_loss: 72.23646 - diff: 24.37ml

Epoch 48: current best loss = 59.74411, at epoch 40
Train batch 1/32 - 118.5ms/batch - loss: 46.60869 - diff: 19.45mlTrain batch 2/32 - 106.8ms/batch - loss: 35.21743 - diff: 17.74mlTrain batch 3/32 - 106.5ms/batch - loss: 33.44684 - diff: 17.75mlTrain batch 4/32 - 105.8ms/batch - loss: 36.87142 - diff: 18.54mlTrain batch 5/32 - 108.9ms/batch - loss: 38.72881 - diff: 19.28mlTrain batch 6/32 - 105.7ms/batch - loss: 42.14881 - diff: 20.04mlTrain batch 7/32 - 106.7ms/batch - loss: 42.47853 - diff: 20.04mlTrain batch 8/32 - 106.2ms/batch - loss: 41.66719 - diff: 19.93mlTrain batch 9/32 - 106.8ms/batch - loss: 39.58233 - diff: 19.39mlTrain batch 10/32 - 103.1ms/batch - loss: 41.72816 - diff: 20.29mlTrain batch 11/32 - 103.1ms/batch - loss: 41.78649 - diff: 20.17mlTrain batch 12/32 - 102.8ms/batch - loss: 41.26040 - diff: 20.12mlTrain batch 13/32 - 112.7ms/batch - loss: 42.38078 - diff: 20.47mlTrain batch 14/32 - 114.1ms/batch - loss: 48.59215 - diff: 20.81mlTrain batch 15/32 - 113.1ms/batch - loss: 91.98517 - diff: 22.82mlTrain batch 16/32 - 114.2ms/batch - loss: 90.01315 - diff: 22.97mlTrain batch 17/32 - 117.2ms/batch - loss: 86.96448 - diff: 22.73mlTrain batch 18/32 - 116.2ms/batch - loss: 83.82518 - diff: 22.36mlTrain batch 19/32 - 118.5ms/batch - loss: 83.39923 - diff: 22.56mlTrain batch 20/32 - 102.6ms/batch - loss: 81.34740 - diff: 22.46mlTrain batch 21/32 - 109.9ms/batch - loss: 79.85149 - diff: 22.38mlTrain batch 22/32 - 114.7ms/batch - loss: 77.61882 - diff: 22.21mlTrain batch 23/32 - 109.9ms/batch - loss: 75.29369 - diff: 21.95mlTrain batch 24/32 - 102.5ms/batch - loss: 73.59940 - diff: 21.81mlTrain batch 25/32 - 110.7ms/batch - loss: 72.92813 - diff: 21.84mlTrain batch 26/32 - 105.8ms/batch - loss: 72.20377 - diff: 21.88mlTrain batch 27/32 - 110.2ms/batch - loss: 70.95545 - diff: 21.80mlTrain batch 28/32 - 112.5ms/batch - loss: 70.39932 - diff: 21.84mlTrain batch 29/32 - 108.0ms/batch - loss: 69.15628 - diff: 21.82mlTrain batch 30/32 - 106.6ms/batch - loss: 69.50882 - diff: 21.77mlTrain batch 31/32 - 102.1ms/batch - loss: 69.70616 - diff: 21.90mlTrain batch 32/32 - 81.6ms/batch - loss: 70.17652 - diff: 21.89mlTrain batch 32/32 - 11.4s 81.6ms/batch - loss: 70.17652 - diff: 21.89ml
Test 0.6s: val_loss: 43.53434 - diff: 18.17ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Systole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 49: current best loss = 43.53434, at epoch 48
Train batch 1/32 - 126.2ms/batch - loss: 34.79111 - diff: 19.22mlTrain batch 2/32 - 112.1ms/batch - loss: 47.31063 - diff: 21.24mlTrain batch 3/32 - 113.8ms/batch - loss: 45.33546 - diff: 20.94mlTrain batch 4/32 - 115.0ms/batch - loss: 40.18691 - diff: 19.75mlTrain batch 5/32 - 90.3ms/batch - loss: 38.61356 - diff: 19.86mlTrain batch 6/32 - 88.4ms/batch - loss: 36.12234 - diff: 18.95mlTrain batch 7/32 - 95.8ms/batch - loss: 38.92534 - diff: 19.58mlTrain batch 8/32 - 105.3ms/batch - loss: 37.79528 - diff: 19.08mlTrain batch 9/32 - 105.4ms/batch - loss: 37.68542 - diff: 18.98mlTrain batch 10/32 - 108.0ms/batch - loss: 41.57569 - diff: 19.25mlTrain batch 11/32 - 115.8ms/batch - loss: 39.77436 - diff: 18.75mlTrain batch 12/32 - 116.3ms/batch - loss: 40.90292 - diff: 19.11mlTrain batch 13/32 - 112.7ms/batch - loss: 39.84115 - diff: 19.02mlTrain batch 14/32 - 94.3ms/batch - loss: 53.57109 - diff: 19.34mlTrain batch 15/32 - 91.8ms/batch - loss: 53.58068 - diff: 19.45mlTrain batch 16/32 - 117.2ms/batch - loss: 53.90732 - diff: 19.78mlTrain batch 17/32 - 88.4ms/batch - loss: 54.69611 - diff: 19.86mlTrain batch 18/32 - 96.9ms/batch - loss: 53.97723 - diff: 19.85mlTrain batch 19/32 - 116.8ms/batch - loss: 53.59269 - diff: 19.93mlTrain batch 20/32 - 110.9ms/batch - loss: 54.82466 - diff: 20.06mlTrain batch 21/32 - 100.4ms/batch - loss: 54.24418 - diff: 20.09mlTrain batch 22/32 - 105.5ms/batch - loss: 54.05515 - diff: 20.16mlTrain batch 23/32 - 106.5ms/batch - loss: 52.50136 - diff: 19.83mlTrain batch 24/32 - 115.9ms/batch - loss: 54.97539 - diff: 20.13mlTrain batch 25/32 - 93.4ms/batch - loss: 58.52135 - diff: 20.60mlTrain batch 26/32 - 91.0ms/batch - loss: 58.15434 - diff: 20.67mlTrain batch 27/32 - 106.4ms/batch - loss: 57.54937 - diff: 20.68mlTrain batch 28/32 - 107.7ms/batch - loss: 58.02276 - diff: 20.85mlTrain batch 29/32 - 103.3ms/batch - loss: 58.75356 - diff: 21.02mlTrain batch 30/32 - 101.8ms/batch - loss: 58.15687 - diff: 20.91mlTrain batch 31/32 - 102.2ms/batch - loss: 57.85939 - diff: 20.94mlTrain batch 32/32 - 76.3ms/batch - loss: 57.78396 - diff: 20.88mlTrain batch 32/32 - 12.1s 76.3ms/batch - loss: 57.78396 - diff: 20.88ml
Test 0.6s: val_loss: 46.00382 - diff: 19.13ml

Epoch 50: current best loss = 43.53434, at epoch 48
Train batch 1/32 - 125.3ms/batch - loss: 52.86357 - diff: 22.93mlTrain batch 2/32 - 104.7ms/batch - loss: 52.72795 - diff: 22.65mlTrain batch 3/32 - 106.9ms/batch - loss: 58.58631 - diff: 23.43mlTrain batch 4/32 - 106.0ms/batch - loss: 50.72571 - diff: 21.40mlTrain batch 5/32 - 108.5ms/batch - loss: 46.21222 - diff: 20.29mlTrain batch 6/32 - 114.6ms/batch - loss: 46.38908 - diff: 20.36mlTrain batch 7/32 - 105.5ms/batch - loss: 45.43524 - diff: 20.29mlTrain batch 8/32 - 106.5ms/batch - loss: 44.43865 - diff: 20.22mlTrain batch 9/32 - 105.9ms/batch - loss: 62.25569 - diff: 21.55mlTrain batch 10/32 - 108.0ms/batch - loss: 58.54182 - diff: 20.94mlTrain batch 11/32 - 102.0ms/batch - loss: 57.69024 - diff: 21.03mlTrain batch 12/32 - 106.6ms/batch - loss: 55.97528 - diff: 21.07mlTrain batch 13/32 - 100.5ms/batch - loss: 57.46454 - diff: 21.62mlTrain batch 14/32 - 105.7ms/batch - loss: 59.99595 - diff: 22.20mlTrain batch 15/32 - 115.6ms/batch - loss: 60.59675 - diff: 22.53mlTrain batch 16/32 - 106.0ms/batch - loss: 58.17182 - diff: 22.01mlTrain batch 17/32 - 114.7ms/batch - loss: 58.12839 - diff: 22.24mlTrain batch 18/32 - 113.8ms/batch - loss: 56.52700 - diff: 21.85mlTrain batch 19/32 - 113.5ms/batch - loss: 56.05539 - diff: 21.69mlTrain batch 20/32 - 112.9ms/batch - loss: 54.68548 - diff: 21.48mlTrain batch 21/32 - 112.3ms/batch - loss: 53.51591 - diff: 21.39mlTrain batch 22/32 - 111.2ms/batch - loss: 54.17089 - diff: 21.52mlTrain batch 23/32 - 112.1ms/batch - loss: 52.36191 - diff: 21.06mlTrain batch 24/32 - 111.4ms/batch - loss: 50.96130 - diff: 20.86mlTrain batch 25/32 - 113.4ms/batch - loss: 50.93834 - diff: 20.82mlTrain batch 26/32 - 111.9ms/batch - loss: 51.05381 - diff: 20.96mlTrain batch 27/32 - 113.5ms/batch - loss: 50.99925 - diff: 20.93mlTrain batch 28/32 - 115.3ms/batch - loss: 52.04375 - diff: 21.10mlTrain batch 29/32 - 106.5ms/batch - loss: 51.61051 - diff: 21.08mlTrain batch 30/32 - 109.2ms/batch - loss: 51.02755 - diff: 21.03mlTrain batch 31/32 - 99.0ms/batch - loss: 50.94583 - diff: 20.96mlTrain batch 32/32 - 81.0ms/batch - loss: 50.80633 - diff: 20.87mlTrain batch 32/32 - 11.8s 81.0ms/batch - loss: 50.80633 - diff: 20.87ml
Test 0.6s: val_loss: 51.47772 - diff: 18.28ml

Epoch 51: current best loss = 43.53434, at epoch 48
Train batch 1/32 - 131.8ms/batch - loss: 34.77963 - diff: 18.94mlTrain batch 2/32 - 112.2ms/batch - loss: 31.77872 - diff: 19.22mlTrain batch 3/32 - 115.0ms/batch - loss: 29.93408 - diff: 18.07mlTrain batch 4/32 - 103.0ms/batch - loss: 32.67730 - diff: 17.60mlTrain batch 5/32 - 98.4ms/batch - loss: 33.29570 - diff: 17.96mlTrain batch 6/32 - 116.6ms/batch - loss: 31.96909 - diff: 17.68mlTrain batch 7/32 - 110.4ms/batch - loss: 33.67709 - diff: 17.82mlTrain batch 8/32 - 102.6ms/batch - loss: 44.49620 - diff: 18.81mlTrain batch 9/32 - 109.2ms/batch - loss: 43.82591 - diff: 18.66mlTrain batch 10/32 - 103.2ms/batch - loss: 41.72954 - diff: 18.25mlTrain batch 11/32 - 109.8ms/batch - loss: 43.15955 - diff: 18.54mlTrain batch 12/32 - 104.9ms/batch - loss: 44.70304 - diff: 18.69mlTrain batch 13/32 - 105.1ms/batch - loss: 44.97271 - diff: 19.02mlTrain batch 14/32 - 103.1ms/batch - loss: 45.19118 - diff: 19.26mlTrain batch 15/32 - 92.5ms/batch - loss: 44.21007 - diff: 19.24mlTrain batch 16/32 - 112.6ms/batch - loss: 46.12913 - diff: 19.75mlTrain batch 17/32 - 113.2ms/batch - loss: 45.75650 - diff: 19.72mlTrain batch 18/32 - 92.7ms/batch - loss: 45.78163 - diff: 19.73mlTrain batch 19/32 - 110.3ms/batch - loss: 45.96226 - diff: 19.84mlTrain batch 20/32 - 101.4ms/batch - loss: 45.42534 - diff: 19.77mlTrain batch 21/32 - 105.2ms/batch - loss: 44.26542 - diff: 19.54mlTrain batch 22/32 - 103.0ms/batch - loss: 43.31294 - diff: 19.39mlTrain batch 23/32 - 103.4ms/batch - loss: 42.23437 - diff: 19.09mlTrain batch 24/32 - 102.0ms/batch - loss: 41.91556 - diff: 19.13mlTrain batch 25/32 - 109.9ms/batch - loss: 44.79558 - diff: 19.67mlTrain batch 26/32 - 102.0ms/batch - loss: 44.27575 - diff: 19.44mlTrain batch 27/32 - 119.3ms/batch - loss: 45.74265 - diff: 19.61mlTrain batch 28/32 - 112.3ms/batch - loss: 46.06664 - diff: 19.67mlTrain batch 29/32 - 113.0ms/batch - loss: 45.41664 - diff: 19.59mlTrain batch 30/32 - 100.6ms/batch - loss: 45.65454 - diff: 19.57mlTrain batch 31/32 - 112.5ms/batch - loss: 51.71720 - diff: 19.95mlTrain batch 32/32 - 68.3ms/batch - loss: 63.15800 - diff: 20.19mlTrain batch 32/32 - 10.6s 68.3ms/batch - loss: 63.15800 - diff: 20.19ml
Test 0.6s: val_loss: 48.27044 - diff: 19.02ml

Epoch 52: current best loss = 43.53434, at epoch 48
Train batch 1/32 - 122.6ms/batch - loss: 41.87724 - diff: 19.55mlTrain batch 2/32 - 107.6ms/batch - loss: 33.88130 - diff: 18.54mlTrain batch 3/32 - 107.2ms/batch - loss: 54.46570 - diff: 22.87mlTrain batch 4/32 - 108.8ms/batch - loss: 58.41826 - diff: 23.75mlTrain batch 5/32 - 107.5ms/batch - loss: 51.64192 - diff: 22.23mlTrain batch 6/32 - 106.6ms/batch - loss: 51.42553 - diff: 22.48mlTrain batch 7/32 - 108.6ms/batch - loss: 54.50795 - diff: 23.06mlTrain batch 8/32 - 108.7ms/batch - loss: 53.95705 - diff: 22.95mlTrain batch 9/32 - 108.4ms/batch - loss: 51.52383 - diff: 22.34mlTrain batch 10/32 - 106.4ms/batch - loss: 51.91730 - diff: 22.23mlTrain batch 11/32 - 107.5ms/batch - loss: 51.35814 - diff: 22.17mlTrain batch 12/32 - 107.5ms/batch - loss: 52.33644 - diff: 22.41mlTrain batch 13/32 - 107.2ms/batch - loss: 51.66923 - diff: 22.30mlTrain batch 14/32 - 100.3ms/batch - loss: 59.04790 - diff: 22.70mlTrain batch 15/32 - 106.0ms/batch - loss: 56.86578 - diff: 22.39mlTrain batch 16/32 - 105.7ms/batch - loss: 55.30789 - diff: 22.11mlTrain batch 17/32 - 115.9ms/batch - loss: 54.11831 - diff: 22.00mlTrain batch 18/32 - 103.8ms/batch - loss: 55.12275 - diff: 22.17mlTrain batch 19/32 - 109.9ms/batch - loss: 55.43649 - diff: 22.27mlTrain batch 20/32 - 91.5ms/batch - loss: 56.16189 - diff: 22.48mlTrain batch 21/32 - 114.0ms/batch - loss: 54.42264 - diff: 22.06mlTrain batch 22/32 - 112.9ms/batch - loss: 53.18705 - diff: 21.82mlTrain batch 23/32 - 119.9ms/batch - loss: 51.57390 - diff: 21.44mlTrain batch 24/32 - 113.8ms/batch - loss: 50.62028 - diff: 21.26mlTrain batch 25/32 - 119.5ms/batch - loss: 50.47762 - diff: 21.31mlTrain batch 26/32 - 112.9ms/batch - loss: 49.57053 - diff: 21.23mlTrain batch 27/32 - 114.1ms/batch - loss: 49.43951 - diff: 21.10mlTrain batch 28/32 - 112.8ms/batch - loss: 49.15978 - diff: 21.13mlTrain batch 29/32 - 112.5ms/batch - loss: 48.89834 - diff: 21.11mlTrain batch 30/32 - 113.1ms/batch - loss: 49.16460 - diff: 21.18mlTrain batch 31/32 - 112.7ms/batch - loss: 48.02195 - diff: 20.86mlTrain batch 32/32 - 77.5ms/batch - loss: 48.46345 - diff: 20.84mlTrain batch 32/32 - 10.7s 77.5ms/batch - loss: 48.46345 - diff: 20.84ml
Test 0.6s: val_loss: 56.50468 - diff: 20.83ml

Epoch 53: current best loss = 43.53434, at epoch 48
Train batch 1/32 - 115.9ms/batch - loss: 23.98161 - diff: 15.35mlTrain batch 2/32 - 107.9ms/batch - loss: 22.12109 - diff: 14.67mlTrain batch 3/32 - 102.8ms/batch - loss: 39.56055 - diff: 18.29mlTrain batch 4/32 - 105.3ms/batch - loss: 46.62288 - diff: 19.45mlTrain batch 5/32 - 105.4ms/batch - loss: 42.42272 - diff: 18.76mlTrain batch 6/32 - 105.8ms/batch - loss: 44.38296 - diff: 19.12mlTrain batch 7/32 - 103.1ms/batch - loss: 58.44900 - diff: 20.43mlTrain batch 8/32 - 114.4ms/batch - loss: 55.58724 - diff: 19.98mlTrain batch 9/32 - 106.8ms/batch - loss: 51.21137 - diff: 19.13mlTrain batch 10/32 - 107.9ms/batch - loss: 47.55467 - diff: 18.46mlTrain batch 11/32 - 108.3ms/batch - loss: 51.33479 - diff: 18.90mlTrain batch 12/32 - 107.0ms/batch - loss: 49.59980 - diff: 18.66mlTrain batch 13/32 - 102.7ms/batch - loss: 48.60898 - diff: 18.87mlTrain batch 14/32 - 92.2ms/batch - loss: 45.97569 - diff: 18.34mlTrain batch 15/32 - 89.1ms/batch - loss: 44.78188 - diff: 18.20mlTrain batch 16/32 - 114.2ms/batch - loss: 43.23535 - diff: 18.00mlTrain batch 17/32 - 118.2ms/batch - loss: 42.97978 - diff: 17.96mlTrain batch 18/32 - 121.6ms/batch - loss: 47.36161 - diff: 18.21mlTrain batch 19/32 - 122.8ms/batch - loss: 47.23757 - diff: 18.33mlTrain batch 20/32 - 112.9ms/batch - loss: 47.36625 - diff: 18.38mlTrain batch 21/32 - 121.4ms/batch - loss: 46.94330 - diff: 18.30mlTrain batch 22/32 - 110.5ms/batch - loss: 45.89950 - diff: 18.16mlTrain batch 23/32 - 114.2ms/batch - loss: 45.29234 - diff: 18.24mlTrain batch 24/32 - 102.1ms/batch - loss: 44.22945 - diff: 18.08mlTrain batch 25/32 - 105.9ms/batch - loss: 44.48273 - diff: 18.23mlTrain batch 26/32 - 102.7ms/batch - loss: 44.12592 - diff: 18.21mlTrain batch 27/32 - 105.0ms/batch - loss: 44.56335 - diff: 18.42mlTrain batch 28/32 - 101.7ms/batch - loss: 43.79146 - diff: 18.30mlTrain batch 29/32 - 100.7ms/batch - loss: 44.26062 - diff: 18.42mlTrain batch 30/32 - 101.8ms/batch - loss: 43.70682 - diff: 18.36mlTrain batch 31/32 - 116.4ms/batch - loss: 43.85029 - diff: 18.35mlTrain batch 32/32 - 80.7ms/batch - loss: 44.32815 - diff: 18.33mlTrain batch 32/32 - 11.3s 80.7ms/batch - loss: 44.32815 - diff: 18.33ml
Test 0.6s: val_loss: 40.94428 - diff: 17.33ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Systole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 54: current best loss = 40.94428, at epoch 53
Train batch 1/32 - 116.6ms/batch - loss: 25.84359 - diff: 16.33mlTrain batch 2/32 - 104.6ms/batch - loss: 36.54611 - diff: 18.31mlTrain batch 3/32 - 103.5ms/batch - loss: 32.61187 - diff: 17.94mlTrain batch 4/32 - 103.9ms/batch - loss: 28.69852 - diff: 17.00mlTrain batch 5/32 - 104.3ms/batch - loss: 41.50806 - diff: 18.82mlTrain batch 6/32 - 103.9ms/batch - loss: 38.03459 - diff: 18.40mlTrain batch 7/32 - 105.6ms/batch - loss: 40.41028 - diff: 18.62mlTrain batch 8/32 - 107.6ms/batch - loss: 40.67871 - diff: 18.80mlTrain batch 9/32 - 106.2ms/batch - loss: 39.57660 - diff: 18.24mlTrain batch 10/32 - 105.1ms/batch - loss: 36.70189 - diff: 17.49mlTrain batch 11/32 - 107.5ms/batch - loss: 38.56379 - diff: 18.13mlTrain batch 12/32 - 104.2ms/batch - loss: 37.01479 - diff: 17.84mlTrain batch 13/32 - 99.4ms/batch - loss: 36.90829 - diff: 18.03mlTrain batch 14/32 - 99.2ms/batch - loss: 40.95507 - diff: 18.76mlTrain batch 15/32 - 99.1ms/batch - loss: 41.09788 - diff: 18.81mlTrain batch 16/32 - 109.0ms/batch - loss: 42.43790 - diff: 18.90mlTrain batch 17/32 - 109.2ms/batch - loss: 41.58381 - diff: 18.75mlTrain batch 18/32 - 117.3ms/batch - loss: 42.21593 - diff: 18.75mlTrain batch 19/32 - 113.9ms/batch - loss: 42.61480 - diff: 18.97mlTrain batch 20/32 - 112.5ms/batch - loss: 45.41739 - diff: 19.24mlTrain batch 21/32 - 105.1ms/batch - loss: 45.99437 - diff: 19.44mlTrain batch 22/32 - 115.3ms/batch - loss: 44.44754 - diff: 19.10mlTrain batch 23/32 - 114.2ms/batch - loss: 43.08129 - diff: 18.78mlTrain batch 24/32 - 121.2ms/batch - loss: 42.32904 - diff: 18.65mlTrain batch 25/32 - 113.2ms/batch - loss: 41.57869 - diff: 18.55mlTrain batch 26/32 - 115.1ms/batch - loss: 40.39876 - diff: 18.25mlTrain batch 27/32 - 113.8ms/batch - loss: 39.82195 - diff: 18.20mlTrain batch 28/32 - 119.6ms/batch - loss: 39.17552 - diff: 18.13mlTrain batch 29/32 - 113.2ms/batch - loss: 39.11329 - diff: 18.13mlTrain batch 30/32 - 110.8ms/batch - loss: 38.47529 - diff: 17.99mlTrain batch 31/32 - 98.0ms/batch - loss: 39.61634 - diff: 18.20mlTrain batch 32/32 - 90.8ms/batch - loss: 40.70505 - diff: 18.23mlTrain batch 32/32 - 11.7s 90.8ms/batch - loss: 40.70505 - diff: 18.23ml
Test 0.6s: val_loss: 57.30306 - diff: 19.82ml

Epoch 55: current best loss = 40.94428, at epoch 53
Train batch 1/32 - 122.5ms/batch - loss: 32.61363 - diff: 18.06mlTrain batch 2/32 - 107.5ms/batch - loss: 34.46294 - diff: 18.06mlTrain batch 3/32 - 107.3ms/batch - loss: 40.95534 - diff: 18.82mlTrain batch 4/32 - 108.3ms/batch - loss: 33.66914 - diff: 16.91mlTrain batch 5/32 - 117.6ms/batch - loss: 31.34062 - diff: 16.45mlTrain batch 6/32 - 118.8ms/batch - loss: 28.48968 - diff: 15.97mlTrain batch 7/32 - 106.1ms/batch - loss: 29.79399 - diff: 16.32mlTrain batch 8/32 - 109.8ms/batch - loss: 37.25635 - diff: 17.71mlTrain batch 9/32 - 107.8ms/batch - loss: 35.91702 - diff: 17.45mlTrain batch 10/32 - 105.8ms/batch - loss: 37.79017 - diff: 17.56mlTrain batch 11/32 - 106.9ms/batch - loss: 39.17068 - diff: 17.69mlTrain batch 12/32 - 106.9ms/batch - loss: 38.22151 - diff: 17.72mlTrain batch 13/32 - 106.3ms/batch - loss: 40.48244 - diff: 18.22mlTrain batch 14/32 - 107.5ms/batch - loss: 40.11575 - diff: 18.31mlTrain batch 15/32 - 107.5ms/batch - loss: 39.84356 - diff: 18.14mlTrain batch 16/32 - 106.9ms/batch - loss: 53.17725 - diff: 19.00mlTrain batch 17/32 - 105.6ms/batch - loss: 70.11183 - diff: 20.39mlTrain batch 18/32 - 106.7ms/batch - loss: 69.11682 - diff: 20.41mlTrain batch 19/32 - 105.6ms/batch - loss: 68.31085 - diff: 20.31mlTrain batch 20/32 - 106.7ms/batch - loss: 67.90108 - diff: 20.29mlTrain batch 21/32 - 105.5ms/batch - loss: 65.43078 - diff: 19.91mlTrain batch 22/32 - 107.2ms/batch - loss: 64.59679 - diff: 20.06mlTrain batch 23/32 - 107.5ms/batch - loss: 62.69567 - diff: 19.76mlTrain batch 24/32 - 105.7ms/batch - loss: 62.12504 - diff: 19.79mlTrain batch 25/32 - 107.2ms/batch - loss: 60.80162 - diff: 19.72mlTrain batch 26/32 - 105.9ms/batch - loss: 60.97964 - diff: 19.99mlTrain batch 27/32 - 106.6ms/batch - loss: 59.42901 - diff: 19.79mlTrain batch 28/32 - 105.5ms/batch - loss: 58.14284 - diff: 19.69mlTrain batch 29/32 - 106.7ms/batch - loss: 57.31194 - diff: 19.69mlTrain batch 30/32 - 107.0ms/batch - loss: 56.12806 - diff: 19.43mlTrain batch 31/32 - 103.1ms/batch - loss: 55.35712 - diff: 19.36mlTrain batch 32/32 - 84.8ms/batch - loss: 55.45377 - diff: 19.31mlTrain batch 32/32 - 11.0s 84.8ms/batch - loss: 55.45377 - diff: 19.31ml
Test 0.6s: val_loss: 69.54785 - diff: 23.00ml

Epoch 56: current best loss = 40.94428, at epoch 53
Train batch 1/32 - 121.1ms/batch - loss: 18.95938 - diff: 13.86mlTrain batch 2/32 - 119.7ms/batch - loss: 34.96035 - diff: 17.49mlTrain batch 3/32 - 117.7ms/batch - loss: 30.62843 - diff: 16.73mlTrain batch 4/32 - 118.7ms/batch - loss: 31.13005 - diff: 17.27mlTrain batch 5/32 - 112.2ms/batch - loss: 27.38376 - diff: 15.97mlTrain batch 6/32 - 121.1ms/batch - loss: 27.96291 - diff: 16.54mlTrain batch 7/32 - 119.0ms/batch - loss: 26.48224 - diff: 16.10mlTrain batch 8/32 - 124.3ms/batch - loss: 26.71374 - diff: 16.39mlTrain batch 9/32 - 117.8ms/batch - loss: 27.95079 - diff: 16.72mlTrain batch 10/32 - 117.5ms/batch - loss: 29.15879 - diff: 16.99mlTrain batch 11/32 - 117.2ms/batch - loss: 52.03423 - diff: 18.76mlTrain batch 12/32 - 117.0ms/batch - loss: 48.63202 - diff: 17.99mlTrain batch 13/32 - 110.6ms/batch - loss: 46.64981 - diff: 17.88mlTrain batch 14/32 - 97.0ms/batch - loss: 45.66130 - diff: 17.97mlTrain batch 15/32 - 90.6ms/batch - loss: 46.16010 - diff: 18.23mlTrain batch 16/32 - 103.5ms/batch - loss: 51.03728 - diff: 19.05mlTrain batch 17/32 - 105.0ms/batch - loss: 62.62792 - diff: 19.90mlTrain batch 18/32 - 102.8ms/batch - loss: 60.89552 - diff: 19.54mlTrain batch 19/32 - 104.8ms/batch - loss: 59.19713 - diff: 19.43mlTrain batch 20/32 - 103.0ms/batch - loss: 58.27623 - diff: 19.60mlTrain batch 21/32 - 104.5ms/batch - loss: 58.65912 - diff: 19.68mlTrain batch 22/32 - 103.4ms/batch - loss: 57.82155 - diff: 19.67mlTrain batch 23/32 - 103.9ms/batch - loss: 56.20931 - diff: 19.44mlTrain batch 24/32 - 112.8ms/batch - loss: 55.60819 - diff: 19.52mlTrain batch 25/32 - 120.8ms/batch - loss: 55.43785 - diff: 19.55mlTrain batch 26/32 - 104.5ms/batch - loss: 55.31478 - diff: 19.55mlTrain batch 27/32 - 111.2ms/batch - loss: 54.39278 - diff: 19.49mlTrain batch 28/32 - 113.7ms/batch - loss: 53.13763 - diff: 19.32mlTrain batch 29/32 - 114.0ms/batch - loss: 53.70541 - diff: 19.39mlTrain batch 30/32 - 104.0ms/batch - loss: 53.90740 - diff: 19.55mlTrain batch 31/32 - 102.7ms/batch - loss: 54.68229 - diff: 19.73mlTrain batch 32/32 - 92.4ms/batch - loss: 56.01438 - diff: 19.76mlTrain batch 32/32 - 11.2s 92.4ms/batch - loss: 56.01438 - diff: 19.76ml
Test 0.6s: val_loss: 58.84059 - diff: 19.29ml

Epoch 57: current best loss = 40.94428, at epoch 53
Train batch 1/32 - 114.4ms/batch - loss: 30.64388 - diff: 15.68mlTrain batch 2/32 - 108.4ms/batch - loss: 36.82229 - diff: 17.04mlTrain batch 3/32 - 107.2ms/batch - loss: 42.85398 - diff: 18.65mlTrain batch 4/32 - 107.3ms/batch - loss: 42.75650 - diff: 18.46mlTrain batch 5/32 - 114.2ms/batch - loss: 39.50608 - diff: 18.05mlTrain batch 6/32 - 105.8ms/batch - loss: 40.70713 - diff: 17.49mlTrain batch 7/32 - 106.7ms/batch - loss: 43.51852 - diff: 18.20mlTrain batch 8/32 - 106.2ms/batch - loss: 43.18519 - diff: 18.47mlTrain batch 9/32 - 106.2ms/batch - loss: 43.82363 - diff: 18.17mlTrain batch 10/32 - 101.7ms/batch - loss: 42.99092 - diff: 18.45mlTrain batch 11/32 - 114.5ms/batch - loss: 46.34924 - diff: 18.80mlTrain batch 12/32 - 116.6ms/batch - loss: 44.09719 - diff: 18.39mlTrain batch 13/32 - 102.5ms/batch - loss: 42.85622 - diff: 18.29mlTrain batch 14/32 - 102.0ms/batch - loss: 43.88033 - diff: 18.55mlTrain batch 15/32 - 99.5ms/batch - loss: 44.38699 - diff: 18.65mlTrain batch 16/32 - 104.5ms/batch - loss: 44.49826 - diff: 18.60mlTrain batch 17/32 - 102.6ms/batch - loss: 43.72902 - diff: 18.59mlTrain batch 18/32 - 101.9ms/batch - loss: 42.48248 - diff: 18.44mlTrain batch 19/32 - 101.7ms/batch - loss: 48.69288 - diff: 19.08mlTrain batch 20/32 - 103.1ms/batch - loss: 49.69485 - diff: 19.25mlTrain batch 21/32 - 102.3ms/batch - loss: 48.27476 - diff: 19.04mlTrain batch 22/32 - 102.6ms/batch - loss: 48.72154 - diff: 19.31mlTrain batch 23/32 - 112.8ms/batch - loss: 47.59536 - diff: 19.22mlTrain batch 24/32 - 120.1ms/batch - loss: 48.39009 - diff: 19.41mlTrain batch 25/32 - 112.6ms/batch - loss: 51.64191 - diff: 19.73mlTrain batch 26/32 - 108.0ms/batch - loss: 50.58571 - diff: 19.57mlTrain batch 27/32 - 106.1ms/batch - loss: 51.20427 - diff: 19.86mlTrain batch 28/32 - 106.8ms/batch - loss: 50.35033 - diff: 19.71mlTrain batch 29/32 - 107.6ms/batch - loss: 49.60504 - diff: 19.59mlTrain batch 30/32 - 106.1ms/batch - loss: 49.25118 - diff: 19.56mlTrain batch 31/32 - 105.6ms/batch - loss: 49.17884 - diff: 19.66mlTrain batch 32/32 - 87.4ms/batch - loss: 50.27911 - diff: 19.71mlTrain batch 32/32 - 11.5s 87.4ms/batch - loss: 50.27911 - diff: 19.71ml
Test 0.6s: val_loss: 51.49410 - diff: 19.65ml

Epoch 58: current best loss = 40.94428, at epoch 53
Train batch 1/32 - 138.9ms/batch - loss: 16.53672 - diff: 13.00mlTrain batch 2/32 - 93.6ms/batch - loss: 16.65707 - diff: 12.79mlTrain batch 3/32 - 107.2ms/batch - loss: 22.19861 - diff: 14.64mlTrain batch 4/32 - 106.0ms/batch - loss: 27.57027 - diff: 16.85mlTrain batch 5/32 - 106.9ms/batch - loss: 29.94402 - diff: 17.13mlTrain batch 6/32 - 104.5ms/batch - loss: 26.80374 - diff: 16.23mlTrain batch 7/32 - 106.5ms/batch - loss: 27.34575 - diff: 16.51mlTrain batch 8/32 - 106.0ms/batch - loss: 27.94023 - diff: 16.72mlTrain batch 9/32 - 91.3ms/batch - loss: 27.02504 - diff: 16.53mlTrain batch 10/32 - 97.6ms/batch - loss: 28.73199 - diff: 17.04mlTrain batch 11/32 - 107.7ms/batch - loss: 29.72784 - diff: 17.35mlTrain batch 12/32 - 111.4ms/batch - loss: 30.90681 - diff: 17.73mlTrain batch 13/32 - 115.4ms/batch - loss: 30.73210 - diff: 17.66mlTrain batch 14/32 - 106.9ms/batch - loss: 31.45380 - diff: 17.53mlTrain batch 15/32 - 108.9ms/batch - loss: 31.21294 - diff: 17.41mlTrain batch 16/32 - 106.7ms/batch - loss: 31.85820 - diff: 17.30mlTrain batch 17/32 - 113.9ms/batch - loss: 30.65082 - diff: 16.85mlTrain batch 18/32 - 115.6ms/batch - loss: 29.86727 - diff: 16.56mlTrain batch 19/32 - 113.4ms/batch - loss: 31.38672 - diff: 16.92mlTrain batch 20/32 - 112.3ms/batch - loss: 30.65108 - diff: 16.67mlTrain batch 21/32 - 114.2ms/batch - loss: 30.41415 - diff: 16.67mlTrain batch 22/32 - 114.0ms/batch - loss: 29.95283 - diff: 16.65mlTrain batch 23/32 - 112.8ms/batch - loss: 30.32565 - diff: 16.72mlTrain batch 24/32 - 112.6ms/batch - loss: 30.72034 - diff: 16.90mlTrain batch 25/32 - 112.3ms/batch - loss: 30.11838 - diff: 16.71mlTrain batch 26/32 - 117.1ms/batch - loss: 30.14022 - diff: 16.68mlTrain batch 27/32 - 112.6ms/batch - loss: 30.29393 - diff: 16.76mlTrain batch 28/32 - 110.9ms/batch - loss: 34.16870 - diff: 16.93mlTrain batch 29/32 - 118.1ms/batch - loss: 34.84485 - diff: 17.08mlTrain batch 30/32 - 107.2ms/batch - loss: 34.71022 - diff: 17.03mlTrain batch 31/32 - 116.2ms/batch - loss: 34.40625 - diff: 16.97mlTrain batch 32/32 - 70.0ms/batch - loss: 35.15465 - diff: 16.99mlTrain batch 32/32 - 11.1s 70.0ms/batch - loss: 35.15465 - diff: 16.99ml
Test 0.6s: val_loss: 47.71332 - diff: 18.90ml

Epoch 59: current best loss = 40.94428, at epoch 53
Train batch 1/32 - 120.1ms/batch - loss: 31.02746 - diff: 16.96mlTrain batch 2/32 - 109.2ms/batch - loss: 30.56545 - diff: 18.14mlTrain batch 3/32 - 106.7ms/batch - loss: 27.01606 - diff: 17.15mlTrain batch 4/32 - 107.2ms/batch - loss: 30.68472 - diff: 17.83mlTrain batch 5/32 - 108.2ms/batch - loss: 30.90404 - diff: 17.93mlTrain batch 6/32 - 110.6ms/batch - loss: 32.13077 - diff: 18.32mlTrain batch 7/32 - 106.2ms/batch - loss: 29.25357 - diff: 17.35mlTrain batch 8/32 - 107.0ms/batch - loss: 34.16435 - diff: 18.51mlTrain batch 9/32 - 106.7ms/batch - loss: 46.93650 - diff: 19.86mlTrain batch 10/32 - 105.6ms/batch - loss: 43.35041 - diff: 19.02mlTrain batch 11/32 - 116.2ms/batch - loss: 42.23258 - diff: 18.64mlTrain batch 12/32 - 115.6ms/batch - loss: 40.91493 - diff: 18.40mlTrain batch 13/32 - 101.8ms/batch - loss: 50.21029 - diff: 18.89mlTrain batch 14/32 - 111.7ms/batch - loss: 49.72732 - diff: 18.96mlTrain batch 15/32 - 117.9ms/batch - loss: 50.26420 - diff: 19.13mlTrain batch 16/32 - 122.1ms/batch - loss: 48.82373 - diff: 18.93mlTrain batch 17/32 - 119.6ms/batch - loss: 48.59675 - diff: 19.15mlTrain batch 18/32 - 113.0ms/batch - loss: 46.79836 - diff: 18.76mlTrain batch 19/32 - 101.9ms/batch - loss: 45.60541 - diff: 18.56mlTrain batch 20/32 - 103.1ms/batch - loss: 44.10538 - diff: 18.34mlTrain batch 21/32 - 108.6ms/batch - loss: 43.89242 - diff: 18.33mlTrain batch 22/32 - 107.5ms/batch - loss: 44.09212 - diff: 18.53mlTrain batch 23/32 - 113.0ms/batch - loss: 43.08751 - diff: 18.37mlTrain batch 24/32 - 112.1ms/batch - loss: 44.26154 - diff: 18.73mlTrain batch 25/32 - 113.0ms/batch - loss: 43.36237 - diff: 18.65mlTrain batch 26/32 - 113.3ms/batch - loss: 42.21674 - diff: 18.38mlTrain batch 27/32 - 114.2ms/batch - loss: 42.08134 - diff: 18.39mlTrain batch 28/32 - 113.0ms/batch - loss: 42.25753 - diff: 18.47mlTrain batch 29/32 - 114.1ms/batch - loss: 42.69440 - diff: 18.55mlTrain batch 30/32 - 112.1ms/batch - loss: 42.34962 - diff: 18.56mlTrain batch 31/32 - 112.2ms/batch - loss: 43.36064 - diff: 18.67mlTrain batch 32/32 - 99.9ms/batch - loss: 43.33910 - diff: 18.61mlTrain batch 32/32 - 11.5s 99.9ms/batch - loss: 43.33910 - diff: 18.61ml
Test 0.7s: val_loss: 46.51375 - diff: 18.52ml

Epoch 60: current best loss = 40.94428, at epoch 53
Train batch 1/32 - 115.4ms/batch - loss: 30.96839 - diff: 18.20mlTrain batch 2/32 - 103.1ms/batch - loss: 23.10650 - diff: 15.52mlTrain batch 3/32 - 103.1ms/batch - loss: 23.11175 - diff: 15.44mlTrain batch 4/32 - 102.6ms/batch - loss: 20.27148 - diff: 14.25mlTrain batch 5/32 - 103.3ms/batch - loss: 25.90986 - diff: 15.80mlTrain batch 6/32 - 103.9ms/batch - loss: 27.15176 - diff: 16.16mlTrain batch 7/32 - 100.5ms/batch - loss: 25.79255 - diff: 15.71mlTrain batch 8/32 - 102.2ms/batch - loss: 26.88528 - diff: 16.14mlTrain batch 9/32 - 103.3ms/batch - loss: 26.96515 - diff: 16.36mlTrain batch 10/32 - 90.5ms/batch - loss: 27.13449 - diff: 16.31mlTrain batch 11/32 - 93.3ms/batch - loss: 27.52419 - diff: 16.37mlTrain batch 12/32 - 107.1ms/batch - loss: 29.60990 - diff: 16.97mlTrain batch 13/32 - 113.7ms/batch - loss: 30.96065 - diff: 17.30mlTrain batch 14/32 - 122.2ms/batch - loss: 30.68913 - diff: 17.34mlTrain batch 15/32 - 113.9ms/batch - loss: 29.95724 - diff: 17.18mlTrain batch 16/32 - 127.1ms/batch - loss: 33.95488 - diff: 17.37mlTrain batch 17/32 - 116.2ms/batch - loss: 35.26975 - diff: 17.48mlTrain batch 18/32 - 114.5ms/batch - loss: 37.38229 - diff: 17.99mlTrain batch 19/32 - 106.3ms/batch - loss: 39.29977 - diff: 18.38mlTrain batch 20/32 - 110.3ms/batch - loss: 39.15372 - diff: 18.42mlTrain batch 21/32 - 106.2ms/batch - loss: 39.33244 - diff: 18.40mlTrain batch 22/32 - 105.9ms/batch - loss: 38.29739 - diff: 18.19mlTrain batch 23/32 - 105.0ms/batch - loss: 37.97819 - diff: 18.21mlTrain batch 24/32 - 106.2ms/batch - loss: 37.88592 - diff: 18.21mlTrain batch 25/32 - 105.6ms/batch - loss: 37.43377 - diff: 18.07mlTrain batch 26/32 - 106.4ms/batch - loss: 36.39899 - diff: 17.78mlTrain batch 27/32 - 106.2ms/batch - loss: 35.74546 - diff: 17.57mlTrain batch 28/32 - 106.1ms/batch - loss: 35.86173 - diff: 17.71mlTrain batch 29/32 - 92.8ms/batch - loss: 35.29463 - diff: 17.60mlTrain batch 30/32 - 100.9ms/batch - loss: 34.99064 - diff: 17.53mlTrain batch 31/32 - 91.0ms/batch - loss: 36.69868 - diff: 17.78mlTrain batch 32/32 - 90.4ms/batch - loss: 41.14856 - diff: 17.92mlTrain batch 32/32 - 11.6s 90.4ms/batch - loss: 41.14856 - diff: 17.92ml
Test 0.6s: val_loss: 60.31736 - diff: 21.19ml

Epoch 61: current best loss = 40.94428, at epoch 53
Train batch 1/32 - 121.6ms/batch - loss: 190.71271 - diff: 27.76mlTrain batch 2/32 - 107.1ms/batch - loss: 109.30478 - diff: 22.92mlTrain batch 3/32 - 105.9ms/batch - loss: 81.30264 - diff: 20.78mlTrain batch 4/32 - 107.4ms/batch - loss: 66.04039 - diff: 19.06mlTrain batch 5/32 - 107.1ms/batch - loss: 65.83694 - diff: 20.19mlTrain batch 6/32 - 102.9ms/batch - loss: 59.80516 - diff: 19.63mlTrain batch 7/32 - 102.6ms/batch - loss: 56.65141 - diff: 19.60mlTrain batch 8/32 - 101.3ms/batch - loss: 52.12741 - diff: 18.98mlTrain batch 9/32 - 92.5ms/batch - loss: 52.00024 - diff: 19.46mlTrain batch 10/32 - 112.7ms/batch - loss: 50.03549 - diff: 19.13mlTrain batch 11/32 - 106.6ms/batch - loss: 57.83670 - diff: 20.99mlTrain batch 12/32 - 108.3ms/batch - loss: 56.65173 - diff: 20.99mlTrain batch 13/32 - 104.6ms/batch - loss: 54.42146 - diff: 20.65mlTrain batch 14/32 - 106.8ms/batch - loss: 52.81249 - diff: 20.48mlTrain batch 15/32 - 106.4ms/batch - loss: 51.16625 - diff: 20.11mlTrain batch 16/32 - 106.3ms/batch - loss: 49.83223 - diff: 19.91mlTrain batch 17/32 - 106.6ms/batch - loss: 48.47022 - diff: 19.81mlTrain batch 18/32 - 106.7ms/batch - loss: 47.33531 - diff: 19.59mlTrain batch 19/32 - 106.8ms/batch - loss: 46.35440 - diff: 19.46mlTrain batch 20/32 - 111.4ms/batch - loss: 45.97265 - diff: 19.45mlTrain batch 21/32 - 111.3ms/batch - loss: 45.28208 - diff: 19.33mlTrain batch 22/32 - 106.8ms/batch - loss: 44.60521 - diff: 19.07mlTrain batch 23/32 - 105.8ms/batch - loss: 43.69375 - diff: 18.93mlTrain batch 24/32 - 113.2ms/batch - loss: 49.64539 - diff: 19.23mlTrain batch 25/32 - 113.2ms/batch - loss: 48.44252 - diff: 19.03mlTrain batch 26/32 - 114.2ms/batch - loss: 47.65486 - diff: 18.85mlTrain batch 27/32 - 113.7ms/batch - loss: 46.88435 - diff: 18.69mlTrain batch 28/32 - 114.2ms/batch - loss: 47.59191 - diff: 18.71mlTrain batch 29/32 - 114.4ms/batch - loss: 46.71469 - diff: 18.56mlTrain batch 30/32 - 97.7ms/batch - loss: 45.62808 - diff: 18.28mlTrain batch 31/32 - 88.8ms/batch - loss: 44.70363 - diff: 18.07mlTrain batch 32/32 - 66.5ms/batch - loss: 49.20468 - diff: 18.24mlTrain batch 32/32 - 11.0s 66.5ms/batch - loss: 49.20468 - diff: 18.24ml
Test 0.6s: val_loss: 39.51385 - diff: 17.59ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Systole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 62: current best loss = 39.51385, at epoch 61
Train batch 1/32 - 103.6ms/batch - loss: 28.32316 - diff: 17.11mlTrain batch 2/32 - 97.7ms/batch - loss: 39.97060 - diff: 19.04mlTrain batch 3/32 - 114.8ms/batch - loss: 38.04170 - diff: 19.52mlTrain batch 4/32 - 123.0ms/batch - loss: 34.50962 - diff: 18.66mlTrain batch 5/32 - 114.1ms/batch - loss: 30.68666 - diff: 17.59mlTrain batch 6/32 - 103.6ms/batch - loss: 28.74373 - diff: 16.95mlTrain batch 7/32 - 114.5ms/batch - loss: 27.45200 - diff: 16.75mlTrain batch 8/32 - 110.6ms/batch - loss: 27.40669 - diff: 16.81mlTrain batch 9/32 - 113.1ms/batch - loss: 28.93031 - diff: 17.31mlTrain batch 10/32 - 92.8ms/batch - loss: 28.78984 - diff: 17.27mlTrain batch 11/32 - 100.3ms/batch - loss: 28.40967 - diff: 16.99mlTrain batch 12/32 - 104.0ms/batch - loss: 34.53193 - diff: 18.15mlTrain batch 13/32 - 93.0ms/batch - loss: 36.23626 - diff: 18.39mlTrain batch 14/32 - 97.7ms/batch - loss: 36.22620 - diff: 18.47mlTrain batch 15/32 - 98.2ms/batch - loss: 35.67875 - diff: 18.18mlTrain batch 16/32 - 110.2ms/batch - loss: 35.12975 - diff: 18.19mlTrain batch 17/32 - 113.5ms/batch - loss: 34.30453 - diff: 18.03mlTrain batch 18/32 - 111.5ms/batch - loss: 33.28431 - diff: 17.77mlTrain batch 19/32 - 111.2ms/batch - loss: 32.75149 - diff: 17.66mlTrain batch 20/32 - 111.9ms/batch - loss: 33.77933 - diff: 17.68mlTrain batch 21/32 - 111.5ms/batch - loss: 33.65880 - diff: 17.65mlTrain batch 22/32 - 112.3ms/batch - loss: 32.93210 - diff: 17.46mlTrain batch 23/32 - 111.3ms/batch - loss: 32.46341 - diff: 17.37mlTrain batch 24/32 - 112.6ms/batch - loss: 36.99946 - diff: 17.85mlTrain batch 25/32 - 111.7ms/batch - loss: 36.09240 - diff: 17.63mlTrain batch 26/32 - 112.7ms/batch - loss: 36.16630 - diff: 17.54mlTrain batch 27/32 - 112.3ms/batch - loss: 35.78073 - diff: 17.56mlTrain batch 28/32 - 111.7ms/batch - loss: 34.91023 - diff: 17.32mlTrain batch 29/32 - 112.4ms/batch - loss: 35.83506 - diff: 17.20mlTrain batch 30/32 - 110.9ms/batch - loss: 35.48504 - diff: 17.16mlTrain batch 31/32 - 102.9ms/batch - loss: 36.12029 - diff: 17.30mlTrain batch 32/32 - 83.4ms/batch - loss: 36.57089 - diff: 17.30mlTrain batch 32/32 - 11.3s 83.4ms/batch - loss: 36.57089 - diff: 17.30ml
Test 0.6s: val_loss: 52.47142 - diff: 20.89ml

Epoch 63: current best loss = 39.51385, at epoch 61
Train batch 1/32 - 106.8ms/batch - loss: 33.57487 - diff: 18.40mlTrain batch 2/32 - 96.5ms/batch - loss: 38.13362 - diff: 19.41mlTrain batch 3/32 - 110.2ms/batch - loss: 39.83108 - diff: 19.39mlTrain batch 4/32 - 103.9ms/batch - loss: 39.49900 - diff: 19.50mlTrain batch 5/32 - 104.1ms/batch - loss: 37.42655 - diff: 18.97mlTrain batch 6/32 - 104.4ms/batch - loss: 38.90222 - diff: 19.08mlTrain batch 7/32 - 104.0ms/batch - loss: 37.93574 - diff: 19.03mlTrain batch 8/32 - 103.6ms/batch - loss: 36.14070 - diff: 18.72mlTrain batch 9/32 - 103.3ms/batch - loss: 34.79071 - diff: 18.45mlTrain batch 10/32 - 103.8ms/batch - loss: 39.82468 - diff: 18.68mlTrain batch 11/32 - 103.8ms/batch - loss: 41.78299 - diff: 18.82mlTrain batch 12/32 - 103.4ms/batch - loss: 40.26576 - diff: 18.45mlTrain batch 13/32 - 102.4ms/batch - loss: 40.74538 - diff: 18.52mlTrain batch 14/32 - 103.5ms/batch - loss: 39.46737 - diff: 18.29mlTrain batch 15/32 - 103.5ms/batch - loss: 39.62117 - diff: 18.37mlTrain batch 16/32 - 103.8ms/batch - loss: 37.97701 - diff: 18.00mlTrain batch 17/32 - 107.6ms/batch - loss: 37.52374 - diff: 17.97mlTrain batch 18/32 - 97.1ms/batch - loss: 41.31718 - diff: 18.70mlTrain batch 19/32 - 103.0ms/batch - loss: 45.93991 - diff: 19.04mlTrain batch 20/32 - 103.4ms/batch - loss: 45.33121 - diff: 19.01mlTrain batch 21/32 - 102.5ms/batch - loss: 44.83587 - diff: 18.95mlTrain batch 22/32 - 102.5ms/batch - loss: 43.58168 - diff: 18.64mlTrain batch 23/32 - 102.8ms/batch - loss: 42.52658 - diff: 18.37mlTrain batch 24/32 - 102.4ms/batch - loss: 41.39133 - diff: 18.19mlTrain batch 25/32 - 102.9ms/batch - loss: 40.60433 - diff: 18.04mlTrain batch 26/32 - 103.9ms/batch - loss: 39.71839 - diff: 17.82mlTrain batch 27/32 - 102.6ms/batch - loss: 39.75626 - diff: 17.78mlTrain batch 28/32 - 104.0ms/batch - loss: 39.50375 - diff: 17.81mlTrain batch 29/32 - 102.2ms/batch - loss: 39.63046 - diff: 17.89mlTrain batch 30/32 - 105.3ms/batch - loss: 39.70172 - diff: 17.84mlTrain batch 31/32 - 88.2ms/batch - loss: 39.01890 - diff: 17.69mlTrain batch 32/32 - 66.1ms/batch - loss: 41.63521 - diff: 17.83mlTrain batch 32/32 - 10.8s 66.1ms/batch - loss: 41.63521 - diff: 17.83ml
Test 0.6s: val_loss: 41.25890 - diff: 18.06ml

Epoch 64: current best loss = 39.51385, at epoch 61
Train batch 1/32 - 118.4ms/batch - loss: 46.94738 - diff: 18.69mlTrain batch 2/32 - 107.7ms/batch - loss: 31.42328 - diff: 14.60mlTrain batch 3/32 - 110.5ms/batch - loss: 24.81260 - diff: 13.56mlTrain batch 4/32 - 108.7ms/batch - loss: 22.84926 - diff: 13.29mlTrain batch 5/32 - 112.4ms/batch - loss: 21.05548 - diff: 12.96mlTrain batch 6/32 - 116.1ms/batch - loss: 20.06561 - diff: 12.88mlTrain batch 7/32 - 114.5ms/batch - loss: 18.92963 - diff: 12.75mlTrain batch 8/32 - 113.3ms/batch - loss: 22.46261 - diff: 13.50mlTrain batch 9/32 - 112.5ms/batch - loss: 29.15943 - diff: 14.50mlTrain batch 10/32 - 113.6ms/batch - loss: 28.37914 - diff: 14.60mlTrain batch 11/32 - 112.5ms/batch - loss: 27.50383 - diff: 14.53mlTrain batch 12/32 - 104.2ms/batch - loss: 26.72898 - diff: 14.45mlTrain batch 13/32 - 101.6ms/batch - loss: 25.37575 - diff: 14.08mlTrain batch 14/32 - 89.1ms/batch - loss: 25.89044 - diff: 14.33mlTrain batch 15/32 - 89.9ms/batch - loss: 28.54438 - diff: 15.05mlTrain batch 16/32 - 118.6ms/batch - loss: 30.02470 - diff: 15.37mlTrain batch 17/32 - 108.4ms/batch - loss: 29.98213 - diff: 15.40mlTrain batch 18/32 - 113.1ms/batch - loss: 29.77467 - diff: 15.44mlTrain batch 19/32 - 112.4ms/batch - loss: 30.86181 - diff: 15.68mlTrain batch 20/32 - 120.2ms/batch - loss: 30.21194 - diff: 15.64mlTrain batch 21/32 - 112.5ms/batch - loss: 30.20668 - diff: 15.79mlTrain batch 22/32 - 113.0ms/batch - loss: 30.03574 - diff: 15.81mlTrain batch 23/32 - 112.1ms/batch - loss: 30.04867 - diff: 15.90mlTrain batch 24/32 - 113.2ms/batch - loss: 29.73708 - diff: 15.86mlTrain batch 25/32 - 113.0ms/batch - loss: 28.87446 - diff: 15.60mlTrain batch 26/32 - 114.4ms/batch - loss: 28.48918 - diff: 15.57mlTrain batch 27/32 - 119.3ms/batch - loss: 28.00568 - diff: 15.42mlTrain batch 28/32 - 113.8ms/batch - loss: 28.00893 - diff: 15.45mlTrain batch 29/32 - 113.0ms/batch - loss: 31.64514 - diff: 15.87mlTrain batch 30/32 - 102.1ms/batch - loss: 31.48444 - diff: 15.85mlTrain batch 31/32 - 102.1ms/batch - loss: 30.86735 - diff: 15.71mlTrain batch 32/32 - 71.6ms/batch - loss: 36.00280 - diff: 15.85mlTrain batch 32/32 - 11.7s 71.6ms/batch - loss: 36.00280 - diff: 15.85ml
Test 0.6s: val_loss: 51.08655 - diff: 19.54ml

Epoch 65: current best loss = 39.51385, at epoch 61
Train batch 1/32 - 120.0ms/batch - loss: 44.21514 - diff: 21.92mlTrain batch 2/32 - 108.1ms/batch - loss: 27.90021 - diff: 16.34mlTrain batch 3/32 - 106.4ms/batch - loss: 25.43879 - diff: 15.17mlTrain batch 4/32 - 103.8ms/batch - loss: 23.13505 - diff: 14.76mlTrain batch 5/32 - 102.6ms/batch - loss: 22.49429 - diff: 14.52mlTrain batch 6/32 - 113.6ms/batch - loss: 23.48105 - diff: 14.80mlTrain batch 7/32 - 112.9ms/batch - loss: 23.71563 - diff: 15.07mlTrain batch 8/32 - 113.8ms/batch - loss: 23.02589 - diff: 14.88mlTrain batch 9/32 - 114.2ms/batch - loss: 22.44600 - diff: 14.72mlTrain batch 10/32 - 114.0ms/batch - loss: 25.50340 - diff: 15.53mlTrain batch 11/32 - 112.5ms/batch - loss: 26.29491 - diff: 15.64mlTrain batch 12/32 - 106.9ms/batch - loss: 50.07472 - diff: 17.28mlTrain batch 13/32 - 112.5ms/batch - loss: 47.16659 - diff: 16.89mlTrain batch 14/32 - 107.4ms/batch - loss: 45.60110 - diff: 16.78mlTrain batch 15/32 - 99.7ms/batch - loss: 43.20625 - diff: 16.41mlTrain batch 16/32 - 103.4ms/batch - loss: 41.76675 - diff: 16.31mlTrain batch 17/32 - 104.4ms/batch - loss: 41.24288 - diff: 16.36mlTrain batch 18/32 - 102.5ms/batch - loss: 42.59220 - diff: 17.04mlTrain batch 19/32 - 103.0ms/batch - loss: 40.89092 - diff: 16.74mlTrain batch 20/32 - 106.6ms/batch - loss: 42.36303 - diff: 17.24mlTrain batch 21/32 - 107.1ms/batch - loss: 42.57220 - diff: 17.46mlTrain batch 22/32 - 115.5ms/batch - loss: 41.30506 - diff: 17.24mlTrain batch 23/32 - 116.5ms/batch - loss: 41.61196 - diff: 17.43mlTrain batch 24/32 - 115.8ms/batch - loss: 40.44448 - diff: 17.12mlTrain batch 25/32 - 112.5ms/batch - loss: 41.74093 - diff: 17.47mlTrain batch 26/32 - 106.4ms/batch - loss: 41.37084 - diff: 17.45mlTrain batch 27/32 - 110.3ms/batch - loss: 40.30587 - diff: 17.21mlTrain batch 28/32 - 106.4ms/batch - loss: 39.84081 - diff: 17.07mlTrain batch 29/32 - 102.3ms/batch - loss: 40.70684 - diff: 17.28mlTrain batch 30/32 - 104.4ms/batch - loss: 40.77112 - diff: 17.28mlTrain batch 31/32 - 97.1ms/batch - loss: 40.75275 - diff: 17.30mlTrain batch 32/32 - 94.3ms/batch - loss: 42.73012 - diff: 17.42mlTrain batch 32/32 - 11.4s 94.3ms/batch - loss: 42.73012 - diff: 17.42ml
Test 0.6s: val_loss: 47.72458 - diff: 18.89ml

Epoch 66: current best loss = 39.51385, at epoch 61
Train batch 1/32 - 119.4ms/batch - loss: 31.18196 - diff: 17.96mlTrain batch 2/32 - 103.8ms/batch - loss: 27.00057 - diff: 16.50mlTrain batch 3/32 - 102.8ms/batch - loss: 21.90608 - diff: 13.89mlTrain batch 4/32 - 104.4ms/batch - loss: 21.85339 - diff: 14.23mlTrain batch 5/32 - 110.2ms/batch - loss: 25.60323 - diff: 15.05mlTrain batch 6/32 - 104.3ms/batch - loss: 31.87019 - diff: 16.14mlTrain batch 7/32 - 103.5ms/batch - loss: 33.25041 - diff: 16.33mlTrain batch 8/32 - 110.5ms/batch - loss: 31.54039 - diff: 16.03mlTrain batch 9/32 - 115.4ms/batch - loss: 34.69343 - diff: 17.02mlTrain batch 10/32 - 115.2ms/batch - loss: 33.66504 - diff: 16.77mlTrain batch 11/32 - 116.9ms/batch - loss: 32.04111 - diff: 16.43mlTrain batch 12/32 - 112.1ms/batch - loss: 31.17576 - diff: 16.19mlTrain batch 13/32 - 119.5ms/batch - loss: 31.19193 - diff: 16.22mlTrain batch 14/32 - 104.7ms/batch - loss: 31.29188 - diff: 16.40mlTrain batch 15/32 - 108.3ms/batch - loss: 31.48541 - diff: 16.47mlTrain batch 16/32 - 107.0ms/batch - loss: 31.40958 - diff: 16.58mlTrain batch 17/32 - 110.7ms/batch - loss: 30.35320 - diff: 16.35mlTrain batch 18/32 - 107.2ms/batch - loss: 30.28209 - diff: 16.39mlTrain batch 19/32 - 105.7ms/batch - loss: 30.26369 - diff: 16.32mlTrain batch 20/32 - 106.8ms/batch - loss: 32.25231 - diff: 16.63mlTrain batch 21/32 - 108.5ms/batch - loss: 31.85753 - diff: 16.58mlTrain batch 22/32 - 108.3ms/batch - loss: 36.14068 - diff: 16.97mlTrain batch 23/32 - 107.5ms/batch - loss: 35.26792 - diff: 16.80mlTrain batch 24/32 - 106.4ms/batch - loss: 34.18538 - diff: 16.50mlTrain batch 25/32 - 108.2ms/batch - loss: 33.11442 - diff: 16.15mlTrain batch 26/32 - 105.5ms/batch - loss: 32.52312 - diff: 16.04mlTrain batch 27/32 - 113.3ms/batch - loss: 31.76695 - diff: 15.86mlTrain batch 28/32 - 105.6ms/batch - loss: 32.39446 - diff: 16.01mlTrain batch 29/32 - 108.1ms/batch - loss: 39.35012 - diff: 16.69mlTrain batch 30/32 - 103.1ms/batch - loss: 38.94396 - diff: 16.69mlTrain batch 31/32 - 103.8ms/batch - loss: 38.01212 - diff: 16.50mlTrain batch 32/32 - 86.3ms/batch - loss: 38.47091 - diff: 16.50mlTrain batch 32/32 - 10.7s 86.3ms/batch - loss: 38.47091 - diff: 16.50ml
Test 0.6s: val_loss: 45.29256 - diff: 19.91ml

Epoch 67: current best loss = 39.51385, at epoch 61
Train batch 1/32 - 125.7ms/batch - loss: 11.09427 - diff: 10.94mlTrain batch 2/32 - 103.0ms/batch - loss: 11.82024 - diff: 11.22mlTrain batch 3/32 - 110.4ms/batch - loss: 25.52318 - diff: 13.80mlTrain batch 4/32 - 102.7ms/batch - loss: 23.84220 - diff: 13.66mlTrain batch 5/32 - 109.9ms/batch - loss: 22.57963 - diff: 13.87mlTrain batch 6/32 - 102.9ms/batch - loss: 23.63650 - diff: 14.39mlTrain batch 7/32 - 109.1ms/batch - loss: 22.29557 - diff: 14.18mlTrain batch 8/32 - 101.9ms/batch - loss: 22.50369 - diff: 14.08mlTrain batch 9/32 - 109.7ms/batch - loss: 22.43882 - diff: 14.26mlTrain batch 10/32 - 101.5ms/batch - loss: 23.15764 - diff: 14.69mlTrain batch 11/32 - 111.3ms/batch - loss: 23.32698 - diff: 14.90mlTrain batch 12/32 - 102.5ms/batch - loss: 24.47901 - diff: 15.19mlTrain batch 13/32 - 110.2ms/batch - loss: 25.36768 - diff: 15.29mlTrain batch 14/32 - 102.2ms/batch - loss: 24.40977 - diff: 14.99mlTrain batch 15/32 - 104.0ms/batch - loss: 28.10319 - diff: 15.63mlTrain batch 16/32 - 110.1ms/batch - loss: 27.46542 - diff: 15.49mlTrain batch 17/32 - 107.1ms/batch - loss: 33.61614 - diff: 16.49mlTrain batch 18/32 - 105.3ms/batch - loss: 32.85017 - diff: 16.37mlTrain batch 19/32 - 115.3ms/batch - loss: 32.19536 - diff: 16.22mlTrain batch 20/32 - 105.6ms/batch - loss: 32.68567 - diff: 16.31mlTrain batch 21/32 - 113.5ms/batch - loss: 32.72302 - diff: 16.40mlTrain batch 22/32 - 105.0ms/batch - loss: 31.89597 - diff: 16.21mlTrain batch 23/32 - 113.6ms/batch - loss: 30.92490 - diff: 15.94mlTrain batch 24/32 - 105.1ms/batch - loss: 31.08475 - diff: 15.87mlTrain batch 25/32 - 113.6ms/batch - loss: 30.43225 - diff: 15.73mlTrain batch 26/32 - 105.3ms/batch - loss: 29.95990 - diff: 15.60mlTrain batch 27/32 - 114.3ms/batch - loss: 30.21447 - diff: 15.71mlTrain batch 28/32 - 105.2ms/batch - loss: 29.45466 - diff: 15.50mlTrain batch 29/32 - 114.7ms/batch - loss: 29.30287 - diff: 15.51mlTrain batch 30/32 - 103.5ms/batch - loss: 28.92011 - diff: 15.45mlTrain batch 31/32 - 91.5ms/batch - loss: 29.53469 - diff: 15.69mlTrain batch 32/32 - 73.2ms/batch - loss: 30.44305 - diff: 15.71mlTrain batch 32/32 - 10.8s 73.2ms/batch - loss: 30.44305 - diff: 15.71ml
Test 0.6s: val_loss: 49.45810 - diff: 21.74ml

Epoch 68: current best loss = 39.51385, at epoch 61
Train batch 1/32 - 118.4ms/batch - loss: 15.34932 - diff: 12.26mlTrain batch 2/32 - 103.3ms/batch - loss: 23.14982 - diff: 14.50mlTrain batch 3/32 - 107.2ms/batch - loss: 21.02321 - diff: 14.20mlTrain batch 4/32 - 102.9ms/batch - loss: 19.84755 - diff: 13.23mlTrain batch 5/32 - 106.9ms/batch - loss: 17.14176 - diff: 12.12mlTrain batch 6/32 - 102.3ms/batch - loss: 17.75701 - diff: 12.58mlTrain batch 7/32 - 113.6ms/batch - loss: 17.75294 - diff: 12.68mlTrain batch 8/32 - 116.9ms/batch - loss: 22.00960 - diff: 13.83mlTrain batch 9/32 - 117.6ms/batch - loss: 20.92602 - diff: 13.66mlTrain batch 10/32 - 116.8ms/batch - loss: 22.00092 - diff: 14.31mlTrain batch 11/32 - 106.3ms/batch - loss: 28.98403 - diff: 15.25mlTrain batch 12/32 - 88.2ms/batch - loss: 28.22115 - diff: 14.97mlTrain batch 13/32 - 118.3ms/batch - loss: 26.73065 - diff: 14.62mlTrain batch 14/32 - 112.1ms/batch - loss: 26.67516 - diff: 14.71mlTrain batch 15/32 - 108.7ms/batch - loss: 27.07227 - diff: 15.08mlTrain batch 16/32 - 111.9ms/batch - loss: 27.18853 - diff: 15.01mlTrain batch 17/32 - 107.3ms/batch - loss: 25.89439 - diff: 14.57mlTrain batch 18/32 - 115.6ms/batch - loss: 27.51853 - diff: 14.82mlTrain batch 19/32 - 107.9ms/batch - loss: 26.94637 - diff: 14.68mlTrain batch 20/32 - 103.1ms/batch - loss: 26.05480 - diff: 14.45mlTrain batch 21/32 - 114.5ms/batch - loss: 26.40311 - diff: 14.70mlTrain batch 22/32 - 113.5ms/batch - loss: 27.46973 - diff: 15.08mlTrain batch 23/32 - 116.5ms/batch - loss: 27.25168 - diff: 15.09mlTrain batch 24/32 - 115.7ms/batch - loss: 26.73249 - diff: 14.88mlTrain batch 25/32 - 115.0ms/batch - loss: 26.59755 - diff: 14.82mlTrain batch 26/32 - 113.8ms/batch - loss: 26.73422 - diff: 14.91mlTrain batch 27/32 - 115.1ms/batch - loss: 26.70011 - diff: 14.92mlTrain batch 28/32 - 113.8ms/batch - loss: 27.67621 - diff: 15.09mlTrain batch 29/32 - 114.7ms/batch - loss: 28.39031 - diff: 15.28mlTrain batch 30/32 - 113.7ms/batch - loss: 27.75658 - diff: 15.07mlTrain batch 31/32 - 106.7ms/batch - loss: 27.40444 - diff: 15.04mlTrain batch 32/32 - 69.9ms/batch - loss: 27.71463 - diff: 15.05mlTrain batch 32/32 - 10.7s 69.9ms/batch - loss: 27.71463 - diff: 15.05ml
Test 0.6s: val_loss: 39.71291 - diff: 17.53ml

Epoch 69: current best loss = 39.51385, at epoch 61
Train batch 1/32 - 128.0ms/batch - loss: 9.05027 - diff: 10.15mlTrain batch 2/32 - 104.8ms/batch - loss: 22.13748 - diff: 14.64mlTrain batch 3/32 - 103.1ms/batch - loss: 20.39454 - diff: 14.01mlTrain batch 4/32 - 106.8ms/batch - loss: 19.03456 - diff: 13.61mlTrain batch 5/32 - 109.1ms/batch - loss: 17.71441 - diff: 13.10mlTrain batch 6/32 - 116.2ms/batch - loss: 19.46151 - diff: 13.83mlTrain batch 7/32 - 106.4ms/batch - loss: 19.41048 - diff: 13.75mlTrain batch 8/32 - 107.0ms/batch - loss: 21.30515 - diff: 14.42mlTrain batch 9/32 - 106.9ms/batch - loss: 21.88606 - diff: 14.51mlTrain batch 10/32 - 106.4ms/batch - loss: 21.06216 - diff: 14.19mlTrain batch 11/32 - 106.6ms/batch - loss: 20.03155 - diff: 13.84mlTrain batch 12/32 - 109.6ms/batch - loss: 21.28970 - diff: 14.12mlTrain batch 13/32 - 109.4ms/batch - loss: 20.53543 - diff: 13.87mlTrain batch 14/32 - 94.7ms/batch - loss: 20.12636 - diff: 13.81mlTrain batch 15/32 - 102.8ms/batch - loss: 20.67482 - diff: 14.03mlTrain batch 16/32 - 100.0ms/batch - loss: 20.05663 - diff: 13.86mlTrain batch 17/32 - 88.4ms/batch - loss: 19.80640 - diff: 13.83mlTrain batch 18/32 - 115.2ms/batch - loss: 19.72041 - diff: 13.79mlTrain batch 19/32 - 115.6ms/batch - loss: 21.86869 - diff: 14.43mlTrain batch 20/32 - 116.7ms/batch - loss: 22.92367 - diff: 14.77mlTrain batch 21/32 - 112.0ms/batch - loss: 23.27650 - diff: 14.82mlTrain batch 22/32 - 114.1ms/batch - loss: 24.15606 - diff: 15.10mlTrain batch 23/32 - 112.5ms/batch - loss: 24.09094 - diff: 15.07mlTrain batch 24/32 - 103.4ms/batch - loss: 24.11628 - diff: 15.04mlTrain batch 25/32 - 104.9ms/batch - loss: 23.70932 - diff: 14.92mlTrain batch 26/32 - 103.0ms/batch - loss: 23.73819 - diff: 14.89mlTrain batch 27/32 - 102.9ms/batch - loss: 23.80260 - diff: 14.96mlTrain batch 28/32 - 104.2ms/batch - loss: 23.77101 - diff: 14.95mlTrain batch 29/32 - 103.0ms/batch - loss: 23.57055 - diff: 14.95mlTrain batch 30/32 - 103.6ms/batch - loss: 23.54513 - diff: 14.96mlTrain batch 31/32 - 115.8ms/batch - loss: 23.08619 - diff: 14.78mlTrain batch 32/32 - 99.1ms/batch - loss: 23.51479 - diff: 14.77mlTrain batch 32/32 - 11.7s 99.1ms/batch - loss: 23.51479 - diff: 14.77ml
Test 0.6s: val_loss: 35.83543 - diff: 17.70ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Systole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 70: current best loss = 35.83543, at epoch 69
Train batch 1/32 - 121.5ms/batch - loss: 10.98240 - diff: 10.89mlTrain batch 2/32 - 104.8ms/batch - loss: 8.28474 - diff: 9.07mlTrain batch 3/32 - 105.7ms/batch - loss: 15.66694 - diff: 11.17mlTrain batch 4/32 - 106.4ms/batch - loss: 14.50352 - diff: 10.87mlTrain batch 5/32 - 103.0ms/batch - loss: 16.20924 - diff: 11.54mlTrain batch 6/32 - 102.5ms/batch - loss: 15.42904 - diff: 11.28mlTrain batch 7/32 - 107.2ms/batch - loss: 14.52261 - diff: 11.10mlTrain batch 8/32 - 106.9ms/batch - loss: 15.24461 - diff: 11.45mlTrain batch 9/32 - 103.3ms/batch - loss: 17.37120 - diff: 12.30mlTrain batch 10/32 - 103.0ms/batch - loss: 17.01480 - diff: 12.25mlTrain batch 11/32 - 103.1ms/batch - loss: 16.92198 - diff: 12.32mlTrain batch 12/32 - 106.5ms/batch - loss: 16.81485 - diff: 12.43mlTrain batch 13/32 - 97.5ms/batch - loss: 16.37332 - diff: 12.20mlTrain batch 14/32 - 99.0ms/batch - loss: 16.82046 - diff: 12.42mlTrain batch 15/32 - 99.6ms/batch - loss: 16.52421 - diff: 12.31mlTrain batch 16/32 - 105.7ms/batch - loss: 16.52051 - diff: 12.36mlTrain batch 17/32 - 113.9ms/batch - loss: 16.40839 - diff: 12.26mlTrain batch 18/32 - 113.9ms/batch - loss: 17.63867 - diff: 12.73mlTrain batch 19/32 - 115.5ms/batch - loss: 17.40753 - diff: 12.66mlTrain batch 20/32 - 113.8ms/batch - loss: 17.07354 - diff: 12.54mlTrain batch 21/32 - 115.7ms/batch - loss: 16.52918 - diff: 12.32mlTrain batch 22/32 - 113.9ms/batch - loss: 16.92680 - diff: 12.52mlTrain batch 23/32 - 107.7ms/batch - loss: 16.88606 - diff: 12.52mlTrain batch 24/32 - 107.0ms/batch - loss: 16.56156 - diff: 12.43mlTrain batch 25/32 - 112.8ms/batch - loss: 16.46638 - diff: 12.40mlTrain batch 26/32 - 113.1ms/batch - loss: 16.45858 - diff: 12.39mlTrain batch 27/32 - 101.4ms/batch - loss: 16.48801 - diff: 12.43mlTrain batch 28/32 - 99.8ms/batch - loss: 16.55350 - diff: 12.45mlTrain batch 29/32 - 112.3ms/batch - loss: 16.89798 - diff: 12.52mlTrain batch 30/32 - 113.0ms/batch - loss: 16.72756 - diff: 12.48mlTrain batch 31/32 - 105.9ms/batch - loss: 16.59943 - diff: 12.38mlTrain batch 32/32 - 79.9ms/batch - loss: 16.86333 - diff: 12.37mlTrain batch 32/32 - 11.3s 79.9ms/batch - loss: 16.86333 - diff: 12.37ml
Test 0.6s: val_loss: 37.30359 - diff: 17.36ml

Epoch 71: current best loss = 35.83543, at epoch 69
Train batch 1/32 - 118.0ms/batch - loss: 16.86375 - diff: 12.31mlTrain batch 2/32 - 103.9ms/batch - loss: 12.36685 - diff: 10.71mlTrain batch 3/32 - 104.0ms/batch - loss: 11.23734 - diff: 10.52mlTrain batch 4/32 - 103.8ms/batch - loss: 12.40098 - diff: 11.04mlTrain batch 5/32 - 106.1ms/batch - loss: 16.92061 - diff: 12.49mlTrain batch 6/32 - 112.7ms/batch - loss: 16.71003 - diff: 12.26mlTrain batch 7/32 - 107.6ms/batch - loss: 15.91919 - diff: 12.08mlTrain batch 8/32 - 103.6ms/batch - loss: 19.16522 - diff: 13.04mlTrain batch 9/32 - 103.3ms/batch - loss: 19.05817 - diff: 13.20mlTrain batch 10/32 - 112.7ms/batch - loss: 18.29489 - diff: 13.01mlTrain batch 11/32 - 103.5ms/batch - loss: 17.46207 - diff: 12.56mlTrain batch 12/32 - 112.3ms/batch - loss: 17.00510 - diff: 12.52mlTrain batch 13/32 - 91.6ms/batch - loss: 16.80553 - diff: 12.37mlTrain batch 14/32 - 100.7ms/batch - loss: 19.87955 - diff: 13.04mlTrain batch 15/32 - 109.8ms/batch - loss: 19.22182 - diff: 12.87mlTrain batch 16/32 - 112.1ms/batch - loss: 20.47659 - diff: 13.18mlTrain batch 17/32 - 102.5ms/batch - loss: 21.88844 - diff: 13.66mlTrain batch 18/32 - 112.1ms/batch - loss: 21.23555 - diff: 13.46mlTrain batch 19/32 - 103.8ms/batch - loss: 20.87720 - diff: 13.41mlTrain batch 20/32 - 104.2ms/batch - loss: 20.41389 - diff: 13.29mlTrain batch 21/32 - 107.6ms/batch - loss: 21.49942 - diff: 13.76mlTrain batch 22/32 - 102.3ms/batch - loss: 21.30605 - diff: 13.65mlTrain batch 23/32 - 103.6ms/batch - loss: 20.96863 - diff: 13.57mlTrain batch 24/32 - 110.7ms/batch - loss: 21.05948 - diff: 13.64mlTrain batch 25/32 - 103.2ms/batch - loss: 21.15203 - diff: 13.62mlTrain batch 26/32 - 109.7ms/batch - loss: 21.28622 - diff: 13.69mlTrain batch 27/32 - 104.0ms/batch - loss: 21.14532 - diff: 13.68mlTrain batch 28/32 - 110.0ms/batch - loss: 22.04966 - diff: 13.80mlTrain batch 29/32 - 102.7ms/batch - loss: 21.59500 - diff: 13.63mlTrain batch 30/32 - 103.0ms/batch - loss: 22.33736 - diff: 13.95mlTrain batch 31/32 - 98.1ms/batch - loss: 21.91213 - diff: 13.81mlTrain batch 32/32 - 80.9ms/batch - loss: 24.44015 - diff: 13.96mlTrain batch 32/32 - 11.4s 80.9ms/batch - loss: 24.44015 - diff: 13.96ml
Test 0.6s: val_loss: 31.70499 - diff: 16.18ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Systole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 72: current best loss = 31.70499, at epoch 71
Train batch 1/32 - 121.9ms/batch - loss: 19.71024 - diff: 14.11mlTrain batch 2/32 - 108.1ms/batch - loss: 15.68707 - diff: 11.88mlTrain batch 3/32 - 107.1ms/batch - loss: 17.23686 - diff: 12.15mlTrain batch 4/32 - 107.8ms/batch - loss: 19.10289 - diff: 12.74mlTrain batch 5/32 - 114.6ms/batch - loss: 16.76275 - diff: 11.91mlTrain batch 6/32 - 114.3ms/batch - loss: 18.78994 - diff: 12.72mlTrain batch 7/32 - 117.4ms/batch - loss: 17.25178 - diff: 12.17mlTrain batch 8/32 - 103.9ms/batch - loss: 16.64985 - diff: 12.29mlTrain batch 9/32 - 97.1ms/batch - loss: 15.99514 - diff: 11.92mlTrain batch 10/32 - 103.7ms/batch - loss: 15.40538 - diff: 11.84mlTrain batch 11/32 - 108.3ms/batch - loss: 15.32602 - diff: 11.91mlTrain batch 12/32 - 101.4ms/batch - loss: 15.57007 - diff: 12.13mlTrain batch 13/32 - 104.6ms/batch - loss: 14.78542 - diff: 11.76mlTrain batch 14/32 - 110.9ms/batch - loss: 15.30270 - diff: 11.93mlTrain batch 15/32 - 104.6ms/batch - loss: 15.83058 - diff: 12.17mlTrain batch 16/32 - 114.4ms/batch - loss: 16.66752 - diff: 12.48mlTrain batch 17/32 - 103.0ms/batch - loss: 16.39268 - diff: 12.49mlTrain batch 18/32 - 107.4ms/batch - loss: 16.02148 - diff: 12.37mlTrain batch 19/32 - 103.7ms/batch - loss: 18.52889 - diff: 12.76mlTrain batch 20/32 - 103.2ms/batch - loss: 19.12088 - diff: 12.97mlTrain batch 21/32 - 103.1ms/batch - loss: 19.41968 - diff: 13.01mlTrain batch 22/32 - 103.9ms/batch - loss: 19.80050 - diff: 13.22mlTrain batch 23/32 - 103.7ms/batch - loss: 19.32469 - diff: 13.06mlTrain batch 24/32 - 103.8ms/batch - loss: 19.50213 - diff: 13.19mlTrain batch 25/32 - 103.2ms/batch - loss: 19.55510 - diff: 13.21mlTrain batch 26/32 - 107.7ms/batch - loss: 19.90087 - diff: 13.34mlTrain batch 27/32 - 102.6ms/batch - loss: 20.69917 - diff: 13.65mlTrain batch 28/32 - 111.2ms/batch - loss: 21.24210 - diff: 13.85mlTrain batch 29/32 - 105.9ms/batch - loss: 20.95236 - diff: 13.78mlTrain batch 30/32 - 105.5ms/batch - loss: 20.70924 - diff: 13.72mlTrain batch 31/32 - 88.6ms/batch - loss: 20.56592 - diff: 13.68mlTrain batch 32/32 - 73.0ms/batch - loss: 21.15051 - diff: 13.69mlTrain batch 32/32 - 11.5s 73.0ms/batch - loss: 21.15051 - diff: 13.69ml
Test 0.6s: val_loss: 38.35872 - diff: 16.62ml

Epoch 73: current best loss = 31.70499, at epoch 71
Train batch 1/32 - 122.3ms/batch - loss: 18.39024 - diff: 14.61mlTrain batch 2/32 - 104.6ms/batch - loss: 14.38369 - diff: 12.36mlTrain batch 3/32 - 106.9ms/batch - loss: 13.03909 - diff: 11.70mlTrain batch 4/32 - 103.3ms/batch - loss: 14.56527 - diff: 12.68mlTrain batch 5/32 - 104.5ms/batch - loss: 16.24147 - diff: 13.20mlTrain batch 6/32 - 102.8ms/batch - loss: 16.81094 - diff: 13.11mlTrain batch 7/32 - 103.0ms/batch - loss: 16.75136 - diff: 13.14mlTrain batch 8/32 - 102.7ms/batch - loss: 16.06610 - diff: 12.75mlTrain batch 9/32 - 106.6ms/batch - loss: 15.41117 - diff: 12.34mlTrain batch 10/32 - 105.2ms/batch - loss: 19.10273 - diff: 13.80mlTrain batch 11/32 - 105.5ms/batch - loss: 19.57278 - diff: 13.96mlTrain batch 12/32 - 109.0ms/batch - loss: 20.52626 - diff: 14.25mlTrain batch 13/32 - 103.0ms/batch - loss: 20.59064 - diff: 14.03mlTrain batch 14/32 - 112.8ms/batch - loss: 20.38645 - diff: 14.05mlTrain batch 15/32 - 109.6ms/batch - loss: 20.33290 - diff: 14.03mlTrain batch 16/32 - 104.4ms/batch - loss: 19.76073 - diff: 13.88mlTrain batch 17/32 - 113.1ms/batch - loss: 20.04651 - diff: 13.89mlTrain batch 18/32 - 113.2ms/batch - loss: 19.50587 - diff: 13.66mlTrain batch 19/32 - 116.7ms/batch - loss: 19.15358 - diff: 13.57mlTrain batch 20/32 - 117.4ms/batch - loss: 18.77927 - diff: 13.46mlTrain batch 21/32 - 109.1ms/batch - loss: 19.21368 - diff: 13.69mlTrain batch 22/32 - 117.6ms/batch - loss: 19.06717 - diff: 13.61mlTrain batch 23/32 - 118.7ms/batch - loss: 19.92439 - diff: 13.63mlTrain batch 24/32 - 117.4ms/batch - loss: 19.69345 - diff: 13.64mlTrain batch 25/32 - 113.6ms/batch - loss: 19.36049 - diff: 13.51mlTrain batch 26/32 - 117.8ms/batch - loss: 20.25809 - diff: 13.69mlTrain batch 27/32 - 117.1ms/batch - loss: 20.16985 - diff: 13.68mlTrain batch 28/32 - 116.5ms/batch - loss: 19.75560 - diff: 13.52mlTrain batch 29/32 - 117.3ms/batch - loss: 19.32649 - diff: 13.33mlTrain batch 30/32 - 113.8ms/batch - loss: 19.33560 - diff: 13.38mlTrain batch 31/32 - 113.4ms/batch - loss: 19.17817 - diff: 13.35mlTrain batch 32/32 - 104.8ms/batch - loss: 19.17403 - diff: 13.30mlTrain batch 32/32 - 11.0s 104.8ms/batch - loss: 19.17403 - diff: 13.30ml
Test 0.6s: val_loss: 31.80896 - diff: 16.30ml

Epoch 74: current best loss = 31.70499, at epoch 71
Train batch 1/32 - 121.6ms/batch - loss: 30.50889 - diff: 19.27mlTrain batch 2/32 - 113.4ms/batch - loss: 20.37633 - diff: 14.94mlTrain batch 3/32 - 113.4ms/batch - loss: 19.69976 - diff: 14.20mlTrain batch 4/32 - 115.4ms/batch - loss: 18.20662 - diff: 13.51mlTrain batch 5/32 - 114.9ms/batch - loss: 15.72647 - diff: 12.42mlTrain batch 6/32 - 97.3ms/batch - loss: 15.12445 - diff: 11.95mlTrain batch 7/32 - 107.3ms/batch - loss: 15.22460 - diff: 12.01mlTrain batch 8/32 - 106.7ms/batch - loss: 15.73233 - diff: 12.13mlTrain batch 9/32 - 107.0ms/batch - loss: 14.88235 - diff: 11.76mlTrain batch 10/32 - 106.9ms/batch - loss: 14.44885 - diff: 11.70mlTrain batch 11/32 - 106.3ms/batch - loss: 14.17930 - diff: 11.65mlTrain batch 12/32 - 106.1ms/batch - loss: 15.33785 - diff: 12.18mlTrain batch 13/32 - 106.1ms/batch - loss: 15.63164 - diff: 12.38mlTrain batch 14/32 - 107.4ms/batch - loss: 19.83379 - diff: 13.12mlTrain batch 15/32 - 107.4ms/batch - loss: 19.20847 - diff: 13.01mlTrain batch 16/32 - 107.0ms/batch - loss: 19.03465 - diff: 12.96mlTrain batch 17/32 - 97.2ms/batch - loss: 18.57521 - diff: 12.83mlTrain batch 18/32 - 107.2ms/batch - loss: 18.42140 - diff: 12.79mlTrain batch 19/32 - 105.3ms/batch - loss: 17.87430 - diff: 12.62mlTrain batch 20/32 - 106.4ms/batch - loss: 17.85463 - diff: 12.62mlTrain batch 21/32 - 107.0ms/batch - loss: 17.77100 - diff: 12.62mlTrain batch 22/32 - 106.6ms/batch - loss: 17.30582 - diff: 12.45mlTrain batch 23/32 - 109.4ms/batch - loss: 16.94744 - diff: 12.34mlTrain batch 24/32 - 107.9ms/batch - loss: 17.00694 - diff: 12.48mlTrain batch 25/32 - 103.8ms/batch - loss: 16.92692 - diff: 12.43mlTrain batch 26/32 - 101.7ms/batch - loss: 16.82272 - diff: 12.39mlTrain batch 27/32 - 102.9ms/batch - loss: 16.39575 - diff: 12.21mlTrain batch 28/32 - 104.7ms/batch - loss: 16.53778 - diff: 12.31mlTrain batch 29/32 - 102.4ms/batch - loss: 16.63938 - diff: 12.26mlTrain batch 30/32 - 102.3ms/batch - loss: 16.65757 - diff: 12.28mlTrain batch 31/32 - 103.1ms/batch - loss: 16.74403 - diff: 12.35mlTrain batch 32/32 - 94.2ms/batch - loss: 17.44856 - diff: 12.38mlTrain batch 32/32 - 11.7s 94.2ms/batch - loss: 17.44856 - diff: 12.38ml
Test 0.6s: val_loss: 28.27297 - diff: 15.17ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Systole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 75: current best loss = 28.27297, at epoch 74
Train batch 1/32 - 127.0ms/batch - loss: 15.12736 - diff: 12.30mlTrain batch 2/32 - 113.6ms/batch - loss: 10.82957 - diff: 9.95mlTrain batch 3/32 - 115.1ms/batch - loss: 10.86897 - diff: 10.12mlTrain batch 4/32 - 115.2ms/batch - loss: 36.34197 - diff: 16.60mlTrain batch 5/32 - 113.5ms/batch - loss: 31.29073 - diff: 15.34mlTrain batch 6/32 - 108.7ms/batch - loss: 27.80716 - diff: 14.50mlTrain batch 7/32 - 108.0ms/batch - loss: 25.74794 - diff: 14.29mlTrain batch 8/32 - 114.8ms/batch - loss: 23.55435 - diff: 13.72mlTrain batch 9/32 - 106.7ms/batch - loss: 23.40529 - diff: 13.38mlTrain batch 10/32 - 107.1ms/batch - loss: 23.48226 - diff: 13.36mlTrain batch 11/32 - 120.3ms/batch - loss: 22.38610 - diff: 13.00mlTrain batch 12/32 - 106.5ms/batch - loss: 21.68825 - diff: 12.85mlTrain batch 13/32 - 119.3ms/batch - loss: 20.71197 - diff: 12.60mlTrain batch 14/32 - 113.6ms/batch - loss: 20.52519 - diff: 12.70mlTrain batch 15/32 - 107.1ms/batch - loss: 20.68671 - diff: 12.89mlTrain batch 16/32 - 102.5ms/batch - loss: 21.63401 - diff: 13.20mlTrain batch 17/32 - 104.2ms/batch - loss: 23.35092 - diff: 13.79mlTrain batch 18/32 - 110.2ms/batch - loss: 22.94665 - diff: 13.68mlTrain batch 19/32 - 104.2ms/batch - loss: 22.14149 - diff: 13.40mlTrain batch 20/32 - 111.7ms/batch - loss: 21.78758 - diff: 13.32mlTrain batch 21/32 - 112.3ms/batch - loss: 21.60803 - diff: 13.33mlTrain batch 22/32 - 103.9ms/batch - loss: 22.04391 - diff: 13.42mlTrain batch 23/32 - 105.2ms/batch - loss: 21.86187 - diff: 13.47mlTrain batch 24/32 - 110.1ms/batch - loss: 22.48239 - diff: 13.64mlTrain batch 25/32 - 103.5ms/batch - loss: 22.11502 - diff: 13.56mlTrain batch 26/32 - 111.3ms/batch - loss: 22.00640 - diff: 13.54mlTrain batch 27/32 - 112.5ms/batch - loss: 21.92429 - diff: 13.50mlTrain batch 28/32 - 119.3ms/batch - loss: 23.43440 - diff: 13.88mlTrain batch 29/32 - 107.2ms/batch - loss: 23.39279 - diff: 13.90mlTrain batch 30/32 - 94.6ms/batch - loss: 22.96647 - diff: 13.74mlTrain batch 31/32 - 98.4ms/batch - loss: 23.36990 - diff: 13.84mlTrain batch 32/32 - 96.7ms/batch - loss: 25.77790 - diff: 13.98mlTrain batch 32/32 - 11.6s 96.7ms/batch - loss: 25.77790 - diff: 13.98ml
Test 0.7s: val_loss: 26.71204 - diff: 15.12ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Systole_DenseNet121_0_Adam-0.001_MSE_DA3_best

Epoch 76: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 118.8ms/batch - loss: 30.08922 - diff: 16.79mlTrain batch 2/32 - 104.8ms/batch - loss: 26.96399 - diff: 14.62mlTrain batch 3/32 - 105.3ms/batch - loss: 22.22444 - diff: 13.55mlTrain batch 4/32 - 104.7ms/batch - loss: 20.93630 - diff: 13.92mlTrain batch 5/32 - 105.7ms/batch - loss: 22.06858 - diff: 14.35mlTrain batch 6/32 - 104.6ms/batch - loss: 19.79085 - diff: 13.64mlTrain batch 7/32 - 105.3ms/batch - loss: 19.06167 - diff: 13.20mlTrain batch 8/32 - 102.3ms/batch - loss: 18.02638 - diff: 12.93mlTrain batch 9/32 - 123.0ms/batch - loss: 18.06816 - diff: 12.97mlTrain batch 10/32 - 105.5ms/batch - loss: 18.26934 - diff: 12.97mlTrain batch 11/32 - 107.0ms/batch - loss: 17.26023 - diff: 12.57mlTrain batch 12/32 - 103.9ms/batch - loss: 17.02271 - diff: 12.47mlTrain batch 13/32 - 104.0ms/batch - loss: 16.47741 - diff: 12.24mlTrain batch 14/32 - 103.7ms/batch - loss: 16.73852 - diff: 12.47mlTrain batch 15/32 - 109.2ms/batch - loss: 16.72786 - diff: 12.51mlTrain batch 16/32 - 107.7ms/batch - loss: 16.74608 - diff: 12.57mlTrain batch 17/32 - 108.8ms/batch - loss: 17.66035 - diff: 12.75mlTrain batch 18/32 - 107.8ms/batch - loss: 17.25750 - diff: 12.67mlTrain batch 19/32 - 108.7ms/batch - loss: 17.11906 - diff: 12.64mlTrain batch 20/32 - 113.6ms/batch - loss: 20.51785 - diff: 13.29mlTrain batch 21/32 - 109.9ms/batch - loss: 19.97293 - diff: 13.15mlTrain batch 22/32 - 110.9ms/batch - loss: 19.27448 - diff: 12.90mlTrain batch 23/32 - 109.1ms/batch - loss: 19.03244 - diff: 12.89mlTrain batch 24/32 - 110.0ms/batch - loss: 18.86221 - diff: 12.91mlTrain batch 25/32 - 106.6ms/batch - loss: 19.28823 - diff: 13.02mlTrain batch 26/32 - 112.9ms/batch - loss: 19.10442 - diff: 13.02mlTrain batch 27/32 - 103.2ms/batch - loss: 19.19721 - diff: 13.19mlTrain batch 28/32 - 104.2ms/batch - loss: 19.56808 - diff: 13.30mlTrain batch 29/32 - 103.6ms/batch - loss: 19.21649 - diff: 13.17mlTrain batch 30/32 - 105.2ms/batch - loss: 18.91075 - diff: 13.03mlTrain batch 31/32 - 90.3ms/batch - loss: 18.80912 - diff: 12.97mlTrain batch 32/32 - 75.9ms/batch - loss: 19.02144 - diff: 12.97mlTrain batch 32/32 - 10.7s 75.9ms/batch - loss: 19.02144 - diff: 12.97ml
Test 0.6s: val_loss: 34.73868 - diff: 16.37ml

Epoch 77: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 136.4ms/batch - loss: 8.43093 - diff: 9.95mlTrain batch 2/32 - 114.2ms/batch - loss: 13.65587 - diff: 12.26mlTrain batch 3/32 - 121.5ms/batch - loss: 14.10704 - diff: 12.42mlTrain batch 4/32 - 114.8ms/batch - loss: 16.78119 - diff: 12.60mlTrain batch 5/32 - 114.9ms/batch - loss: 15.71435 - diff: 12.12mlTrain batch 6/32 - 106.7ms/batch - loss: 18.56079 - diff: 13.61mlTrain batch 7/32 - 117.3ms/batch - loss: 17.45214 - diff: 13.10mlTrain batch 8/32 - 116.5ms/batch - loss: 18.84937 - diff: 13.51mlTrain batch 9/32 - 121.9ms/batch - loss: 19.30252 - diff: 13.67mlTrain batch 10/32 - 116.8ms/batch - loss: 19.06290 - diff: 13.76mlTrain batch 11/32 - 127.5ms/batch - loss: 18.48292 - diff: 13.56mlTrain batch 12/32 - 116.8ms/batch - loss: 18.58398 - diff: 13.62mlTrain batch 13/32 - 109.7ms/batch - loss: 18.63934 - diff: 13.48mlTrain batch 14/32 - 116.8ms/batch - loss: 18.40498 - diff: 13.35mlTrain batch 15/32 - 117.7ms/batch - loss: 17.97608 - diff: 13.21mlTrain batch 16/32 - 116.5ms/batch - loss: 18.33261 - diff: 13.44mlTrain batch 17/32 - 117.3ms/batch - loss: 17.71736 - diff: 13.21mlTrain batch 18/32 - 110.5ms/batch - loss: 17.64483 - diff: 13.21mlTrain batch 19/32 - 109.4ms/batch - loss: 17.24855 - diff: 13.08mlTrain batch 20/32 - 116.6ms/batch - loss: 26.93280 - diff: 14.46mlTrain batch 21/32 - 107.9ms/batch - loss: 25.91013 - diff: 14.15mlTrain batch 22/32 - 105.6ms/batch - loss: 25.29917 - diff: 13.99mlTrain batch 23/32 - 107.0ms/batch - loss: 26.14774 - diff: 14.37mlTrain batch 24/32 - 106.6ms/batch - loss: 26.04579 - diff: 14.41mlTrain batch 25/32 - 108.1ms/batch - loss: 26.51319 - diff: 14.62mlTrain batch 26/32 - 108.8ms/batch - loss: 26.34541 - diff: 14.62mlTrain batch 27/32 - 112.3ms/batch - loss: 26.28794 - diff: 14.66mlTrain batch 28/32 - 94.0ms/batch - loss: 25.99097 - diff: 14.61mlTrain batch 29/32 - 91.7ms/batch - loss: 26.20291 - diff: 14.76mlTrain batch 30/32 - 112.7ms/batch - loss: 25.72697 - diff: 14.57mlTrain batch 31/32 - 106.5ms/batch - loss: 25.32608 - diff: 14.48mlTrain batch 32/32 - 73.4ms/batch - loss: 25.92189 - diff: 14.49mlTrain batch 32/32 - 11.9s 73.4ms/batch - loss: 25.92189 - diff: 14.49ml
Test 0.6s: val_loss: 81.85233 - diff: 27.88ml

Epoch 78: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 116.9ms/batch - loss: 8.46846 - diff: 9.12mlTrain batch 2/32 - 103.0ms/batch - loss: 14.20410 - diff: 12.02mlTrain batch 3/32 - 113.7ms/batch - loss: 14.00392 - diff: 11.96mlTrain batch 4/32 - 115.1ms/batch - loss: 15.95506 - diff: 12.65mlTrain batch 5/32 - 112.5ms/batch - loss: 19.15573 - diff: 13.41mlTrain batch 6/32 - 95.9ms/batch - loss: 17.74508 - diff: 13.00mlTrain batch 7/32 - 114.2ms/batch - loss: 20.78184 - diff: 13.99mlTrain batch 8/32 - 113.4ms/batch - loss: 20.34925 - diff: 13.99mlTrain batch 9/32 - 112.6ms/batch - loss: 20.47565 - diff: 14.04mlTrain batch 10/32 - 112.6ms/batch - loss: 20.04545 - diff: 14.01mlTrain batch 11/32 - 103.6ms/batch - loss: 19.31950 - diff: 13.81mlTrain batch 12/32 - 102.1ms/batch - loss: 22.69532 - diff: 14.35mlTrain batch 13/32 - 103.6ms/batch - loss: 22.14084 - diff: 14.27mlTrain batch 14/32 - 104.2ms/batch - loss: 23.23142 - diff: 14.50mlTrain batch 15/32 - 103.1ms/batch - loss: 23.18180 - diff: 14.51mlTrain batch 16/32 - 103.8ms/batch - loss: 22.69691 - diff: 14.45mlTrain batch 17/32 - 103.1ms/batch - loss: 22.13553 - diff: 14.33mlTrain batch 18/32 - 102.4ms/batch - loss: 22.70873 - diff: 14.36mlTrain batch 19/32 - 103.8ms/batch - loss: 22.58806 - diff: 14.40mlTrain batch 20/32 - 102.0ms/batch - loss: 26.01012 - diff: 14.70mlTrain batch 21/32 - 102.4ms/batch - loss: 25.20617 - diff: 14.43mlTrain batch 22/32 - 102.8ms/batch - loss: 25.00374 - diff: 14.39mlTrain batch 23/32 - 103.9ms/batch - loss: 26.31687 - diff: 14.88mlTrain batch 24/32 - 103.3ms/batch - loss: 26.06723 - diff: 14.84mlTrain batch 25/32 - 103.9ms/batch - loss: 25.48357 - diff: 14.68mlTrain batch 26/32 - 105.4ms/batch - loss: 24.98543 - diff: 14.60mlTrain batch 27/32 - 103.1ms/batch - loss: 24.69897 - diff: 14.55mlTrain batch 28/32 - 102.8ms/batch - loss: 25.19811 - diff: 14.67mlTrain batch 29/32 - 99.4ms/batch - loss: 25.03530 - diff: 14.57mlTrain batch 30/32 - 87.9ms/batch - loss: 24.54913 - diff: 14.40mlTrain batch 31/32 - 92.4ms/batch - loss: 23.98028 - diff: 14.20mlTrain batch 32/32 - 79.9ms/batch - loss: 24.44124 - diff: 14.21mlTrain batch 32/32 - 10.9s 79.9ms/batch - loss: 24.44124 - diff: 14.21ml
Test 0.6s: val_loss: 32.10850 - diff: 15.74ml

Epoch 79: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 122.3ms/batch - loss: 5.53006 - diff: 7.12mlTrain batch 2/32 - 108.8ms/batch - loss: 21.05442 - diff: 12.04mlTrain batch 3/32 - 92.7ms/batch - loss: 17.38390 - diff: 11.67mlTrain batch 4/32 - 108.4ms/batch - loss: 15.36911 - diff: 11.26mlTrain batch 5/32 - 108.7ms/batch - loss: 16.64192 - diff: 11.79mlTrain batch 6/32 - 108.9ms/batch - loss: 16.21086 - diff: 11.85mlTrain batch 7/32 - 105.0ms/batch - loss: 17.71349 - diff: 12.12mlTrain batch 8/32 - 115.9ms/batch - loss: 18.73469 - diff: 12.35mlTrain batch 9/32 - 115.7ms/batch - loss: 18.14789 - diff: 12.33mlTrain batch 10/32 - 118.9ms/batch - loss: 17.93140 - diff: 12.45mlTrain batch 11/32 - 117.1ms/batch - loss: 17.15609 - diff: 12.22mlTrain batch 12/32 - 117.3ms/batch - loss: 17.34160 - diff: 12.46mlTrain batch 13/32 - 111.6ms/batch - loss: 17.24184 - diff: 12.41mlTrain batch 14/32 - 106.1ms/batch - loss: 16.98756 - diff: 12.40mlTrain batch 15/32 - 107.0ms/batch - loss: 16.45811 - diff: 12.14mlTrain batch 16/32 - 106.5ms/batch - loss: 18.37138 - diff: 12.51mlTrain batch 17/32 - 105.7ms/batch - loss: 19.49025 - diff: 12.77mlTrain batch 18/32 - 94.3ms/batch - loss: 19.07508 - diff: 12.73mlTrain batch 19/32 - 104.8ms/batch - loss: 19.08705 - diff: 12.79mlTrain batch 20/32 - 103.9ms/batch - loss: 18.95727 - diff: 12.69mlTrain batch 21/32 - 107.5ms/batch - loss: 19.20463 - diff: 12.82mlTrain batch 22/32 - 114.7ms/batch - loss: 19.07978 - diff: 12.84mlTrain batch 23/32 - 111.5ms/batch - loss: 20.41803 - diff: 13.33mlTrain batch 24/32 - 114.8ms/batch - loss: 20.05168 - diff: 13.27mlTrain batch 25/32 - 116.2ms/batch - loss: 19.65634 - diff: 13.13mlTrain batch 26/32 - 114.0ms/batch - loss: 19.44731 - diff: 13.11mlTrain batch 27/32 - 115.8ms/batch - loss: 19.86296 - diff: 13.09mlTrain batch 28/32 - 103.0ms/batch - loss: 19.67228 - diff: 13.08mlTrain batch 29/32 - 98.3ms/batch - loss: 19.73145 - diff: 13.15mlTrain batch 30/32 - 102.5ms/batch - loss: 20.91640 - diff: 13.43mlTrain batch 31/32 - 88.9ms/batch - loss: 20.99467 - diff: 13.51mlTrain batch 32/32 - 82.3ms/batch - loss: 21.73385 - diff: 13.54mlTrain batch 32/32 - 11.0s 82.3ms/batch - loss: 21.73385 - diff: 13.54ml
Test 0.6s: val_loss: 48.71030 - diff: 20.19ml

Epoch 80: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 120.5ms/batch - loss: 22.04016 - diff: 12.68mlTrain batch 2/32 - 107.2ms/batch - loss: 28.07224 - diff: 14.09mlTrain batch 3/32 - 107.0ms/batch - loss: 20.73471 - diff: 11.90mlTrain batch 4/32 - 99.0ms/batch - loss: 19.47523 - diff: 12.14mlTrain batch 5/32 - 103.3ms/batch - loss: 40.16274 - diff: 13.45mlTrain batch 6/32 - 103.5ms/batch - loss: 38.95402 - diff: 14.54mlTrain batch 7/32 - 105.3ms/batch - loss: 35.48521 - diff: 14.31mlTrain batch 8/32 - 114.6ms/batch - loss: 32.86973 - diff: 14.17mlTrain batch 9/32 - 115.1ms/batch - loss: 33.08305 - diff: 14.44mlTrain batch 10/32 - 116.5ms/batch - loss: 34.65796 - diff: 15.26mlTrain batch 11/32 - 116.2ms/batch - loss: 33.33920 - diff: 15.19mlTrain batch 12/32 - 101.1ms/batch - loss: 31.98484 - diff: 15.06mlTrain batch 13/32 - 104.4ms/batch - loss: 30.98091 - diff: 14.98mlTrain batch 14/32 - 104.3ms/batch - loss: 29.98477 - diff: 14.89mlTrain batch 15/32 - 104.7ms/batch - loss: 28.84111 - diff: 14.66mlTrain batch 16/32 - 104.4ms/batch - loss: 29.71034 - diff: 15.03mlTrain batch 17/32 - 110.2ms/batch - loss: 29.52644 - diff: 15.18mlTrain batch 18/32 - 108.4ms/batch - loss: 28.32354 - diff: 14.86mlTrain batch 19/32 - 115.5ms/batch - loss: 27.45086 - diff: 14.70mlTrain batch 20/32 - 115.7ms/batch - loss: 27.30484 - diff: 14.72mlTrain batch 21/32 - 116.1ms/batch - loss: 28.38391 - diff: 15.04mlTrain batch 22/32 - 115.3ms/batch - loss: 27.91769 - diff: 14.96mlTrain batch 23/32 - 115.3ms/batch - loss: 27.55560 - diff: 14.85mlTrain batch 24/32 - 114.9ms/batch - loss: 27.04107 - diff: 14.75mlTrain batch 25/32 - 116.9ms/batch - loss: 26.75416 - diff: 14.70mlTrain batch 26/32 - 115.4ms/batch - loss: 26.61300 - diff: 14.71mlTrain batch 27/32 - 114.8ms/batch - loss: 26.08639 - diff: 14.60mlTrain batch 28/32 - 105.9ms/batch - loss: 27.70242 - diff: 14.84mlTrain batch 29/32 - 104.3ms/batch - loss: 27.72668 - diff: 14.89mlTrain batch 30/32 - 104.8ms/batch - loss: 26.99658 - diff: 14.67mlTrain batch 31/32 - 95.7ms/batch - loss: 26.90015 - diff: 14.60mlTrain batch 32/32 - 80.9ms/batch - loss: 27.04701 - diff: 14.57mlTrain batch 32/32 - 11.1s 80.9ms/batch - loss: 27.04701 - diff: 14.57ml
Test 0.6s: val_loss: 40.53397 - diff: 17.86ml

Epoch 81: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 121.7ms/batch - loss: 4.55829 - diff: 6.92mlTrain batch 2/32 - 115.0ms/batch - loss: 23.75189 - diff: 13.38mlTrain batch 3/32 - 108.0ms/batch - loss: 21.42747 - diff: 13.12mlTrain batch 4/32 - 108.7ms/batch - loss: 19.46904 - diff: 12.47mlTrain batch 5/32 - 109.6ms/batch - loss: 21.86126 - diff: 13.52mlTrain batch 6/32 - 105.1ms/batch - loss: 19.88389 - diff: 12.97mlTrain batch 7/32 - 105.4ms/batch - loss: 19.35106 - diff: 12.84mlTrain batch 8/32 - 104.8ms/batch - loss: 20.09111 - diff: 13.27mlTrain batch 9/32 - 104.4ms/batch - loss: 30.80891 - diff: 14.71mlTrain batch 10/32 - 105.3ms/batch - loss: 29.06459 - diff: 14.44mlTrain batch 11/32 - 104.3ms/batch - loss: 27.74155 - diff: 14.26mlTrain batch 12/32 - 101.6ms/batch - loss: 26.98598 - diff: 14.26mlTrain batch 13/32 - 104.8ms/batch - loss: 25.61417 - diff: 13.90mlTrain batch 14/32 - 95.1ms/batch - loss: 24.62187 - diff: 13.74mlTrain batch 15/32 - 105.3ms/batch - loss: 24.78738 - diff: 13.88mlTrain batch 16/32 - 111.7ms/batch - loss: 23.97606 - diff: 13.74mlTrain batch 17/32 - 114.9ms/batch - loss: 23.84201 - diff: 13.78mlTrain batch 18/32 - 115.6ms/batch - loss: 24.75829 - diff: 14.04mlTrain batch 19/32 - 114.3ms/batch - loss: 24.39549 - diff: 13.96mlTrain batch 20/32 - 117.1ms/batch - loss: 25.39466 - diff: 14.16mlTrain batch 21/32 - 106.7ms/batch - loss: 25.23739 - diff: 14.16mlTrain batch 22/32 - 109.0ms/batch - loss: 25.63425 - diff: 14.33mlTrain batch 23/32 - 107.1ms/batch - loss: 27.08061 - diff: 14.61mlTrain batch 24/32 - 106.7ms/batch - loss: 26.82427 - diff: 14.62mlTrain batch 25/32 - 106.0ms/batch - loss: 26.73545 - diff: 14.68mlTrain batch 26/32 - 106.9ms/batch - loss: 27.04935 - diff: 14.76mlTrain batch 27/32 - 105.7ms/batch - loss: 26.50131 - diff: 14.62mlTrain batch 28/32 - 106.9ms/batch - loss: 26.04619 - diff: 14.55mlTrain batch 29/32 - 106.2ms/batch - loss: 25.64894 - diff: 14.48mlTrain batch 30/32 - 106.6ms/batch - loss: 25.46363 - diff: 14.47mlTrain batch 31/32 - 100.9ms/batch - loss: 25.51961 - diff: 14.62mlTrain batch 32/32 - 81.2ms/batch - loss: 26.07214 - diff: 14.65mlTrain batch 32/32 - 11.0s 81.2ms/batch - loss: 26.07214 - diff: 14.65ml
Test 0.6s: val_loss: 41.70348 - diff: 16.61ml

Epoch 82: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 141.3ms/batch - loss: 17.50881 - diff: 13.49mlTrain batch 2/32 - 118.1ms/batch - loss: 15.78905 - diff: 12.89mlTrain batch 3/32 - 124.6ms/batch - loss: 19.76350 - diff: 13.37mlTrain batch 4/32 - 117.5ms/batch - loss: 18.21461 - diff: 12.81mlTrain batch 5/32 - 124.7ms/batch - loss: 21.49581 - diff: 14.23mlTrain batch 6/32 - 116.9ms/batch - loss: 19.87711 - diff: 13.72mlTrain batch 7/32 - 116.9ms/batch - loss: 19.04271 - diff: 13.45mlTrain batch 8/32 - 116.7ms/batch - loss: 25.06470 - diff: 15.15mlTrain batch 9/32 - 118.4ms/batch - loss: 25.67165 - diff: 14.98mlTrain batch 10/32 - 117.5ms/batch - loss: 24.61926 - diff: 14.81mlTrain batch 11/32 - 113.6ms/batch - loss: 24.40110 - diff: 14.72mlTrain batch 12/32 - 114.7ms/batch - loss: 24.62794 - diff: 14.75mlTrain batch 13/32 - 111.5ms/batch - loss: 24.20523 - diff: 14.68mlTrain batch 14/32 - 106.6ms/batch - loss: 23.07984 - diff: 14.23mlTrain batch 15/32 - 102.8ms/batch - loss: 22.45288 - diff: 13.93mlTrain batch 16/32 - 104.3ms/batch - loss: 21.81002 - diff: 13.80mlTrain batch 17/32 - 107.7ms/batch - loss: 21.50764 - diff: 13.78mlTrain batch 18/32 - 121.3ms/batch - loss: 21.96516 - diff: 13.93mlTrain batch 19/32 - 106.6ms/batch - loss: 21.40406 - diff: 13.76mlTrain batch 20/32 - 121.8ms/batch - loss: 22.47124 - diff: 14.26mlTrain batch 21/32 - 116.1ms/batch - loss: 22.25835 - diff: 14.21mlTrain batch 22/32 - 125.3ms/batch - loss: 22.35956 - diff: 14.25mlTrain batch 23/32 - 116.2ms/batch - loss: 24.84913 - diff: 14.88mlTrain batch 24/32 - 115.0ms/batch - loss: 25.34865 - diff: 15.14mlTrain batch 25/32 - 116.3ms/batch - loss: 25.09645 - diff: 15.04mlTrain batch 26/32 - 115.8ms/batch - loss: 25.63742 - diff: 15.20mlTrain batch 27/32 - 113.1ms/batch - loss: 32.41161 - diff: 15.59mlTrain batch 28/32 - 112.7ms/batch - loss: 31.68793 - diff: 15.47mlTrain batch 29/32 - 112.9ms/batch - loss: 31.21216 - diff: 15.46mlTrain batch 30/32 - 113.9ms/batch - loss: 30.64361 - diff: 15.39mlTrain batch 31/32 - 97.4ms/batch - loss: 30.12498 - diff: 15.32mlTrain batch 32/32 - 90.7ms/batch - loss: 30.91371 - diff: 15.37mlTrain batch 32/32 - 11.4s 90.7ms/batch - loss: 30.91371 - diff: 15.37ml
Test 0.6s: val_loss: 55.13595 - diff: 18.75ml

Epoch 83: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 117.7ms/batch - loss: 7.81668 - diff: 9.30mlTrain batch 2/32 - 104.1ms/batch - loss: 10.59773 - diff: 10.55mlTrain batch 3/32 - 107.2ms/batch - loss: 16.06508 - diff: 12.45mlTrain batch 4/32 - 107.2ms/batch - loss: 18.74256 - diff: 13.62mlTrain batch 5/32 - 107.8ms/batch - loss: 19.33086 - diff: 13.79mlTrain batch 6/32 - 106.4ms/batch - loss: 18.74415 - diff: 13.61mlTrain batch 7/32 - 107.5ms/batch - loss: 22.21060 - diff: 14.35mlTrain batch 8/32 - 105.9ms/batch - loss: 20.74871 - diff: 13.72mlTrain batch 9/32 - 107.3ms/batch - loss: 20.50367 - diff: 13.70mlTrain batch 10/32 - 105.5ms/batch - loss: 19.57505 - diff: 13.47mlTrain batch 11/32 - 101.3ms/batch - loss: 18.65523 - diff: 13.06mlTrain batch 12/32 - 91.4ms/batch - loss: 18.96840 - diff: 13.25mlTrain batch 13/32 - 102.9ms/batch - loss: 20.75364 - diff: 13.57mlTrain batch 14/32 - 107.8ms/batch - loss: 21.74873 - diff: 13.94mlTrain batch 15/32 - 98.8ms/batch - loss: 21.74955 - diff: 13.94mlTrain batch 16/32 - 102.5ms/batch - loss: 22.40433 - diff: 14.20mlTrain batch 17/32 - 111.3ms/batch - loss: 21.96136 - diff: 14.03mlTrain batch 18/32 - 114.5ms/batch - loss: 21.18199 - diff: 13.74mlTrain batch 19/32 - 103.5ms/batch - loss: 21.28055 - diff: 13.87mlTrain batch 20/32 - 103.3ms/batch - loss: 20.63313 - diff: 13.65mlTrain batch 21/32 - 104.1ms/batch - loss: 20.43483 - diff: 13.67mlTrain batch 22/32 - 102.9ms/batch - loss: 19.88770 - diff: 13.43mlTrain batch 23/32 - 102.6ms/batch - loss: 21.08951 - diff: 13.75mlTrain batch 24/32 - 103.8ms/batch - loss: 21.15083 - diff: 13.84mlTrain batch 25/32 - 102.9ms/batch - loss: 21.25401 - diff: 13.94mlTrain batch 26/32 - 102.4ms/batch - loss: 21.12794 - diff: 13.95mlTrain batch 27/32 - 102.1ms/batch - loss: 21.51273 - diff: 14.13mlTrain batch 28/32 - 103.7ms/batch - loss: 21.37283 - diff: 14.09mlTrain batch 29/32 - 104.3ms/batch - loss: 21.02915 - diff: 14.00mlTrain batch 30/32 - 111.2ms/batch - loss: 21.31108 - diff: 14.11mlTrain batch 31/32 - 114.8ms/batch - loss: 23.71470 - diff: 14.34mlTrain batch 32/32 - 108.3ms/batch - loss: 23.78936 - diff: 14.30mlTrain batch 32/32 - 11.1s 108.3ms/batch - loss: 23.78936 - diff: 14.30ml
Test 0.7s: val_loss: 38.03133 - diff: 17.09ml

Epoch 84: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 121.0ms/batch - loss: 28.55002 - diff: 17.77mlTrain batch 2/32 - 108.1ms/batch - loss: 23.33665 - diff: 16.11mlTrain batch 3/32 - 108.1ms/batch - loss: 21.65363 - diff: 14.50mlTrain batch 4/32 - 108.4ms/batch - loss: 18.50012 - diff: 13.41mlTrain batch 5/32 - 101.8ms/batch - loss: 18.73991 - diff: 13.49mlTrain batch 6/32 - 106.2ms/batch - loss: 19.38806 - diff: 13.90mlTrain batch 7/32 - 119.9ms/batch - loss: 20.10467 - diff: 14.31mlTrain batch 8/32 - 106.6ms/batch - loss: 19.10346 - diff: 13.97mlTrain batch 9/32 - 100.6ms/batch - loss: 18.80678 - diff: 13.97mlTrain batch 10/32 - 99.3ms/batch - loss: 18.46626 - diff: 13.78mlTrain batch 11/32 - 97.4ms/batch - loss: 20.64198 - diff: 14.19mlTrain batch 12/32 - 111.6ms/batch - loss: 20.09196 - diff: 13.89mlTrain batch 13/32 - 110.9ms/batch - loss: 24.34490 - diff: 14.44mlTrain batch 14/32 - 102.4ms/batch - loss: 23.12531 - diff: 14.05mlTrain batch 15/32 - 103.3ms/batch - loss: 22.33674 - diff: 13.74mlTrain batch 16/32 - 110.2ms/batch - loss: 21.81360 - diff: 13.63mlTrain batch 17/32 - 103.6ms/batch - loss: 20.90221 - diff: 13.35mlTrain batch 18/32 - 111.0ms/batch - loss: 20.28253 - diff: 13.23mlTrain batch 19/32 - 103.1ms/batch - loss: 20.25508 - diff: 13.27mlTrain batch 20/32 - 109.5ms/batch - loss: 20.61451 - diff: 13.42mlTrain batch 21/32 - 109.8ms/batch - loss: 19.82148 - diff: 13.10mlTrain batch 22/32 - 101.9ms/batch - loss: 19.93290 - diff: 13.14mlTrain batch 23/32 - 103.1ms/batch - loss: 19.76693 - diff: 13.09mlTrain batch 24/32 - 110.5ms/batch - loss: 19.63102 - diff: 13.14mlTrain batch 25/32 - 114.3ms/batch - loss: 19.66536 - diff: 13.18mlTrain batch 26/32 - 105.5ms/batch - loss: 19.92613 - diff: 13.35mlTrain batch 27/32 - 108.5ms/batch - loss: 19.88562 - diff: 13.34mlTrain batch 28/32 - 114.3ms/batch - loss: 19.57662 - diff: 13.20mlTrain batch 29/32 - 106.6ms/batch - loss: 19.58684 - diff: 13.24mlTrain batch 30/32 - 88.5ms/batch - loss: 19.53615 - diff: 13.27mlTrain batch 31/32 - 99.7ms/batch - loss: 19.48782 - diff: 13.28mlTrain batch 32/32 - 67.0ms/batch - loss: 20.46846 - diff: 13.29mlTrain batch 32/32 - 11.7s 67.0ms/batch - loss: 20.46846 - diff: 13.29ml
Test 0.6s: val_loss: 34.67413 - diff: 15.92ml

Epoch 85: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 117.6ms/batch - loss: 8.85999 - diff: 9.55mlTrain batch 2/32 - 104.8ms/batch - loss: 13.49146 - diff: 11.01mlTrain batch 3/32 - 104.2ms/batch - loss: 11.42420 - diff: 10.28mlTrain batch 4/32 - 105.7ms/batch - loss: 10.59321 - diff: 10.18mlTrain batch 5/32 - 104.7ms/batch - loss: 9.86838 - diff: 9.74mlTrain batch 6/32 - 103.1ms/batch - loss: 10.31659 - diff: 9.99mlTrain batch 7/32 - 106.1ms/batch - loss: 13.47556 - diff: 10.79mlTrain batch 8/32 - 106.4ms/batch - loss: 14.22137 - diff: 11.09mlTrain batch 9/32 - 106.1ms/batch - loss: 14.10293 - diff: 11.19mlTrain batch 10/32 - 106.1ms/batch - loss: 13.52564 - diff: 10.93mlTrain batch 11/32 - 106.5ms/batch - loss: 14.12467 - diff: 11.30mlTrain batch 12/32 - 95.2ms/batch - loss: 14.37067 - diff: 11.54mlTrain batch 13/32 - 108.2ms/batch - loss: 15.29004 - diff: 11.82mlTrain batch 14/32 - 91.4ms/batch - loss: 15.14067 - diff: 11.82mlTrain batch 15/32 - 105.9ms/batch - loss: 19.36932 - diff: 12.51mlTrain batch 16/32 - 105.8ms/batch - loss: 20.02325 - diff: 12.75mlTrain batch 17/32 - 105.9ms/batch - loss: 19.43284 - diff: 12.52mlTrain batch 18/32 - 107.9ms/batch - loss: 20.11950 - diff: 12.72mlTrain batch 19/32 - 106.1ms/batch - loss: 19.81081 - diff: 12.75mlTrain batch 20/32 - 111.8ms/batch - loss: 20.02200 - diff: 12.85mlTrain batch 21/32 - 103.3ms/batch - loss: 20.52470 - diff: 13.07mlTrain batch 22/32 - 110.8ms/batch - loss: 20.93648 - diff: 13.24mlTrain batch 23/32 - 106.2ms/batch - loss: 21.29913 - diff: 13.31mlTrain batch 24/32 - 110.8ms/batch - loss: 21.27017 - diff: 13.34mlTrain batch 25/32 - 106.6ms/batch - loss: 21.41454 - diff: 13.38mlTrain batch 26/32 - 107.1ms/batch - loss: 21.25749 - diff: 13.38mlTrain batch 27/32 - 106.4ms/batch - loss: 21.06783 - diff: 13.38mlTrain batch 28/32 - 106.2ms/batch - loss: 20.65238 - diff: 13.22mlTrain batch 29/32 - 106.5ms/batch - loss: 20.95937 - diff: 13.27mlTrain batch 30/32 - 107.5ms/batch - loss: 20.69766 - diff: 13.25mlTrain batch 31/32 - 107.3ms/batch - loss: 20.56284 - diff: 13.29mlTrain batch 32/32 - 87.4ms/batch - loss: 26.23194 - diff: 13.47mlTrain batch 32/32 - 10.7s 87.4ms/batch - loss: 26.23194 - diff: 13.47ml
Test 0.6s: val_loss: 32.27934 - diff: 16.55ml

Epoch 86: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 128.2ms/batch - loss: 15.53746 - diff: 13.52mlTrain batch 2/32 - 113.9ms/batch - loss: 18.42024 - diff: 13.92mlTrain batch 3/32 - 121.9ms/batch - loss: 15.21260 - diff: 12.66mlTrain batch 4/32 - 112.9ms/batch - loss: 13.73795 - diff: 12.00mlTrain batch 5/32 - 99.6ms/batch - loss: 18.67895 - diff: 12.95mlTrain batch 6/32 - 87.4ms/batch - loss: 22.98878 - diff: 13.72mlTrain batch 7/32 - 106.5ms/batch - loss: 26.54897 - diff: 14.72mlTrain batch 8/32 - 112.7ms/batch - loss: 28.76440 - diff: 15.36mlTrain batch 9/32 - 112.3ms/batch - loss: 29.36110 - diff: 15.45mlTrain batch 10/32 - 111.9ms/batch - loss: 28.65700 - diff: 15.45mlTrain batch 11/32 - 113.1ms/batch - loss: 26.99620 - diff: 15.07mlTrain batch 12/32 - 112.3ms/batch - loss: 25.93878 - diff: 14.90mlTrain batch 13/32 - 107.6ms/batch - loss: 26.67056 - diff: 15.15mlTrain batch 14/32 - 116.8ms/batch - loss: 26.28984 - diff: 14.99mlTrain batch 15/32 - 112.0ms/batch - loss: 26.23508 - diff: 15.02mlTrain batch 16/32 - 111.8ms/batch - loss: 37.35910 - diff: 16.01mlTrain batch 17/32 - 112.9ms/batch - loss: 35.94904 - diff: 15.75mlTrain batch 18/32 - 112.5ms/batch - loss: 35.71540 - diff: 15.76mlTrain batch 19/32 - 111.9ms/batch - loss: 35.18681 - diff: 15.75mlTrain batch 20/32 - 119.3ms/batch - loss: 36.70569 - diff: 16.20mlTrain batch 21/32 - 116.8ms/batch - loss: 35.63744 - diff: 15.99mlTrain batch 22/32 - 113.7ms/batch - loss: 36.19219 - diff: 16.11mlTrain batch 23/32 - 113.6ms/batch - loss: 36.35590 - diff: 16.26mlTrain batch 24/32 - 113.0ms/batch - loss: 37.42387 - diff: 16.61mlTrain batch 25/32 - 113.2ms/batch - loss: 36.44830 - diff: 16.40mlTrain batch 26/32 - 112.2ms/batch - loss: 35.60942 - diff: 16.26mlTrain batch 27/32 - 112.5ms/batch - loss: 34.87358 - diff: 16.19mlTrain batch 28/32 - 112.2ms/batch - loss: 34.27236 - diff: 16.11mlTrain batch 29/32 - 114.6ms/batch - loss: 34.57463 - diff: 16.16mlTrain batch 30/32 - 96.6ms/batch - loss: 34.88063 - diff: 16.21mlTrain batch 31/32 - 91.6ms/batch - loss: 34.20810 - diff: 16.04mlTrain batch 32/32 - 81.5ms/batch - loss: 35.27106 - diff: 16.07mlTrain batch 32/32 - 11.8s 81.5ms/batch - loss: 35.27106 - diff: 16.07ml
Test 0.6s: val_loss: 44.10026 - diff: 18.20ml
Epoch    87: reducing learning rate of group 0 to 2.5000e-04.

Epoch 87: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 125.1ms/batch - loss: 12.56112 - diff: 11.95mlTrain batch 2/32 - 104.2ms/batch - loss: 9.87046 - diff: 10.24mlTrain batch 3/32 - 109.8ms/batch - loss: 9.76820 - diff: 9.82mlTrain batch 4/32 - 110.3ms/batch - loss: 11.27177 - diff: 10.63mlTrain batch 5/32 - 98.4ms/batch - loss: 20.87101 - diff: 12.67mlTrain batch 6/32 - 92.1ms/batch - loss: 23.33972 - diff: 13.50mlTrain batch 7/32 - 113.9ms/batch - loss: 22.98239 - diff: 13.64mlTrain batch 8/32 - 121.7ms/batch - loss: 21.68701 - diff: 13.28mlTrain batch 9/32 - 107.1ms/batch - loss: 20.41008 - diff: 12.90mlTrain batch 10/32 - 108.3ms/batch - loss: 21.96173 - diff: 13.65mlTrain batch 11/32 - 107.7ms/batch - loss: 25.06760 - diff: 13.98mlTrain batch 12/32 - 106.6ms/batch - loss: 24.33982 - diff: 13.90mlTrain batch 13/32 - 113.7ms/batch - loss: 25.22626 - diff: 14.14mlTrain batch 14/32 - 106.8ms/batch - loss: 24.88038 - diff: 14.13mlTrain batch 15/32 - 106.4ms/batch - loss: 24.91712 - diff: 14.11mlTrain batch 16/32 - 115.8ms/batch - loss: 24.82221 - diff: 14.27mlTrain batch 17/32 - 107.1ms/batch - loss: 23.91635 - diff: 14.04mlTrain batch 18/32 - 113.6ms/batch - loss: 23.99963 - diff: 13.92mlTrain batch 19/32 - 106.8ms/batch - loss: 23.47079 - diff: 13.81mlTrain batch 20/32 - 113.1ms/batch - loss: 23.25692 - diff: 13.84mlTrain batch 21/32 - 113.5ms/batch - loss: 22.75132 - diff: 13.72mlTrain batch 22/32 - 117.9ms/batch - loss: 22.34871 - diff: 13.62mlTrain batch 23/32 - 88.4ms/batch - loss: 22.21334 - diff: 13.64mlTrain batch 24/32 - 97.3ms/batch - loss: 21.95714 - diff: 13.61mlTrain batch 25/32 - 102.6ms/batch - loss: 21.54404 - diff: 13.51mlTrain batch 26/32 - 106.9ms/batch - loss: 21.57655 - diff: 13.54mlTrain batch 27/32 - 102.2ms/batch - loss: 21.20307 - diff: 13.46mlTrain batch 28/32 - 105.2ms/batch - loss: 20.80090 - diff: 13.32mlTrain batch 29/32 - 107.3ms/batch - loss: 21.02326 - diff: 13.50mlTrain batch 30/32 - 101.9ms/batch - loss: 21.12681 - diff: 13.57mlTrain batch 31/32 - 90.2ms/batch - loss: 20.88922 - diff: 13.48mlTrain batch 32/32 - 89.9ms/batch - loss: 21.29227 - diff: 13.51mlTrain batch 32/32 - 11.4s 89.9ms/batch - loss: 21.29227 - diff: 13.51ml
Test 0.7s: val_loss: 35.10597 - diff: 16.49ml

Epoch 88: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 142.3ms/batch - loss: 32.02715 - diff: 18.80mlTrain batch 2/32 - 117.5ms/batch - loss: 29.01499 - diff: 17.41mlTrain batch 3/32 - 114.4ms/batch - loss: 23.91866 - diff: 15.72mlTrain batch 4/32 - 109.8ms/batch - loss: 27.80927 - diff: 16.70mlTrain batch 5/32 - 106.9ms/batch - loss: 25.35422 - diff: 15.81mlTrain batch 6/32 - 106.5ms/batch - loss: 22.74326 - diff: 14.89mlTrain batch 7/32 - 94.0ms/batch - loss: 21.36483 - diff: 14.39mlTrain batch 8/32 - 100.6ms/batch - loss: 20.50547 - diff: 14.20mlTrain batch 9/32 - 111.0ms/batch - loss: 26.80516 - diff: 14.43mlTrain batch 10/32 - 115.2ms/batch - loss: 25.68962 - diff: 14.32mlTrain batch 11/32 - 116.5ms/batch - loss: 24.64950 - diff: 14.16mlTrain batch 12/32 - 112.0ms/batch - loss: 23.75566 - diff: 13.97mlTrain batch 13/32 - 104.6ms/batch - loss: 22.46738 - diff: 13.55mlTrain batch 14/32 - 107.1ms/batch - loss: 21.46903 - diff: 13.31mlTrain batch 15/32 - 112.6ms/batch - loss: 20.64711 - diff: 13.06mlTrain batch 16/32 - 116.9ms/batch - loss: 20.01961 - diff: 12.90mlTrain batch 17/32 - 106.1ms/batch - loss: 20.49237 - diff: 13.26mlTrain batch 18/32 - 103.0ms/batch - loss: 20.47207 - diff: 13.25mlTrain batch 19/32 - 107.2ms/batch - loss: 20.80191 - diff: 13.32mlTrain batch 20/32 - 107.5ms/batch - loss: 21.06846 - diff: 13.52mlTrain batch 21/32 - 112.5ms/batch - loss: 22.40709 - diff: 13.89mlTrain batch 22/32 - 107.8ms/batch - loss: 21.72761 - diff: 13.68mlTrain batch 23/32 - 107.4ms/batch - loss: 21.56008 - diff: 13.66mlTrain batch 24/32 - 106.4ms/batch - loss: 20.85630 - diff: 13.39mlTrain batch 25/32 - 124.1ms/batch - loss: 20.43376 - diff: 13.31mlTrain batch 26/32 - 116.2ms/batch - loss: 19.86493 - diff: 13.11mlTrain batch 27/32 - 120.3ms/batch - loss: 19.38594 - diff: 12.91mlTrain batch 28/32 - 106.5ms/batch - loss: 19.12162 - diff: 12.85mlTrain batch 29/32 - 114.3ms/batch - loss: 20.13943 - diff: 13.13mlTrain batch 30/32 - 112.8ms/batch - loss: 19.98818 - diff: 13.12mlTrain batch 31/32 - 91.4ms/batch - loss: 19.76938 - diff: 13.09mlTrain batch 32/32 - 69.4ms/batch - loss: 20.25695 - diff: 13.08mlTrain batch 32/32 - 11.3s 69.4ms/batch - loss: 20.25695 - diff: 13.08ml
Test 0.6s: val_loss: 35.35308 - diff: 17.10ml

Epoch 89: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 132.7ms/batch - loss: 14.71146 - diff: 10.50mlTrain batch 2/32 - 117.6ms/batch - loss: 19.76027 - diff: 13.03mlTrain batch 3/32 - 113.7ms/batch - loss: 15.71843 - diff: 11.26mlTrain batch 4/32 - 113.6ms/batch - loss: 13.85916 - diff: 10.83mlTrain batch 5/32 - 113.7ms/batch - loss: 13.38774 - diff: 10.91mlTrain batch 6/32 - 113.5ms/batch - loss: 11.76917 - diff: 10.12mlTrain batch 7/32 - 104.1ms/batch - loss: 14.30568 - diff: 11.16mlTrain batch 8/32 - 102.9ms/batch - loss: 15.47883 - diff: 11.86mlTrain batch 9/32 - 104.9ms/batch - loss: 15.07476 - diff: 11.78mlTrain batch 10/32 - 93.1ms/batch - loss: 14.28202 - diff: 11.44mlTrain batch 11/32 - 106.6ms/batch - loss: 14.55535 - diff: 11.45mlTrain batch 12/32 - 123.9ms/batch - loss: 14.14722 - diff: 11.37mlTrain batch 13/32 - 112.6ms/batch - loss: 13.59901 - diff: 11.17mlTrain batch 14/32 - 112.4ms/batch - loss: 15.50094 - diff: 11.91mlTrain batch 15/32 - 103.2ms/batch - loss: 15.21727 - diff: 11.81mlTrain batch 16/32 - 111.7ms/batch - loss: 15.41855 - diff: 11.89mlTrain batch 17/32 - 113.1ms/batch - loss: 15.15042 - diff: 11.82mlTrain batch 18/32 - 113.2ms/batch - loss: 14.65908 - diff: 11.61mlTrain batch 19/32 - 112.8ms/batch - loss: 14.42902 - diff: 11.51mlTrain batch 20/32 - 113.0ms/batch - loss: 14.57698 - diff: 11.64mlTrain batch 21/32 - 112.9ms/batch - loss: 14.40644 - diff: 11.53mlTrain batch 22/32 - 112.4ms/batch - loss: 14.83800 - diff: 11.69mlTrain batch 23/32 - 113.8ms/batch - loss: 15.59209 - diff: 11.89mlTrain batch 24/32 - 112.4ms/batch - loss: 15.15092 - diff: 11.65mlTrain batch 25/32 - 113.1ms/batch - loss: 14.93045 - diff: 11.52mlTrain batch 26/32 - 113.4ms/batch - loss: 15.61784 - diff: 11.75mlTrain batch 27/32 - 113.3ms/batch - loss: 15.41826 - diff: 11.65mlTrain batch 28/32 - 113.2ms/batch - loss: 15.97748 - diff: 11.89mlTrain batch 29/32 - 105.4ms/batch - loss: 15.76668 - diff: 11.82mlTrain batch 30/32 - 90.9ms/batch - loss: 15.47291 - diff: 11.69mlTrain batch 31/32 - 91.1ms/batch - loss: 15.15833 - diff: 11.59mlTrain batch 32/32 - 71.9ms/batch - loss: 15.24251 - diff: 11.56mlTrain batch 32/32 - 11.1s 71.9ms/batch - loss: 15.24251 - diff: 11.56ml
Test 0.6s: val_loss: 32.70464 - diff: 15.89ml

Epoch 90: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 123.0ms/batch - loss: 22.60440 - diff: 15.33mlTrain batch 2/32 - 105.8ms/batch - loss: 20.64716 - diff: 13.35mlTrain batch 3/32 - 105.1ms/batch - loss: 16.82358 - diff: 12.18mlTrain batch 4/32 - 103.1ms/batch - loss: 15.67471 - diff: 11.99mlTrain batch 5/32 - 110.0ms/batch - loss: 14.05400 - diff: 10.95mlTrain batch 6/32 - 103.4ms/batch - loss: 16.08135 - diff: 11.85mlTrain batch 7/32 - 107.1ms/batch - loss: 14.43353 - diff: 11.17mlTrain batch 8/32 - 107.3ms/batch - loss: 15.46645 - diff: 11.73mlTrain batch 9/32 - 115.8ms/batch - loss: 14.78349 - diff: 11.32mlTrain batch 10/32 - 116.7ms/batch - loss: 13.79071 - diff: 10.94mlTrain batch 11/32 - 109.1ms/batch - loss: 12.84217 - diff: 10.54mlTrain batch 12/32 - 120.6ms/batch - loss: 12.52423 - diff: 10.51mlTrain batch 13/32 - 107.5ms/batch - loss: 12.10323 - diff: 10.40mlTrain batch 14/32 - 109.8ms/batch - loss: 11.96145 - diff: 10.35mlTrain batch 15/32 - 109.5ms/batch - loss: 11.89133 - diff: 10.34mlTrain batch 16/32 - 115.2ms/batch - loss: 11.85783 - diff: 10.34mlTrain batch 17/32 - 106.4ms/batch - loss: 11.59277 - diff: 10.24mlTrain batch 18/32 - 104.6ms/batch - loss: 11.52880 - diff: 10.19mlTrain batch 19/32 - 98.9ms/batch - loss: 11.79807 - diff: 10.42mlTrain batch 20/32 - 102.9ms/batch - loss: 11.56365 - diff: 10.30mlTrain batch 21/32 - 103.4ms/batch - loss: 11.51095 - diff: 10.38mlTrain batch 22/32 - 110.5ms/batch - loss: 11.69986 - diff: 10.45mlTrain batch 23/32 - 103.0ms/batch - loss: 13.23646 - diff: 11.01mlTrain batch 24/32 - 110.4ms/batch - loss: 13.45545 - diff: 11.08mlTrain batch 25/32 - 114.3ms/batch - loss: 15.69927 - diff: 11.70mlTrain batch 26/32 - 115.6ms/batch - loss: 15.29756 - diff: 11.54mlTrain batch 27/32 - 101.7ms/batch - loss: 14.88698 - diff: 11.34mlTrain batch 28/32 - 113.0ms/batch - loss: 14.79722 - diff: 11.36mlTrain batch 29/32 - 107.0ms/batch - loss: 14.58142 - diff: 11.33mlTrain batch 30/32 - 105.8ms/batch - loss: 15.01377 - diff: 11.42mlTrain batch 31/32 - 116.9ms/batch - loss: 14.87999 - diff: 11.39mlTrain batch 32/32 - 109.9ms/batch - loss: 15.02981 - diff: 11.38mlTrain batch 32/32 - 12.1s 109.9ms/batch - loss: 15.02981 - diff: 11.38ml
Test 0.7s: val_loss: 36.27341 - diff: 16.84ml

Epoch 91: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 138.9ms/batch - loss: 16.12671 - diff: 12.75mlTrain batch 2/32 - 114.1ms/batch - loss: 18.43014 - diff: 13.82mlTrain batch 3/32 - 112.1ms/batch - loss: 15.05070 - diff: 11.85mlTrain batch 4/32 - 106.3ms/batch - loss: 12.94346 - diff: 11.03mlTrain batch 5/32 - 103.7ms/batch - loss: 11.62573 - diff: 10.36mlTrain batch 6/32 - 103.5ms/batch - loss: 12.04139 - diff: 10.69mlTrain batch 7/32 - 104.7ms/batch - loss: 11.10585 - diff: 10.25mlTrain batch 8/32 - 103.3ms/batch - loss: 10.16354 - diff: 9.72mlTrain batch 9/32 - 103.7ms/batch - loss: 9.72993 - diff: 9.59mlTrain batch 10/32 - 104.0ms/batch - loss: 9.91124 - diff: 9.69mlTrain batch 11/32 - 121.5ms/batch - loss: 11.55182 - diff: 10.25mlTrain batch 12/32 - 107.8ms/batch - loss: 12.04712 - diff: 10.52mlTrain batch 13/32 - 103.3ms/batch - loss: 11.77374 - diff: 10.45mlTrain batch 14/32 - 101.5ms/batch - loss: 11.67177 - diff: 10.41mlTrain batch 15/32 - 110.4ms/batch - loss: 11.69968 - diff: 10.32mlTrain batch 16/32 - 102.8ms/batch - loss: 11.38314 - diff: 10.23mlTrain batch 17/32 - 103.9ms/batch - loss: 11.75215 - diff: 10.49mlTrain batch 18/32 - 102.5ms/batch - loss: 11.58296 - diff: 10.43mlTrain batch 19/32 - 102.4ms/batch - loss: 12.22665 - diff: 10.73mlTrain batch 20/32 - 102.7ms/batch - loss: 12.56169 - diff: 10.87mlTrain batch 21/32 - 102.8ms/batch - loss: 12.45678 - diff: 10.87mlTrain batch 22/32 - 102.3ms/batch - loss: 12.23165 - diff: 10.81mlTrain batch 23/32 - 103.0ms/batch - loss: 12.51769 - diff: 10.99mlTrain batch 24/32 - 102.5ms/batch - loss: 12.28407 - diff: 10.89mlTrain batch 25/32 - 103.4ms/batch - loss: 12.35087 - diff: 10.90mlTrain batch 26/32 - 102.3ms/batch - loss: 12.41543 - diff: 10.99mlTrain batch 27/32 - 107.4ms/batch - loss: 12.33698 - diff: 10.96mlTrain batch 28/32 - 113.7ms/batch - loss: 12.31314 - diff: 10.90mlTrain batch 29/32 - 113.2ms/batch - loss: 12.15916 - diff: 10.86mlTrain batch 30/32 - 107.8ms/batch - loss: 12.29875 - diff: 10.95mlTrain batch 31/32 - 110.0ms/batch - loss: 12.40932 - diff: 10.99mlTrain batch 32/32 - 109.1ms/batch - loss: 13.07355 - diff: 11.03mlTrain batch 32/32 - 10.9s 109.1ms/batch - loss: 13.07355 - diff: 11.03ml
Test 0.6s: val_loss: 34.57377 - diff: 16.14ml

Epoch 92: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 121.5ms/batch - loss: 27.76832 - diff: 13.93mlTrain batch 2/32 - 109.0ms/batch - loss: 16.13409 - diff: 10.33mlTrain batch 3/32 - 99.6ms/batch - loss: 15.42879 - diff: 10.15mlTrain batch 4/32 - 107.9ms/batch - loss: 14.95911 - diff: 10.54mlTrain batch 5/32 - 110.8ms/batch - loss: 15.60055 - diff: 10.94mlTrain batch 6/32 - 108.5ms/batch - loss: 15.90383 - diff: 11.54mlTrain batch 7/32 - 109.0ms/batch - loss: 14.79660 - diff: 11.29mlTrain batch 8/32 - 95.3ms/batch - loss: 15.35494 - diff: 11.53mlTrain batch 9/32 - 108.9ms/batch - loss: 15.16383 - diff: 11.66mlTrain batch 10/32 - 108.3ms/batch - loss: 15.23611 - diff: 11.93mlTrain batch 11/32 - 109.0ms/batch - loss: 14.83356 - diff: 11.78mlTrain batch 12/32 - 94.3ms/batch - loss: 14.65394 - diff: 11.64mlTrain batch 13/32 - 117.9ms/batch - loss: 15.04318 - diff: 11.59mlTrain batch 14/32 - 108.2ms/batch - loss: 14.65668 - diff: 11.50mlTrain batch 15/32 - 108.4ms/batch - loss: 14.10547 - diff: 11.25mlTrain batch 16/32 - 109.3ms/batch - loss: 13.94608 - diff: 11.26mlTrain batch 17/32 - 109.9ms/batch - loss: 13.85459 - diff: 11.13mlTrain batch 18/32 - 111.7ms/batch - loss: 13.36788 - diff: 10.94mlTrain batch 19/32 - 109.4ms/batch - loss: 13.33863 - diff: 10.99mlTrain batch 20/32 - 117.5ms/batch - loss: 13.79217 - diff: 10.94mlTrain batch 21/32 - 106.4ms/batch - loss: 13.53852 - diff: 10.88mlTrain batch 22/32 - 106.9ms/batch - loss: 15.83774 - diff: 10.94mlTrain batch 23/32 - 106.7ms/batch - loss: 15.93638 - diff: 11.13mlTrain batch 24/32 - 103.3ms/batch - loss: 16.16972 - diff: 11.33mlTrain batch 25/32 - 103.6ms/batch - loss: 15.91403 - diff: 11.28mlTrain batch 26/32 - 103.2ms/batch - loss: 15.64966 - diff: 11.24mlTrain batch 27/32 - 103.3ms/batch - loss: 15.47232 - diff: 11.23mlTrain batch 28/32 - 102.7ms/batch - loss: 15.15321 - diff: 11.11mlTrain batch 29/32 - 89.7ms/batch - loss: 15.17403 - diff: 11.21mlTrain batch 30/32 - 108.2ms/batch - loss: 14.82628 - diff: 11.08mlTrain batch 31/32 - 110.1ms/batch - loss: 14.49618 - diff: 10.95mlTrain batch 32/32 - 97.5ms/batch - loss: 14.44531 - diff: 10.90mlTrain batch 32/32 - 11.7s 97.5ms/batch - loss: 14.44531 - diff: 10.90ml
Test 0.6s: val_loss: 32.26809 - diff: 15.79ml

Epoch 93: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 124.9ms/batch - loss: 14.46889 - diff: 12.27mlTrain batch 2/32 - 106.3ms/batch - loss: 10.13567 - diff: 9.79mlTrain batch 3/32 - 116.0ms/batch - loss: 12.77209 - diff: 10.80mlTrain batch 4/32 - 104.2ms/batch - loss: 12.40396 - diff: 10.54mlTrain batch 5/32 - 106.6ms/batch - loss: 12.93714 - diff: 10.89mlTrain batch 6/32 - 104.4ms/batch - loss: 13.91761 - diff: 11.03mlTrain batch 7/32 - 105.3ms/batch - loss: 12.94035 - diff: 10.70mlTrain batch 8/32 - 102.9ms/batch - loss: 14.42056 - diff: 11.43mlTrain batch 9/32 - 102.6ms/batch - loss: 13.96379 - diff: 11.38mlTrain batch 10/32 - 103.0ms/batch - loss: 13.50647 - diff: 11.12mlTrain batch 11/32 - 103.8ms/batch - loss: 12.88710 - diff: 10.86mlTrain batch 12/32 - 99.9ms/batch - loss: 12.89302 - diff: 10.86mlTrain batch 13/32 - 114.4ms/batch - loss: 12.87896 - diff: 10.85mlTrain batch 14/32 - 109.6ms/batch - loss: 13.74585 - diff: 11.24mlTrain batch 15/32 - 106.6ms/batch - loss: 13.21414 - diff: 10.99mlTrain batch 16/32 - 116.7ms/batch - loss: 12.97061 - diff: 10.95mlTrain batch 17/32 - 108.5ms/batch - loss: 13.26521 - diff: 11.09mlTrain batch 18/32 - 114.3ms/batch - loss: 13.28852 - diff: 11.09mlTrain batch 19/32 - 98.0ms/batch - loss: 13.55574 - diff: 11.16mlTrain batch 20/32 - 108.6ms/batch - loss: 18.91317 - diff: 12.61mlTrain batch 21/32 - 113.3ms/batch - loss: 19.19412 - diff: 12.72mlTrain batch 22/32 - 104.9ms/batch - loss: 18.86851 - diff: 12.69mlTrain batch 23/32 - 88.6ms/batch - loss: 18.41302 - diff: 12.49mlTrain batch 24/32 - 102.8ms/batch - loss: 18.03544 - diff: 12.38mlTrain batch 25/32 - 104.6ms/batch - loss: 17.75273 - diff: 12.30mlTrain batch 26/32 - 102.8ms/batch - loss: 17.24122 - diff: 12.08mlTrain batch 27/32 - 108.0ms/batch - loss: 17.26977 - diff: 12.01mlTrain batch 28/32 - 102.9ms/batch - loss: 16.86311 - diff: 11.86mlTrain batch 29/32 - 111.5ms/batch - loss: 16.53307 - diff: 11.74mlTrain batch 30/32 - 111.6ms/batch - loss: 16.26114 - diff: 11.65mlTrain batch 31/32 - 113.7ms/batch - loss: 16.06526 - diff: 11.58mlTrain batch 32/32 - 84.5ms/batch - loss: 16.02169 - diff: 11.53mlTrain batch 32/32 - 12.0s 84.5ms/batch - loss: 16.02169 - diff: 11.53ml
Test 0.6s: val_loss: 33.26220 - diff: 15.85ml

Epoch 94: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 129.2ms/batch - loss: 13.07099 - diff: 11.28mlTrain batch 2/32 - 115.3ms/batch - loss: 20.60405 - diff: 13.75mlTrain batch 3/32 - 115.4ms/batch - loss: 15.62737 - diff: 11.59mlTrain batch 4/32 - 114.9ms/batch - loss: 13.78828 - diff: 10.93mlTrain batch 5/32 - 114.0ms/batch - loss: 11.93314 - diff: 10.19mlTrain batch 6/32 - 113.0ms/batch - loss: 13.05115 - diff: 10.13mlTrain batch 7/32 - 113.7ms/batch - loss: 12.15328 - diff: 9.92mlTrain batch 8/32 - 113.3ms/batch - loss: 12.25588 - diff: 10.14mlTrain batch 9/32 - 112.4ms/batch - loss: 11.43879 - diff: 9.80mlTrain batch 10/32 - 114.8ms/batch - loss: 11.09221 - diff: 9.72mlTrain batch 11/32 - 109.4ms/batch - loss: 12.31614 - diff: 9.98mlTrain batch 12/32 - 103.3ms/batch - loss: 12.33421 - diff: 10.01mlTrain batch 13/32 - 117.6ms/batch - loss: 12.10691 - diff: 10.01mlTrain batch 14/32 - 116.9ms/batch - loss: 11.77747 - diff: 9.85mlTrain batch 15/32 - 112.4ms/batch - loss: 11.36390 - diff: 9.74mlTrain batch 16/32 - 112.6ms/batch - loss: 10.88676 - diff: 9.52mlTrain batch 17/32 - 101.1ms/batch - loss: 10.69739 - diff: 9.52mlTrain batch 18/32 - 116.6ms/batch - loss: 10.50700 - diff: 9.51mlTrain batch 19/32 - 107.2ms/batch - loss: 11.22325 - diff: 9.94mlTrain batch 20/32 - 115.4ms/batch - loss: 11.26768 - diff: 10.03mlTrain batch 21/32 - 117.4ms/batch - loss: 11.43153 - diff: 10.04mlTrain batch 22/32 - 118.1ms/batch - loss: 12.02154 - diff: 10.34mlTrain batch 23/32 - 115.5ms/batch - loss: 11.59902 - diff: 10.12mlTrain batch 24/32 - 111.1ms/batch - loss: 12.04721 - diff: 10.42mlTrain batch 25/32 - 103.9ms/batch - loss: 11.70391 - diff: 10.24mlTrain batch 26/32 - 108.5ms/batch - loss: 11.67905 - diff: 10.25mlTrain batch 27/32 - 112.2ms/batch - loss: 11.36683 - diff: 10.08mlTrain batch 28/32 - 112.3ms/batch - loss: 11.29743 - diff: 10.05mlTrain batch 29/32 - 103.3ms/batch - loss: 11.18072 - diff: 10.02mlTrain batch 30/32 - 110.3ms/batch - loss: 11.21041 - diff: 10.04mlTrain batch 31/32 - 108.6ms/batch - loss: 11.18873 - diff: 10.08mlTrain batch 32/32 - 97.4ms/batch - loss: 18.11402 - diff: 10.41mlTrain batch 32/32 - 11.3s 97.4ms/batch - loss: 18.11402 - diff: 10.41ml
Test 0.6s: val_loss: 33.20411 - diff: 15.54ml

Epoch 95: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 122.0ms/batch - loss: 15.04254 - diff: 11.64mlTrain batch 2/32 - 106.4ms/batch - loss: 17.83280 - diff: 13.96mlTrain batch 3/32 - 117.1ms/batch - loss: 16.25569 - diff: 13.22mlTrain batch 4/32 - 116.4ms/batch - loss: 14.14302 - diff: 11.83mlTrain batch 5/32 - 111.5ms/batch - loss: 13.23573 - diff: 11.48mlTrain batch 6/32 - 111.2ms/batch - loss: 12.50059 - diff: 11.07mlTrain batch 7/32 - 122.3ms/batch - loss: 12.07661 - diff: 10.94mlTrain batch 8/32 - 121.2ms/batch - loss: 13.45872 - diff: 11.34mlTrain batch 9/32 - 106.3ms/batch - loss: 15.26059 - diff: 12.04mlTrain batch 10/32 - 105.3ms/batch - loss: 16.43157 - diff: 12.47mlTrain batch 11/32 - 107.2ms/batch - loss: 15.87076 - diff: 12.35mlTrain batch 12/32 - 107.6ms/batch - loss: 15.24092 - diff: 11.96mlTrain batch 13/32 - 103.5ms/batch - loss: 14.66077 - diff: 11.69mlTrain batch 14/32 - 103.9ms/batch - loss: 14.93072 - diff: 11.71mlTrain batch 15/32 - 102.5ms/batch - loss: 14.49623 - diff: 11.53mlTrain batch 16/32 - 103.4ms/batch - loss: 14.08453 - diff: 11.36mlTrain batch 17/32 - 103.4ms/batch - loss: 13.88260 - diff: 11.25mlTrain batch 18/32 - 101.4ms/batch - loss: 13.60350 - diff: 11.08mlTrain batch 19/32 - 112.0ms/batch - loss: 13.35217 - diff: 11.02mlTrain batch 20/32 - 109.7ms/batch - loss: 13.11882 - diff: 10.91mlTrain batch 21/32 - 102.5ms/batch - loss: 13.14357 - diff: 10.93mlTrain batch 22/32 - 102.6ms/batch - loss: 12.96459 - diff: 10.87mlTrain batch 23/32 - 96.8ms/batch - loss: 12.79424 - diff: 10.82mlTrain batch 24/32 - 88.4ms/batch - loss: 13.09307 - diff: 10.98mlTrain batch 25/32 - 117.7ms/batch - loss: 13.30135 - diff: 10.98mlTrain batch 26/32 - 114.4ms/batch - loss: 14.34730 - diff: 11.18mlTrain batch 27/32 - 102.4ms/batch - loss: 14.61096 - diff: 11.28mlTrain batch 28/32 - 103.4ms/batch - loss: 14.80825 - diff: 11.39mlTrain batch 29/32 - 112.5ms/batch - loss: 14.53312 - diff: 11.31mlTrain batch 30/32 - 112.2ms/batch - loss: 14.33682 - diff: 11.27mlTrain batch 31/32 - 112.7ms/batch - loss: 14.31605 - diff: 11.29mlTrain batch 32/32 - 105.0ms/batch - loss: 15.04966 - diff: 11.34mlTrain batch 32/32 - 11.3s 105.0ms/batch - loss: 15.04966 - diff: 11.34ml
Test 0.6s: val_loss: 32.75395 - diff: 16.02ml

Epoch 96: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 139.2ms/batch - loss: 7.64254 - diff: 9.46mlTrain batch 2/32 - 113.2ms/batch - loss: 14.03978 - diff: 12.38mlTrain batch 3/32 - 112.9ms/batch - loss: 14.59534 - diff: 12.71mlTrain batch 4/32 - 113.0ms/batch - loss: 15.25949 - diff: 12.45mlTrain batch 5/32 - 114.4ms/batch - loss: 17.18431 - diff: 12.86mlTrain batch 6/32 - 113.7ms/batch - loss: 15.53601 - diff: 12.06mlTrain batch 7/32 - 118.3ms/batch - loss: 14.98234 - diff: 11.83mlTrain batch 8/32 - 113.1ms/batch - loss: 13.93583 - diff: 11.40mlTrain batch 9/32 - 99.7ms/batch - loss: 14.37480 - diff: 11.20mlTrain batch 10/32 - 87.9ms/batch - loss: 14.01019 - diff: 11.16mlTrain batch 11/32 - 103.9ms/batch - loss: 13.13944 - diff: 10.73mlTrain batch 12/32 - 113.2ms/batch - loss: 12.56841 - diff: 10.52mlTrain batch 13/32 - 102.4ms/batch - loss: 12.44143 - diff: 10.54mlTrain batch 14/32 - 106.0ms/batch - loss: 11.96535 - diff: 10.35mlTrain batch 15/32 - 103.1ms/batch - loss: 11.82363 - diff: 10.37mlTrain batch 16/32 - 102.3ms/batch - loss: 11.66115 - diff: 10.32mlTrain batch 17/32 - 106.3ms/batch - loss: 11.17464 - diff: 10.08mlTrain batch 18/32 - 106.7ms/batch - loss: 10.96300 - diff: 9.99mlTrain batch 19/32 - 103.3ms/batch - loss: 11.02194 - diff: 10.08mlTrain batch 20/32 - 114.5ms/batch - loss: 11.27917 - diff: 10.20mlTrain batch 21/32 - 113.7ms/batch - loss: 11.08029 - diff: 10.15mlTrain batch 22/32 - 115.3ms/batch - loss: 10.99614 - diff: 10.15mlTrain batch 23/32 - 112.2ms/batch - loss: 10.94475 - diff: 10.14mlTrain batch 24/32 - 110.3ms/batch - loss: 11.36003 - diff: 10.34mlTrain batch 25/32 - 112.9ms/batch - loss: 12.25188 - diff: 10.65mlTrain batch 26/32 - 121.0ms/batch - loss: 12.56260 - diff: 10.78mlTrain batch 27/32 - 113.4ms/batch - loss: 12.41002 - diff: 10.73mlTrain batch 28/32 - 120.8ms/batch - loss: 12.71666 - diff: 10.89mlTrain batch 29/32 - 111.2ms/batch - loss: 12.56397 - diff: 10.81mlTrain batch 30/32 - 111.0ms/batch - loss: 13.27149 - diff: 11.16mlTrain batch 31/32 - 99.9ms/batch - loss: 13.42214 - diff: 11.26mlTrain batch 32/32 - 98.0ms/batch - loss: 13.56261 - diff: 11.25mlTrain batch 32/32 - 11.5s 98.0ms/batch - loss: 13.56261 - diff: 11.25ml
Test 0.7s: val_loss: 33.91841 - diff: 16.11ml

Epoch 97: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 128.9ms/batch - loss: 20.06499 - diff: 10.25mlTrain batch 2/32 - 97.6ms/batch - loss: 15.08047 - diff: 9.30mlTrain batch 3/32 - 103.4ms/batch - loss: 13.97472 - diff: 9.24mlTrain batch 4/32 - 105.9ms/batch - loss: 12.04552 - diff: 8.88mlTrain batch 5/32 - 104.6ms/batch - loss: 14.10085 - diff: 9.96mlTrain batch 6/32 - 114.5ms/batch - loss: 13.51586 - diff: 10.27mlTrain batch 7/32 - 103.1ms/batch - loss: 13.35350 - diff: 10.34mlTrain batch 8/32 - 109.2ms/batch - loss: 12.70154 - diff: 10.12mlTrain batch 9/32 - 110.6ms/batch - loss: 12.24749 - diff: 10.02mlTrain batch 10/32 - 106.9ms/batch - loss: 11.49944 - diff: 9.62mlTrain batch 11/32 - 107.0ms/batch - loss: 11.93286 - diff: 9.89mlTrain batch 12/32 - 106.6ms/batch - loss: 13.23574 - diff: 10.50mlTrain batch 13/32 - 106.9ms/batch - loss: 12.94969 - diff: 10.47mlTrain batch 14/32 - 108.9ms/batch - loss: 12.51756 - diff: 10.36mlTrain batch 15/32 - 106.6ms/batch - loss: 12.05721 - diff: 10.24mlTrain batch 16/32 - 106.7ms/batch - loss: 17.80145 - diff: 10.99mlTrain batch 17/32 - 104.4ms/batch - loss: 17.84074 - diff: 11.25mlTrain batch 18/32 - 106.3ms/batch - loss: 17.70486 - diff: 11.15mlTrain batch 19/32 - 105.7ms/batch - loss: 17.68032 - diff: 11.29mlTrain batch 20/32 - 109.8ms/batch - loss: 17.56106 - diff: 11.27mlTrain batch 21/32 - 108.0ms/batch - loss: 17.02145 - diff: 11.09mlTrain batch 22/32 - 118.9ms/batch - loss: 16.67756 - diff: 11.04mlTrain batch 23/32 - 119.1ms/batch - loss: 16.20117 - diff: 10.86mlTrain batch 24/32 - 119.1ms/batch - loss: 16.00836 - diff: 10.76mlTrain batch 25/32 - 119.2ms/batch - loss: 15.82935 - diff: 10.79mlTrain batch 26/32 - 115.0ms/batch - loss: 15.72055 - diff: 10.85mlTrain batch 27/32 - 112.0ms/batch - loss: 15.23892 - diff: 10.64mlTrain batch 28/32 - 105.0ms/batch - loss: 15.21139 - diff: 10.67mlTrain batch 29/32 - 109.1ms/batch - loss: 15.07458 - diff: 10.64mlTrain batch 30/32 - 110.0ms/batch - loss: 15.43307 - diff: 10.71mlTrain batch 31/32 - 94.5ms/batch - loss: 15.34593 - diff: 10.73mlTrain batch 32/32 - 97.5ms/batch - loss: 15.78931 - diff: 10.77mlTrain batch 32/32 - 11.2s 97.5ms/batch - loss: 15.78931 - diff: 10.77ml
Test 0.6s: val_loss: 29.55362 - diff: 15.65ml
Epoch    98: reducing learning rate of group 0 to 1.2500e-04.

Epoch 98: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 117.0ms/batch - loss: 5.21777 - diff: 6.41mlTrain batch 2/32 - 103.7ms/batch - loss: 9.20552 - diff: 8.79mlTrain batch 3/32 - 103.6ms/batch - loss: 8.28794 - diff: 8.50mlTrain batch 4/32 - 104.7ms/batch - loss: 7.25714 - diff: 7.90mlTrain batch 5/32 - 104.0ms/batch - loss: 6.77269 - diff: 7.83mlTrain batch 6/32 - 104.0ms/batch - loss: 7.45688 - diff: 8.31mlTrain batch 7/32 - 103.2ms/batch - loss: 7.37291 - diff: 8.18mlTrain batch 8/32 - 105.1ms/batch - loss: 7.10026 - diff: 7.99mlTrain batch 9/32 - 106.5ms/batch - loss: 8.52548 - diff: 8.64mlTrain batch 10/32 - 97.8ms/batch - loss: 8.11495 - diff: 8.39mlTrain batch 11/32 - 114.6ms/batch - loss: 7.63597 - diff: 8.04mlTrain batch 12/32 - 103.3ms/batch - loss: 7.41684 - diff: 7.98mlTrain batch 13/32 - 110.0ms/batch - loss: 8.21236 - diff: 8.53mlTrain batch 14/32 - 105.0ms/batch - loss: 8.33272 - diff: 8.60mlTrain batch 15/32 - 111.7ms/batch - loss: 8.43793 - diff: 8.65mlTrain batch 16/32 - 103.6ms/batch - loss: 8.66012 - diff: 8.78mlTrain batch 17/32 - 103.0ms/batch - loss: 11.21004 - diff: 9.25mlTrain batch 18/32 - 102.6ms/batch - loss: 11.67145 - diff: 9.58mlTrain batch 19/32 - 102.4ms/batch - loss: 11.50610 - diff: 9.54mlTrain batch 20/32 - 104.6ms/batch - loss: 11.75169 - diff: 9.62mlTrain batch 21/32 - 104.3ms/batch - loss: 11.89280 - diff: 9.68mlTrain batch 22/32 - 112.3ms/batch - loss: 11.75660 - diff: 9.65mlTrain batch 23/32 - 92.2ms/batch - loss: 11.76059 - diff: 9.73mlTrain batch 24/32 - 97.2ms/batch - loss: 11.35083 - diff: 9.51mlTrain batch 25/32 - 92.9ms/batch - loss: 11.18169 - diff: 9.46mlTrain batch 26/32 - 97.9ms/batch - loss: 11.41772 - diff: 9.59mlTrain batch 27/32 - 113.6ms/batch - loss: 11.29589 - diff: 9.60mlTrain batch 28/32 - 106.3ms/batch - loss: 11.15950 - diff: 9.59mlTrain batch 29/32 - 101.1ms/batch - loss: 11.10817 - diff: 9.62mlTrain batch 30/32 - 91.1ms/batch - loss: 10.86244 - diff: 9.50mlTrain batch 31/32 - 106.7ms/batch - loss: 10.83802 - diff: 9.55mlTrain batch 32/32 - 84.7ms/batch - loss: 11.14960 - diff: 9.57mlTrain batch 32/32 - 11.8s 84.7ms/batch - loss: 11.14960 - diff: 9.57ml
Test 0.6s: val_loss: 31.70353 - diff: 15.97ml

Epoch 99: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 139.8ms/batch - loss: 7.96287 - diff: 9.20mlTrain batch 2/32 - 118.7ms/batch - loss: 8.49260 - diff: 9.45mlTrain batch 3/32 - 121.1ms/batch - loss: 8.16419 - diff: 9.21mlTrain batch 4/32 - 107.6ms/batch - loss: 8.24602 - diff: 9.36mlTrain batch 5/32 - 119.4ms/batch - loss: 7.75275 - diff: 9.03mlTrain batch 6/32 - 111.9ms/batch - loss: 9.58590 - diff: 9.13mlTrain batch 7/32 - 126.0ms/batch - loss: 13.08811 - diff: 10.40mlTrain batch 8/32 - 116.3ms/batch - loss: 11.90988 - diff: 9.91mlTrain batch 9/32 - 113.1ms/batch - loss: 11.14548 - diff: 9.56mlTrain batch 10/32 - 102.6ms/batch - loss: 11.52802 - diff: 9.78mlTrain batch 11/32 - 109.0ms/batch - loss: 10.82105 - diff: 9.47mlTrain batch 12/32 - 103.7ms/batch - loss: 10.50816 - diff: 9.31mlTrain batch 13/32 - 106.3ms/batch - loss: 10.02494 - diff: 9.15mlTrain batch 14/32 - 103.0ms/batch - loss: 9.69352 - diff: 9.05mlTrain batch 15/32 - 106.9ms/batch - loss: 11.39377 - diff: 9.90mlTrain batch 16/32 - 103.0ms/batch - loss: 10.90681 - diff: 9.66mlTrain batch 17/32 - 104.6ms/batch - loss: 10.72834 - diff: 9.65mlTrain batch 18/32 - 103.1ms/batch - loss: 10.36637 - diff: 9.49mlTrain batch 19/32 - 114.7ms/batch - loss: 10.41777 - diff: 9.55mlTrain batch 20/32 - 102.6ms/batch - loss: 10.37839 - diff: 9.57mlTrain batch 21/32 - 111.9ms/batch - loss: 10.02131 - diff: 9.38mlTrain batch 22/32 - 102.2ms/batch - loss: 9.84250 - diff: 9.32mlTrain batch 23/32 - 112.0ms/batch - loss: 9.78686 - diff: 9.30mlTrain batch 24/32 - 103.1ms/batch - loss: 9.72649 - diff: 9.32mlTrain batch 25/32 - 103.8ms/batch - loss: 10.23421 - diff: 9.60mlTrain batch 26/32 - 103.1ms/batch - loss: 10.50218 - diff: 9.78mlTrain batch 27/32 - 102.8ms/batch - loss: 10.30319 - diff: 9.69mlTrain batch 28/32 - 103.5ms/batch - loss: 10.03651 - diff: 9.53mlTrain batch 29/32 - 102.8ms/batch - loss: 10.20695 - diff: 9.62mlTrain batch 30/32 - 98.4ms/batch - loss: 10.40615 - diff: 9.77mlTrain batch 31/32 - 86.5ms/batch - loss: 10.27129 - diff: 9.69mlTrain batch 32/32 - 78.3ms/batch - loss: 10.65542 - diff: 9.73mlTrain batch 32/32 - 10.9s 78.3ms/batch - loss: 10.65542 - diff: 9.73ml
Test 0.6s: val_loss: 34.81185 - diff: 15.54ml

Epoch 100: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 119.2ms/batch - loss: 18.07304 - diff: 15.62mlTrain batch 2/32 - 103.5ms/batch - loss: 11.26361 - diff: 11.45mlTrain batch 3/32 - 104.6ms/batch - loss: 9.29666 - diff: 9.85mlTrain batch 4/32 - 105.3ms/batch - loss: 8.51459 - diff: 9.31mlTrain batch 5/32 - 106.0ms/batch - loss: 7.54677 - diff: 8.86mlTrain batch 6/32 - 116.4ms/batch - loss: 11.11630 - diff: 10.68mlTrain batch 7/32 - 114.5ms/batch - loss: 10.71941 - diff: 10.32mlTrain batch 8/32 - 108.2ms/batch - loss: 11.55121 - diff: 10.65mlTrain batch 9/32 - 103.7ms/batch - loss: 11.19124 - diff: 10.47mlTrain batch 10/32 - 94.9ms/batch - loss: 10.66506 - diff: 10.27mlTrain batch 11/32 - 117.7ms/batch - loss: 10.15812 - diff: 10.06mlTrain batch 12/32 - 114.9ms/batch - loss: 9.64556 - diff: 9.76mlTrain batch 13/32 - 104.4ms/batch - loss: 9.04741 - diff: 9.36mlTrain batch 14/32 - 105.6ms/batch - loss: 9.27180 - diff: 9.53mlTrain batch 15/32 - 105.5ms/batch - loss: 8.82842 - diff: 9.25mlTrain batch 16/32 - 104.8ms/batch - loss: 12.18936 - diff: 9.86mlTrain batch 17/32 - 105.2ms/batch - loss: 11.98998 - diff: 9.87mlTrain batch 18/32 - 107.0ms/batch - loss: 11.69451 - diff: 9.80mlTrain batch 19/32 - 104.7ms/batch - loss: 11.88546 - diff: 9.86mlTrain batch 20/32 - 111.8ms/batch - loss: 11.72755 - diff: 9.89mlTrain batch 21/32 - 111.3ms/batch - loss: 11.51795 - diff: 9.79mlTrain batch 22/32 - 103.2ms/batch - loss: 11.43902 - diff: 9.82mlTrain batch 23/32 - 103.3ms/batch - loss: 11.20287 - diff: 9.72mlTrain batch 24/32 - 104.6ms/batch - loss: 10.96611 - diff: 9.63mlTrain batch 25/32 - 113.5ms/batch - loss: 11.64314 - diff: 9.75mlTrain batch 26/32 - 99.7ms/batch - loss: 11.42791 - diff: 9.65mlTrain batch 27/32 - 113.9ms/batch - loss: 11.42097 - diff: 9.69mlTrain batch 28/32 - 93.8ms/batch - loss: 11.43423 - diff: 9.71mlTrain batch 29/32 - 87.7ms/batch - loss: 11.20725 - diff: 9.61mlTrain batch 30/32 - 113.2ms/batch - loss: 10.95420 - diff: 9.48mlTrain batch 31/32 - 110.7ms/batch - loss: 10.84050 - diff: 9.45mlTrain batch 32/32 - 70.3ms/batch - loss: 11.14689 - diff: 9.47mlTrain batch 32/32 - 11.9s 70.3ms/batch - loss: 11.14689 - diff: 9.47ml
Test 0.6s: val_loss: 32.94558 - diff: 15.48ml

Epoch 101: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 117.3ms/batch - loss: 9.80492 - diff: 10.25mlTrain batch 2/32 - 104.2ms/batch - loss: 10.23488 - diff: 9.83mlTrain batch 3/32 - 103.1ms/batch - loss: 10.52975 - diff: 10.23mlTrain batch 4/32 - 110.0ms/batch - loss: 9.89410 - diff: 10.09mlTrain batch 5/32 - 108.3ms/batch - loss: 10.59818 - diff: 10.38mlTrain batch 6/32 - 128.3ms/batch - loss: 10.17796 - diff: 10.19mlTrain batch 7/32 - 113.1ms/batch - loss: 9.68300 - diff: 9.85mlTrain batch 8/32 - 92.4ms/batch - loss: 9.55618 - diff: 9.75mlTrain batch 9/32 - 93.9ms/batch - loss: 10.13777 - diff: 9.89mlTrain batch 10/32 - 92.4ms/batch - loss: 9.59748 - diff: 9.60mlTrain batch 11/32 - 98.3ms/batch - loss: 9.00731 - diff: 9.27mlTrain batch 12/32 - 113.5ms/batch - loss: 8.84847 - diff: 9.27mlTrain batch 13/32 - 114.6ms/batch - loss: 8.60325 - diff: 9.12mlTrain batch 14/32 - 107.4ms/batch - loss: 8.62096 - diff: 9.16mlTrain batch 15/32 - 106.6ms/batch - loss: 8.98684 - diff: 9.34mlTrain batch 16/32 - 113.8ms/batch - loss: 9.81218 - diff: 9.81mlTrain batch 17/32 - 114.2ms/batch - loss: 9.49463 - diff: 9.66mlTrain batch 18/32 - 114.8ms/batch - loss: 11.43345 - diff: 10.40mlTrain batch 19/32 - 114.0ms/batch - loss: 11.17727 - diff: 10.32mlTrain batch 20/32 - 121.3ms/batch - loss: 10.84062 - diff: 10.16mlTrain batch 21/32 - 113.7ms/batch - loss: 10.49660 - diff: 9.99mlTrain batch 22/32 - 120.0ms/batch - loss: 10.33522 - diff: 9.93mlTrain batch 23/32 - 112.6ms/batch - loss: 10.22774 - diff: 9.84mlTrain batch 24/32 - 121.5ms/batch - loss: 9.93998 - diff: 9.71mlTrain batch 25/32 - 112.7ms/batch - loss: 10.07012 - diff: 9.75mlTrain batch 26/32 - 119.9ms/batch - loss: 9.90305 - diff: 9.67mlTrain batch 27/32 - 113.0ms/batch - loss: 10.07944 - diff: 9.83mlTrain batch 28/32 - 119.0ms/batch - loss: 9.98183 - diff: 9.78mlTrain batch 29/32 - 112.4ms/batch - loss: 9.87754 - diff: 9.74mlTrain batch 30/32 - 115.3ms/batch - loss: 10.58747 - diff: 10.07mlTrain batch 31/32 - 108.1ms/batch - loss: 10.87142 - diff: 10.14mlTrain batch 32/32 - 97.8ms/batch - loss: 12.02277 - diff: 10.25mlTrain batch 32/32 - 10.8s 97.8ms/batch - loss: 12.02277 - diff: 10.25ml
Test 0.7s: val_loss: 35.28557 - diff: 15.87ml

Epoch 102: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 118.7ms/batch - loss: 9.24218 - diff: 9.91mlTrain batch 2/32 - 104.5ms/batch - loss: 8.54567 - diff: 9.92mlTrain batch 3/32 - 104.3ms/batch - loss: 11.13985 - diff: 9.72mlTrain batch 4/32 - 104.6ms/batch - loss: 12.26163 - diff: 10.89mlTrain batch 5/32 - 104.8ms/batch - loss: 11.06471 - diff: 10.34mlTrain batch 6/32 - 102.8ms/batch - loss: 10.87841 - diff: 10.33mlTrain batch 7/32 - 106.5ms/batch - loss: 11.22138 - diff: 10.68mlTrain batch 8/32 - 106.3ms/batch - loss: 10.30658 - diff: 10.10mlTrain batch 9/32 - 107.5ms/batch - loss: 10.02327 - diff: 10.05mlTrain batch 10/32 - 105.9ms/batch - loss: 9.49253 - diff: 9.75mlTrain batch 11/32 - 118.4ms/batch - loss: 9.35447 - diff: 9.77mlTrain batch 12/32 - 103.8ms/batch - loss: 9.01091 - diff: 9.54mlTrain batch 13/32 - 114.0ms/batch - loss: 8.71249 - diff: 9.43mlTrain batch 14/32 - 102.9ms/batch - loss: 8.38230 - diff: 9.23mlTrain batch 15/32 - 110.6ms/batch - loss: 8.20508 - diff: 9.12mlTrain batch 16/32 - 103.5ms/batch - loss: 8.20482 - diff: 9.16mlTrain batch 17/32 - 112.2ms/batch - loss: 11.73255 - diff: 9.87mlTrain batch 18/32 - 103.4ms/batch - loss: 11.31788 - diff: 9.68mlTrain batch 19/32 - 110.5ms/batch - loss: 11.05621 - diff: 9.61mlTrain batch 20/32 - 103.0ms/batch - loss: 11.38622 - diff: 9.71mlTrain batch 21/32 - 107.3ms/batch - loss: 11.08848 - diff: 9.54mlTrain batch 22/32 - 106.1ms/batch - loss: 10.92416 - diff: 9.48mlTrain batch 23/32 - 103.2ms/batch - loss: 10.61938 - diff: 9.35mlTrain batch 24/32 - 104.2ms/batch - loss: 10.55055 - diff: 9.40mlTrain batch 25/32 - 107.1ms/batch - loss: 10.33650 - diff: 9.28mlTrain batch 26/32 - 106.8ms/batch - loss: 10.38027 - diff: 9.33mlTrain batch 27/32 - 106.7ms/batch - loss: 10.14590 - diff: 9.22mlTrain batch 28/32 - 106.6ms/batch - loss: 10.56477 - diff: 9.50mlTrain batch 29/32 - 109.4ms/batch - loss: 10.50151 - diff: 9.43mlTrain batch 30/32 - 105.2ms/batch - loss: 10.32065 - diff: 9.36mlTrain batch 31/32 - 90.7ms/batch - loss: 10.15579 - diff: 9.32mlTrain batch 32/32 - 83.9ms/batch - loss: 10.73909 - diff: 9.37mlTrain batch 32/32 - 10.8s 83.9ms/batch - loss: 10.73909 - diff: 9.37ml
Test 0.6s: val_loss: 35.06920 - diff: 15.48ml

Epoch 103: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 136.8ms/batch - loss: 4.84634 - diff: 6.87mlTrain batch 2/32 - 114.1ms/batch - loss: 6.11155 - diff: 7.53mlTrain batch 3/32 - 113.4ms/batch - loss: 7.84904 - diff: 8.63mlTrain batch 4/32 - 114.5ms/batch - loss: 7.46089 - diff: 8.56mlTrain batch 5/32 - 114.3ms/batch - loss: 10.37333 - diff: 10.15mlTrain batch 6/32 - 112.8ms/batch - loss: 9.15105 - diff: 9.32mlTrain batch 7/32 - 113.9ms/batch - loss: 8.48875 - diff: 8.97mlTrain batch 8/32 - 113.2ms/batch - loss: 8.62261 - diff: 9.24mlTrain batch 9/32 - 113.4ms/batch - loss: 8.86395 - diff: 9.22mlTrain batch 10/32 - 113.7ms/batch - loss: 11.86170 - diff: 10.51mlTrain batch 11/32 - 112.8ms/batch - loss: 11.44177 - diff: 10.29mlTrain batch 12/32 - 113.4ms/batch - loss: 11.44245 - diff: 10.48mlTrain batch 13/32 - 108.9ms/batch - loss: 12.68689 - diff: 10.71mlTrain batch 14/32 - 104.9ms/batch - loss: 12.52379 - diff: 10.71mlTrain batch 15/32 - 107.4ms/batch - loss: 12.21640 - diff: 10.61mlTrain batch 16/32 - 101.1ms/batch - loss: 11.92936 - diff: 10.49mlTrain batch 17/32 - 116.3ms/batch - loss: 11.49989 - diff: 10.26mlTrain batch 18/32 - 106.1ms/batch - loss: 12.06143 - diff: 10.39mlTrain batch 19/32 - 110.5ms/batch - loss: 11.73602 - diff: 10.25mlTrain batch 20/32 - 105.0ms/batch - loss: 11.37232 - diff: 10.07mlTrain batch 21/32 - 107.9ms/batch - loss: 11.08956 - diff: 9.92mlTrain batch 22/32 - 104.7ms/batch - loss: 11.26068 - diff: 10.03mlTrain batch 23/32 - 105.9ms/batch - loss: 10.98495 - diff: 9.92mlTrain batch 24/32 - 104.7ms/batch - loss: 10.72227 - diff: 9.79mlTrain batch 25/32 - 104.8ms/batch - loss: 10.72758 - diff: 9.79mlTrain batch 26/32 - 104.3ms/batch - loss: 10.52955 - diff: 9.75mlTrain batch 27/32 - 105.1ms/batch - loss: 10.32674 - diff: 9.66mlTrain batch 28/32 - 105.3ms/batch - loss: 10.35525 - diff: 9.65mlTrain batch 29/32 - 105.5ms/batch - loss: 10.25149 - diff: 9.63mlTrain batch 30/32 - 103.6ms/batch - loss: 10.08977 - diff: 9.54mlTrain batch 31/32 - 94.3ms/batch - loss: 10.23200 - diff: 9.59mlTrain batch 32/32 - 78.4ms/batch - loss: 10.33511 - diff: 9.58mlTrain batch 32/32 - 10.7s 78.4ms/batch - loss: 10.33511 - diff: 9.58ml
Test 0.6s: val_loss: 29.44245 - diff: 15.12ml

Epoch 104: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 130.2ms/batch - loss: 10.14924 - diff: 11.12mlTrain batch 2/32 - 113.7ms/batch - loss: 7.95552 - diff: 9.76mlTrain batch 3/32 - 104.4ms/batch - loss: 8.01727 - diff: 9.86mlTrain batch 4/32 - 104.2ms/batch - loss: 8.54052 - diff: 10.15mlTrain batch 5/32 - 106.0ms/batch - loss: 8.38629 - diff: 9.84mlTrain batch 6/32 - 104.0ms/batch - loss: 14.22374 - diff: 11.47mlTrain batch 7/32 - 111.9ms/batch - loss: 12.94034 - diff: 10.98mlTrain batch 8/32 - 102.6ms/batch - loss: 12.57588 - diff: 10.84mlTrain batch 9/32 - 111.2ms/batch - loss: 11.60703 - diff: 10.40mlTrain batch 10/32 - 102.8ms/batch - loss: 11.11340 - diff: 10.18mlTrain batch 11/32 - 102.9ms/batch - loss: 10.58011 - diff: 9.91mlTrain batch 12/32 - 103.2ms/batch - loss: 10.32326 - diff: 9.83mlTrain batch 13/32 - 98.4ms/batch - loss: 10.33759 - diff: 9.82mlTrain batch 14/32 - 102.2ms/batch - loss: 11.11349 - diff: 10.31mlTrain batch 15/32 - 112.3ms/batch - loss: 10.66640 - diff: 10.11mlTrain batch 16/32 - 102.7ms/batch - loss: 10.29710 - diff: 9.94mlTrain batch 17/32 - 113.2ms/batch - loss: 9.96210 - diff: 9.78mlTrain batch 18/32 - 104.2ms/batch - loss: 9.83592 - diff: 9.77mlTrain batch 19/32 - 99.9ms/batch - loss: 9.90400 - diff: 9.89mlTrain batch 20/32 - 106.5ms/batch - loss: 9.54602 - diff: 9.66mlTrain batch 21/32 - 107.1ms/batch - loss: 9.41647 - diff: 9.56mlTrain batch 22/32 - 106.7ms/batch - loss: 9.24145 - diff: 9.40mlTrain batch 23/32 - 105.6ms/batch - loss: 9.81190 - diff: 9.74mlTrain batch 24/32 - 104.8ms/batch - loss: 9.73677 - diff: 9.71mlTrain batch 25/32 - 109.3ms/batch - loss: 9.54886 - diff: 9.64mlTrain batch 26/32 - 109.0ms/batch - loss: 9.36353 - diff: 9.49mlTrain batch 27/32 - 109.8ms/batch - loss: 9.25513 - diff: 9.41mlTrain batch 28/32 - 90.8ms/batch - loss: 9.07471 - diff: 9.33mlTrain batch 29/32 - 91.0ms/batch - loss: 9.06283 - diff: 9.36mlTrain batch 30/32 - 102.6ms/batch - loss: 9.25160 - diff: 9.34mlTrain batch 31/32 - 96.7ms/batch - loss: 9.04857 - diff: 9.22mlTrain batch 32/32 - 76.0ms/batch - loss: 9.22736 - diff: 9.23mlTrain batch 32/32 - 11.2s 76.0ms/batch - loss: 9.22736 - diff: 9.23ml
Test 0.6s: val_loss: 30.63506 - diff: 14.88ml

Epoch 105: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 121.3ms/batch - loss: 6.01190 - diff: 7.47mlTrain batch 2/32 - 108.3ms/batch - loss: 7.61898 - diff: 9.11mlTrain batch 3/32 - 114.5ms/batch - loss: 7.02608 - diff: 8.95mlTrain batch 4/32 - 121.8ms/batch - loss: 8.04002 - diff: 9.37mlTrain batch 5/32 - 116.9ms/batch - loss: 7.46023 - diff: 9.01mlTrain batch 6/32 - 114.1ms/batch - loss: 7.09011 - diff: 8.77mlTrain batch 7/32 - 99.5ms/batch - loss: 7.16218 - diff: 8.92mlTrain batch 8/32 - 88.4ms/batch - loss: 7.94655 - diff: 9.38mlTrain batch 9/32 - 116.2ms/batch - loss: 8.09229 - diff: 9.52mlTrain batch 10/32 - 89.8ms/batch - loss: 7.67333 - diff: 9.23mlTrain batch 11/32 - 119.1ms/batch - loss: 8.90725 - diff: 9.57mlTrain batch 12/32 - 122.6ms/batch - loss: 9.39570 - diff: 9.77mlTrain batch 13/32 - 109.9ms/batch - loss: 9.02747 - diff: 9.51mlTrain batch 14/32 - 104.8ms/batch - loss: 12.24028 - diff: 10.75mlTrain batch 15/32 - 103.3ms/batch - loss: 12.09923 - diff: 10.69mlTrain batch 16/32 - 105.0ms/batch - loss: 12.32723 - diff: 10.87mlTrain batch 17/32 - 104.0ms/batch - loss: 11.88217 - diff: 10.61mlTrain batch 18/32 - 112.1ms/batch - loss: 12.79821 - diff: 11.00mlTrain batch 19/32 - 103.8ms/batch - loss: 12.50290 - diff: 10.89mlTrain batch 20/32 - 111.0ms/batch - loss: 12.08935 - diff: 10.65mlTrain batch 21/32 - 110.5ms/batch - loss: 11.86998 - diff: 10.59mlTrain batch 22/32 - 103.9ms/batch - loss: 11.43573 - diff: 10.33mlTrain batch 23/32 - 104.2ms/batch - loss: 11.14351 - diff: 10.18mlTrain batch 24/32 - 111.8ms/batch - loss: 10.89993 - diff: 10.07mlTrain batch 25/32 - 105.1ms/batch - loss: 10.52201 - diff: 9.81mlTrain batch 26/32 - 109.8ms/batch - loss: 10.42240 - diff: 9.80mlTrain batch 27/32 - 110.8ms/batch - loss: 10.21175 - diff: 9.68mlTrain batch 28/32 - 103.8ms/batch - loss: 9.91660 - diff: 9.48mlTrain batch 29/32 - 97.3ms/batch - loss: 9.70681 - diff: 9.37mlTrain batch 30/32 - 107.0ms/batch - loss: 9.71239 - diff: 9.37mlTrain batch 31/32 - 90.9ms/batch - loss: 10.86178 - diff: 9.85mlTrain batch 32/32 - 67.6ms/batch - loss: 11.38537 - diff: 9.88mlTrain batch 32/32 - 11.5s 67.6ms/batch - loss: 11.38537 - diff: 9.88ml
Test 0.6s: val_loss: 31.07013 - diff: 15.24ml

Epoch 106: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 118.6ms/batch - loss: 4.62062 - diff: 7.29mlTrain batch 2/32 - 103.9ms/batch - loss: 3.95055 - diff: 6.77mlTrain batch 3/32 - 104.0ms/batch - loss: 6.52573 - diff: 7.45mlTrain batch 4/32 - 104.1ms/batch - loss: 6.48170 - diff: 7.39mlTrain batch 5/32 - 104.5ms/batch - loss: 7.71784 - diff: 8.52mlTrain batch 6/32 - 105.1ms/batch - loss: 8.44140 - diff: 9.14mlTrain batch 7/32 - 96.9ms/batch - loss: 9.00052 - diff: 9.42mlTrain batch 8/32 - 103.4ms/batch - loss: 10.46604 - diff: 10.12mlTrain batch 9/32 - 107.1ms/batch - loss: 12.29700 - diff: 10.93mlTrain batch 10/32 - 106.7ms/batch - loss: 12.18119 - diff: 10.93mlTrain batch 11/32 - 92.1ms/batch - loss: 13.35034 - diff: 11.53mlTrain batch 12/32 - 101.9ms/batch - loss: 12.85838 - diff: 11.27mlTrain batch 13/32 - 112.2ms/batch - loss: 12.87917 - diff: 11.23mlTrain batch 14/32 - 103.4ms/batch - loss: 12.20758 - diff: 10.89mlTrain batch 15/32 - 103.9ms/batch - loss: 11.89707 - diff: 10.70mlTrain batch 16/32 - 111.0ms/batch - loss: 11.75631 - diff: 10.56mlTrain batch 17/32 - 103.6ms/batch - loss: 11.59483 - diff: 10.48mlTrain batch 18/32 - 107.8ms/batch - loss: 13.65524 - diff: 10.73mlTrain batch 19/32 - 105.6ms/batch - loss: 13.27357 - diff: 10.60mlTrain batch 20/32 - 108.1ms/batch - loss: 12.81253 - diff: 10.41mlTrain batch 21/32 - 113.2ms/batch - loss: 12.87260 - diff: 10.57mlTrain batch 22/32 - 105.3ms/batch - loss: 12.57749 - diff: 10.45mlTrain batch 23/32 - 114.7ms/batch - loss: 12.15556 - diff: 10.25mlTrain batch 24/32 - 106.6ms/batch - loss: 11.85955 - diff: 10.11mlTrain batch 25/32 - 106.9ms/batch - loss: 11.68159 - diff: 10.05mlTrain batch 26/32 - 115.2ms/batch - loss: 11.54287 - diff: 10.02mlTrain batch 27/32 - 106.8ms/batch - loss: 11.39345 - diff: 10.01mlTrain batch 28/32 - 104.4ms/batch - loss: 11.24521 - diff: 9.97mlTrain batch 29/32 - 117.0ms/batch - loss: 11.35348 - diff: 10.11mlTrain batch 30/32 - 113.4ms/batch - loss: 11.70266 - diff: 10.33mlTrain batch 31/32 - 115.3ms/batch - loss: 11.47689 - diff: 10.24mlTrain batch 32/32 - 109.7ms/batch - loss: 11.95215 - diff: 10.29mlTrain batch 32/32 - 11.8s 109.7ms/batch - loss: 11.95215 - diff: 10.29ml
Test 0.7s: val_loss: 32.11369 - diff: 15.62ml

Epoch 107: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 124.2ms/batch - loss: 13.81330 - diff: 12.48mlTrain batch 2/32 - 108.0ms/batch - loss: 13.49547 - diff: 13.17mlTrain batch 3/32 - 107.7ms/batch - loss: 10.21684 - diff: 10.93mlTrain batch 4/32 - 104.0ms/batch - loss: 8.43374 - diff: 9.49mlTrain batch 5/32 - 104.9ms/batch - loss: 10.05116 - diff: 10.24mlTrain batch 6/32 - 104.7ms/batch - loss: 10.34846 - diff: 10.18mlTrain batch 7/32 - 105.1ms/batch - loss: 10.50102 - diff: 10.34mlTrain batch 8/32 - 114.7ms/batch - loss: 10.34101 - diff: 10.30mlTrain batch 9/32 - 116.3ms/batch - loss: 9.40983 - diff: 9.67mlTrain batch 10/32 - 114.6ms/batch - loss: 10.80615 - diff: 10.18mlTrain batch 11/32 - 114.2ms/batch - loss: 9.99850 - diff: 9.67mlTrain batch 12/32 - 112.8ms/batch - loss: 9.55965 - diff: 9.43mlTrain batch 13/32 - 117.8ms/batch - loss: 9.40772 - diff: 9.33mlTrain batch 14/32 - 120.1ms/batch - loss: 9.14541 - diff: 9.20mlTrain batch 15/32 - 94.2ms/batch - loss: 9.05835 - diff: 9.20mlTrain batch 16/32 - 105.6ms/batch - loss: 9.09963 - diff: 9.14mlTrain batch 17/32 - 104.7ms/batch - loss: 8.83274 - diff: 9.00mlTrain batch 18/32 - 105.3ms/batch - loss: 8.73036 - diff: 8.87mlTrain batch 19/32 - 104.2ms/batch - loss: 8.48079 - diff: 8.72mlTrain batch 20/32 - 105.7ms/batch - loss: 8.28664 - diff: 8.64mlTrain batch 21/32 - 105.4ms/batch - loss: 8.31375 - diff: 8.66mlTrain batch 22/32 - 106.1ms/batch - loss: 8.42487 - diff: 8.66mlTrain batch 23/32 - 104.1ms/batch - loss: 8.29877 - diff: 8.62mlTrain batch 24/32 - 105.7ms/batch - loss: 8.21146 - diff: 8.60mlTrain batch 25/32 - 104.2ms/batch - loss: 8.04707 - diff: 8.52mlTrain batch 26/32 - 105.1ms/batch - loss: 8.00556 - diff: 8.52mlTrain batch 27/32 - 105.2ms/batch - loss: 7.93281 - diff: 8.50mlTrain batch 28/32 - 106.1ms/batch - loss: 8.03936 - diff: 8.59mlTrain batch 29/32 - 104.8ms/batch - loss: 7.99052 - diff: 8.56mlTrain batch 30/32 - 108.9ms/batch - loss: 8.01561 - diff: 8.56mlTrain batch 31/32 - 103.1ms/batch - loss: 7.94907 - diff: 8.51mlTrain batch 32/32 - 96.5ms/batch - loss: 8.43299 - diff: 8.54mlTrain batch 32/32 - 11.0s 96.5ms/batch - loss: 8.43299 - diff: 8.54ml
Test 0.7s: val_loss: 32.08818 - diff: 15.52ml

Epoch 108: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 130.1ms/batch - loss: 4.95398 - diff: 7.00mlTrain batch 2/32 - 107.8ms/batch - loss: 13.16545 - diff: 12.05mlTrain batch 3/32 - 108.2ms/batch - loss: 12.56754 - diff: 11.87mlTrain batch 4/32 - 107.0ms/batch - loss: 10.61859 - diff: 10.63mlTrain batch 5/32 - 108.1ms/batch - loss: 9.24545 - diff: 9.63mlTrain batch 6/32 - 108.4ms/batch - loss: 10.59447 - diff: 10.39mlTrain batch 7/32 - 109.3ms/batch - loss: 11.13158 - diff: 10.83mlTrain batch 8/32 - 116.7ms/batch - loss: 12.01344 - diff: 11.43mlTrain batch 9/32 - 111.4ms/batch - loss: 11.82627 - diff: 11.29mlTrain batch 10/32 - 116.2ms/batch - loss: 11.74339 - diff: 11.26mlTrain batch 11/32 - 115.2ms/batch - loss: 12.59675 - diff: 11.62mlTrain batch 12/32 - 114.9ms/batch - loss: 12.04930 - diff: 11.34mlTrain batch 13/32 - 114.2ms/batch - loss: 11.73835 - diff: 10.96mlTrain batch 14/32 - 106.4ms/batch - loss: 11.34932 - diff: 10.70mlTrain batch 15/32 - 107.8ms/batch - loss: 10.95568 - diff: 10.43mlTrain batch 16/32 - 102.1ms/batch - loss: 10.58892 - diff: 10.20mlTrain batch 17/32 - 101.8ms/batch - loss: 10.50553 - diff: 10.19mlTrain batch 18/32 - 88.7ms/batch - loss: 10.12271 - diff: 9.96mlTrain batch 19/32 - 113.4ms/batch - loss: 9.82046 - diff: 9.79mlTrain batch 20/32 - 108.1ms/batch - loss: 9.96396 - diff: 9.83mlTrain batch 21/32 - 102.7ms/batch - loss: 9.89843 - diff: 9.80mlTrain batch 22/32 - 102.5ms/batch - loss: 9.72070 - diff: 9.72mlTrain batch 23/32 - 103.3ms/batch - loss: 9.65928 - diff: 9.73mlTrain batch 24/32 - 99.8ms/batch - loss: 9.37719 - diff: 9.55mlTrain batch 25/32 - 103.4ms/batch - loss: 9.10549 - diff: 9.35mlTrain batch 26/32 - 102.4ms/batch - loss: 9.93454 - diff: 9.73mlTrain batch 27/32 - 102.2ms/batch - loss: 9.89850 - diff: 9.69mlTrain batch 28/32 - 103.7ms/batch - loss: 9.72073 - diff: 9.58mlTrain batch 29/32 - 118.8ms/batch - loss: 9.45580 - diff: 9.41mlTrain batch 30/32 - 104.9ms/batch - loss: 9.79391 - diff: 9.61mlTrain batch 31/32 - 115.3ms/batch - loss: 10.18336 - diff: 9.84mlTrain batch 32/32 - 108.8ms/batch - loss: 10.19658 - diff: 9.80mlTrain batch 32/32 - 11.9s 108.8ms/batch - loss: 10.19658 - diff: 9.80ml
Test 0.6s: val_loss: 33.50329 - diff: 15.74ml
Epoch   109: reducing learning rate of group 0 to 6.2500e-05.

Epoch 109: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 131.6ms/batch - loss: 6.28896 - diff: 8.38mlTrain batch 2/32 - 116.1ms/batch - loss: 7.70876 - diff: 8.89mlTrain batch 3/32 - 116.0ms/batch - loss: 7.41041 - diff: 9.07mlTrain batch 4/32 - 116.0ms/batch - loss: 6.63223 - diff: 8.51mlTrain batch 5/32 - 115.9ms/batch - loss: 6.19082 - diff: 8.10mlTrain batch 6/32 - 116.4ms/batch - loss: 6.53082 - diff: 8.37mlTrain batch 7/32 - 115.3ms/batch - loss: 6.18804 - diff: 8.12mlTrain batch 8/32 - 115.0ms/batch - loss: 7.99871 - diff: 8.89mlTrain batch 9/32 - 115.1ms/batch - loss: 7.74753 - diff: 8.74mlTrain batch 10/32 - 95.4ms/batch - loss: 7.74767 - diff: 8.83mlTrain batch 11/32 - 96.6ms/batch - loss: 7.68178 - diff: 8.80mlTrain batch 12/32 - 99.0ms/batch - loss: 7.44663 - diff: 8.70mlTrain batch 13/32 - 117.0ms/batch - loss: 7.38456 - diff: 8.43mlTrain batch 14/32 - 116.4ms/batch - loss: 7.62879 - diff: 8.67mlTrain batch 15/32 - 111.5ms/batch - loss: 8.40349 - diff: 8.88mlTrain batch 16/32 - 113.3ms/batch - loss: 8.26500 - diff: 8.83mlTrain batch 17/32 - 107.2ms/batch - loss: 8.23533 - diff: 8.79mlTrain batch 18/32 - 106.8ms/batch - loss: 7.98788 - diff: 8.59mlTrain batch 19/32 - 106.8ms/batch - loss: 7.85146 - diff: 8.56mlTrain batch 20/32 - 106.4ms/batch - loss: 7.82819 - diff: 8.52mlTrain batch 21/32 - 104.4ms/batch - loss: 7.68970 - diff: 8.47mlTrain batch 22/32 - 102.3ms/batch - loss: 7.76925 - diff: 8.57mlTrain batch 23/32 - 104.0ms/batch - loss: 7.54831 - diff: 8.41mlTrain batch 24/32 - 88.0ms/batch - loss: 7.41737 - diff: 8.37mlTrain batch 25/32 - 103.5ms/batch - loss: 8.48141 - diff: 8.90mlTrain batch 26/32 - 104.5ms/batch - loss: 8.28795 - diff: 8.81mlTrain batch 27/32 - 104.0ms/batch - loss: 8.24624 - diff: 8.84mlTrain batch 28/32 - 103.5ms/batch - loss: 8.29981 - diff: 8.93mlTrain batch 29/32 - 106.4ms/batch - loss: 8.39420 - diff: 8.99mlTrain batch 30/32 - 102.5ms/batch - loss: 8.25159 - diff: 8.91mlTrain batch 31/32 - 88.2ms/batch - loss: 8.39905 - diff: 8.99mlTrain batch 32/32 - 78.1ms/batch - loss: 8.51245 - diff: 8.99mlTrain batch 32/32 - 10.9s 78.1ms/batch - loss: 8.51245 - diff: 8.99ml
Test 0.6s: val_loss: 36.82619 - diff: 15.48ml

Epoch 110: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 128.8ms/batch - loss: 3.80126 - diff: 6.15mlTrain batch 2/32 - 114.3ms/batch - loss: 5.05931 - diff: 7.50mlTrain batch 3/32 - 107.2ms/batch - loss: 5.37423 - diff: 7.92mlTrain batch 4/32 - 107.9ms/batch - loss: 5.85496 - diff: 8.09mlTrain batch 5/32 - 108.4ms/batch - loss: 7.15764 - diff: 9.06mlTrain batch 6/32 - 106.6ms/batch - loss: 7.16268 - diff: 9.06mlTrain batch 7/32 - 108.2ms/batch - loss: 6.84134 - diff: 8.68mlTrain batch 8/32 - 114.1ms/batch - loss: 6.19883 - diff: 8.12mlTrain batch 9/32 - 105.5ms/batch - loss: 6.58738 - diff: 8.35mlTrain batch 10/32 - 109.6ms/batch - loss: 6.99360 - diff: 8.51mlTrain batch 11/32 - 107.6ms/batch - loss: 7.45609 - diff: 8.80mlTrain batch 12/32 - 107.7ms/batch - loss: 7.32422 - diff: 8.74mlTrain batch 13/32 - 116.2ms/batch - loss: 7.06310 - diff: 8.59mlTrain batch 14/32 - 108.3ms/batch - loss: 6.72938 - diff: 8.33mlTrain batch 15/32 - 107.1ms/batch - loss: 6.64720 - diff: 8.30mlTrain batch 16/32 - 115.5ms/batch - loss: 6.68614 - diff: 8.33mlTrain batch 17/32 - 108.7ms/batch - loss: 7.14163 - diff: 8.34mlTrain batch 18/32 - 116.3ms/batch - loss: 7.14413 - diff: 8.40mlTrain batch 19/32 - 102.9ms/batch - loss: 6.89877 - diff: 8.22mlTrain batch 20/32 - 103.3ms/batch - loss: 7.13801 - diff: 8.38mlTrain batch 21/32 - 98.0ms/batch - loss: 7.13988 - diff: 8.42mlTrain batch 22/32 - 104.7ms/batch - loss: 7.99245 - diff: 8.90mlTrain batch 23/32 - 107.7ms/batch - loss: 7.97475 - diff: 8.89mlTrain batch 24/32 - 106.1ms/batch - loss: 8.05625 - diff: 9.01mlTrain batch 25/32 - 108.6ms/batch - loss: 7.81815 - diff: 8.82mlTrain batch 26/32 - 106.9ms/batch - loss: 7.64309 - diff: 8.72mlTrain batch 27/32 - 116.1ms/batch - loss: 7.79943 - diff: 8.69mlTrain batch 28/32 - 116.9ms/batch - loss: 7.74679 - diff: 8.69mlTrain batch 29/32 - 116.3ms/batch - loss: 7.64563 - diff: 8.64mlTrain batch 30/32 - 116.1ms/batch - loss: 8.26886 - diff: 9.00mlTrain batch 31/32 - 116.3ms/batch - loss: 8.22614 - diff: 8.95mlTrain batch 32/32 - 110.1ms/batch - loss: 10.64110 - diff: 9.14mlTrain batch 32/32 - 12.0s 110.1ms/batch - loss: 10.64110 - diff: 9.14ml
Test 0.7s: val_loss: 39.16158 - diff: 15.89ml

Epoch 111: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 134.3ms/batch - loss: 8.78370 - diff: 10.02mlTrain batch 2/32 - 118.3ms/batch - loss: 6.48516 - diff: 8.01mlTrain batch 3/32 - 117.1ms/batch - loss: 6.06144 - diff: 7.77mlTrain batch 4/32 - 116.7ms/batch - loss: 6.08599 - diff: 7.89mlTrain batch 5/32 - 114.3ms/batch - loss: 6.05197 - diff: 7.70mlTrain batch 6/32 - 113.5ms/batch - loss: 7.04553 - diff: 8.42mlTrain batch 7/32 - 119.6ms/batch - loss: 7.23356 - diff: 8.58mlTrain batch 8/32 - 115.7ms/batch - loss: 7.26006 - diff: 8.52mlTrain batch 9/32 - 116.2ms/batch - loss: 7.30993 - diff: 8.66mlTrain batch 10/32 - 108.0ms/batch - loss: 6.73798 - diff: 8.21mlTrain batch 11/32 - 107.6ms/batch - loss: 6.54959 - diff: 8.11mlTrain batch 12/32 - 101.6ms/batch - loss: 6.67949 - diff: 8.13mlTrain batch 13/32 - 107.8ms/batch - loss: 6.56362 - diff: 8.03mlTrain batch 14/32 - 102.2ms/batch - loss: 6.47431 - diff: 7.94mlTrain batch 15/32 - 104.2ms/batch - loss: 6.57263 - diff: 8.04mlTrain batch 16/32 - 103.8ms/batch - loss: 6.71016 - diff: 8.13mlTrain batch 17/32 - 104.5ms/batch - loss: 6.79078 - diff: 8.23mlTrain batch 18/32 - 103.1ms/batch - loss: 7.57220 - diff: 8.58mlTrain batch 19/32 - 102.9ms/batch - loss: 7.49466 - diff: 8.52mlTrain batch 20/32 - 103.6ms/batch - loss: 7.52566 - diff: 8.54mlTrain batch 21/32 - 104.5ms/batch - loss: 7.33191 - diff: 8.41mlTrain batch 22/32 - 104.2ms/batch - loss: 7.69542 - diff: 8.59mlTrain batch 23/32 - 103.1ms/batch - loss: 7.83566 - diff: 8.72mlTrain batch 24/32 - 102.9ms/batch - loss: 7.60914 - diff: 8.56mlTrain batch 25/32 - 108.4ms/batch - loss: 7.84151 - diff: 8.74mlTrain batch 26/32 - 107.4ms/batch - loss: 7.76792 - diff: 8.72mlTrain batch 27/32 - 108.0ms/batch - loss: 7.70315 - diff: 8.69mlTrain batch 28/32 - 108.0ms/batch - loss: 7.62535 - diff: 8.66mlTrain batch 29/32 - 107.5ms/batch - loss: 7.45477 - diff: 8.54mlTrain batch 30/32 - 111.6ms/batch - loss: 7.27960 - diff: 8.42mlTrain batch 31/32 - 105.9ms/batch - loss: 7.48075 - diff: 8.52mlTrain batch 32/32 - 73.8ms/batch - loss: 8.05345 - diff: 8.58mlTrain batch 32/32 - 11.0s 73.8ms/batch - loss: 8.05345 - diff: 8.58ml
Test 0.6s: val_loss: 33.62830 - diff: 15.50ml

Epoch 112: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 128.3ms/batch - loss: 5.78624 - diff: 8.26mlTrain batch 2/32 - 107.8ms/batch - loss: 3.70590 - diff: 6.26mlTrain batch 3/32 - 107.8ms/batch - loss: 4.54921 - diff: 6.84mlTrain batch 4/32 - 107.0ms/batch - loss: 8.49028 - diff: 9.22mlTrain batch 5/32 - 103.8ms/batch - loss: 8.53418 - diff: 9.27mlTrain batch 6/32 - 103.6ms/batch - loss: 8.70428 - diff: 9.01mlTrain batch 7/32 - 104.3ms/batch - loss: 8.98851 - diff: 9.04mlTrain batch 8/32 - 111.5ms/batch - loss: 8.22211 - diff: 8.54mlTrain batch 9/32 - 106.6ms/batch - loss: 8.40769 - diff: 8.67mlTrain batch 10/32 - 106.6ms/batch - loss: 7.92494 - diff: 8.39mlTrain batch 11/32 - 125.1ms/batch - loss: 7.43642 - diff: 8.12mlTrain batch 12/32 - 120.2ms/batch - loss: 7.51212 - diff: 8.22mlTrain batch 13/32 - 113.5ms/batch - loss: 7.11326 - diff: 7.97mlTrain batch 14/32 - 118.4ms/batch - loss: 7.34693 - diff: 8.13mlTrain batch 15/32 - 113.8ms/batch - loss: 7.45319 - diff: 8.29mlTrain batch 16/32 - 109.1ms/batch - loss: 8.24366 - diff: 8.54mlTrain batch 17/32 - 116.2ms/batch - loss: 8.65225 - diff: 8.86mlTrain batch 18/32 - 122.0ms/batch - loss: 10.28590 - diff: 9.60mlTrain batch 19/32 - 105.8ms/batch - loss: 10.01787 - diff: 9.50mlTrain batch 20/32 - 109.7ms/batch - loss: 9.65497 - diff: 9.31mlTrain batch 21/32 - 106.9ms/batch - loss: 9.94892 - diff: 9.56mlTrain batch 22/32 - 106.6ms/batch - loss: 9.86327 - diff: 9.55mlTrain batch 23/32 - 106.4ms/batch - loss: 10.92310 - diff: 10.06mlTrain batch 24/32 - 104.2ms/batch - loss: 11.28403 - diff: 10.30mlTrain batch 25/32 - 106.5ms/batch - loss: 11.24142 - diff: 10.34mlTrain batch 26/32 - 109.7ms/batch - loss: 10.94655 - diff: 10.18mlTrain batch 27/32 - 102.6ms/batch - loss: 10.90975 - diff: 10.22mlTrain batch 28/32 - 108.7ms/batch - loss: 10.59846 - diff: 10.02mlTrain batch 29/32 - 93.7ms/batch - loss: 10.39014 - diff: 9.92mlTrain batch 30/32 - 104.3ms/batch - loss: 10.32209 - diff: 9.87mlTrain batch 31/32 - 87.6ms/batch - loss: 10.10155 - diff: 9.73mlTrain batch 32/32 - 76.8ms/batch - loss: 10.65130 - diff: 9.79mlTrain batch 32/32 - 11.3s 76.8ms/batch - loss: 10.65130 - diff: 9.79ml
Test 0.6s: val_loss: 30.93957 - diff: 15.36ml

Epoch 113: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 128.0ms/batch - loss: 9.49140 - diff: 9.98mlTrain batch 2/32 - 112.2ms/batch - loss: 6.83388 - diff: 8.36mlTrain batch 3/32 - 123.0ms/batch - loss: 6.47921 - diff: 8.05mlTrain batch 4/32 - 106.7ms/batch - loss: 6.75357 - diff: 7.50mlTrain batch 5/32 - 125.1ms/batch - loss: 5.83435 - diff: 6.95mlTrain batch 6/32 - 108.2ms/batch - loss: 5.62122 - diff: 6.71mlTrain batch 7/32 - 116.5ms/batch - loss: 5.54387 - diff: 6.81mlTrain batch 8/32 - 117.8ms/batch - loss: 5.48213 - diff: 6.89mlTrain batch 9/32 - 110.7ms/batch - loss: 5.79402 - diff: 7.28mlTrain batch 10/32 - 104.5ms/batch - loss: 5.63486 - diff: 7.14mlTrain batch 11/32 - 103.9ms/batch - loss: 5.39191 - diff: 7.01mlTrain batch 12/32 - 103.2ms/batch - loss: 5.30772 - diff: 7.01mlTrain batch 13/32 - 90.1ms/batch - loss: 5.88401 - diff: 7.40mlTrain batch 14/32 - 99.1ms/batch - loss: 5.97551 - diff: 7.54mlTrain batch 15/32 - 95.0ms/batch - loss: 5.69682 - diff: 7.34mlTrain batch 16/32 - 107.4ms/batch - loss: 5.65247 - diff: 7.32mlTrain batch 17/32 - 108.1ms/batch - loss: 5.73995 - diff: 7.43mlTrain batch 18/32 - 97.0ms/batch - loss: 5.67341 - diff: 7.38mlTrain batch 19/32 - 106.1ms/batch - loss: 5.80307 - diff: 7.51mlTrain batch 20/32 - 103.8ms/batch - loss: 5.77854 - diff: 7.49mlTrain batch 21/32 - 103.4ms/batch - loss: 5.69537 - diff: 7.45mlTrain batch 22/32 - 108.3ms/batch - loss: 5.57797 - diff: 7.38mlTrain batch 23/32 - 111.3ms/batch - loss: 5.57144 - diff: 7.36mlTrain batch 24/32 - 116.9ms/batch - loss: 6.15893 - diff: 7.64mlTrain batch 25/32 - 118.1ms/batch - loss: 6.35356 - diff: 7.75mlTrain batch 26/32 - 119.5ms/batch - loss: 6.26056 - diff: 7.71mlTrain batch 27/32 - 116.5ms/batch - loss: 6.35724 - diff: 7.77mlTrain batch 28/32 - 116.0ms/batch - loss: 6.46954 - diff: 7.80mlTrain batch 29/32 - 116.6ms/batch - loss: 6.46449 - diff: 7.81mlTrain batch 30/32 - 115.7ms/batch - loss: 6.40430 - diff: 7.76mlTrain batch 31/32 - 115.7ms/batch - loss: 6.55870 - diff: 7.87mlTrain batch 32/32 - 110.2ms/batch - loss: 6.71314 - diff: 7.87mlTrain batch 32/32 - 11.7s 110.2ms/batch - loss: 6.71314 - diff: 7.87ml
Test 0.7s: val_loss: 31.22820 - diff: 15.26ml

Epoch 114: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 128.8ms/batch - loss: 3.58868 - diff: 6.06mlTrain batch 2/32 - 108.6ms/batch - loss: 21.23350 - diff: 14.83mlTrain batch 3/32 - 111.3ms/batch - loss: 15.10789 - diff: 11.60mlTrain batch 4/32 - 106.1ms/batch - loss: 13.92631 - diff: 11.53mlTrain batch 5/32 - 108.1ms/batch - loss: 12.39118 - diff: 10.81mlTrain batch 6/32 - 106.0ms/batch - loss: 10.63610 - diff: 9.79mlTrain batch 7/32 - 110.4ms/batch - loss: 11.94048 - diff: 10.54mlTrain batch 8/32 - 105.7ms/batch - loss: 11.30733 - diff: 10.21mlTrain batch 9/32 - 108.8ms/batch - loss: 10.32085 - diff: 9.53mlTrain batch 10/32 - 107.2ms/batch - loss: 10.20963 - diff: 9.18mlTrain batch 11/32 - 115.3ms/batch - loss: 9.79191 - diff: 8.98mlTrain batch 12/32 - 107.5ms/batch - loss: 9.87086 - diff: 9.15mlTrain batch 13/32 - 115.7ms/batch - loss: 9.69664 - diff: 9.09mlTrain batch 14/32 - 106.6ms/batch - loss: 9.47165 - diff: 8.97mlTrain batch 15/32 - 125.2ms/batch - loss: 9.51745 - diff: 9.13mlTrain batch 16/32 - 96.9ms/batch - loss: 9.25715 - diff: 9.06mlTrain batch 17/32 - 120.0ms/batch - loss: 8.97469 - diff: 8.93mlTrain batch 18/32 - 106.1ms/batch - loss: 8.87008 - diff: 8.96mlTrain batch 19/32 - 109.1ms/batch - loss: 8.80064 - diff: 8.98mlTrain batch 20/32 - 107.8ms/batch - loss: 8.54291 - diff: 8.85mlTrain batch 21/32 - 120.1ms/batch - loss: 8.30018 - diff: 8.72mlTrain batch 22/32 - 116.7ms/batch - loss: 8.32608 - diff: 8.68mlTrain batch 23/32 - 127.9ms/batch - loss: 8.19084 - diff: 8.63mlTrain batch 24/32 - 116.2ms/batch - loss: 8.30689 - diff: 8.60mlTrain batch 25/32 - 113.0ms/batch - loss: 8.18525 - diff: 8.55mlTrain batch 26/32 - 105.8ms/batch - loss: 8.57608 - diff: 8.74mlTrain batch 27/32 - 113.2ms/batch - loss: 8.52064 - diff: 8.74mlTrain batch 28/32 - 106.9ms/batch - loss: 8.32145 - diff: 8.62mlTrain batch 29/32 - 107.9ms/batch - loss: 8.18310 - diff: 8.55mlTrain batch 30/32 - 105.4ms/batch - loss: 8.01996 - diff: 8.48mlTrain batch 31/32 - 108.5ms/batch - loss: 7.84691 - diff: 8.34mlTrain batch 32/32 - 109.6ms/batch - loss: 8.44583 - diff: 8.41mlTrain batch 32/32 - 10.8s 109.6ms/batch - loss: 8.44583 - diff: 8.41ml
Test 0.6s: val_loss: 33.25293 - diff: 15.67ml

Epoch 115: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 118.3ms/batch - loss: 4.35188 - diff: 6.49mlTrain batch 2/32 - 107.5ms/batch - loss: 3.71737 - diff: 5.82mlTrain batch 3/32 - 108.7ms/batch - loss: 3.31670 - diff: 5.51mlTrain batch 4/32 - 109.3ms/batch - loss: 3.61358 - diff: 6.03mlTrain batch 5/32 - 107.5ms/batch - loss: 9.36847 - diff: 8.16mlTrain batch 6/32 - 112.9ms/batch - loss: 8.42587 - diff: 7.75mlTrain batch 7/32 - 115.5ms/batch - loss: 8.38065 - diff: 7.81mlTrain batch 8/32 - 112.9ms/batch - loss: 8.71814 - diff: 8.24mlTrain batch 9/32 - 106.9ms/batch - loss: 8.28875 - diff: 8.20mlTrain batch 10/32 - 106.6ms/batch - loss: 8.69037 - diff: 8.52mlTrain batch 11/32 - 107.8ms/batch - loss: 8.20535 - diff: 8.28mlTrain batch 12/32 - 98.3ms/batch - loss: 7.99524 - diff: 8.26mlTrain batch 13/32 - 115.7ms/batch - loss: 7.55219 - diff: 7.95mlTrain batch 14/32 - 114.5ms/batch - loss: 7.49857 - diff: 7.98mlTrain batch 15/32 - 122.2ms/batch - loss: 7.42512 - diff: 8.00mlTrain batch 16/32 - 121.8ms/batch - loss: 7.26646 - diff: 7.97mlTrain batch 17/32 - 115.4ms/batch - loss: 7.06571 - diff: 7.90mlTrain batch 18/32 - 115.2ms/batch - loss: 7.89127 - diff: 8.45mlTrain batch 19/32 - 110.4ms/batch - loss: 7.64841 - diff: 8.30mlTrain batch 20/32 - 105.4ms/batch - loss: 7.43965 - diff: 8.19mlTrain batch 21/32 - 103.7ms/batch - loss: 7.32723 - diff: 8.13mlTrain batch 22/32 - 103.4ms/batch - loss: 7.25156 - diff: 8.10mlTrain batch 23/32 - 103.7ms/batch - loss: 7.11290 - diff: 8.02mlTrain batch 24/32 - 102.7ms/batch - loss: 7.41974 - diff: 8.25mlTrain batch 25/32 - 103.5ms/batch - loss: 7.34223 - diff: 8.21mlTrain batch 26/32 - 102.8ms/batch - loss: 7.33015 - diff: 8.22mlTrain batch 27/32 - 103.6ms/batch - loss: 7.34708 - diff: 8.29mlTrain batch 28/32 - 104.0ms/batch - loss: 7.45987 - diff: 8.38mlTrain batch 29/32 - 100.7ms/batch - loss: 7.44832 - diff: 8.41mlTrain batch 30/32 - 107.2ms/batch - loss: 8.01713 - diff: 8.73mlTrain batch 31/32 - 101.3ms/batch - loss: 7.84429 - diff: 8.63mlTrain batch 32/32 - 81.5ms/batch - loss: 9.20645 - diff: 8.76mlTrain batch 32/32 - 11.5s 81.5ms/batch - loss: 9.20645 - diff: 8.76ml
Test 0.6s: val_loss: 34.27786 - diff: 15.87ml

Epoch 116: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 123.4ms/batch - loss: 13.75322 - diff: 12.78mlTrain batch 2/32 - 104.9ms/batch - loss: 9.32589 - diff: 10.01mlTrain batch 3/32 - 121.8ms/batch - loss: 7.17820 - diff: 8.24mlTrain batch 4/32 - 114.5ms/batch - loss: 6.91951 - diff: 8.14mlTrain batch 5/32 - 121.7ms/batch - loss: 6.30347 - diff: 7.75mlTrain batch 6/32 - 115.2ms/batch - loss: 5.85395 - diff: 7.30mlTrain batch 7/32 - 120.7ms/batch - loss: 7.97136 - diff: 8.52mlTrain batch 8/32 - 113.7ms/batch - loss: 7.60267 - diff: 8.28mlTrain batch 9/32 - 117.1ms/batch - loss: 7.21991 - diff: 8.03mlTrain batch 10/32 - 116.5ms/batch - loss: 7.05228 - diff: 7.99mlTrain batch 11/32 - 116.9ms/batch - loss: 6.97828 - diff: 7.93mlTrain batch 12/32 - 105.9ms/batch - loss: 7.43487 - diff: 8.08mlTrain batch 13/32 - 117.9ms/batch - loss: 7.44075 - diff: 8.08mlTrain batch 14/32 - 106.0ms/batch - loss: 7.11449 - diff: 7.93mlTrain batch 15/32 - 99.3ms/batch - loss: 6.84918 - diff: 7.81mlTrain batch 16/32 - 104.6ms/batch - loss: 6.66705 - diff: 7.69mlTrain batch 17/32 - 100.3ms/batch - loss: 6.85552 - diff: 7.87mlTrain batch 18/32 - 107.5ms/batch - loss: 6.62992 - diff: 7.71mlTrain batch 19/32 - 106.0ms/batch - loss: 6.99182 - diff: 7.94mlTrain batch 20/32 - 108.6ms/batch - loss: 7.18707 - diff: 8.08mlTrain batch 21/32 - 109.7ms/batch - loss: 8.01454 - diff: 8.53mlTrain batch 22/32 - 109.1ms/batch - loss: 8.01519 - diff: 8.59mlTrain batch 23/32 - 106.9ms/batch - loss: 7.85790 - diff: 8.51mlTrain batch 24/32 - 116.6ms/batch - loss: 7.75791 - diff: 8.46mlTrain batch 25/32 - 115.2ms/batch - loss: 7.61291 - diff: 8.39mlTrain batch 26/32 - 116.8ms/batch - loss: 7.69089 - diff: 8.38mlTrain batch 27/32 - 116.0ms/batch - loss: 7.56504 - diff: 8.31mlTrain batch 28/32 - 115.7ms/batch - loss: 7.49231 - diff: 8.30mlTrain batch 29/32 - 117.4ms/batch - loss: 7.44699 - diff: 8.27mlTrain batch 30/32 - 117.3ms/batch - loss: 7.32699 - diff: 8.18mlTrain batch 31/32 - 107.0ms/batch - loss: 7.19339 - diff: 8.09mlTrain batch 32/32 - 82.0ms/batch - loss: 7.25123 - diff: 8.09mlTrain batch 32/32 - 10.8s 82.0ms/batch - loss: 7.25123 - diff: 8.09ml
Test 0.7s: val_loss: 33.22201 - diff: 15.19ml

Epoch 117: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 127.8ms/batch - loss: 7.92711 - diff: 9.07mlTrain batch 2/32 - 113.6ms/batch - loss: 9.89469 - diff: 10.76mlTrain batch 3/32 - 113.2ms/batch - loss: 8.35441 - diff: 9.65mlTrain batch 4/32 - 116.1ms/batch - loss: 7.57536 - diff: 9.02mlTrain batch 5/32 - 115.0ms/batch - loss: 11.58945 - diff: 10.89mlTrain batch 6/32 - 117.9ms/batch - loss: 10.22597 - diff: 10.12mlTrain batch 7/32 - 114.4ms/batch - loss: 10.46810 - diff: 10.35mlTrain batch 8/32 - 114.4ms/batch - loss: 9.60747 - diff: 9.76mlTrain batch 9/32 - 114.9ms/batch - loss: 8.87900 - diff: 9.29mlTrain batch 10/32 - 115.0ms/batch - loss: 9.99984 - diff: 9.89mlTrain batch 11/32 - 114.6ms/batch - loss: 9.66272 - diff: 9.72mlTrain batch 12/32 - 112.7ms/batch - loss: 9.34641 - diff: 9.63mlTrain batch 13/32 - 111.8ms/batch - loss: 9.38001 - diff: 9.49mlTrain batch 14/32 - 112.6ms/batch - loss: 9.22526 - diff: 9.39mlTrain batch 15/32 - 112.7ms/batch - loss: 8.95399 - diff: 9.23mlTrain batch 16/32 - 116.4ms/batch - loss: 8.64887 - diff: 9.04mlTrain batch 17/32 - 113.6ms/batch - loss: 8.40347 - diff: 8.92mlTrain batch 18/32 - 112.6ms/batch - loss: 8.23027 - diff: 8.86mlTrain batch 19/32 - 112.4ms/batch - loss: 9.22960 - diff: 9.36mlTrain batch 20/32 - 112.0ms/batch - loss: 9.07819 - diff: 9.32mlTrain batch 21/32 - 111.7ms/batch - loss: 8.92747 - diff: 9.25mlTrain batch 22/32 - 107.5ms/batch - loss: 8.88308 - diff: 9.19mlTrain batch 23/32 - 112.5ms/batch - loss: 8.83576 - diff: 9.17mlTrain batch 24/32 - 115.8ms/batch - loss: 8.58789 - diff: 8.99mlTrain batch 25/32 - 115.9ms/batch - loss: 8.47166 - diff: 8.94mlTrain batch 26/32 - 123.2ms/batch - loss: 8.34355 - diff: 8.91mlTrain batch 27/32 - 115.7ms/batch - loss: 8.30398 - diff: 8.88mlTrain batch 28/32 - 117.5ms/batch - loss: 8.60098 - diff: 9.10mlTrain batch 29/32 - 115.5ms/batch - loss: 8.52244 - diff: 9.01mlTrain batch 30/32 - 117.3ms/batch - loss: 8.38865 - diff: 8.94mlTrain batch 31/32 - 117.2ms/batch - loss: 8.16956 - diff: 8.80mlTrain batch 32/32 - 84.4ms/batch - loss: 8.16143 - diff: 8.77mlTrain batch 32/32 - 11.3s 84.4ms/batch - loss: 8.16143 - diff: 8.77ml
Test 0.7s: val_loss: 33.79304 - diff: 15.55ml

Epoch 118: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 118.3ms/batch - loss: 6.51825 - diff: 8.07mlTrain batch 2/32 - 104.7ms/batch - loss: 24.02983 - diff: 15.65mlTrain batch 3/32 - 104.2ms/batch - loss: 21.27088 - diff: 15.06mlTrain batch 4/32 - 105.6ms/batch - loss: 17.56765 - diff: 13.55mlTrain batch 5/32 - 105.5ms/batch - loss: 14.93789 - diff: 12.03mlTrain batch 6/32 - 108.0ms/batch - loss: 14.13776 - diff: 11.94mlTrain batch 7/32 - 108.4ms/batch - loss: 13.35589 - diff: 11.65mlTrain batch 8/32 - 108.7ms/batch - loss: 12.30007 - diff: 11.10mlTrain batch 9/32 - 109.8ms/batch - loss: 11.73265 - diff: 10.85mlTrain batch 10/32 - 107.1ms/batch - loss: 11.47618 - diff: 10.81mlTrain batch 11/32 - 106.4ms/batch - loss: 10.77468 - diff: 10.33mlTrain batch 12/32 - 107.2ms/batch - loss: 10.27991 - diff: 10.03mlTrain batch 13/32 - 117.7ms/batch - loss: 10.78467 - diff: 10.26mlTrain batch 14/32 - 108.0ms/batch - loss: 10.31087 - diff: 9.99mlTrain batch 15/32 - 111.1ms/batch - loss: 9.96146 - diff: 9.79mlTrain batch 16/32 - 107.6ms/batch - loss: 10.25451 - diff: 9.92mlTrain batch 17/32 - 109.7ms/batch - loss: 9.77925 - diff: 9.62mlTrain batch 18/32 - 107.7ms/batch - loss: 9.37819 - diff: 9.38mlTrain batch 19/32 - 114.6ms/batch - loss: 9.07515 - diff: 9.20mlTrain batch 20/32 - 102.9ms/batch - loss: 8.79955 - diff: 9.03mlTrain batch 21/32 - 112.0ms/batch - loss: 8.69107 - diff: 8.96mlTrain batch 22/32 - 103.8ms/batch - loss: 8.47015 - diff: 8.86mlTrain batch 23/32 - 116.2ms/batch - loss: 8.82143 - diff: 8.93mlTrain batch 24/32 - 114.1ms/batch - loss: 8.64143 - diff: 8.85mlTrain batch 25/32 - 107.3ms/batch - loss: 8.42529 - diff: 8.72mlTrain batch 26/32 - 104.7ms/batch - loss: 8.29974 - diff: 8.67mlTrain batch 27/32 - 114.0ms/batch - loss: 8.13620 - diff: 8.56mlTrain batch 28/32 - 103.8ms/batch - loss: 8.02481 - diff: 8.51mlTrain batch 29/32 - 112.7ms/batch - loss: 7.89203 - diff: 8.43mlTrain batch 30/32 - 89.5ms/batch - loss: 7.90352 - diff: 8.48mlTrain batch 31/32 - 88.9ms/batch - loss: 7.95450 - diff: 8.52mlTrain batch 32/32 - 72.0ms/batch - loss: 8.30452 - diff: 8.57mlTrain batch 32/32 - 11.4s 72.0ms/batch - loss: 8.30452 - diff: 8.57ml
Test 0.6s: val_loss: 32.85682 - diff: 15.82ml

Epoch 119: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 128.6ms/batch - loss: 14.65342 - diff: 11.80mlTrain batch 2/32 - 104.7ms/batch - loss: 8.78837 - diff: 8.28mlTrain batch 3/32 - 104.6ms/batch - loss: 8.48831 - diff: 8.44mlTrain batch 4/32 - 103.9ms/batch - loss: 9.74349 - diff: 9.66mlTrain batch 5/32 - 104.3ms/batch - loss: 10.11597 - diff: 10.01mlTrain batch 6/32 - 102.6ms/batch - loss: 9.18256 - diff: 9.46mlTrain batch 7/32 - 104.0ms/batch - loss: 8.73526 - diff: 9.14mlTrain batch 8/32 - 103.7ms/batch - loss: 11.48295 - diff: 9.91mlTrain batch 9/32 - 103.6ms/batch - loss: 10.79547 - diff: 9.66mlTrain batch 10/32 - 103.0ms/batch - loss: 9.99543 - diff: 9.22mlTrain batch 11/32 - 103.6ms/batch - loss: 9.28860 - diff: 8.78mlTrain batch 12/32 - 102.5ms/batch - loss: 9.65518 - diff: 9.07mlTrain batch 13/32 - 104.3ms/batch - loss: 9.26625 - diff: 8.91mlTrain batch 14/32 - 102.1ms/batch - loss: 9.00984 - diff: 8.82mlTrain batch 15/32 - 103.4ms/batch - loss: 9.03710 - diff: 8.93mlTrain batch 16/32 - 102.9ms/batch - loss: 9.05255 - diff: 9.02mlTrain batch 17/32 - 100.8ms/batch - loss: 8.69167 - diff: 8.81mlTrain batch 18/32 - 108.8ms/batch - loss: 8.45838 - diff: 8.70mlTrain batch 19/32 - 112.7ms/batch - loss: 8.41850 - diff: 8.75mlTrain batch 20/32 - 112.0ms/batch - loss: 8.21387 - diff: 8.66mlTrain batch 21/32 - 103.7ms/batch - loss: 8.24991 - diff: 8.71mlTrain batch 22/32 - 104.6ms/batch - loss: 8.00828 - diff: 8.55mlTrain batch 23/32 - 103.3ms/batch - loss: 7.97045 - diff: 8.54mlTrain batch 24/32 - 103.5ms/batch - loss: 7.98825 - diff: 8.59mlTrain batch 25/32 - 104.0ms/batch - loss: 7.77175 - diff: 8.46mlTrain batch 26/32 - 103.6ms/batch - loss: 7.93679 - diff: 8.55mlTrain batch 27/32 - 104.8ms/batch - loss: 8.26622 - diff: 8.71mlTrain batch 28/32 - 106.4ms/batch - loss: 8.17250 - diff: 8.65mlTrain batch 29/32 - 107.3ms/batch - loss: 8.38170 - diff: 8.72mlTrain batch 30/32 - 108.8ms/batch - loss: 8.27647 - diff: 8.66mlTrain batch 31/32 - 93.7ms/batch - loss: 8.10210 - diff: 8.55mlTrain batch 32/32 - 82.7ms/batch - loss: 9.72185 - diff: 8.69mlTrain batch 32/32 - 10.9s 82.7ms/batch - loss: 9.72185 - diff: 8.69ml
Test 0.6s: val_loss: 35.54607 - diff: 15.49ml
Epoch   120: reducing learning rate of group 0 to 3.1250e-05.

Epoch 120: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 121.5ms/batch - loss: 2.30097 - diff: 4.83mlTrain batch 2/32 - 106.5ms/batch - loss: 5.10688 - diff: 6.71mlTrain batch 3/32 - 107.2ms/batch - loss: 4.76519 - diff: 6.85mlTrain batch 4/32 - 108.6ms/batch - loss: 4.77664 - diff: 6.67mlTrain batch 5/32 - 115.3ms/batch - loss: 4.74051 - diff: 6.52mlTrain batch 6/32 - 107.9ms/batch - loss: 4.16621 - diff: 6.08mlTrain batch 7/32 - 106.5ms/batch - loss: 4.76619 - diff: 6.43mlTrain batch 8/32 - 107.0ms/batch - loss: 4.48731 - diff: 6.28mlTrain batch 9/32 - 106.0ms/batch - loss: 7.94888 - diff: 8.06mlTrain batch 10/32 - 107.1ms/batch - loss: 8.04326 - diff: 8.21mlTrain batch 11/32 - 106.9ms/batch - loss: 8.36374 - diff: 8.54mlTrain batch 12/32 - 108.5ms/batch - loss: 8.50816 - diff: 8.73mlTrain batch 13/32 - 107.4ms/batch - loss: 8.02855 - diff: 8.39mlTrain batch 14/32 - 105.9ms/batch - loss: 7.89676 - diff: 8.39mlTrain batch 15/32 - 108.4ms/batch - loss: 7.66611 - diff: 8.31mlTrain batch 16/32 - 117.5ms/batch - loss: 8.04259 - diff: 8.56mlTrain batch 17/32 - 110.6ms/batch - loss: 8.35774 - diff: 8.53mlTrain batch 18/32 - 113.9ms/batch - loss: 8.35330 - diff: 8.63mlTrain batch 19/32 - 112.6ms/batch - loss: 8.08059 - diff: 8.52mlTrain batch 20/32 - 114.2ms/batch - loss: 7.99036 - diff: 8.46mlTrain batch 21/32 - 115.5ms/batch - loss: 8.82014 - diff: 8.71mlTrain batch 22/32 - 114.1ms/batch - loss: 8.55347 - diff: 8.60mlTrain batch 23/32 - 113.1ms/batch - loss: 8.28855 - diff: 8.44mlTrain batch 24/32 - 113.8ms/batch - loss: 8.07916 - diff: 8.35mlTrain batch 25/32 - 113.0ms/batch - loss: 7.89845 - diff: 8.25mlTrain batch 26/32 - 117.3ms/batch - loss: 8.03661 - diff: 8.39mlTrain batch 27/32 - 112.9ms/batch - loss: 7.93564 - diff: 8.37mlTrain batch 28/32 - 114.4ms/batch - loss: 7.83963 - diff: 8.33mlTrain batch 29/32 - 113.4ms/batch - loss: 7.79963 - diff: 8.37mlTrain batch 30/32 - 115.2ms/batch - loss: 7.72661 - diff: 8.35mlTrain batch 31/32 - 100.3ms/batch - loss: 8.32842 - diff: 8.67mlTrain batch 32/32 - 80.6ms/batch - loss: 8.42687 - diff: 8.67mlTrain batch 32/32 - 12.1s 80.6ms/batch - loss: 8.42687 - diff: 8.67ml
Test 0.7s: val_loss: 34.19501 - diff: 15.47ml

Epoch 121: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 130.3ms/batch - loss: 8.52425 - diff: 10.33mlTrain batch 2/32 - 115.1ms/batch - loss: 10.86205 - diff: 11.14mlTrain batch 3/32 - 113.8ms/batch - loss: 12.98353 - diff: 12.53mlTrain batch 4/32 - 106.0ms/batch - loss: 10.63952 - diff: 11.11mlTrain batch 5/32 - 115.3ms/batch - loss: 10.33754 - diff: 10.67mlTrain batch 6/32 - 115.7ms/batch - loss: 9.56024 - diff: 10.24mlTrain batch 7/32 - 112.2ms/batch - loss: 9.24830 - diff: 9.97mlTrain batch 8/32 - 116.0ms/batch - loss: 10.09349 - diff: 10.55mlTrain batch 9/32 - 115.5ms/batch - loss: 9.44033 - diff: 10.14mlTrain batch 10/32 - 116.9ms/batch - loss: 8.74000 - diff: 9.64mlTrain batch 11/32 - 117.6ms/batch - loss: 9.48927 - diff: 10.06mlTrain batch 12/32 - 116.2ms/batch - loss: 9.52691 - diff: 10.18mlTrain batch 13/32 - 116.7ms/batch - loss: 8.92515 - diff: 9.69mlTrain batch 14/32 - 113.0ms/batch - loss: 8.66207 - diff: 9.50mlTrain batch 15/32 - 92.5ms/batch - loss: 8.54587 - diff: 9.37mlTrain batch 16/32 - 98.8ms/batch - loss: 8.66785 - diff: 9.43mlTrain batch 17/32 - 102.2ms/batch - loss: 8.66442 - diff: 9.46mlTrain batch 18/32 - 103.1ms/batch - loss: 8.36227 - diff: 9.27mlTrain batch 19/32 - 103.8ms/batch - loss: 8.10778 - diff: 9.10mlTrain batch 20/32 - 106.6ms/batch - loss: 7.99495 - diff: 9.05mlTrain batch 21/32 - 107.4ms/batch - loss: 7.86210 - diff: 8.97mlTrain batch 22/32 - 117.0ms/batch - loss: 7.89159 - diff: 9.00mlTrain batch 23/32 - 116.7ms/batch - loss: 7.91589 - diff: 9.05mlTrain batch 24/32 - 116.1ms/batch - loss: 7.74697 - diff: 8.95mlTrain batch 25/32 - 108.3ms/batch - loss: 7.59802 - diff: 8.85mlTrain batch 26/32 - 112.3ms/batch - loss: 7.43080 - diff: 8.74mlTrain batch 27/32 - 112.2ms/batch - loss: 7.41152 - diff: 8.72mlTrain batch 28/32 - 112.8ms/batch - loss: 7.48162 - diff: 8.79mlTrain batch 29/32 - 112.9ms/batch - loss: 7.43692 - diff: 8.76mlTrain batch 30/32 - 107.2ms/batch - loss: 7.40849 - diff: 8.73mlTrain batch 31/32 - 97.7ms/batch - loss: 7.73212 - diff: 8.85mlTrain batch 32/32 - 80.5ms/batch - loss: 7.89593 - diff: 8.85mlTrain batch 32/32 - 11.3s 80.5ms/batch - loss: 7.89593 - diff: 8.85ml
Test 0.7s: val_loss: 33.75010 - diff: 15.36ml

Epoch 122: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 120.9ms/batch - loss: 8.81196 - diff: 9.59mlTrain batch 2/32 - 107.3ms/batch - loss: 14.89719 - diff: 13.17mlTrain batch 3/32 - 107.4ms/batch - loss: 11.82969 - diff: 11.34mlTrain batch 4/32 - 107.1ms/batch - loss: 9.75567 - diff: 9.94mlTrain batch 5/32 - 107.4ms/batch - loss: 9.25203 - diff: 9.66mlTrain batch 6/32 - 117.0ms/batch - loss: 8.86649 - diff: 9.24mlTrain batch 7/32 - 106.9ms/batch - loss: 10.22973 - diff: 10.29mlTrain batch 8/32 - 115.7ms/batch - loss: 9.33097 - diff: 9.76mlTrain batch 9/32 - 107.2ms/batch - loss: 9.18009 - diff: 9.78mlTrain batch 10/32 - 116.1ms/batch - loss: 8.83546 - diff: 9.49mlTrain batch 11/32 - 106.7ms/batch - loss: 8.57740 - diff: 9.30mlTrain batch 12/32 - 116.6ms/batch - loss: 8.29507 - diff: 9.07mlTrain batch 13/32 - 106.9ms/batch - loss: 9.92668 - diff: 9.67mlTrain batch 14/32 - 108.7ms/batch - loss: 9.43713 - diff: 9.41mlTrain batch 15/32 - 92.8ms/batch - loss: 9.71771 - diff: 9.62mlTrain batch 16/32 - 91.2ms/batch - loss: 9.44180 - diff: 9.47mlTrain batch 17/32 - 108.9ms/batch - loss: 9.18082 - diff: 9.33mlTrain batch 18/32 - 125.3ms/batch - loss: 9.05339 - diff: 9.35mlTrain batch 19/32 - 113.2ms/batch - loss: 9.54578 - diff: 9.67mlTrain batch 20/32 - 118.6ms/batch - loss: 9.45256 - diff: 9.68mlTrain batch 21/32 - 119.5ms/batch - loss: 9.12049 - diff: 9.45mlTrain batch 22/32 - 115.1ms/batch - loss: 9.28853 - diff: 9.61mlTrain batch 23/32 - 114.3ms/batch - loss: 9.01423 - diff: 9.44mlTrain batch 24/32 - 105.7ms/batch - loss: 8.77299 - diff: 9.30mlTrain batch 25/32 - 107.8ms/batch - loss: 8.61749 - diff: 9.19mlTrain batch 26/32 - 111.9ms/batch - loss: 8.50222 - diff: 9.09mlTrain batch 27/32 - 108.0ms/batch - loss: 8.47460 - diff: 9.11mlTrain batch 28/32 - 109.2ms/batch - loss: 8.69241 - diff: 9.19mlTrain batch 29/32 - 107.1ms/batch - loss: 8.68651 - diff: 9.19mlTrain batch 30/32 - 107.6ms/batch - loss: 8.50716 - diff: 9.10mlTrain batch 31/32 - 91.4ms/batch - loss: 8.34341 - diff: 9.01mlTrain batch 32/32 - 84.6ms/batch - loss: 22.30961 - diff: 9.41mlTrain batch 32/32 - 11.4s 84.6ms/batch - loss: 22.30961 - diff: 9.41ml
Test 0.7s: val_loss: 35.47663 - diff: 15.48ml

Epoch 123: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 129.9ms/batch - loss: 3.07824 - diff: 6.02mlTrain batch 2/32 - 108.4ms/batch - loss: 3.84081 - diff: 6.26mlTrain batch 3/32 - 107.1ms/batch - loss: 4.13928 - diff: 6.45mlTrain batch 4/32 - 103.9ms/batch - loss: 4.60754 - diff: 6.76mlTrain batch 5/32 - 99.3ms/batch - loss: 5.62187 - diff: 7.41mlTrain batch 6/32 - 104.2ms/batch - loss: 6.24478 - diff: 7.27mlTrain batch 7/32 - 104.4ms/batch - loss: 6.67224 - diff: 7.74mlTrain batch 8/32 - 103.5ms/batch - loss: 6.24990 - diff: 7.57mlTrain batch 9/32 - 103.8ms/batch - loss: 6.10069 - diff: 7.56mlTrain batch 10/32 - 103.3ms/batch - loss: 6.48707 - diff: 7.76mlTrain batch 11/32 - 103.6ms/batch - loss: 6.36544 - diff: 7.76mlTrain batch 12/32 - 103.6ms/batch - loss: 7.09023 - diff: 8.03mlTrain batch 13/32 - 105.7ms/batch - loss: 7.97870 - diff: 8.35mlTrain batch 14/32 - 104.0ms/batch - loss: 7.93443 - diff: 8.40mlTrain batch 15/32 - 106.5ms/batch - loss: 7.65270 - diff: 8.22mlTrain batch 16/32 - 103.3ms/batch - loss: 7.41256 - diff: 8.14mlTrain batch 17/32 - 110.4ms/batch - loss: 7.70718 - diff: 8.32mlTrain batch 18/32 - 119.0ms/batch - loss: 7.45022 - diff: 8.15mlTrain batch 19/32 - 113.5ms/batch - loss: 7.62747 - diff: 8.21mlTrain batch 20/32 - 113.8ms/batch - loss: 7.57787 - diff: 8.23mlTrain batch 21/32 - 113.2ms/batch - loss: 7.83002 - diff: 8.19mlTrain batch 22/32 - 113.5ms/batch - loss: 7.61586 - diff: 8.09mlTrain batch 23/32 - 114.9ms/batch - loss: 7.38347 - diff: 7.94mlTrain batch 24/32 - 113.3ms/batch - loss: 7.27300 - diff: 7.89mlTrain batch 25/32 - 113.8ms/batch - loss: 7.34420 - diff: 7.97mlTrain batch 26/32 - 103.7ms/batch - loss: 7.55581 - diff: 8.16mlTrain batch 27/32 - 103.5ms/batch - loss: 7.66158 - diff: 8.23mlTrain batch 28/32 - 104.9ms/batch - loss: 7.61418 - diff: 8.21mlTrain batch 29/32 - 104.6ms/batch - loss: 7.46787 - diff: 8.11mlTrain batch 30/32 - 105.9ms/batch - loss: 7.34100 - diff: 8.07mlTrain batch 31/32 - 110.8ms/batch - loss: 8.74173 - diff: 8.35mlTrain batch 32/32 - 85.5ms/batch - loss: 8.90441 - diff: 8.35mlTrain batch 32/32 - 10.7s 85.5ms/batch - loss: 8.90441 - diff: 8.35ml
Test 0.6s: val_loss: 32.42877 - diff: 15.38ml

Epoch 124: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 139.4ms/batch - loss: 18.83994 - diff: 14.76mlTrain batch 2/32 - 114.1ms/batch - loss: 12.76442 - diff: 11.98mlTrain batch 3/32 - 104.1ms/batch - loss: 9.97455 - diff: 10.29mlTrain batch 4/32 - 93.4ms/batch - loss: 8.26700 - diff: 9.01mlTrain batch 5/32 - 108.0ms/batch - loss: 9.06677 - diff: 9.33mlTrain batch 6/32 - 97.6ms/batch - loss: 8.60118 - diff: 9.17mlTrain batch 7/32 - 117.3ms/batch - loss: 8.44768 - diff: 9.11mlTrain batch 8/32 - 117.2ms/batch - loss: 7.86453 - diff: 8.73mlTrain batch 9/32 - 118.1ms/batch - loss: 7.57208 - diff: 8.49mlTrain batch 10/32 - 117.5ms/batch - loss: 8.52378 - diff: 9.04mlTrain batch 11/32 - 114.2ms/batch - loss: 8.17690 - diff: 8.82mlTrain batch 12/32 - 114.8ms/batch - loss: 7.66034 - diff: 8.44mlTrain batch 13/32 - 115.0ms/batch - loss: 8.22895 - diff: 8.91mlTrain batch 14/32 - 113.8ms/batch - loss: 7.80932 - diff: 8.59mlTrain batch 15/32 - 110.3ms/batch - loss: 7.50712 - diff: 8.41mlTrain batch 16/32 - 116.1ms/batch - loss: 7.46660 - diff: 8.33mlTrain batch 17/32 - 102.7ms/batch - loss: 7.26692 - diff: 8.23mlTrain batch 18/32 - 103.1ms/batch - loss: 7.22328 - diff: 8.15mlTrain batch 19/32 - 104.2ms/batch - loss: 7.38622 - diff: 8.24mlTrain batch 20/32 - 104.1ms/batch - loss: 7.21238 - diff: 8.16mlTrain batch 21/32 - 102.7ms/batch - loss: 7.31908 - diff: 8.23mlTrain batch 22/32 - 103.6ms/batch - loss: 7.34957 - diff: 8.29mlTrain batch 23/32 - 102.4ms/batch - loss: 7.56047 - diff: 8.45mlTrain batch 24/32 - 103.3ms/batch - loss: 7.34096 - diff: 8.29mlTrain batch 25/32 - 102.8ms/batch - loss: 7.30889 - diff: 8.28mlTrain batch 26/32 - 109.9ms/batch - loss: 7.29187 - diff: 8.35mlTrain batch 27/32 - 102.3ms/batch - loss: 7.27179 - diff: 8.34mlTrain batch 28/32 - 108.6ms/batch - loss: 7.40386 - diff: 8.45mlTrain batch 29/32 - 101.3ms/batch - loss: 7.74968 - diff: 8.60mlTrain batch 30/32 - 100.0ms/batch - loss: 7.57757 - diff: 8.48mlTrain batch 31/32 - 91.2ms/batch - loss: 7.45622 - diff: 8.41mlTrain batch 32/32 - 83.2ms/batch - loss: 7.68969 - diff: 8.43mlTrain batch 32/32 - 11.6s 83.2ms/batch - loss: 7.68969 - diff: 8.43ml
Test 0.6s: val_loss: 33.39134 - diff: 15.66ml

Epoch 125: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 117.5ms/batch - loss: 6.78456 - diff: 8.13mlTrain batch 2/32 - 103.3ms/batch - loss: 4.29228 - diff: 6.27mlTrain batch 3/32 - 103.2ms/batch - loss: 4.43289 - diff: 6.39mlTrain batch 4/32 - 103.8ms/batch - loss: 4.03388 - diff: 6.21mlTrain batch 5/32 - 114.1ms/batch - loss: 4.91527 - diff: 6.91mlTrain batch 6/32 - 113.0ms/batch - loss: 5.13762 - diff: 7.09mlTrain batch 7/32 - 121.0ms/batch - loss: 5.33413 - diff: 7.04mlTrain batch 8/32 - 113.0ms/batch - loss: 4.88064 - diff: 6.71mlTrain batch 9/32 - 113.8ms/batch - loss: 4.94888 - diff: 6.75mlTrain batch 10/32 - 113.4ms/batch - loss: 5.01522 - diff: 6.77mlTrain batch 11/32 - 91.4ms/batch - loss: 7.39850 - diff: 7.99mlTrain batch 12/32 - 114.7ms/batch - loss: 7.66583 - diff: 8.13mlTrain batch 13/32 - 113.2ms/batch - loss: 7.87611 - diff: 8.42mlTrain batch 14/32 - 112.5ms/batch - loss: 7.99035 - diff: 8.57mlTrain batch 15/32 - 112.4ms/batch - loss: 7.83687 - diff: 8.49mlTrain batch 16/32 - 112.1ms/batch - loss: 7.58641 - diff: 8.36mlTrain batch 17/32 - 107.6ms/batch - loss: 7.80238 - diff: 8.56mlTrain batch 18/32 - 105.6ms/batch - loss: 7.60184 - diff: 8.47mlTrain batch 19/32 - 104.7ms/batch - loss: 9.25753 - diff: 9.14mlTrain batch 20/32 - 104.5ms/batch - loss: 9.75672 - diff: 9.20mlTrain batch 21/32 - 104.4ms/batch - loss: 9.39097 - diff: 8.99mlTrain batch 22/32 - 104.7ms/batch - loss: 9.60993 - diff: 9.23mlTrain batch 23/32 - 105.2ms/batch - loss: 9.36875 - diff: 9.11mlTrain batch 24/32 - 104.5ms/batch - loss: 9.43012 - diff: 9.09mlTrain batch 25/32 - 104.6ms/batch - loss: 9.25672 - diff: 9.02mlTrain batch 26/32 - 106.1ms/batch - loss: 9.04787 - diff: 8.89mlTrain batch 27/32 - 105.4ms/batch - loss: 8.80619 - diff: 8.73mlTrain batch 28/32 - 105.8ms/batch - loss: 8.65994 - diff: 8.67mlTrain batch 29/32 - 104.3ms/batch - loss: 8.57482 - diff: 8.67mlTrain batch 30/32 - 106.0ms/batch - loss: 8.37942 - diff: 8.57mlTrain batch 31/32 - 94.0ms/batch - loss: 8.25865 - diff: 8.51mlTrain batch 32/32 - 90.6ms/batch - loss: 8.32552 - diff: 8.48mlTrain batch 32/32 - 11.0s 90.6ms/batch - loss: 8.32552 - diff: 8.48ml
Test 0.7s: val_loss: 37.88226 - diff: 15.73ml

Epoch 126: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 122.4ms/batch - loss: 2.08380 - diff: 4.66mlTrain batch 2/32 - 108.0ms/batch - loss: 2.12798 - diff: 4.84mlTrain batch 3/32 - 108.6ms/batch - loss: 3.98143 - diff: 5.77mlTrain batch 4/32 - 109.3ms/batch - loss: 3.49620 - diff: 5.45mlTrain batch 5/32 - 109.9ms/batch - loss: 4.67754 - diff: 6.53mlTrain batch 6/32 - 113.4ms/batch - loss: 4.32951 - diff: 6.37mlTrain batch 7/32 - 109.2ms/batch - loss: 4.69892 - diff: 6.75mlTrain batch 8/32 - 106.4ms/batch - loss: 5.25513 - diff: 7.30mlTrain batch 9/32 - 106.7ms/batch - loss: 5.21368 - diff: 7.32mlTrain batch 10/32 - 106.5ms/batch - loss: 5.00970 - diff: 7.13mlTrain batch 11/32 - 107.2ms/batch - loss: 5.20952 - diff: 7.31mlTrain batch 12/32 - 106.3ms/batch - loss: 5.76847 - diff: 7.70mlTrain batch 13/32 - 106.5ms/batch - loss: 5.86609 - diff: 7.70mlTrain batch 14/32 - 92.4ms/batch - loss: 5.75362 - diff: 7.66mlTrain batch 15/32 - 90.4ms/batch - loss: 5.57991 - diff: 7.52mlTrain batch 16/32 - 114.3ms/batch - loss: 6.22645 - diff: 7.64mlTrain batch 17/32 - 114.1ms/batch - loss: 6.11119 - diff: 7.60mlTrain batch 18/32 - 113.4ms/batch - loss: 6.31362 - diff: 7.75mlTrain batch 19/32 - 113.7ms/batch - loss: 6.99612 - diff: 8.17mlTrain batch 20/32 - 114.5ms/batch - loss: 7.17331 - diff: 8.25mlTrain batch 21/32 - 114.1ms/batch - loss: 7.16874 - diff: 8.27mlTrain batch 22/32 - 103.8ms/batch - loss: 7.06533 - diff: 8.20mlTrain batch 23/32 - 103.5ms/batch - loss: 7.02709 - diff: 8.18mlTrain batch 24/32 - 107.4ms/batch - loss: 6.91065 - diff: 8.10mlTrain batch 25/32 - 106.2ms/batch - loss: 6.77861 - diff: 8.02mlTrain batch 26/32 - 99.2ms/batch - loss: 7.37076 - diff: 8.34mlTrain batch 27/32 - 100.5ms/batch - loss: 8.34555 - diff: 8.78mlTrain batch 28/32 - 103.9ms/batch - loss: 8.19659 - diff: 8.69mlTrain batch 29/32 - 103.6ms/batch - loss: 8.00577 - diff: 8.59mlTrain batch 30/32 - 103.7ms/batch - loss: 8.01022 - diff: 8.64mlTrain batch 31/32 - 97.2ms/batch - loss: 7.89132 - diff: 8.56mlTrain batch 32/32 - 91.0ms/batch - loss: 7.97450 - diff: 8.55mlTrain batch 32/32 - 11.3s 91.0ms/batch - loss: 7.97450 - diff: 8.55ml
Test 0.7s: val_loss: 36.54769 - diff: 15.43ml

Epoch 127: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 120.7ms/batch - loss: 2.98098 - diff: 5.34mlTrain batch 2/32 - 105.3ms/batch - loss: 3.04947 - diff: 5.71mlTrain batch 3/32 - 103.9ms/batch - loss: 6.03061 - diff: 6.63mlTrain batch 4/32 - 103.6ms/batch - loss: 6.20959 - diff: 7.15mlTrain batch 5/32 - 99.5ms/batch - loss: 6.06851 - diff: 7.37mlTrain batch 6/32 - 107.5ms/batch - loss: 5.45866 - diff: 7.02mlTrain batch 7/32 - 104.8ms/batch - loss: 5.79092 - diff: 7.36mlTrain batch 8/32 - 104.1ms/batch - loss: 5.33650 - diff: 7.09mlTrain batch 9/32 - 104.3ms/batch - loss: 5.19594 - diff: 7.00mlTrain batch 10/32 - 103.6ms/batch - loss: 5.18598 - diff: 7.07mlTrain batch 11/32 - 102.7ms/batch - loss: 5.22020 - diff: 7.14mlTrain batch 12/32 - 130.6ms/batch - loss: 5.08891 - diff: 7.10mlTrain batch 13/32 - 115.1ms/batch - loss: 4.91546 - diff: 6.97mlTrain batch 14/32 - 113.2ms/batch - loss: 4.78850 - diff: 6.88mlTrain batch 15/32 - 107.5ms/batch - loss: 4.59739 - diff: 6.73mlTrain batch 16/32 - 106.1ms/batch - loss: 5.42881 - diff: 7.28mlTrain batch 17/32 - 101.9ms/batch - loss: 5.20516 - diff: 7.08mlTrain batch 18/32 - 109.9ms/batch - loss: 5.12911 - diff: 7.07mlTrain batch 19/32 - 113.9ms/batch - loss: 4.99502 - diff: 6.98mlTrain batch 20/32 - 101.0ms/batch - loss: 5.02679 - diff: 7.01mlTrain batch 21/32 - 105.9ms/batch - loss: 4.91226 - diff: 6.93mlTrain batch 22/32 - 106.9ms/batch - loss: 4.91279 - diff: 6.96mlTrain batch 23/32 - 112.6ms/batch - loss: 4.92526 - diff: 6.99mlTrain batch 24/32 - 115.5ms/batch - loss: 5.27004 - diff: 7.06mlTrain batch 25/32 - 111.2ms/batch - loss: 5.14053 - diff: 6.98mlTrain batch 26/32 - 112.4ms/batch - loss: 5.29142 - diff: 7.09mlTrain batch 27/32 - 115.2ms/batch - loss: 5.25465 - diff: 7.06mlTrain batch 28/32 - 116.9ms/batch - loss: 5.23485 - diff: 7.05mlTrain batch 29/32 - 115.8ms/batch - loss: 5.17831 - diff: 7.02mlTrain batch 30/32 - 107.6ms/batch - loss: 5.17872 - diff: 7.04mlTrain batch 31/32 - 104.0ms/batch - loss: 5.39391 - diff: 7.22mlTrain batch 32/32 - 66.5ms/batch - loss: 5.56396 - diff: 7.23mlTrain batch 32/32 - 12.6s 66.5ms/batch - loss: 5.56396 - diff: 7.23ml
Test 0.7s: val_loss: 34.22043 - diff: 15.54ml

Epoch 128: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 122.6ms/batch - loss: 3.85981 - diff: 6.41mlTrain batch 2/32 - 104.1ms/batch - loss: 5.53318 - diff: 7.58mlTrain batch 3/32 - 107.2ms/batch - loss: 8.44060 - diff: 9.57mlTrain batch 4/32 - 111.1ms/batch - loss: 10.44883 - diff: 10.59mlTrain batch 5/32 - 109.6ms/batch - loss: 8.97121 - diff: 9.71mlTrain batch 6/32 - 125.9ms/batch - loss: 7.74804 - diff: 8.83mlTrain batch 7/32 - 116.9ms/batch - loss: 8.18842 - diff: 9.04mlTrain batch 8/32 - 115.8ms/batch - loss: 8.80573 - diff: 9.54mlTrain batch 9/32 - 115.4ms/batch - loss: 8.32843 - diff: 9.27mlTrain batch 10/32 - 108.3ms/batch - loss: 7.97548 - diff: 9.15mlTrain batch 11/32 - 107.5ms/batch - loss: 7.75300 - diff: 8.97mlTrain batch 12/32 - 116.8ms/batch - loss: 7.60258 - diff: 8.88mlTrain batch 13/32 - 112.0ms/batch - loss: 7.49158 - diff: 8.86mlTrain batch 14/32 - 125.5ms/batch - loss: 7.27952 - diff: 8.77mlTrain batch 15/32 - 114.9ms/batch - loss: 7.05193 - diff: 8.64mlTrain batch 16/32 - 112.9ms/batch - loss: 7.04638 - diff: 8.58mlTrain batch 17/32 - 114.4ms/batch - loss: 6.77030 - diff: 8.37mlTrain batch 18/32 - 112.7ms/batch - loss: 6.63235 - diff: 8.25mlTrain batch 19/32 - 119.4ms/batch - loss: 6.45673 - diff: 8.13mlTrain batch 20/32 - 112.7ms/batch - loss: 6.37723 - diff: 8.07mlTrain batch 21/32 - 119.8ms/batch - loss: 6.23834 - diff: 7.97mlTrain batch 22/32 - 107.6ms/batch - loss: 6.11144 - diff: 7.83mlTrain batch 23/32 - 109.3ms/batch - loss: 6.18145 - diff: 7.78mlTrain batch 24/32 - 106.9ms/batch - loss: 6.12821 - diff: 7.74mlTrain batch 25/32 - 108.8ms/batch - loss: 6.29336 - diff: 7.83mlTrain batch 26/32 - 103.6ms/batch - loss: 6.19342 - diff: 7.77mlTrain batch 27/32 - 103.4ms/batch - loss: 6.16675 - diff: 7.76mlTrain batch 28/32 - 103.7ms/batch - loss: 6.11535 - diff: 7.71mlTrain batch 29/32 - 103.2ms/batch - loss: 6.12644 - diff: 7.69mlTrain batch 30/32 - 103.2ms/batch - loss: 6.11898 - diff: 7.71mlTrain batch 31/32 - 102.8ms/batch - loss: 6.15110 - diff: 7.77mlTrain batch 32/32 - 76.6ms/batch - loss: 6.30899 - diff: 7.79mlTrain batch 32/32 - 11.7s 76.6ms/batch - loss: 6.30899 - diff: 7.79ml
Test 0.6s: val_loss: 35.79858 - diff: 15.77ml

Epoch 129: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 128.0ms/batch - loss: 4.66311 - diff: 6.14mlTrain batch 2/32 - 103.9ms/batch - loss: 4.17933 - diff: 6.18mlTrain batch 3/32 - 105.9ms/batch - loss: 6.13563 - diff: 7.39mlTrain batch 4/32 - 104.5ms/batch - loss: 6.30344 - diff: 7.78mlTrain batch 5/32 - 105.9ms/batch - loss: 6.25150 - diff: 7.67mlTrain batch 6/32 - 104.0ms/batch - loss: 6.21141 - diff: 7.58mlTrain batch 7/32 - 104.5ms/batch - loss: 5.75225 - diff: 7.30mlTrain batch 8/32 - 101.5ms/batch - loss: 5.96724 - diff: 7.55mlTrain batch 9/32 - 102.5ms/batch - loss: 5.69567 - diff: 7.39mlTrain batch 10/32 - 107.8ms/batch - loss: 5.95856 - diff: 7.48mlTrain batch 11/32 - 106.3ms/batch - loss: 5.76873 - diff: 7.34mlTrain batch 12/32 - 115.6ms/batch - loss: 6.28610 - diff: 7.68mlTrain batch 13/32 - 92.5ms/batch - loss: 6.26420 - diff: 7.65mlTrain batch 14/32 - 124.5ms/batch - loss: 6.06343 - diff: 7.52mlTrain batch 15/32 - 109.5ms/batch - loss: 5.93312 - diff: 7.48mlTrain batch 16/32 - 121.4ms/batch - loss: 6.23459 - diff: 7.73mlTrain batch 17/32 - 113.2ms/batch - loss: 5.96214 - diff: 7.51mlTrain batch 18/32 - 121.7ms/batch - loss: 7.05765 - diff: 8.12mlTrain batch 19/32 - 114.2ms/batch - loss: 6.86695 - diff: 8.04mlTrain batch 20/32 - 116.1ms/batch - loss: 6.79475 - diff: 8.03mlTrain batch 21/32 - 106.1ms/batch - loss: 6.59282 - diff: 7.91mlTrain batch 22/32 - 112.0ms/batch - loss: 6.37697 - diff: 7.75mlTrain batch 23/32 - 102.4ms/batch - loss: 6.47402 - diff: 7.89mlTrain batch 24/32 - 103.0ms/batch - loss: 6.56371 - diff: 7.97mlTrain batch 25/32 - 105.3ms/batch - loss: 6.99072 - diff: 8.08mlTrain batch 26/32 - 122.7ms/batch - loss: 6.94851 - diff: 8.05mlTrain batch 27/32 - 114.5ms/batch - loss: 6.87711 - diff: 8.01mlTrain batch 28/32 - 122.3ms/batch - loss: 6.70974 - diff: 7.89mlTrain batch 29/32 - 113.6ms/batch - loss: 6.73288 - diff: 7.92mlTrain batch 30/32 - 112.3ms/batch - loss: 6.79274 - diff: 7.99mlTrain batch 31/32 - 114.9ms/batch - loss: 6.81453 - diff: 8.03mlTrain batch 32/32 - 98.9ms/batch - loss: 7.12378 - diff: 8.07mlTrain batch 32/32 - 10.9s 98.9ms/batch - loss: 7.12378 - diff: 8.07ml
Test 0.7s: val_loss: 34.77467 - diff: 15.59ml

Epoch 130: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 116.0ms/batch - loss: 2.66327 - diff: 5.24mlTrain batch 2/32 - 92.8ms/batch - loss: 5.62048 - diff: 7.21mlTrain batch 3/32 - 109.4ms/batch - loss: 4.79755 - diff: 6.67mlTrain batch 4/32 - 107.8ms/batch - loss: 5.82093 - diff: 7.69mlTrain batch 5/32 - 108.8ms/batch - loss: 5.26146 - diff: 7.23mlTrain batch 6/32 - 107.2ms/batch - loss: 5.22145 - diff: 7.20mlTrain batch 7/32 - 108.6ms/batch - loss: 4.99892 - diff: 7.01mlTrain batch 8/32 - 107.4ms/batch - loss: 4.88483 - diff: 6.84mlTrain batch 9/32 - 108.4ms/batch - loss: 4.57028 - diff: 6.65mlTrain batch 10/32 - 106.5ms/batch - loss: 6.27799 - diff: 6.89mlTrain batch 11/32 - 107.7ms/batch - loss: 6.20051 - diff: 6.96mlTrain batch 12/32 - 107.6ms/batch - loss: 6.32376 - diff: 7.13mlTrain batch 13/32 - 111.1ms/batch - loss: 6.24618 - diff: 7.20mlTrain batch 14/32 - 102.9ms/batch - loss: 6.01400 - diff: 7.08mlTrain batch 15/32 - 96.6ms/batch - loss: 5.89623 - diff: 7.05mlTrain batch 16/32 - 92.0ms/batch - loss: 6.35996 - diff: 7.18mlTrain batch 17/32 - 112.2ms/batch - loss: 6.33698 - diff: 7.19mlTrain batch 18/32 - 92.4ms/batch - loss: 6.26212 - diff: 7.11mlTrain batch 19/32 - 104.1ms/batch - loss: 6.09287 - diff: 7.01mlTrain batch 20/32 - 104.0ms/batch - loss: 6.05938 - diff: 7.02mlTrain batch 21/32 - 104.3ms/batch - loss: 6.31465 - diff: 7.12mlTrain batch 22/32 - 102.6ms/batch - loss: 6.33943 - diff: 7.21mlTrain batch 23/32 - 102.9ms/batch - loss: 6.30860 - diff: 7.24mlTrain batch 24/32 - 102.6ms/batch - loss: 6.27382 - diff: 7.28mlTrain batch 25/32 - 102.7ms/batch - loss: 6.13792 - diff: 7.20mlTrain batch 26/32 - 104.7ms/batch - loss: 6.32443 - diff: 7.39mlTrain batch 27/32 - 103.2ms/batch - loss: 6.40142 - diff: 7.50mlTrain batch 28/32 - 102.7ms/batch - loss: 6.29371 - diff: 7.43mlTrain batch 29/32 - 105.5ms/batch - loss: 6.16213 - diff: 7.35mlTrain batch 30/32 - 88.3ms/batch - loss: 6.02251 - diff: 7.26mlTrain batch 31/32 - 91.3ms/batch - loss: 6.10842 - diff: 7.33mlTrain batch 32/32 - 93.9ms/batch - loss: 6.62233 - diff: 7.40mlTrain batch 32/32 - 10.8s 93.9ms/batch - loss: 6.62233 - diff: 7.40ml
Test 0.7s: val_loss: 32.22179 - diff: 15.52ml
Epoch   131: reducing learning rate of group 0 to 1.5625e-05.

Epoch 131: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 123.3ms/batch - loss: 8.64673 - diff: 8.82mlTrain batch 2/32 - 108.6ms/batch - loss: 8.13706 - diff: 8.44mlTrain batch 3/32 - 98.3ms/batch - loss: 6.85599 - diff: 8.03mlTrain batch 4/32 - 106.6ms/batch - loss: 6.12330 - diff: 7.56mlTrain batch 5/32 - 107.2ms/batch - loss: 5.33596 - diff: 7.01mlTrain batch 6/32 - 103.9ms/batch - loss: 5.74461 - diff: 7.33mlTrain batch 7/32 - 104.6ms/batch - loss: 5.97686 - diff: 7.50mlTrain batch 8/32 - 110.5ms/batch - loss: 5.65638 - diff: 7.34mlTrain batch 9/32 - 105.7ms/batch - loss: 5.91914 - diff: 7.37mlTrain batch 10/32 - 110.0ms/batch - loss: 6.29766 - diff: 7.63mlTrain batch 11/32 - 104.2ms/batch - loss: 6.65788 - diff: 7.96mlTrain batch 12/32 - 105.8ms/batch - loss: 6.89600 - diff: 8.14mlTrain batch 13/32 - 103.4ms/batch - loss: 7.29409 - diff: 8.46mlTrain batch 14/32 - 103.6ms/batch - loss: 7.21203 - diff: 8.40mlTrain batch 15/32 - 113.2ms/batch - loss: 6.92271 - diff: 8.24mlTrain batch 16/32 - 99.5ms/batch - loss: 6.63771 - diff: 7.99mlTrain batch 17/32 - 107.5ms/batch - loss: 6.50429 - diff: 7.87mlTrain batch 18/32 - 112.4ms/batch - loss: 6.28036 - diff: 7.72mlTrain batch 19/32 - 114.4ms/batch - loss: 6.58684 - diff: 7.97mlTrain batch 20/32 - 109.8ms/batch - loss: 6.56884 - diff: 7.94mlTrain batch 21/32 - 102.6ms/batch - loss: 6.38745 - diff: 7.82mlTrain batch 22/32 - 110.6ms/batch - loss: 6.35349 - diff: 7.82mlTrain batch 23/32 - 103.1ms/batch - loss: 6.73358 - diff: 8.07mlTrain batch 24/32 - 102.7ms/batch - loss: 6.64722 - diff: 8.02mlTrain batch 25/32 - 108.4ms/batch - loss: 6.66527 - diff: 8.05mlTrain batch 26/32 - 102.7ms/batch - loss: 7.07745 - diff: 8.30mlTrain batch 27/32 - 121.3ms/batch - loss: 6.87362 - diff: 8.16mlTrain batch 28/32 - 114.7ms/batch - loss: 7.00880 - diff: 8.29mlTrain batch 29/32 - 112.9ms/batch - loss: 7.14781 - diff: 8.43mlTrain batch 30/32 - 107.8ms/batch - loss: 7.78261 - diff: 8.57mlTrain batch 31/32 - 107.2ms/batch - loss: 7.71698 - diff: 8.54mlTrain batch 32/32 - 99.1ms/batch - loss: 7.77199 - diff: 8.53mlTrain batch 32/32 - 11.0s 99.1ms/batch - loss: 7.77199 - diff: 8.53ml
Test 0.6s: val_loss: 33.17622 - diff: 15.34ml

Epoch 132: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 117.6ms/batch - loss: 5.91649 - diff: 8.46mlTrain batch 2/32 - 103.7ms/batch - loss: 5.34595 - diff: 7.10mlTrain batch 3/32 - 104.1ms/batch - loss: 5.52365 - diff: 7.15mlTrain batch 4/32 - 103.9ms/batch - loss: 8.04162 - diff: 8.72mlTrain batch 5/32 - 103.4ms/batch - loss: 7.00449 - diff: 8.03mlTrain batch 6/32 - 103.9ms/batch - loss: 8.22148 - diff: 8.77mlTrain batch 7/32 - 102.2ms/batch - loss: 7.61729 - diff: 8.57mlTrain batch 8/32 - 104.0ms/batch - loss: 7.40597 - diff: 8.44mlTrain batch 9/32 - 103.4ms/batch - loss: 7.85009 - diff: 8.83mlTrain batch 10/32 - 103.2ms/batch - loss: 7.66894 - diff: 8.65mlTrain batch 11/32 - 106.6ms/batch - loss: 7.52052 - diff: 8.58mlTrain batch 12/32 - 105.8ms/batch - loss: 8.19815 - diff: 8.82mlTrain batch 13/32 - 103.6ms/batch - loss: 7.85013 - diff: 8.64mlTrain batch 14/32 - 103.7ms/batch - loss: 8.79673 - diff: 9.15mlTrain batch 15/32 - 102.8ms/batch - loss: 8.45999 - diff: 8.97mlTrain batch 16/32 - 102.2ms/batch - loss: 8.03087 - diff: 8.65mlTrain batch 17/32 - 103.6ms/batch - loss: 8.27042 - diff: 8.83mlTrain batch 18/32 - 102.5ms/batch - loss: 7.97399 - diff: 8.65mlTrain batch 19/32 - 113.2ms/batch - loss: 7.91908 - diff: 8.64mlTrain batch 20/32 - 114.4ms/batch - loss: 7.60332 - diff: 8.41mlTrain batch 21/32 - 115.8ms/batch - loss: 7.46175 - diff: 8.36mlTrain batch 22/32 - 114.7ms/batch - loss: 7.53101 - diff: 8.39mlTrain batch 23/32 - 115.6ms/batch - loss: 7.42691 - diff: 8.28mlTrain batch 24/32 - 118.4ms/batch - loss: 7.15089 - diff: 8.04mlTrain batch 25/32 - 107.5ms/batch - loss: 6.99827 - diff: 7.98mlTrain batch 26/32 - 107.5ms/batch - loss: 6.86132 - diff: 7.92mlTrain batch 27/32 - 106.9ms/batch - loss: 6.75976 - diff: 7.87mlTrain batch 28/32 - 107.1ms/batch - loss: 6.76471 - diff: 7.89mlTrain batch 29/32 - 107.0ms/batch - loss: 6.60995 - diff: 7.79mlTrain batch 30/32 - 105.8ms/batch - loss: 7.45601 - diff: 8.16mlTrain batch 31/32 - 91.9ms/batch - loss: 7.38071 - diff: 8.15mlTrain batch 32/32 - 82.4ms/batch - loss: 7.41333 - diff: 8.13mlTrain batch 32/32 - 11.1s 82.4ms/batch - loss: 7.41333 - diff: 8.13ml
Test 0.6s: val_loss: 32.79653 - diff: 15.41ml

Epoch 133: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 118.5ms/batch - loss: 2.96024 - diff: 5.43mlTrain batch 2/32 - 104.8ms/batch - loss: 3.88862 - diff: 6.00mlTrain batch 3/32 - 104.1ms/batch - loss: 4.39186 - diff: 6.49mlTrain batch 4/32 - 104.6ms/batch - loss: 4.20674 - diff: 6.24mlTrain batch 5/32 - 107.1ms/batch - loss: 5.42552 - diff: 7.36mlTrain batch 6/32 - 102.0ms/batch - loss: 4.96676 - diff: 7.00mlTrain batch 7/32 - 108.1ms/batch - loss: 5.14342 - diff: 7.25mlTrain batch 8/32 - 107.6ms/batch - loss: 5.20604 - diff: 7.35mlTrain batch 9/32 - 107.4ms/batch - loss: 5.53683 - diff: 7.51mlTrain batch 10/32 - 106.6ms/batch - loss: 5.10717 - diff: 7.14mlTrain batch 11/32 - 107.0ms/batch - loss: 5.09219 - diff: 7.19mlTrain batch 12/32 - 108.0ms/batch - loss: 5.34991 - diff: 7.26mlTrain batch 13/32 - 108.1ms/batch - loss: 5.13430 - diff: 7.02mlTrain batch 14/32 - 109.4ms/batch - loss: 5.13556 - diff: 7.03mlTrain batch 15/32 - 110.1ms/batch - loss: 4.96314 - diff: 6.91mlTrain batch 16/32 - 108.9ms/batch - loss: 5.26104 - diff: 7.06mlTrain batch 17/32 - 110.0ms/batch - loss: 5.46502 - diff: 7.21mlTrain batch 18/32 - 109.9ms/batch - loss: 5.27563 - diff: 7.08mlTrain batch 19/32 - 104.4ms/batch - loss: 5.34081 - diff: 7.13mlTrain batch 20/32 - 114.9ms/batch - loss: 5.31303 - diff: 7.12mlTrain batch 21/32 - 114.8ms/batch - loss: 5.31340 - diff: 7.12mlTrain batch 22/32 - 92.7ms/batch - loss: 5.11345 - diff: 6.94mlTrain batch 23/32 - 107.9ms/batch - loss: 5.10578 - diff: 6.97mlTrain batch 24/32 - 109.0ms/batch - loss: 5.02672 - diff: 6.93mlTrain batch 25/32 - 109.2ms/batch - loss: 5.14418 - diff: 6.95mlTrain batch 26/32 - 106.0ms/batch - loss: 5.16359 - diff: 6.97mlTrain batch 27/32 - 109.8ms/batch - loss: 5.25171 - diff: 7.02mlTrain batch 28/32 - 105.2ms/batch - loss: 5.29284 - diff: 7.03mlTrain batch 29/32 - 105.3ms/batch - loss: 5.25978 - diff: 6.99mlTrain batch 30/32 - 109.9ms/batch - loss: 5.35931 - diff: 7.03mlTrain batch 31/32 - 119.2ms/batch - loss: 5.30356 - diff: 6.99mlTrain batch 32/32 - 111.7ms/batch - loss: 6.21625 - diff: 7.10mlTrain batch 32/32 - 10.9s 111.7ms/batch - loss: 6.21625 - diff: 7.10ml
Test 0.7s: val_loss: 33.56094 - diff: 15.45ml

Epoch 134: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 134.0ms/batch - loss: 2.51729 - diff: 5.25mlTrain batch 2/32 - 113.4ms/batch - loss: 3.50272 - diff: 6.06mlTrain batch 3/32 - 114.5ms/batch - loss: 5.21197 - diff: 7.30mlTrain batch 4/32 - 114.7ms/batch - loss: 5.02772 - diff: 7.31mlTrain batch 5/32 - 114.4ms/batch - loss: 5.22299 - diff: 7.22mlTrain batch 6/32 - 113.8ms/batch - loss: 5.30195 - diff: 7.08mlTrain batch 7/32 - 121.4ms/batch - loss: 4.97012 - diff: 6.87mlTrain batch 8/32 - 119.2ms/batch - loss: 4.64582 - diff: 6.64mlTrain batch 9/32 - 118.6ms/batch - loss: 5.00151 - diff: 7.02mlTrain batch 10/32 - 118.3ms/batch - loss: 5.68058 - diff: 7.54mlTrain batch 11/32 - 117.3ms/batch - loss: 5.48711 - diff: 7.45mlTrain batch 12/32 - 115.6ms/batch - loss: 5.74938 - diff: 7.69mlTrain batch 13/32 - 119.5ms/batch - loss: 5.61871 - diff: 7.60mlTrain batch 14/32 - 115.0ms/batch - loss: 6.06188 - diff: 7.77mlTrain batch 15/32 - 112.0ms/batch - loss: 6.14551 - diff: 7.88mlTrain batch 16/32 - 88.6ms/batch - loss: 6.39776 - diff: 7.96mlTrain batch 17/32 - 102.4ms/batch - loss: 6.26759 - diff: 7.90mlTrain batch 18/32 - 106.5ms/batch - loss: 6.06373 - diff: 7.75mlTrain batch 19/32 - 111.8ms/batch - loss: 5.86892 - diff: 7.60mlTrain batch 20/32 - 107.7ms/batch - loss: 5.93803 - diff: 7.60mlTrain batch 21/32 - 123.1ms/batch - loss: 5.80093 - diff: 7.52mlTrain batch 22/32 - 112.6ms/batch - loss: 5.67154 - diff: 7.43mlTrain batch 23/32 - 113.5ms/batch - loss: 5.64711 - diff: 7.45mlTrain batch 24/32 - 108.2ms/batch - loss: 5.63762 - diff: 7.43mlTrain batch 25/32 - 108.0ms/batch - loss: 5.55146 - diff: 7.38mlTrain batch 26/32 - 99.0ms/batch - loss: 5.47424 - diff: 7.33mlTrain batch 27/32 - 104.1ms/batch - loss: 5.52474 - diff: 7.36mlTrain batch 28/32 - 105.5ms/batch - loss: 5.54430 - diff: 7.39mlTrain batch 29/32 - 102.1ms/batch - loss: 5.50020 - diff: 7.37mlTrain batch 30/32 - 87.0ms/batch - loss: 5.42359 - diff: 7.30mlTrain batch 31/32 - 93.3ms/batch - loss: 6.27956 - diff: 7.59mlTrain batch 32/32 - 77.4ms/batch - loss: 6.43634 - diff: 7.60mlTrain batch 32/32 - 12.2s 77.4ms/batch - loss: 6.43634 - diff: 7.60ml
Test 0.7s: val_loss: 33.46817 - diff: 15.51ml

Epoch 135: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 109.9ms/batch - loss: 5.53461 - diff: 8.17mlTrain batch 2/32 - 89.0ms/batch - loss: 5.50579 - diff: 8.04mlTrain batch 3/32 - 112.0ms/batch - loss: 5.94751 - diff: 7.77mlTrain batch 4/32 - 103.7ms/batch - loss: 7.82096 - diff: 8.71mlTrain batch 5/32 - 115.4ms/batch - loss: 7.86396 - diff: 9.00mlTrain batch 6/32 - 114.5ms/batch - loss: 8.60819 - diff: 9.59mlTrain batch 7/32 - 115.5ms/batch - loss: 8.64485 - diff: 9.75mlTrain batch 8/32 - 114.5ms/batch - loss: 7.91578 - diff: 9.20mlTrain batch 9/32 - 103.2ms/batch - loss: 7.81298 - diff: 9.15mlTrain batch 10/32 - 103.3ms/batch - loss: 8.45266 - diff: 9.56mlTrain batch 11/32 - 108.2ms/batch - loss: 8.10693 - diff: 9.32mlTrain batch 12/32 - 114.3ms/batch - loss: 7.62411 - diff: 8.92mlTrain batch 13/32 - 114.3ms/batch - loss: 7.25598 - diff: 8.69mlTrain batch 14/32 - 109.8ms/batch - loss: 7.05025 - diff: 8.52mlTrain batch 15/32 - 112.4ms/batch - loss: 7.20691 - diff: 8.62mlTrain batch 16/32 - 115.8ms/batch - loss: 7.42407 - diff: 8.78mlTrain batch 17/32 - 113.7ms/batch - loss: 7.23510 - diff: 8.66mlTrain batch 18/32 - 114.4ms/batch - loss: 7.03452 - diff: 8.52mlTrain batch 19/32 - 114.0ms/batch - loss: 6.85894 - diff: 8.33mlTrain batch 20/32 - 109.1ms/batch - loss: 6.73230 - diff: 8.25mlTrain batch 21/32 - 119.5ms/batch - loss: 6.79405 - diff: 8.30mlTrain batch 22/32 - 118.0ms/batch - loss: 6.79542 - diff: 8.32mlTrain batch 23/32 - 106.8ms/batch - loss: 8.10184 - diff: 8.94mlTrain batch 24/32 - 114.0ms/batch - loss: 8.16606 - diff: 8.96mlTrain batch 25/32 - 112.4ms/batch - loss: 7.96145 - diff: 8.84mlTrain batch 26/32 - 112.3ms/batch - loss: 8.39739 - diff: 9.01mlTrain batch 27/32 - 122.2ms/batch - loss: 8.74057 - diff: 9.27mlTrain batch 28/32 - 103.7ms/batch - loss: 8.60070 - diff: 9.21mlTrain batch 29/32 - 103.3ms/batch - loss: 8.54291 - diff: 9.19mlTrain batch 30/32 - 107.8ms/batch - loss: 8.40146 - diff: 9.10mlTrain batch 31/32 - 90.9ms/batch - loss: 8.30281 - diff: 9.05mlTrain batch 32/32 - 95.1ms/batch - loss: 8.33482 - diff: 9.03mlTrain batch 32/32 - 10.9s 95.1ms/batch - loss: 8.33482 - diff: 9.03ml
Test 0.7s: val_loss: 32.83362 - diff: 15.52ml

Epoch 136: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 131.7ms/batch - loss: 8.84631 - diff: 9.76mlTrain batch 2/32 - 116.3ms/batch - loss: 6.22508 - diff: 7.92mlTrain batch 3/32 - 116.7ms/batch - loss: 4.64682 - diff: 6.52mlTrain batch 4/32 - 117.3ms/batch - loss: 4.54545 - diff: 6.73mlTrain batch 5/32 - 108.3ms/batch - loss: 4.15398 - diff: 6.38mlTrain batch 6/32 - 108.3ms/batch - loss: 4.55996 - diff: 6.58mlTrain batch 7/32 - 99.3ms/batch - loss: 5.85521 - diff: 7.61mlTrain batch 8/32 - 103.7ms/batch - loss: 5.57738 - diff: 7.48mlTrain batch 9/32 - 106.7ms/batch - loss: 5.93080 - diff: 7.63mlTrain batch 10/32 - 103.9ms/batch - loss: 5.87759 - diff: 7.50mlTrain batch 11/32 - 104.2ms/batch - loss: 5.65912 - diff: 7.30mlTrain batch 12/32 - 103.8ms/batch - loss: 5.51934 - diff: 7.24mlTrain batch 13/32 - 114.2ms/batch - loss: 5.35537 - diff: 7.15mlTrain batch 14/32 - 115.2ms/batch - loss: 5.38054 - diff: 7.18mlTrain batch 15/32 - 114.1ms/batch - loss: 5.27063 - diff: 7.11mlTrain batch 16/32 - 114.2ms/batch - loss: 5.55532 - diff: 7.33mlTrain batch 17/32 - 109.5ms/batch - loss: 5.83358 - diff: 7.51mlTrain batch 18/32 - 88.0ms/batch - loss: 5.90239 - diff: 7.56mlTrain batch 19/32 - 114.8ms/batch - loss: 5.82490 - diff: 7.53mlTrain batch 20/32 - 112.9ms/batch - loss: 5.84719 - diff: 7.57mlTrain batch 21/32 - 115.8ms/batch - loss: 5.74966 - diff: 7.52mlTrain batch 22/32 - 116.6ms/batch - loss: 6.30482 - diff: 7.73mlTrain batch 23/32 - 107.2ms/batch - loss: 6.29175 - diff: 7.66mlTrain batch 24/32 - 107.2ms/batch - loss: 6.46026 - diff: 7.67mlTrain batch 25/32 - 107.0ms/batch - loss: 6.45250 - diff: 7.67mlTrain batch 26/32 - 107.4ms/batch - loss: 6.30193 - diff: 7.56mlTrain batch 27/32 - 109.9ms/batch - loss: 6.20137 - diff: 7.52mlTrain batch 28/32 - 107.6ms/batch - loss: 6.16552 - diff: 7.53mlTrain batch 29/32 - 106.6ms/batch - loss: 6.11787 - diff: 7.48mlTrain batch 30/32 - 107.4ms/batch - loss: 6.40189 - diff: 7.68mlTrain batch 31/32 - 110.7ms/batch - loss: 6.45797 - diff: 7.74mlTrain batch 32/32 - 88.1ms/batch - loss: 6.44729 - diff: 7.71mlTrain batch 32/32 - 11.7s 88.1ms/batch - loss: 6.44729 - diff: 7.71ml
Test 0.6s: val_loss: 31.43327 - diff: 15.43ml

Epoch 137: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 118.4ms/batch - loss: 6.97643 - diff: 8.94mlTrain batch 2/32 - 104.5ms/batch - loss: 6.08266 - diff: 7.51mlTrain batch 3/32 - 104.0ms/batch - loss: 6.37611 - diff: 7.83mlTrain batch 4/32 - 106.4ms/batch - loss: 6.88010 - diff: 8.54mlTrain batch 5/32 - 116.7ms/batch - loss: 7.34144 - diff: 8.65mlTrain batch 6/32 - 122.0ms/batch - loss: 7.39838 - diff: 8.74mlTrain batch 7/32 - 119.0ms/batch - loss: 6.57256 - diff: 8.09mlTrain batch 8/32 - 113.5ms/batch - loss: 6.96864 - diff: 8.03mlTrain batch 9/32 - 109.7ms/batch - loss: 7.43435 - diff: 8.40mlTrain batch 10/32 - 104.4ms/batch - loss: 6.86553 - diff: 7.99mlTrain batch 11/32 - 89.8ms/batch - loss: 7.51111 - diff: 8.40mlTrain batch 12/32 - 104.3ms/batch - loss: 7.56386 - diff: 8.38mlTrain batch 13/32 - 106.2ms/batch - loss: 7.25314 - diff: 8.19mlTrain batch 14/32 - 108.3ms/batch - loss: 7.06066 - diff: 7.99mlTrain batch 15/32 - 119.6ms/batch - loss: 7.01094 - diff: 8.02mlTrain batch 16/32 - 122.5ms/batch - loss: 7.55625 - diff: 8.46mlTrain batch 17/32 - 107.2ms/batch - loss: 7.28930 - diff: 8.29mlTrain batch 18/32 - 115.4ms/batch - loss: 8.34177 - diff: 8.65mlTrain batch 19/32 - 97.8ms/batch - loss: 8.08993 - diff: 8.53mlTrain batch 20/32 - 103.8ms/batch - loss: 8.27265 - diff: 8.65mlTrain batch 21/32 - 113.9ms/batch - loss: 8.02135 - diff: 8.49mlTrain batch 22/32 - 113.1ms/batch - loss: 8.66619 - diff: 8.87mlTrain batch 23/32 - 113.0ms/batch - loss: 8.55355 - diff: 8.82mlTrain batch 24/32 - 113.3ms/batch - loss: 8.70504 - diff: 8.92mlTrain batch 25/32 - 114.0ms/batch - loss: 8.48721 - diff: 8.80mlTrain batch 26/32 - 113.2ms/batch - loss: 8.29737 - diff: 8.69mlTrain batch 27/32 - 121.4ms/batch - loss: 8.14398 - diff: 8.64mlTrain batch 28/32 - 113.8ms/batch - loss: 7.98406 - diff: 8.54mlTrain batch 29/32 - 114.3ms/batch - loss: 7.92496 - diff: 8.51mlTrain batch 30/32 - 115.0ms/batch - loss: 7.79428 - diff: 8.44mlTrain batch 31/32 - 105.0ms/batch - loss: 7.81064 - diff: 8.49mlTrain batch 32/32 - 79.0ms/batch - loss: 8.34060 - diff: 8.54mlTrain batch 32/32 - 11.9s 79.0ms/batch - loss: 8.34060 - diff: 8.54ml
Test 0.7s: val_loss: 32.83279 - diff: 15.54ml

Epoch 138: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 128.8ms/batch - loss: 6.31212 - diff: 8.23mlTrain batch 2/32 - 113.0ms/batch - loss: 7.71432 - diff: 9.00mlTrain batch 3/32 - 112.9ms/batch - loss: 6.59270 - diff: 7.80mlTrain batch 4/32 - 113.0ms/batch - loss: 5.85999 - diff: 7.41mlTrain batch 5/32 - 120.1ms/batch - loss: 7.22046 - diff: 8.21mlTrain batch 6/32 - 117.4ms/batch - loss: 10.72851 - diff: 9.83mlTrain batch 7/32 - 117.5ms/batch - loss: 9.53852 - diff: 9.19mlTrain batch 8/32 - 113.5ms/batch - loss: 9.01937 - diff: 8.92mlTrain batch 9/32 - 113.0ms/batch - loss: 9.44368 - diff: 9.37mlTrain batch 10/32 - 120.7ms/batch - loss: 9.10549 - diff: 9.30mlTrain batch 11/32 - 111.8ms/batch - loss: 8.91919 - diff: 9.21mlTrain batch 12/32 - 119.6ms/batch - loss: 8.54083 - diff: 9.00mlTrain batch 13/32 - 117.7ms/batch - loss: 7.94153 - diff: 8.51mlTrain batch 14/32 - 119.3ms/batch - loss: 7.81444 - diff: 8.42mlTrain batch 15/32 - 107.4ms/batch - loss: 7.46961 - diff: 8.22mlTrain batch 16/32 - 107.0ms/batch - loss: 7.16996 - diff: 8.05mlTrain batch 17/32 - 104.7ms/batch - loss: 6.89034 - diff: 7.84mlTrain batch 18/32 - 114.6ms/batch - loss: 6.96513 - diff: 7.90mlTrain batch 19/32 - 115.0ms/batch - loss: 6.90052 - diff: 7.87mlTrain batch 20/32 - 112.6ms/batch - loss: 6.79175 - diff: 7.79mlTrain batch 21/32 - 113.2ms/batch - loss: 6.71920 - diff: 7.80mlTrain batch 22/32 - 109.7ms/batch - loss: 6.60593 - diff: 7.76mlTrain batch 23/32 - 113.1ms/batch - loss: 6.53636 - diff: 7.69mlTrain batch 24/32 - 112.9ms/batch - loss: 6.39358 - diff: 7.60mlTrain batch 25/32 - 116.5ms/batch - loss: 6.22332 - diff: 7.48mlTrain batch 26/32 - 112.9ms/batch - loss: 6.13226 - diff: 7.45mlTrain batch 27/32 - 113.2ms/batch - loss: 6.00723 - diff: 7.37mlTrain batch 28/32 - 112.1ms/batch - loss: 5.92041 - diff: 7.30mlTrain batch 29/32 - 112.8ms/batch - loss: 5.76390 - diff: 7.16mlTrain batch 30/32 - 112.4ms/batch - loss: 5.66057 - diff: 7.11mlTrain batch 31/32 - 112.8ms/batch - loss: 6.01120 - diff: 7.35mlTrain batch 32/32 - 69.9ms/batch - loss: 6.30835 - diff: 7.40mlTrain batch 32/32 - 11.4s 69.9ms/batch - loss: 6.30835 - diff: 7.40ml
Test 0.7s: val_loss: 33.81544 - diff: 15.59ml

Epoch 139: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 129.0ms/batch - loss: 10.45998 - diff: 9.94mlTrain batch 2/32 - 114.4ms/batch - loss: 6.20048 - diff: 7.11mlTrain batch 3/32 - 112.7ms/batch - loss: 6.27771 - diff: 7.40mlTrain batch 4/32 - 113.3ms/batch - loss: 6.08906 - diff: 7.28mlTrain batch 5/32 - 112.9ms/batch - loss: 6.85524 - diff: 7.95mlTrain batch 6/32 - 112.2ms/batch - loss: 6.84362 - diff: 8.05mlTrain batch 7/32 - 111.9ms/batch - loss: 6.90330 - diff: 8.13mlTrain batch 8/32 - 112.2ms/batch - loss: 6.23931 - diff: 7.61mlTrain batch 9/32 - 112.6ms/batch - loss: 5.93449 - diff: 7.39mlTrain batch 10/32 - 112.7ms/batch - loss: 7.76280 - diff: 7.91mlTrain batch 11/32 - 112.2ms/batch - loss: 7.70214 - diff: 8.08mlTrain batch 12/32 - 111.6ms/batch - loss: 7.30068 - diff: 7.81mlTrain batch 13/32 - 111.8ms/batch - loss: 6.99780 - diff: 7.71mlTrain batch 14/32 - 111.6ms/batch - loss: 7.06484 - diff: 7.84mlTrain batch 15/32 - 113.8ms/batch - loss: 6.93291 - diff: 7.82mlTrain batch 16/32 - 122.2ms/batch - loss: 6.69698 - diff: 7.72mlTrain batch 17/32 - 116.4ms/batch - loss: 6.47577 - diff: 7.61mlTrain batch 18/32 - 115.7ms/batch - loss: 6.31604 - diff: 7.54mlTrain batch 19/32 - 116.3ms/batch - loss: 6.13219 - diff: 7.44mlTrain batch 20/32 - 115.8ms/batch - loss: 5.99572 - diff: 7.38mlTrain batch 21/32 - 89.7ms/batch - loss: 5.89103 - diff: 7.34mlTrain batch 22/32 - 99.1ms/batch - loss: 6.37757 - diff: 7.58mlTrain batch 23/32 - 115.0ms/batch - loss: 6.32715 - diff: 7.60mlTrain batch 24/32 - 115.2ms/batch - loss: 6.38612 - diff: 7.66mlTrain batch 25/32 - 113.3ms/batch - loss: 6.28929 - diff: 7.60mlTrain batch 26/32 - 113.9ms/batch - loss: 6.12606 - diff: 7.46mlTrain batch 27/32 - 113.8ms/batch - loss: 6.57537 - diff: 7.74mlTrain batch 28/32 - 114.1ms/batch - loss: 6.74684 - diff: 7.88mlTrain batch 29/32 - 107.0ms/batch - loss: 6.67917 - diff: 7.83mlTrain batch 30/32 - 108.7ms/batch - loss: 6.67833 - diff: 7.87mlTrain batch 31/32 - 108.9ms/batch - loss: 6.55502 - diff: 7.79mlTrain batch 32/32 - 98.3ms/batch - loss: 6.70884 - diff: 7.81mlTrain batch 32/32 - 11.0s 98.3ms/batch - loss: 6.70884 - diff: 7.81ml
Test 0.6s: val_loss: 32.99210 - diff: 15.50ml

Epoch 140: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 122.9ms/batch - loss: 4.81962 - diff: 7.04mlTrain batch 2/32 - 110.8ms/batch - loss: 5.13977 - diff: 7.08mlTrain batch 3/32 - 109.8ms/batch - loss: 5.42647 - diff: 7.03mlTrain batch 4/32 - 107.7ms/batch - loss: 5.17681 - diff: 7.03mlTrain batch 5/32 - 107.5ms/batch - loss: 4.74989 - diff: 6.73mlTrain batch 6/32 - 106.7ms/batch - loss: 4.91961 - diff: 6.85mlTrain batch 7/32 - 106.9ms/batch - loss: 5.47256 - diff: 7.40mlTrain batch 8/32 - 105.9ms/batch - loss: 9.34852 - diff: 8.88mlTrain batch 9/32 - 107.2ms/batch - loss: 8.65517 - diff: 8.50mlTrain batch 10/32 - 106.2ms/batch - loss: 8.16258 - diff: 8.25mlTrain batch 11/32 - 105.4ms/batch - loss: 7.80351 - diff: 8.17mlTrain batch 12/32 - 106.7ms/batch - loss: 7.81703 - diff: 8.18mlTrain batch 13/32 - 108.0ms/batch - loss: 7.60724 - diff: 8.12mlTrain batch 14/32 - 93.0ms/batch - loss: 8.04276 - diff: 8.38mlTrain batch 15/32 - 88.7ms/batch - loss: 7.94810 - diff: 8.32mlTrain batch 16/32 - 89.5ms/batch - loss: 8.01649 - diff: 8.38mlTrain batch 17/32 - 125.7ms/batch - loss: 8.15290 - diff: 8.55mlTrain batch 18/32 - 116.6ms/batch - loss: 7.88788 - diff: 8.38mlTrain batch 19/32 - 118.7ms/batch - loss: 7.69832 - diff: 8.30mlTrain batch 20/32 - 112.2ms/batch - loss: 7.60458 - diff: 8.24mlTrain batch 21/32 - 100.6ms/batch - loss: 7.77158 - diff: 8.39mlTrain batch 22/32 - 111.9ms/batch - loss: 7.83603 - diff: 8.44mlTrain batch 23/32 - 103.8ms/batch - loss: 7.68007 - diff: 8.37mlTrain batch 24/32 - 104.1ms/batch - loss: 7.62998 - diff: 8.35mlTrain batch 25/32 - 108.4ms/batch - loss: 8.12372 - diff: 8.55mlTrain batch 26/32 - 106.4ms/batch - loss: 8.48834 - diff: 8.73mlTrain batch 27/32 - 113.3ms/batch - loss: 8.26013 - diff: 8.60mlTrain batch 28/32 - 107.0ms/batch - loss: 8.18861 - diff: 8.57mlTrain batch 29/32 - 111.7ms/batch - loss: 7.99924 - diff: 8.45mlTrain batch 30/32 - 105.9ms/batch - loss: 8.18217 - diff: 8.52mlTrain batch 31/32 - 116.0ms/batch - loss: 8.06168 - diff: 8.46mlTrain batch 32/32 - 111.0ms/batch - loss: 8.27174 - diff: 8.47mlTrain batch 32/32 - 11.6s 111.0ms/batch - loss: 8.27174 - diff: 8.47ml
Test 0.7s: val_loss: 33.37340 - diff: 15.59ml

Epoch 141: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 132.6ms/batch - loss: 2.06367 - diff: 5.08mlTrain batch 2/32 - 117.3ms/batch - loss: 3.14049 - diff: 5.78mlTrain batch 3/32 - 114.5ms/batch - loss: 2.95953 - diff: 5.68mlTrain batch 4/32 - 115.3ms/batch - loss: 3.90798 - diff: 6.31mlTrain batch 5/32 - 112.7ms/batch - loss: 3.85899 - diff: 6.22mlTrain batch 6/32 - 113.0ms/batch - loss: 4.42494 - diff: 6.64mlTrain batch 7/32 - 114.8ms/batch - loss: 4.94822 - diff: 7.07mlTrain batch 8/32 - 112.2ms/batch - loss: 5.07231 - diff: 7.23mlTrain batch 9/32 - 103.1ms/batch - loss: 5.26058 - diff: 7.26mlTrain batch 10/32 - 96.0ms/batch - loss: 5.16651 - diff: 7.21mlTrain batch 11/32 - 107.0ms/batch - loss: 4.90689 - diff: 7.00mlTrain batch 12/32 - 109.4ms/batch - loss: 4.80330 - diff: 6.92mlTrain batch 13/32 - 117.2ms/batch - loss: 5.21206 - diff: 6.96mlTrain batch 14/32 - 123.2ms/batch - loss: 5.05293 - diff: 6.89mlTrain batch 15/32 - 113.5ms/batch - loss: 7.53091 - diff: 8.09mlTrain batch 16/32 - 105.4ms/batch - loss: 7.29143 - diff: 7.93mlTrain batch 17/32 - 102.8ms/batch - loss: 7.94588 - diff: 8.37mlTrain batch 18/32 - 112.6ms/batch - loss: 8.03052 - diff: 8.45mlTrain batch 19/32 - 106.8ms/batch - loss: 7.91375 - diff: 8.43mlTrain batch 20/32 - 115.8ms/batch - loss: 7.73431 - diff: 8.32mlTrain batch 21/32 - 107.5ms/batch - loss: 7.65917 - diff: 8.27mlTrain batch 22/32 - 113.6ms/batch - loss: 8.00539 - diff: 8.47mlTrain batch 23/32 - 98.2ms/batch - loss: 7.82144 - diff: 8.36mlTrain batch 24/32 - 99.9ms/batch - loss: 7.71173 - diff: 8.30mlTrain batch 25/32 - 103.4ms/batch - loss: 7.56687 - diff: 8.22mlTrain batch 26/32 - 114.9ms/batch - loss: 7.45008 - diff: 8.16mlTrain batch 27/32 - 101.7ms/batch - loss: 7.37293 - diff: 8.14mlTrain batch 28/32 - 118.2ms/batch - loss: 7.24313 - diff: 8.09mlTrain batch 29/32 - 110.3ms/batch - loss: 7.09800 - diff: 8.03mlTrain batch 30/32 - 100.6ms/batch - loss: 6.98673 - diff: 7.97mlTrain batch 31/32 - 91.2ms/batch - loss: 6.79169 - diff: 7.82mlTrain batch 32/32 - 85.1ms/batch - loss: 7.10410 - diff: 7.87mlTrain batch 32/32 - 11.5s 85.1ms/batch - loss: 7.10410 - diff: 7.87ml
Test 0.6s: val_loss: 33.12203 - diff: 15.56ml
Epoch   142: reducing learning rate of group 0 to 7.8125e-06.

Epoch 142: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 123.9ms/batch - loss: 6.02034 - diff: 7.88mlTrain batch 2/32 - 108.1ms/batch - loss: 9.09010 - diff: 9.30mlTrain batch 3/32 - 114.5ms/batch - loss: 9.22283 - diff: 9.50mlTrain batch 4/32 - 114.1ms/batch - loss: 8.09403 - diff: 9.03mlTrain batch 5/32 - 114.0ms/batch - loss: 7.36402 - diff: 8.25mlTrain batch 6/32 - 114.1ms/batch - loss: 8.12929 - diff: 8.92mlTrain batch 7/32 - 114.1ms/batch - loss: 7.66498 - diff: 8.71mlTrain batch 8/32 - 113.8ms/batch - loss: 7.13853 - diff: 8.35mlTrain batch 9/32 - 120.3ms/batch - loss: 6.82879 - diff: 8.16mlTrain batch 10/32 - 111.7ms/batch - loss: 6.55901 - diff: 8.00mlTrain batch 11/32 - 93.2ms/batch - loss: 6.49306 - diff: 7.94mlTrain batch 12/32 - 87.9ms/batch - loss: 6.26739 - diff: 7.85mlTrain batch 13/32 - 91.8ms/batch - loss: 6.49995 - diff: 8.00mlTrain batch 14/32 - 95.7ms/batch - loss: 6.35813 - diff: 7.88mlTrain batch 15/32 - 103.4ms/batch - loss: 6.26932 - diff: 7.85mlTrain batch 16/32 - 106.1ms/batch - loss: 6.10565 - diff: 7.76mlTrain batch 17/32 - 103.8ms/batch - loss: 5.85656 - diff: 7.56mlTrain batch 18/32 - 103.3ms/batch - loss: 5.71282 - diff: 7.46mlTrain batch 19/32 - 104.0ms/batch - loss: 5.75858 - diff: 7.54mlTrain batch 20/32 - 104.7ms/batch - loss: 5.60493 - diff: 7.44mlTrain batch 21/32 - 103.4ms/batch - loss: 6.49815 - diff: 7.95mlTrain batch 22/32 - 104.7ms/batch - loss: 6.37279 - diff: 7.88mlTrain batch 23/32 - 104.0ms/batch - loss: 6.88590 - diff: 8.22mlTrain batch 24/32 - 112.0ms/batch - loss: 6.78470 - diff: 8.16mlTrain batch 25/32 - 98.2ms/batch - loss: 6.74774 - diff: 8.12mlTrain batch 26/32 - 104.3ms/batch - loss: 6.61322 - diff: 8.05mlTrain batch 27/32 - 110.0ms/batch - loss: 6.49599 - diff: 7.98mlTrain batch 28/32 - 99.0ms/batch - loss: 6.84255 - diff: 8.11mlTrain batch 29/32 - 107.1ms/batch - loss: 6.98811 - diff: 8.23mlTrain batch 30/32 - 100.9ms/batch - loss: 6.87404 - diff: 8.11mlTrain batch 31/32 - 94.3ms/batch - loss: 6.80881 - diff: 8.08mlTrain batch 32/32 - 83.2ms/batch - loss: 6.98003 - diff: 8.10mlTrain batch 32/32 - 11.3s 83.2ms/batch - loss: 6.98003 - diff: 8.10ml
Test 0.6s: val_loss: 35.07877 - diff: 15.43ml

Epoch 143: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 129.1ms/batch - loss: 4.01876 - diff: 6.92mlTrain batch 2/32 - 105.2ms/batch - loss: 6.73882 - diff: 8.66mlTrain batch 3/32 - 113.0ms/batch - loss: 6.51160 - diff: 8.66mlTrain batch 4/32 - 106.3ms/batch - loss: 8.55748 - diff: 10.17mlTrain batch 5/32 - 112.3ms/batch - loss: 7.86556 - diff: 9.40mlTrain batch 6/32 - 104.3ms/batch - loss: 6.94777 - diff: 8.63mlTrain batch 7/32 - 115.1ms/batch - loss: 6.38018 - diff: 8.07mlTrain batch 8/32 - 103.8ms/batch - loss: 6.13366 - diff: 8.01mlTrain batch 9/32 - 114.0ms/batch - loss: 5.91450 - diff: 7.85mlTrain batch 10/32 - 103.8ms/batch - loss: 5.60008 - diff: 7.54mlTrain batch 11/32 - 106.8ms/batch - loss: 5.43076 - diff: 7.45mlTrain batch 12/32 - 103.3ms/batch - loss: 5.39018 - diff: 7.42mlTrain batch 13/32 - 115.4ms/batch - loss: 5.25837 - diff: 7.35mlTrain batch 14/32 - 90.6ms/batch - loss: 5.49259 - diff: 7.42mlTrain batch 15/32 - 103.9ms/batch - loss: 5.40560 - diff: 7.41mlTrain batch 16/32 - 104.0ms/batch - loss: 6.25963 - diff: 7.89mlTrain batch 17/32 - 104.6ms/batch - loss: 6.63084 - diff: 8.16mlTrain batch 18/32 - 103.2ms/batch - loss: 6.59020 - diff: 8.14mlTrain batch 19/32 - 105.1ms/batch - loss: 6.55536 - diff: 8.13mlTrain batch 20/32 - 103.8ms/batch - loss: 6.44697 - diff: 8.08mlTrain batch 21/32 - 106.5ms/batch - loss: 6.43468 - diff: 8.10mlTrain batch 22/32 - 104.1ms/batch - loss: 6.35165 - diff: 8.07mlTrain batch 23/32 - 105.9ms/batch - loss: 6.18267 - diff: 7.94mlTrain batch 24/32 - 102.9ms/batch - loss: 6.15554 - diff: 7.90mlTrain batch 25/32 - 116.1ms/batch - loss: 6.05131 - diff: 7.83mlTrain batch 26/32 - 107.2ms/batch - loss: 6.07447 - diff: 7.89mlTrain batch 27/32 - 111.7ms/batch - loss: 5.94429 - diff: 7.80mlTrain batch 28/32 - 107.3ms/batch - loss: 5.81612 - diff: 7.70mlTrain batch 29/32 - 108.4ms/batch - loss: 5.75367 - diff: 7.68mlTrain batch 30/32 - 106.8ms/batch - loss: 5.97110 - diff: 7.83mlTrain batch 31/32 - 107.4ms/batch - loss: 6.31411 - diff: 8.07mlTrain batch 32/32 - 82.2ms/batch - loss: 6.52246 - diff: 8.08mlTrain batch 32/32 - 10.8s 82.2ms/batch - loss: 6.52246 - diff: 8.08ml
Test 0.6s: val_loss: 32.40394 - diff: 15.44ml

Epoch 144: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 129.5ms/batch - loss: 14.74432 - diff: 9.80mlTrain batch 2/32 - 104.4ms/batch - loss: 9.89783 - diff: 7.59mlTrain batch 3/32 - 114.2ms/batch - loss: 8.12827 - diff: 7.30mlTrain batch 4/32 - 103.9ms/batch - loss: 6.69998 - diff: 6.81mlTrain batch 5/32 - 116.9ms/batch - loss: 6.24199 - diff: 6.79mlTrain batch 6/32 - 107.1ms/batch - loss: 6.18965 - diff: 7.06mlTrain batch 7/32 - 126.5ms/batch - loss: 6.07564 - diff: 7.15mlTrain batch 8/32 - 116.0ms/batch - loss: 6.12869 - diff: 7.14mlTrain batch 9/32 - 127.1ms/batch - loss: 5.99893 - diff: 7.21mlTrain batch 10/32 - 115.1ms/batch - loss: 5.86919 - diff: 7.14mlTrain batch 11/32 - 104.1ms/batch - loss: 5.74739 - diff: 7.15mlTrain batch 12/32 - 106.8ms/batch - loss: 5.52077 - diff: 7.01mlTrain batch 13/32 - 120.8ms/batch - loss: 5.48714 - diff: 7.06mlTrain batch 14/32 - 108.2ms/batch - loss: 6.78127 - diff: 7.44mlTrain batch 15/32 - 106.1ms/batch - loss: 6.72710 - diff: 7.47mlTrain batch 16/32 - 107.2ms/batch - loss: 6.52511 - diff: 7.39mlTrain batch 17/32 - 107.4ms/batch - loss: 6.34637 - diff: 7.27mlTrain batch 18/32 - 106.1ms/batch - loss: 6.10263 - diff: 7.10mlTrain batch 19/32 - 106.4ms/batch - loss: 6.22587 - diff: 7.25mlTrain batch 20/32 - 108.8ms/batch - loss: 6.09071 - diff: 7.23mlTrain batch 21/32 - 105.7ms/batch - loss: 6.03333 - diff: 7.25mlTrain batch 22/32 - 107.4ms/batch - loss: 6.16929 - diff: 7.41mlTrain batch 23/32 - 111.5ms/batch - loss: 6.08062 - diff: 7.36mlTrain batch 24/32 - 116.4ms/batch - loss: 5.94111 - diff: 7.29mlTrain batch 25/32 - 117.1ms/batch - loss: 6.07960 - diff: 7.40mlTrain batch 26/32 - 116.0ms/batch - loss: 6.46247 - diff: 7.71mlTrain batch 27/32 - 111.1ms/batch - loss: 6.32257 - diff: 7.63mlTrain batch 28/32 - 103.9ms/batch - loss: 6.17878 - diff: 7.54mlTrain batch 29/32 - 103.6ms/batch - loss: 6.89696 - diff: 7.97mlTrain batch 30/32 - 102.7ms/batch - loss: 6.79431 - diff: 7.92mlTrain batch 31/32 - 102.8ms/batch - loss: 6.74945 - diff: 7.90mlTrain batch 32/32 - 93.8ms/batch - loss: 7.46877 - diff: 7.97mlTrain batch 32/32 - 11.8s 93.8ms/batch - loss: 7.46877 - diff: 7.97ml
Test 0.6s: val_loss: 35.16946 - diff: 15.67ml

Epoch 145: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 110.4ms/batch - loss: 19.24466 - diff: 15.54mlTrain batch 2/32 - 100.1ms/batch - loss: 10.78219 - diff: 10.11mlTrain batch 3/32 - 115.7ms/batch - loss: 10.98410 - diff: 9.67mlTrain batch 4/32 - 102.9ms/batch - loss: 9.10137 - diff: 8.68mlTrain batch 5/32 - 114.1ms/batch - loss: 7.91301 - diff: 8.11mlTrain batch 6/32 - 103.2ms/batch - loss: 7.10100 - diff: 7.68mlTrain batch 7/32 - 113.0ms/batch - loss: 6.50669 - diff: 7.30mlTrain batch 8/32 - 103.7ms/batch - loss: 6.79920 - diff: 7.55mlTrain batch 9/32 - 102.0ms/batch - loss: 6.80197 - diff: 7.64mlTrain batch 10/32 - 87.7ms/batch - loss: 7.42108 - diff: 7.81mlTrain batch 11/32 - 95.6ms/batch - loss: 7.43065 - diff: 7.94mlTrain batch 12/32 - 99.6ms/batch - loss: 7.56942 - diff: 8.15mlTrain batch 13/32 - 95.7ms/batch - loss: 7.13225 - diff: 7.86mlTrain batch 14/32 - 94.7ms/batch - loss: 7.44800 - diff: 8.10mlTrain batch 15/32 - 110.4ms/batch - loss: 7.39111 - diff: 8.06mlTrain batch 16/32 - 107.9ms/batch - loss: 8.79312 - diff: 8.76mlTrain batch 17/32 - 108.7ms/batch - loss: 8.59016 - diff: 8.61mlTrain batch 18/32 - 107.9ms/batch - loss: 8.20724 - diff: 8.35mlTrain batch 19/32 - 107.2ms/batch - loss: 8.17141 - diff: 8.38mlTrain batch 20/32 - 108.2ms/batch - loss: 7.90815 - diff: 8.25mlTrain batch 21/32 - 107.7ms/batch - loss: 7.80044 - diff: 8.23mlTrain batch 22/32 - 107.1ms/batch - loss: 7.67224 - diff: 8.18mlTrain batch 23/32 - 109.3ms/batch - loss: 7.47850 - diff: 8.08mlTrain batch 24/32 - 110.0ms/batch - loss: 7.60888 - diff: 8.23mlTrain batch 25/32 - 107.4ms/batch - loss: 7.44755 - diff: 8.13mlTrain batch 26/32 - 107.7ms/batch - loss: 7.48025 - diff: 8.15mlTrain batch 27/32 - 106.9ms/batch - loss: 7.51938 - diff: 8.20mlTrain batch 28/32 - 107.6ms/batch - loss: 7.32317 - diff: 8.05mlTrain batch 29/32 - 116.3ms/batch - loss: 7.17886 - diff: 7.95mlTrain batch 30/32 - 107.7ms/batch - loss: 7.11578 - diff: 7.90mlTrain batch 31/32 - 92.7ms/batch - loss: 7.13433 - diff: 7.92mlTrain batch 32/32 - 83.5ms/batch - loss: 9.41044 - diff: 8.08mlTrain batch 32/32 - 10.8s 83.5ms/batch - loss: 9.41044 - diff: 8.08ml
Test 0.7s: val_loss: 34.36559 - diff: 15.54ml

Epoch 146: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 125.4ms/batch - loss: 5.07668 - diff: 6.85mlTrain batch 2/32 - 108.1ms/batch - loss: 6.25683 - diff: 6.56mlTrain batch 3/32 - 113.6ms/batch - loss: 7.47203 - diff: 8.04mlTrain batch 4/32 - 115.9ms/batch - loss: 7.04061 - diff: 7.77mlTrain batch 5/32 - 107.3ms/batch - loss: 6.88295 - diff: 7.82mlTrain batch 6/32 - 107.1ms/batch - loss: 6.44115 - diff: 7.63mlTrain batch 7/32 - 104.4ms/batch - loss: 6.88658 - diff: 8.09mlTrain batch 8/32 - 104.4ms/batch - loss: 6.77884 - diff: 8.19mlTrain batch 9/32 - 107.8ms/batch - loss: 6.42180 - diff: 7.95mlTrain batch 10/32 - 114.1ms/batch - loss: 7.73472 - diff: 8.71mlTrain batch 11/32 - 91.8ms/batch - loss: 7.23731 - diff: 8.30mlTrain batch 12/32 - 91.1ms/batch - loss: 8.21423 - diff: 8.85mlTrain batch 13/32 - 107.6ms/batch - loss: 8.75456 - diff: 8.79mlTrain batch 14/32 - 109.3ms/batch - loss: 9.27349 - diff: 9.12mlTrain batch 15/32 - 108.4ms/batch - loss: 8.94783 - diff: 9.02mlTrain batch 16/32 - 100.8ms/batch - loss: 8.55846 - diff: 8.82mlTrain batch 17/32 - 106.5ms/batch - loss: 8.97779 - diff: 8.99mlTrain batch 18/32 - 111.5ms/batch - loss: 8.54696 - diff: 8.70mlTrain batch 19/32 - 107.6ms/batch - loss: 8.32496 - diff: 8.57mlTrain batch 20/32 - 107.8ms/batch - loss: 8.43357 - diff: 8.67mlTrain batch 21/32 - 107.6ms/batch - loss: 8.25120 - diff: 8.59mlTrain batch 22/32 - 106.5ms/batch - loss: 8.20639 - diff: 8.62mlTrain batch 23/32 - 107.9ms/batch - loss: 8.25820 - diff: 8.73mlTrain batch 24/32 - 111.5ms/batch - loss: 8.14712 - diff: 8.67mlTrain batch 25/32 - 119.7ms/batch - loss: 8.17604 - diff: 8.75mlTrain batch 26/32 - 117.1ms/batch - loss: 8.55565 - diff: 8.96mlTrain batch 27/32 - 116.7ms/batch - loss: 8.36416 - diff: 8.86mlTrain batch 28/32 - 117.2ms/batch - loss: 8.22344 - diff: 8.77mlTrain batch 29/32 - 116.5ms/batch - loss: 8.37661 - diff: 8.91mlTrain batch 30/32 - 115.9ms/batch - loss: 8.36539 - diff: 8.92mlTrain batch 31/32 - 109.5ms/batch - loss: 8.27934 - diff: 8.88mlTrain batch 32/32 - 84.3ms/batch - loss: 8.45368 - diff: 8.89mlTrain batch 32/32 - 11.7s 84.3ms/batch - loss: 8.45368 - diff: 8.89ml
Test 0.7s: val_loss: 32.15379 - diff: 15.50ml

Epoch 147: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 117.5ms/batch - loss: 3.42131 - diff: 5.91mlTrain batch 2/32 - 104.2ms/batch - loss: 2.74873 - diff: 5.44mlTrain batch 3/32 - 104.3ms/batch - loss: 4.17325 - diff: 6.05mlTrain batch 4/32 - 105.6ms/batch - loss: 4.70635 - diff: 6.59mlTrain batch 5/32 - 105.5ms/batch - loss: 7.15589 - diff: 7.83mlTrain batch 6/32 - 104.6ms/batch - loss: 6.25793 - diff: 7.20mlTrain batch 7/32 - 94.2ms/batch - loss: 6.07014 - diff: 7.08mlTrain batch 8/32 - 118.1ms/batch - loss: 6.05384 - diff: 7.19mlTrain batch 9/32 - 122.1ms/batch - loss: 5.54908 - diff: 6.81mlTrain batch 10/32 - 112.9ms/batch - loss: 5.71845 - diff: 6.96mlTrain batch 11/32 - 104.0ms/batch - loss: 5.40907 - diff: 6.79mlTrain batch 12/32 - 108.8ms/batch - loss: 5.65944 - diff: 7.06mlTrain batch 13/32 - 104.3ms/batch - loss: 6.48649 - diff: 7.46mlTrain batch 14/32 - 113.6ms/batch - loss: 6.16011 - diff: 7.22mlTrain batch 15/32 - 103.3ms/batch - loss: 6.45818 - diff: 7.40mlTrain batch 16/32 - 115.6ms/batch - loss: 6.53336 - diff: 7.45mlTrain batch 17/32 - 106.8ms/batch - loss: 6.50668 - diff: 7.46mlTrain batch 18/32 - 116.7ms/batch - loss: 6.76705 - diff: 7.69mlTrain batch 19/32 - 107.0ms/batch - loss: 6.62083 - diff: 7.62mlTrain batch 20/32 - 115.4ms/batch - loss: 6.50056 - diff: 7.52mlTrain batch 21/32 - 106.7ms/batch - loss: 6.33612 - diff: 7.43mlTrain batch 22/32 - 114.7ms/batch - loss: 6.23280 - diff: 7.40mlTrain batch 23/32 - 106.1ms/batch - loss: 6.09013 - diff: 7.33mlTrain batch 24/32 - 108.5ms/batch - loss: 5.94822 - diff: 7.24mlTrain batch 25/32 - 106.0ms/batch - loss: 6.04295 - diff: 7.35mlTrain batch 26/32 - 115.3ms/batch - loss: 5.91679 - diff: 7.30mlTrain batch 27/32 - 112.5ms/batch - loss: 5.81154 - diff: 7.23mlTrain batch 28/32 - 105.1ms/batch - loss: 5.86449 - diff: 7.31mlTrain batch 29/32 - 108.0ms/batch - loss: 5.80644 - diff: 7.27mlTrain batch 30/32 - 101.5ms/batch - loss: 5.73651 - diff: 7.22mlTrain batch 31/32 - 92.2ms/batch - loss: 5.77398 - diff: 7.26mlTrain batch 32/32 - 84.5ms/batch - loss: 5.77616 - diff: 7.24mlTrain batch 32/32 - 10.9s 84.5ms/batch - loss: 5.77616 - diff: 7.24ml
Test 0.8s: val_loss: 32.95261 - diff: 15.54ml

Epoch 148: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 119.3ms/batch - loss: 9.77875 - diff: 9.93mlTrain batch 2/32 - 105.9ms/batch - loss: 6.69380 - diff: 8.12mlTrain batch 3/32 - 104.9ms/batch - loss: 5.45833 - diff: 7.23mlTrain batch 4/32 - 106.8ms/batch - loss: 5.68903 - diff: 7.37mlTrain batch 5/32 - 104.6ms/batch - loss: 5.10179 - diff: 7.03mlTrain batch 6/32 - 108.4ms/batch - loss: 5.99307 - diff: 7.80mlTrain batch 7/32 - 104.3ms/batch - loss: 5.92730 - diff: 7.81mlTrain batch 8/32 - 103.5ms/batch - loss: 5.71857 - diff: 7.67mlTrain batch 9/32 - 103.2ms/batch - loss: 5.49363 - diff: 7.48mlTrain batch 10/32 - 88.1ms/batch - loss: 5.43740 - diff: 7.45mlTrain batch 11/32 - 88.2ms/batch - loss: 5.36190 - diff: 7.35mlTrain batch 12/32 - 92.0ms/batch - loss: 5.10156 - diff: 7.17mlTrain batch 13/32 - 97.4ms/batch - loss: 4.93440 - diff: 7.03mlTrain batch 14/32 - 107.7ms/batch - loss: 4.96496 - diff: 6.95mlTrain batch 15/32 - 107.7ms/batch - loss: 4.96417 - diff: 6.95mlTrain batch 16/32 - 105.4ms/batch - loss: 4.98554 - diff: 7.00mlTrain batch 17/32 - 103.6ms/batch - loss: 4.89853 - diff: 6.95mlTrain batch 18/32 - 104.2ms/batch - loss: 4.95873 - diff: 7.02mlTrain batch 19/32 - 106.0ms/batch - loss: 4.82477 - diff: 6.90mlTrain batch 20/32 - 103.2ms/batch - loss: 4.76101 - diff: 6.85mlTrain batch 21/32 - 105.9ms/batch - loss: 4.96107 - diff: 6.94mlTrain batch 22/32 - 104.2ms/batch - loss: 4.90409 - diff: 6.90mlTrain batch 23/32 - 105.5ms/batch - loss: 5.30783 - diff: 7.14mlTrain batch 24/32 - 103.3ms/batch - loss: 5.36111 - diff: 7.18mlTrain batch 25/32 - 105.0ms/batch - loss: 5.61010 - diff: 7.29mlTrain batch 26/32 - 102.7ms/batch - loss: 5.91323 - diff: 7.47mlTrain batch 27/32 - 104.3ms/batch - loss: 5.76477 - diff: 7.36mlTrain batch 28/32 - 103.7ms/batch - loss: 5.85868 - diff: 7.45mlTrain batch 29/32 - 105.3ms/batch - loss: 6.24041 - diff: 7.61mlTrain batch 30/32 - 102.8ms/batch - loss: 6.19494 - diff: 7.58mlTrain batch 31/32 - 104.8ms/batch - loss: 6.27100 - diff: 7.58mlTrain batch 32/32 - 78.2ms/batch - loss: 8.17054 - diff: 7.71mlTrain batch 32/32 - 11.0s 78.2ms/batch - loss: 8.17054 - diff: 7.71ml
Test 0.7s: val_loss: 34.15470 - diff: 15.56ml

Epoch 149: current best loss = 26.71204, at epoch 75
Train batch 1/32 - 119.9ms/batch - loss: 2.67436 - diff: 5.59mlTrain batch 2/32 - 104.9ms/batch - loss: 6.01133 - diff: 8.27mlTrain batch 3/32 - 105.1ms/batch - loss: 5.37598 - diff: 7.65mlTrain batch 4/32 - 104.2ms/batch - loss: 5.47795 - diff: 7.47mlTrain batch 5/32 - 111.4ms/batch - loss: 5.54200 - diff: 7.62mlTrain batch 6/32 - 103.6ms/batch - loss: 5.46086 - diff: 7.59mlTrain batch 7/32 - 107.0ms/batch - loss: 5.57485 - diff: 7.64mlTrain batch 8/32 - 103.4ms/batch - loss: 5.21400 - diff: 7.36mlTrain batch 9/32 - 110.7ms/batch - loss: 5.04733 - diff: 7.18mlTrain batch 10/32 - 105.1ms/batch - loss: 5.12688 - diff: 7.20mlTrain batch 11/32 - 111.8ms/batch - loss: 4.91327 - diff: 7.04mlTrain batch 12/32 - 107.9ms/batch - loss: 6.79155 - diff: 7.96mlTrain batch 13/32 - 112.3ms/batch - loss: 6.75828 - diff: 8.01mlTrain batch 14/32 - 112.4ms/batch - loss: 6.73090 - diff: 8.04mlTrain batch 15/32 - 104.3ms/batch - loss: 6.76851 - diff: 8.09mlTrain batch 16/32 - 103.7ms/batch - loss: 6.48846 - diff: 7.88mlTrain batch 17/32 - 105.3ms/batch - loss: 6.55458 - diff: 7.92mlTrain batch 18/32 - 103.7ms/batch - loss: 6.52191 - diff: 7.95mlTrain batch 19/32 - 104.4ms/batch - loss: 6.81321 - diff: 8.15mlTrain batch 20/32 - 104.5ms/batch - loss: 6.90087 - diff: 8.09mlTrain batch 21/32 - 105.0ms/batch - loss: 6.87825 - diff: 8.07mlTrain batch 22/32 - 103.7ms/batch - loss: 7.08849 - diff: 8.23mlTrain batch 23/32 - 104.8ms/batch - loss: 6.91913 - diff: 8.12mlTrain batch 24/32 - 104.0ms/batch - loss: 6.69642 - diff: 7.94mlTrain batch 25/32 - 103.7ms/batch - loss: 6.85721 - diff: 8.04mlTrain batch 26/32 - 103.9ms/batch - loss: 6.79829 - diff: 7.97mlTrain batch 27/32 - 114.4ms/batch - loss: 6.94228 - diff: 8.13mlTrain batch 28/32 - 113.6ms/batch - loss: 6.86666 - diff: 8.11mlTrain batch 29/32 - 103.2ms/batch - loss: 7.11166 - diff: 8.29mlTrain batch 30/32 - 88.0ms/batch - loss: 7.00111 - diff: 8.21mlTrain batch 31/32 - 108.4ms/batch - loss: 6.89434 - diff: 8.13mlTrain batch 32/32 - 102.6ms/batch - loss: 7.76612 - diff: 8.22mlTrain batch 32/32 - 10.9s 102.6ms/batch - loss: 7.76612 - diff: 8.22ml
Test 0.7s: val_loss: 34.08545 - diff: 15.58ml

Traceback (most recent call last):
  File "train.py", line 266, in <module>
    plot_results(train_losses, test_losses, title=f"Loss {target_label}", save_as=exp_name + "_loss")
  File "/home/allochi/tfm/workspace/lib/utils.py", line 393, in plot_results
    plt.plot(train_values, "r", label="train")
  File "/home/allochi/.conda/envs/DL_env/lib/python3.8/site-packages/matplotlib/pyplot.py", line 2824, in plot
    return gca().plot(
  File "/home/allochi/.conda/envs/DL_env/lib/python3.8/site-packages/matplotlib/pyplot.py", line 2352, in gca
    return gcf().gca(**kwargs)
  File "/home/allochi/.conda/envs/DL_env/lib/python3.8/site-packages/matplotlib/pyplot.py", line 731, in gcf
    return figure()
  File "/home/allochi/.conda/envs/DL_env/lib/python3.8/site-packages/matplotlib/pyplot.py", line 671, in figure
    figManager = new_figure_manager(num, figsize=figsize,
  File "/home/allochi/.conda/envs/DL_env/lib/python3.8/site-packages/matplotlib/pyplot.py", line 299, in new_figure_manager
    return _backend_mod.new_figure_manager(*args, **kwargs)
  File "/home/allochi/.conda/envs/DL_env/lib/python3.8/site-packages/matplotlib/backend_bases.py", line 3494, in new_figure_manager
    return cls.new_figure_manager_given_figure(num, fig)
  File "/home/allochi/.conda/envs/DL_env/lib/python3.8/site-packages/matplotlib/backends/_backend_tk.py", line 868, in new_figure_manager_given_figure
    window = tk.Tk(className="matplotlib")
  File "/home/allochi/.conda/envs/DL_env/lib/python3.8/tkinter/__init__.py", line 2261, in __init__
    self.tk = _tkinter.create(screenName, baseName, className, interactive, wantobjects, useTk, sync, use)
_tkinter.TclError: couldn't connect to display "localhost:10.0"
