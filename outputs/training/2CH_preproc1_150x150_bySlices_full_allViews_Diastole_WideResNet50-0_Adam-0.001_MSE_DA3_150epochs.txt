nohup: ignoring input
Running experiment 2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3
Going to train with the GPU in the slot 1 -> device model: GeForce RTX 2080
Model architecture:
 WideResNet50_0(
  (first_conv): Conv2d(30, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (pretrained_block): Sequential(
    (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (5): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (reduction_block): Sequential(
    (0): AdaptiveAvgPool2d(output_size=(1, 1))
    (1): Flatten()
  )
  (dense_block): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.4, inplace=False)
    (3): Linear(in_features=512, out_features=1, bias=True)
  )
) 


############################
# TRAIN PHASE:  150 epochs #
############################

Epoch 0: 
Train batch 1/32 - 211.7ms/batch - loss: 2081.89233 - diff: 174.80mlTrain batch 2/32 - 185.0ms/batch - loss: 1964.41101 - diff: 165.97mlTrain batch 3/32 - 172.0ms/batch - loss: 1827.90507 - diff: 160.63mlTrain batch 4/32 - 168.2ms/batch - loss: 1822.14453 - diff: 160.60mlTrain batch 5/32 - 168.3ms/batch - loss: 1761.62036 - diff: 158.55mlTrain batch 6/32 - 168.5ms/batch - loss: 1835.60901 - diff: 162.18mlTrain batch 7/32 - 168.2ms/batch - loss: 1902.52431 - diff: 165.50mlTrain batch 8/32 - 168.3ms/batch - loss: 1900.31126 - diff: 165.49mlTrain batch 9/32 - 168.3ms/batch - loss: 1876.24215 - diff: 164.39mlTrain batch 10/32 - 168.2ms/batch - loss: 1844.59684 - diff: 162.59mlTrain batch 11/32 - 168.5ms/batch - loss: 1830.64270 - diff: 161.91mlTrain batch 12/32 - 168.5ms/batch - loss: 1793.82592 - diff: 160.18mlTrain batch 13/32 - 168.5ms/batch - loss: 1795.58166 - diff: 160.10mlTrain batch 14/32 - 168.5ms/batch - loss: 1794.95223 - diff: 160.54mlTrain batch 15/32 - 168.4ms/batch - loss: 1796.25946 - diff: 160.35mlTrain batch 16/32 - 168.3ms/batch - loss: 1792.32220 - diff: 160.16mlTrain batch 17/32 - 168.5ms/batch - loss: 1790.88915 - diff: 159.92mlTrain batch 18/32 - 168.4ms/batch - loss: 1807.22506 - diff: 160.87mlTrain batch 19/32 - 168.6ms/batch - loss: 1821.75140 - diff: 161.16mlTrain batch 20/32 - 168.4ms/batch - loss: 1837.55699 - diff: 161.57mlTrain batch 21/32 - 168.5ms/batch - loss: 1841.06391 - diff: 161.96mlTrain batch 22/32 - 168.0ms/batch - loss: 1884.06718 - diff: 162.92mlTrain batch 23/32 - 168.4ms/batch - loss: 1905.88279 - diff: 163.92mlTrain batch 24/32 - 168.6ms/batch - loss: 1885.71263 - diff: 162.94mlTrain batch 25/32 - 168.6ms/batch - loss: 1894.71368 - diff: 163.42mlTrain batch 26/32 - 168.6ms/batch - loss: 1863.44478 - diff: 162.00mlTrain batch 27/32 - 168.5ms/batch - loss: 1847.09423 - diff: 160.92mlTrain batch 28/32 - 168.7ms/batch - loss: 1844.13812 - diff: 160.84mlTrain batch 29/32 - 168.6ms/batch - loss: 1831.02710 - diff: 160.39mlTrain batch 30/32 - 168.5ms/batch - loss: 1821.16006 - diff: 159.92mlTrain batch 31/32 - 168.6ms/batch - loss: 1803.02435 - diff: 159.05mlTrain batch 32/32 - 51.7ms/batch - loss: 1854.67668 - diff: 159.20mlTrain batch 32/32 - 13.4s 51.7ms/batch - loss: 1854.67668 - diff: 159.20ml
Test 1.0s: val_loss: 1535.04261 - diff: 140.95ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 1: current best loss = 1535.04261, at epoch 0
Train batch 1/32 - 168.6ms/batch - loss: 1860.81946 - diff: 161.67mlTrain batch 2/32 - 168.7ms/batch - loss: 1417.20062 - diff: 138.36mlTrain batch 3/32 - 168.4ms/batch - loss: 1432.97721 - diff: 140.50mlTrain batch 4/32 - 169.0ms/batch - loss: 1606.85480 - diff: 148.21mlTrain batch 5/32 - 168.5ms/batch - loss: 1571.72637 - diff: 147.05mlTrain batch 6/32 - 167.7ms/batch - loss: 1513.56079 - diff: 144.81mlTrain batch 7/32 - 168.7ms/batch - loss: 1505.98537 - diff: 145.25mlTrain batch 8/32 - 168.7ms/batch - loss: 1457.46254 - diff: 142.89mlTrain batch 9/32 - 168.6ms/batch - loss: 1415.59557 - diff: 140.33mlTrain batch 10/32 - 168.8ms/batch - loss: 1447.73601 - diff: 141.89mlTrain batch 11/32 - 168.7ms/batch - loss: 1429.93575 - diff: 140.56mlTrain batch 12/32 - 168.8ms/batch - loss: 1419.85523 - diff: 139.85mlTrain batch 13/32 - 168.5ms/batch - loss: 1381.03723 - diff: 138.15mlTrain batch 14/32 - 168.8ms/batch - loss: 1364.02198 - diff: 136.63mlTrain batch 15/32 - 168.7ms/batch - loss: 1340.42979 - diff: 135.40mlTrain batch 16/32 - 168.7ms/batch - loss: 1337.81493 - diff: 134.94mlTrain batch 17/32 - 168.5ms/batch - loss: 1359.92659 - diff: 135.91mlTrain batch 18/32 - 168.9ms/batch - loss: 1339.47361 - diff: 134.33mlTrain batch 19/32 - 169.0ms/batch - loss: 1315.21493 - diff: 133.20mlTrain batch 20/32 - 169.1ms/batch - loss: 1300.96836 - diff: 132.44mlTrain batch 21/32 - 169.0ms/batch - loss: 1284.65485 - diff: 131.57mlTrain batch 22/32 - 169.0ms/batch - loss: 1260.26487 - diff: 130.20mlTrain batch 23/32 - 169.0ms/batch - loss: 1282.96054 - diff: 130.12mlTrain batch 24/32 - 168.7ms/batch - loss: 1267.67499 - diff: 129.01mlTrain batch 25/32 - 168.9ms/batch - loss: 1260.30733 - diff: 128.30mlTrain batch 26/32 - 169.0ms/batch - loss: 1252.54258 - diff: 127.89mlTrain batch 27/32 - 168.9ms/batch - loss: 1246.54208 - diff: 127.71mlTrain batch 28/32 - 169.1ms/batch - loss: 1229.93422 - diff: 126.76mlTrain batch 29/32 - 168.9ms/batch - loss: 1215.37871 - diff: 125.72mlTrain batch 30/32 - 169.2ms/batch - loss: 1203.73737 - diff: 125.20mlTrain batch 31/32 - 169.0ms/batch - loss: 1188.82760 - diff: 124.26mlTrain batch 32/32 - 51.7ms/batch - loss: 1203.83710 - diff: 124.10mlTrain batch 32/32 - 10.4s 51.7ms/batch - loss: 1203.83710 - diff: 124.10ml
Test 1.1s: val_loss: 663.29560 - diff: 88.58ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 2: current best loss = 663.29560, at epoch 1
Train batch 1/32 - 168.9ms/batch - loss: 929.06592 - diff: 106.83mlTrain batch 2/32 - 169.1ms/batch - loss: 803.70520 - diff: 99.13mlTrain batch 3/32 - 168.8ms/batch - loss: 860.39197 - diff: 103.60mlTrain batch 4/32 - 169.2ms/batch - loss: 828.67188 - diff: 101.02mlTrain batch 5/32 - 169.0ms/batch - loss: 770.56860 - diff: 97.56mlTrain batch 6/32 - 169.1ms/batch - loss: 760.94299 - diff: 96.55mlTrain batch 7/32 - 168.9ms/batch - loss: 722.56130 - diff: 94.04mlTrain batch 8/32 - 168.9ms/batch - loss: 712.55040 - diff: 92.93mlTrain batch 9/32 - 169.2ms/batch - loss: 712.24778 - diff: 90.43mlTrain batch 10/32 - 169.2ms/batch - loss: 675.89916 - diff: 87.34mlTrain batch 11/32 - 169.1ms/batch - loss: 648.97718 - diff: 85.28mlTrain batch 12/32 - 169.3ms/batch - loss: 623.99499 - diff: 83.70mlTrain batch 13/32 - 169.2ms/batch - loss: 597.65462 - diff: 81.42mlTrain batch 14/32 - 169.3ms/batch - loss: 595.38366 - diff: 81.23mlTrain batch 15/32 - 169.0ms/batch - loss: 572.27684 - diff: 79.12mlTrain batch 16/32 - 169.1ms/batch - loss: 556.93217 - diff: 77.77mlTrain batch 17/32 - 169.0ms/batch - loss: 549.11132 - diff: 76.55mlTrain batch 18/32 - 169.1ms/batch - loss: 534.50359 - diff: 75.48mlTrain batch 19/32 - 169.1ms/batch - loss: 517.41036 - diff: 74.02mlTrain batch 20/32 - 169.2ms/batch - loss: 545.03286 - diff: 74.53mlTrain batch 21/32 - 169.0ms/batch - loss: 525.62110 - diff: 72.64mlTrain batch 22/32 - 169.3ms/batch - loss: 509.83692 - diff: 71.07mlTrain batch 23/32 - 169.4ms/batch - loss: 506.19206 - diff: 70.63mlTrain batch 24/32 - 169.6ms/batch - loss: 493.41203 - diff: 69.85mlTrain batch 25/32 - 169.0ms/batch - loss: 481.03067 - diff: 68.82mlTrain batch 26/32 - 169.2ms/batch - loss: 470.08957 - diff: 67.92mlTrain batch 27/32 - 169.2ms/batch - loss: 463.81681 - diff: 67.19mlTrain batch 28/32 - 168.7ms/batch - loss: 456.62399 - diff: 66.22mlTrain batch 29/32 - 169.0ms/batch - loss: 448.18673 - diff: 65.56mlTrain batch 30/32 - 168.6ms/batch - loss: 440.34010 - diff: 64.97mlTrain batch 31/32 - 169.2ms/batch - loss: 437.29069 - diff: 64.45mlTrain batch 32/32 - 51.7ms/batch - loss: 436.03660 - diff: 64.17mlTrain batch 32/32 - 10.4s 51.7ms/batch - loss: 436.03660 - diff: 64.17ml
Test 1.0s: val_loss: 212.33176 - diff: 43.35ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 3: current best loss = 212.33176, at epoch 2
Train batch 1/32 - 169.1ms/batch - loss: 211.54666 - diff: 45.79mlTrain batch 2/32 - 169.1ms/batch - loss: 252.40315 - diff: 49.66mlTrain batch 3/32 - 169.2ms/batch - loss: 252.96859 - diff: 47.50mlTrain batch 4/32 - 169.3ms/batch - loss: 210.14128 - diff: 43.77mlTrain batch 5/32 - 169.2ms/batch - loss: 326.08173 - diff: 48.31mlTrain batch 6/32 - 169.4ms/batch - loss: 315.17089 - diff: 48.35mlTrain batch 7/32 - 169.6ms/batch - loss: 307.56150 - diff: 48.74mlTrain batch 8/32 - 169.6ms/batch - loss: 295.14224 - diff: 47.18mlTrain batch 9/32 - 169.5ms/batch - loss: 271.71449 - diff: 45.12mlTrain batch 10/32 - 169.6ms/batch - loss: 260.71269 - diff: 44.56mlTrain batch 11/32 - 169.6ms/batch - loss: 266.50645 - diff: 45.14mlTrain batch 12/32 - 169.6ms/batch - loss: 258.22664 - diff: 44.61mlTrain batch 13/32 - 169.4ms/batch - loss: 248.92901 - diff: 43.83mlTrain batch 14/32 - 169.2ms/batch - loss: 239.65074 - diff: 43.28mlTrain batch 15/32 - 169.5ms/batch - loss: 235.13294 - diff: 42.96mlTrain batch 16/32 - 169.5ms/batch - loss: 233.34529 - diff: 43.38mlTrain batch 17/32 - 169.6ms/batch - loss: 232.75710 - diff: 43.41mlTrain batch 18/32 - 169.9ms/batch - loss: 228.58970 - diff: 43.35mlTrain batch 19/32 - 169.5ms/batch - loss: 224.54390 - diff: 42.92mlTrain batch 20/32 - 169.7ms/batch - loss: 222.00853 - diff: 42.66mlTrain batch 21/32 - 169.6ms/batch - loss: 218.09187 - diff: 42.60mlTrain batch 22/32 - 169.6ms/batch - loss: 223.02441 - diff: 42.88mlTrain batch 23/32 - 169.6ms/batch - loss: 222.36448 - diff: 43.32mlTrain batch 24/32 - 169.9ms/batch - loss: 224.22241 - diff: 43.51mlTrain batch 25/32 - 169.5ms/batch - loss: 219.35419 - diff: 43.13mlTrain batch 26/32 - 169.8ms/batch - loss: 218.55145 - diff: 43.23mlTrain batch 27/32 - 169.5ms/batch - loss: 221.00236 - diff: 43.66mlTrain batch 28/32 - 168.9ms/batch - loss: 224.16772 - diff: 44.14mlTrain batch 29/32 - 169.6ms/batch - loss: 221.17030 - diff: 44.02mlTrain batch 30/32 - 169.0ms/batch - loss: 218.81264 - diff: 43.75mlTrain batch 31/32 - 169.5ms/batch - loss: 218.74291 - diff: 43.90mlTrain batch 32/32 - 52.0ms/batch - loss: 222.38107 - diff: 43.95mlTrain batch 32/32 - 10.5s 52.0ms/batch - loss: 222.38107 - diff: 43.95ml
Test 1.0s: val_loss: 214.22459 - diff: 44.60ml

Epoch 4: current best loss = 212.33176, at epoch 2
Train batch 1/32 - 169.5ms/batch - loss: 108.40586 - diff: 34.31mlTrain batch 2/32 - 169.4ms/batch - loss: 187.61458 - diff: 41.08mlTrain batch 3/32 - 169.7ms/batch - loss: 205.56763 - diff: 44.15mlTrain batch 4/32 - 169.8ms/batch - loss: 185.16084 - diff: 40.92mlTrain batch 5/32 - 169.8ms/batch - loss: 180.70827 - diff: 40.29mlTrain batch 6/32 - 169.8ms/batch - loss: 186.80343 - diff: 41.37mlTrain batch 7/32 - 169.7ms/batch - loss: 177.46032 - diff: 40.44mlTrain batch 8/32 - 169.7ms/batch - loss: 191.87771 - diff: 41.43mlTrain batch 9/32 - 169.8ms/batch - loss: 203.37152 - diff: 42.05mlTrain batch 10/32 - 169.7ms/batch - loss: 196.51325 - diff: 41.30mlTrain batch 11/32 - 169.8ms/batch - loss: 198.70361 - diff: 41.74mlTrain batch 12/32 - 169.9ms/batch - loss: 198.93306 - diff: 41.95mlTrain batch 13/32 - 169.7ms/batch - loss: 188.87850 - diff: 40.90mlTrain batch 14/32 - 169.9ms/batch - loss: 186.15624 - diff: 40.65mlTrain batch 15/32 - 169.6ms/batch - loss: 195.81386 - diff: 41.90mlTrain batch 16/32 - 169.7ms/batch - loss: 195.33205 - diff: 41.83mlTrain batch 17/32 - 169.7ms/batch - loss: 193.20449 - diff: 41.76mlTrain batch 18/32 - 169.9ms/batch - loss: 193.17160 - diff: 41.66mlTrain batch 19/32 - 169.7ms/batch - loss: 191.00029 - diff: 41.21mlTrain batch 20/32 - 169.9ms/batch - loss: 185.86932 - diff: 40.83mlTrain batch 21/32 - 169.7ms/batch - loss: 181.27278 - diff: 40.18mlTrain batch 22/32 - 170.3ms/batch - loss: 178.40806 - diff: 39.99mlTrain batch 23/32 - 169.7ms/batch - loss: 175.78783 - diff: 39.70mlTrain batch 24/32 - 169.7ms/batch - loss: 183.83545 - diff: 40.43mlTrain batch 25/32 - 169.8ms/batch - loss: 187.72791 - diff: 40.96mlTrain batch 26/32 - 168.8ms/batch - loss: 191.66383 - diff: 41.49mlTrain batch 27/32 - 169.7ms/batch - loss: 189.72906 - diff: 41.25mlTrain batch 28/32 - 169.0ms/batch - loss: 188.52023 - diff: 41.32mlTrain batch 29/32 - 169.7ms/batch - loss: 186.83504 - diff: 41.26mlTrain batch 30/32 - 169.5ms/batch - loss: 188.04237 - diff: 41.53mlTrain batch 31/32 - 170.0ms/batch - loss: 213.93803 - diff: 42.80mlTrain batch 32/32 - 52.3ms/batch - loss: 218.08747 - diff: 42.84mlTrain batch 32/32 - 10.5s 52.3ms/batch - loss: 218.08747 - diff: 42.84ml
Test 1.1s: val_loss: 201.64995 - diff: 43.51ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 5: current best loss = 201.64995, at epoch 4
Train batch 1/32 - 169.7ms/batch - loss: 428.66531 - diff: 58.69mlTrain batch 2/32 - 169.1ms/batch - loss: 315.79396 - diff: 53.56mlTrain batch 3/32 - 169.8ms/batch - loss: 261.32364 - diff: 46.99mlTrain batch 4/32 - 169.7ms/batch - loss: 238.64416 - diff: 45.22mlTrain batch 5/32 - 169.7ms/batch - loss: 255.27381 - diff: 47.19mlTrain batch 6/32 - 170.0ms/batch - loss: 262.47412 - diff: 48.29mlTrain batch 7/32 - 169.9ms/batch - loss: 251.72337 - diff: 48.24mlTrain batch 8/32 - 170.0ms/batch - loss: 246.90413 - diff: 47.18mlTrain batch 9/32 - 169.8ms/batch - loss: 237.01835 - diff: 46.15mlTrain batch 10/32 - 170.0ms/batch - loss: 225.60271 - diff: 45.37mlTrain batch 11/32 - 170.0ms/batch - loss: 218.97043 - diff: 45.07mlTrain batch 12/32 - 170.1ms/batch - loss: 211.18290 - diff: 44.64mlTrain batch 13/32 - 169.7ms/batch - loss: 203.22805 - diff: 44.03mlTrain batch 14/32 - 169.7ms/batch - loss: 204.32132 - diff: 43.63mlTrain batch 15/32 - 169.8ms/batch - loss: 201.51141 - diff: 43.53mlTrain batch 16/32 - 169.2ms/batch - loss: 242.50978 - diff: 45.18mlTrain batch 17/32 - 169.8ms/batch - loss: 236.73191 - diff: 44.69mlTrain batch 18/32 - 170.0ms/batch - loss: 240.91033 - diff: 45.28mlTrain batch 19/32 - 169.9ms/batch - loss: 237.49583 - diff: 45.22mlTrain batch 20/32 - 170.5ms/batch - loss: 232.00396 - diff: 44.71mlTrain batch 21/32 - 170.1ms/batch - loss: 232.06550 - diff: 44.52mlTrain batch 22/32 - 170.2ms/batch - loss: 223.71792 - diff: 43.55mlTrain batch 23/32 - 170.1ms/batch - loss: 220.52462 - diff: 43.34mlTrain batch 24/32 - 170.7ms/batch - loss: 216.55497 - diff: 43.13mlTrain batch 25/32 - 170.2ms/batch - loss: 214.01825 - diff: 42.86mlTrain batch 26/32 - 170.5ms/batch - loss: 211.30929 - diff: 42.63mlTrain batch 27/32 - 170.1ms/batch - loss: 214.68753 - diff: 42.89mlTrain batch 28/32 - 170.8ms/batch - loss: 212.56067 - diff: 42.77mlTrain batch 29/32 - 170.2ms/batch - loss: 214.48724 - diff: 43.00mlTrain batch 30/32 - 170.0ms/batch - loss: 211.99421 - diff: 42.84mlTrain batch 31/32 - 170.2ms/batch - loss: 209.22976 - diff: 42.65mlTrain batch 32/32 - 52.4ms/batch - loss: 212.36163 - diff: 42.68mlTrain batch 32/32 - 10.5s 52.4ms/batch - loss: 212.36163 - diff: 42.68ml
Test 1.1s: val_loss: 191.63348 - diff: 41.98ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 6: current best loss = 191.63348, at epoch 5
Train batch 1/32 - 170.1ms/batch - loss: 210.12805 - diff: 42.06mlTrain batch 2/32 - 169.7ms/batch - loss: 195.98354 - diff: 40.58mlTrain batch 3/32 - 170.2ms/batch - loss: 171.97218 - diff: 39.60mlTrain batch 4/32 - 169.9ms/batch - loss: 306.32426 - diff: 44.27mlTrain batch 5/32 - 170.1ms/batch - loss: 255.45103 - diff: 40.33mlTrain batch 6/32 - 170.2ms/batch - loss: 232.81289 - diff: 39.10mlTrain batch 7/32 - 170.1ms/batch - loss: 219.69382 - diff: 39.01mlTrain batch 8/32 - 170.5ms/batch - loss: 210.88974 - diff: 38.56mlTrain batch 9/32 - 170.3ms/batch - loss: 202.15524 - diff: 38.37mlTrain batch 10/32 - 170.4ms/batch - loss: 188.84223 - diff: 37.46mlTrain batch 11/32 - 170.1ms/batch - loss: 210.71599 - diff: 39.81mlTrain batch 12/32 - 170.3ms/batch - loss: 209.43524 - diff: 40.48mlTrain batch 13/32 - 170.1ms/batch - loss: 207.60291 - diff: 40.83mlTrain batch 14/32 - 170.4ms/batch - loss: 208.02855 - diff: 41.00mlTrain batch 15/32 - 170.2ms/batch - loss: 204.13226 - diff: 40.98mlTrain batch 16/32 - 170.3ms/batch - loss: 221.61515 - diff: 42.27mlTrain batch 17/32 - 170.2ms/batch - loss: 217.72738 - diff: 42.20mlTrain batch 18/32 - 170.5ms/batch - loss: 209.36871 - diff: 41.40mlTrain batch 19/32 - 170.4ms/batch - loss: 209.14684 - diff: 41.60mlTrain batch 20/32 - 170.4ms/batch - loss: 212.77050 - diff: 41.88mlTrain batch 21/32 - 170.2ms/batch - loss: 212.90209 - diff: 42.11mlTrain batch 22/32 - 170.4ms/batch - loss: 210.31029 - diff: 41.86mlTrain batch 23/32 - 170.1ms/batch - loss: 206.28060 - diff: 41.74mlTrain batch 24/32 - 170.6ms/batch - loss: 202.74172 - diff: 41.42mlTrain batch 25/32 - 170.4ms/batch - loss: 206.48344 - diff: 41.88mlTrain batch 26/32 - 170.6ms/batch - loss: 204.42840 - diff: 41.61mlTrain batch 27/32 - 170.1ms/batch - loss: 207.16499 - diff: 42.16mlTrain batch 28/32 - 170.5ms/batch - loss: 208.40103 - diff: 42.37mlTrain batch 29/32 - 170.2ms/batch - loss: 206.78856 - diff: 42.33mlTrain batch 30/32 - 170.5ms/batch - loss: 208.39435 - diff: 42.57mlTrain batch 31/32 - 170.2ms/batch - loss: 207.49234 - diff: 42.43mlTrain batch 32/32 - 52.5ms/batch - loss: 210.40060 - diff: 42.44mlTrain batch 32/32 - 10.4s 52.5ms/batch - loss: 210.40060 - diff: 42.44ml
Test 1.1s: val_loss: 184.58173 - diff: 42.04ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 7: current best loss = 184.58173, at epoch 6
Train batch 1/32 - 170.4ms/batch - loss: 168.14984 - diff: 40.31mlTrain batch 2/32 - 170.0ms/batch - loss: 159.81164 - diff: 40.32mlTrain batch 3/32 - 170.2ms/batch - loss: 179.48889 - diff: 42.06mlTrain batch 4/32 - 170.3ms/batch - loss: 160.67062 - diff: 40.00mlTrain batch 5/32 - 170.2ms/batch - loss: 195.17673 - diff: 43.22mlTrain batch 6/32 - 169.9ms/batch - loss: 190.06853 - diff: 43.07mlTrain batch 7/32 - 170.3ms/batch - loss: 193.59898 - diff: 43.66mlTrain batch 8/32 - 170.0ms/batch - loss: 187.62831 - diff: 43.12mlTrain batch 9/32 - 170.4ms/batch - loss: 177.88457 - diff: 41.87mlTrain batch 10/32 - 170.3ms/batch - loss: 170.06391 - diff: 41.20mlTrain batch 11/32 - 170.3ms/batch - loss: 169.92879 - diff: 41.21mlTrain batch 12/32 - 170.9ms/batch - loss: 176.69560 - diff: 42.14mlTrain batch 13/32 - 170.2ms/batch - loss: 169.69294 - diff: 41.27mlTrain batch 14/32 - 170.4ms/batch - loss: 179.36093 - diff: 42.33mlTrain batch 15/32 - 170.3ms/batch - loss: 175.21721 - diff: 41.83mlTrain batch 16/32 - 170.3ms/batch - loss: 178.54385 - diff: 41.97mlTrain batch 17/32 - 170.6ms/batch - loss: 176.76885 - diff: 41.67mlTrain batch 18/32 - 170.8ms/batch - loss: 178.32668 - diff: 41.92mlTrain batch 19/32 - 170.3ms/batch - loss: 180.59306 - diff: 41.88mlTrain batch 20/32 - 170.9ms/batch - loss: 179.33532 - diff: 41.78mlTrain batch 21/32 - 170.3ms/batch - loss: 176.43947 - diff: 41.19mlTrain batch 22/32 - 170.1ms/batch - loss: 181.32151 - diff: 41.89mlTrain batch 23/32 - 170.2ms/batch - loss: 179.38070 - diff: 41.61mlTrain batch 24/32 - 169.7ms/batch - loss: 182.45080 - diff: 41.95mlTrain batch 25/32 - 170.4ms/batch - loss: 203.54600 - diff: 42.81mlTrain batch 26/32 - 170.4ms/batch - loss: 205.70118 - diff: 43.28mlTrain batch 27/32 - 170.4ms/batch - loss: 202.57902 - diff: 42.99mlTrain batch 28/32 - 170.8ms/batch - loss: 200.06331 - diff: 42.85mlTrain batch 29/32 - 170.3ms/batch - loss: 201.83961 - diff: 43.05mlTrain batch 30/32 - 170.6ms/batch - loss: 209.62759 - diff: 43.51mlTrain batch 31/32 - 170.4ms/batch - loss: 207.60216 - diff: 43.21mlTrain batch 32/32 - 52.5ms/batch - loss: 206.32920 - diff: 42.97mlTrain batch 32/32 - 10.4s 52.5ms/batch - loss: 206.32920 - diff: 42.97ml
Test 1.1s: val_loss: 194.33217 - diff: 42.63ml

Epoch 8: current best loss = 184.58173, at epoch 6
Train batch 1/32 - 170.4ms/batch - loss: 262.74719 - diff: 52.05mlTrain batch 2/32 - 170.8ms/batch - loss: 240.31917 - diff: 47.32mlTrain batch 3/32 - 170.5ms/batch - loss: 214.32143 - diff: 45.05mlTrain batch 4/32 - 171.0ms/batch - loss: 231.31279 - diff: 46.84mlTrain batch 5/32 - 170.3ms/batch - loss: 220.20608 - diff: 46.38mlTrain batch 6/32 - 170.3ms/batch - loss: 230.71654 - diff: 47.94mlTrain batch 7/32 - 170.6ms/batch - loss: 227.39377 - diff: 46.79mlTrain batch 8/32 - 170.6ms/batch - loss: 242.36023 - diff: 48.26mlTrain batch 9/32 - 170.3ms/batch - loss: 295.83169 - diff: 49.50mlTrain batch 10/32 - 170.6ms/batch - loss: 275.55996 - diff: 47.68mlTrain batch 11/32 - 170.3ms/batch - loss: 267.62857 - diff: 46.75mlTrain batch 12/32 - 170.7ms/batch - loss: 262.31369 - diff: 46.57mlTrain batch 13/32 - 170.4ms/batch - loss: 244.84577 - diff: 44.48mlTrain batch 14/32 - 170.7ms/batch - loss: 237.88897 - diff: 44.20mlTrain batch 15/32 - 170.4ms/batch - loss: 230.28863 - diff: 43.87mlTrain batch 16/32 - 170.7ms/batch - loss: 237.52002 - diff: 44.13mlTrain batch 17/32 - 170.5ms/batch - loss: 230.55148 - diff: 43.66mlTrain batch 18/32 - 171.0ms/batch - loss: 223.17181 - diff: 43.10mlTrain batch 19/32 - 170.3ms/batch - loss: 223.43102 - diff: 43.33mlTrain batch 20/32 - 170.7ms/batch - loss: 230.18942 - diff: 44.11mlTrain batch 21/32 - 170.5ms/batch - loss: 229.66341 - diff: 44.13mlTrain batch 22/32 - 169.5ms/batch - loss: 230.03960 - diff: 44.24mlTrain batch 23/32 - 170.4ms/batch - loss: 225.06703 - diff: 43.84mlTrain batch 24/32 - 170.2ms/batch - loss: 223.72573 - diff: 43.95mlTrain batch 25/32 - 170.4ms/batch - loss: 222.04007 - diff: 43.82mlTrain batch 26/32 - 170.8ms/batch - loss: 216.27589 - diff: 43.26mlTrain batch 27/32 - 170.5ms/batch - loss: 211.48318 - diff: 42.86mlTrain batch 28/32 - 170.7ms/batch - loss: 213.90859 - diff: 42.94mlTrain batch 29/32 - 170.4ms/batch - loss: 210.42052 - diff: 42.47mlTrain batch 30/32 - 171.1ms/batch - loss: 206.39252 - diff: 42.11mlTrain batch 31/32 - 170.5ms/batch - loss: 204.75884 - diff: 42.10mlTrain batch 32/32 - 52.5ms/batch - loss: 205.09608 - diff: 41.98mlTrain batch 32/32 - 10.5s 52.5ms/batch - loss: 205.09608 - diff: 41.98ml
Test 1.1s: val_loss: 184.19477 - diff: 41.99ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 9: current best loss = 184.19477, at epoch 8
Train batch 1/32 - 170.5ms/batch - loss: 194.48207 - diff: 37.66mlTrain batch 2/32 - 170.7ms/batch - loss: 253.00040 - diff: 42.41mlTrain batch 3/32 - 170.3ms/batch - loss: 201.01370 - diff: 38.82mlTrain batch 4/32 - 169.9ms/batch - loss: 173.54219 - diff: 36.99mlTrain batch 5/32 - 170.4ms/batch - loss: 160.75883 - diff: 36.22mlTrain batch 6/32 - 170.0ms/batch - loss: 161.81562 - diff: 36.20mlTrain batch 7/32 - 170.4ms/batch - loss: 158.69640 - diff: 35.75mlTrain batch 8/32 - 170.8ms/batch - loss: 174.44909 - diff: 36.78mlTrain batch 9/32 - 170.5ms/batch - loss: 167.41862 - diff: 36.69mlTrain batch 10/32 - 170.8ms/batch - loss: 164.01914 - diff: 36.55mlTrain batch 11/32 - 170.3ms/batch - loss: 160.66573 - diff: 36.75mlTrain batch 12/32 - 170.5ms/batch - loss: 155.80331 - diff: 36.40mlTrain batch 13/32 - 170.4ms/batch - loss: 199.59278 - diff: 38.01mlTrain batch 14/32 - 170.9ms/batch - loss: 199.12819 - diff: 38.39mlTrain batch 15/32 - 170.5ms/batch - loss: 203.85647 - diff: 39.50mlTrain batch 16/32 - 170.9ms/batch - loss: 202.18543 - diff: 39.82mlTrain batch 17/32 - 170.7ms/batch - loss: 204.84694 - diff: 40.33mlTrain batch 18/32 - 170.3ms/batch - loss: 205.23509 - diff: 40.89mlTrain batch 19/32 - 170.7ms/batch - loss: 198.59114 - diff: 40.27mlTrain batch 20/32 - 171.0ms/batch - loss: 199.41666 - diff: 40.42mlTrain batch 21/32 - 170.9ms/batch - loss: 199.42045 - diff: 40.73mlTrain batch 22/32 - 170.8ms/batch - loss: 197.14405 - diff: 40.86mlTrain batch 23/32 - 170.8ms/batch - loss: 194.79009 - diff: 40.86mlTrain batch 24/32 - 170.9ms/batch - loss: 191.60170 - diff: 40.56mlTrain batch 25/32 - 170.8ms/batch - loss: 194.22799 - diff: 41.10mlTrain batch 26/32 - 170.8ms/batch - loss: 198.73211 - diff: 41.77mlTrain batch 27/32 - 170.7ms/batch - loss: 196.57179 - diff: 41.61mlTrain batch 28/32 - 171.5ms/batch - loss: 193.61737 - diff: 41.36mlTrain batch 29/32 - 170.9ms/batch - loss: 197.52866 - diff: 41.68mlTrain batch 30/32 - 171.1ms/batch - loss: 203.08184 - diff: 42.32mlTrain batch 31/32 - 171.1ms/batch - loss: 199.43216 - diff: 41.85mlTrain batch 32/32 - 52.6ms/batch - loss: 200.94190 - diff: 41.82mlTrain batch 32/32 - 10.5s 52.6ms/batch - loss: 200.94190 - diff: 41.82ml
Test 1.1s: val_loss: 188.25164 - diff: 41.96ml

Epoch 10: current best loss = 184.19477, at epoch 8
Train batch 1/32 - 170.8ms/batch - loss: 91.95354 - diff: 31.54mlTrain batch 2/32 - 171.1ms/batch - loss: 144.60252 - diff: 35.75mlTrain batch 3/32 - 171.0ms/batch - loss: 130.11405 - diff: 34.18mlTrain batch 4/32 - 171.2ms/batch - loss: 131.18696 - diff: 34.56mlTrain batch 5/32 - 170.9ms/batch - loss: 122.37866 - diff: 34.02mlTrain batch 6/32 - 171.3ms/batch - loss: 127.92013 - diff: 34.11mlTrain batch 7/32 - 170.7ms/batch - loss: 138.62205 - diff: 35.94mlTrain batch 8/32 - 171.4ms/batch - loss: 150.87855 - diff: 37.29mlTrain batch 9/32 - 170.7ms/batch - loss: 148.66550 - diff: 37.25mlTrain batch 10/32 - 171.5ms/batch - loss: 161.23715 - diff: 38.43mlTrain batch 11/32 - 170.7ms/batch - loss: 171.42736 - diff: 39.33mlTrain batch 12/32 - 170.7ms/batch - loss: 169.59252 - diff: 39.49mlTrain batch 13/32 - 170.8ms/batch - loss: 174.77803 - diff: 39.64mlTrain batch 14/32 - 170.9ms/batch - loss: 169.46248 - diff: 39.43mlTrain batch 15/32 - 171.0ms/batch - loss: 167.39624 - diff: 39.06mlTrain batch 16/32 - 171.3ms/batch - loss: 162.57548 - diff: 38.53mlTrain batch 17/32 - 171.1ms/batch - loss: 161.40128 - diff: 38.34mlTrain batch 18/32 - 171.1ms/batch - loss: 165.99432 - diff: 39.05mlTrain batch 19/32 - 171.1ms/batch - loss: 166.45260 - diff: 39.29mlTrain batch 20/32 - 171.1ms/batch - loss: 174.86684 - diff: 40.17mlTrain batch 21/32 - 171.2ms/batch - loss: 174.87220 - diff: 40.30mlTrain batch 22/32 - 171.0ms/batch - loss: 177.50347 - diff: 40.76mlTrain batch 23/32 - 170.7ms/batch - loss: 183.66915 - diff: 41.16mlTrain batch 24/32 - 171.2ms/batch - loss: 180.57996 - diff: 40.74mlTrain batch 25/32 - 171.0ms/batch - loss: 177.22911 - diff: 40.48mlTrain batch 26/32 - 171.2ms/batch - loss: 179.45106 - diff: 40.86mlTrain batch 27/32 - 170.7ms/batch - loss: 183.39635 - diff: 41.27mlTrain batch 28/32 - 171.2ms/batch - loss: 184.81193 - diff: 41.55mlTrain batch 29/32 - 170.8ms/batch - loss: 184.93650 - diff: 41.53mlTrain batch 30/32 - 171.3ms/batch - loss: 204.88923 - diff: 42.21mlTrain batch 31/32 - 171.1ms/batch - loss: 202.26605 - diff: 42.10mlTrain batch 32/32 - 52.6ms/batch - loss: 202.55427 - diff: 41.95mlTrain batch 32/32 - 10.5s 52.6ms/batch - loss: 202.55427 - diff: 41.95ml
Test 1.1s: val_loss: 178.65438 - diff: 41.66ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 11: current best loss = 178.65438, at epoch 10
Train batch 1/32 - 171.2ms/batch - loss: 138.87701 - diff: 41.01mlTrain batch 2/32 - 170.6ms/batch - loss: 137.26685 - diff: 38.62mlTrain batch 3/32 - 171.3ms/batch - loss: 140.75055 - diff: 37.56mlTrain batch 4/32 - 170.1ms/batch - loss: 147.18260 - diff: 38.56mlTrain batch 5/32 - 171.0ms/batch - loss: 160.97465 - diff: 40.23mlTrain batch 6/32 - 171.0ms/batch - loss: 166.91850 - diff: 41.32mlTrain batch 7/32 - 171.1ms/batch - loss: 167.90164 - diff: 40.55mlTrain batch 8/32 - 171.4ms/batch - loss: 162.14315 - diff: 40.05mlTrain batch 9/32 - 171.0ms/batch - loss: 157.86594 - diff: 39.53mlTrain batch 10/32 - 171.2ms/batch - loss: 154.37684 - diff: 38.52mlTrain batch 11/32 - 170.8ms/batch - loss: 168.73317 - diff: 40.08mlTrain batch 12/32 - 171.6ms/batch - loss: 164.77838 - diff: 39.58mlTrain batch 13/32 - 171.0ms/batch - loss: 163.11659 - diff: 38.91mlTrain batch 14/32 - 171.4ms/batch - loss: 160.25465 - diff: 38.81mlTrain batch 15/32 - 171.1ms/batch - loss: 168.75620 - diff: 39.36mlTrain batch 16/32 - 171.0ms/batch - loss: 169.65395 - diff: 39.67mlTrain batch 17/32 - 170.4ms/batch - loss: 177.15651 - diff: 40.46mlTrain batch 18/32 - 170.1ms/batch - loss: 173.11968 - diff: 40.03mlTrain batch 19/32 - 171.0ms/batch - loss: 169.24514 - diff: 39.70mlTrain batch 20/32 - 170.7ms/batch - loss: 168.22816 - diff: 39.79mlTrain batch 21/32 - 170.7ms/batch - loss: 176.50035 - diff: 40.56mlTrain batch 22/32 - 171.3ms/batch - loss: 178.76885 - diff: 40.76mlTrain batch 23/32 - 170.9ms/batch - loss: 178.03742 - diff: 40.85mlTrain batch 24/32 - 171.1ms/batch - loss: 180.29531 - diff: 41.02mlTrain batch 25/32 - 170.9ms/batch - loss: 178.18488 - diff: 40.69mlTrain batch 26/32 - 171.4ms/batch - loss: 181.47285 - diff: 41.12mlTrain batch 27/32 - 171.2ms/batch - loss: 201.29123 - diff: 41.92mlTrain batch 28/32 - 171.6ms/batch - loss: 202.71535 - diff: 41.98mlTrain batch 29/32 - 171.2ms/batch - loss: 205.90613 - diff: 42.23mlTrain batch 30/32 - 171.4ms/batch - loss: 202.99483 - diff: 41.97mlTrain batch 31/32 - 171.0ms/batch - loss: 200.19239 - diff: 41.68mlTrain batch 32/32 - 52.6ms/batch - loss: 206.08306 - diff: 41.74mlTrain batch 32/32 - 10.5s 52.6ms/batch - loss: 206.08306 - diff: 41.74ml
Test 1.1s: val_loss: 197.41392 - diff: 43.38ml

Epoch 12: current best loss = 178.65438, at epoch 10
Train batch 1/32 - 171.2ms/batch - loss: 179.28198 - diff: 40.99mlTrain batch 2/32 - 171.3ms/batch - loss: 179.36479 - diff: 43.61mlTrain batch 3/32 - 171.2ms/batch - loss: 406.98845 - diff: 53.43mlTrain batch 4/32 - 171.2ms/batch - loss: 398.91180 - diff: 53.96mlTrain batch 5/32 - 171.2ms/batch - loss: 340.35511 - diff: 49.34mlTrain batch 6/32 - 171.4ms/batch - loss: 303.53098 - diff: 46.60mlTrain batch 7/32 - 170.8ms/batch - loss: 279.64979 - diff: 45.76mlTrain batch 8/32 - 171.5ms/batch - loss: 267.08654 - diff: 45.06mlTrain batch 9/32 - 171.0ms/batch - loss: 265.69288 - diff: 45.65mlTrain batch 10/32 - 171.4ms/batch - loss: 261.94615 - diff: 45.65mlTrain batch 11/32 - 171.2ms/batch - loss: 264.96869 - diff: 46.70mlTrain batch 12/32 - 170.7ms/batch - loss: 273.14138 - diff: 47.35mlTrain batch 13/32 - 171.1ms/batch - loss: 268.10159 - diff: 47.29mlTrain batch 14/32 - 171.4ms/batch - loss: 260.99885 - diff: 46.81mlTrain batch 15/32 - 171.2ms/batch - loss: 265.98113 - diff: 47.36mlTrain batch 16/32 - 171.2ms/batch - loss: 266.81264 - diff: 47.16mlTrain batch 17/32 - 171.1ms/batch - loss: 254.36632 - diff: 45.81mlTrain batch 18/32 - 171.3ms/batch - loss: 250.59755 - diff: 45.56mlTrain batch 19/32 - 171.0ms/batch - loss: 249.67112 - diff: 45.91mlTrain batch 20/32 - 171.5ms/batch - loss: 244.92607 - diff: 45.65mlTrain batch 21/32 - 170.9ms/batch - loss: 238.02542 - diff: 45.11mlTrain batch 22/32 - 171.4ms/batch - loss: 234.95945 - diff: 44.72mlTrain batch 23/32 - 171.2ms/batch - loss: 228.77731 - diff: 44.14mlTrain batch 24/32 - 171.8ms/batch - loss: 225.73357 - diff: 44.00mlTrain batch 25/32 - 171.2ms/batch - loss: 221.83669 - diff: 43.75mlTrain batch 26/32 - 171.1ms/batch - loss: 220.55641 - diff: 43.80mlTrain batch 27/32 - 171.2ms/batch - loss: 217.14403 - diff: 43.45mlTrain batch 28/32 - 171.3ms/batch - loss: 214.54741 - diff: 43.37mlTrain batch 29/32 - 170.9ms/batch - loss: 209.77355 - diff: 42.89mlTrain batch 30/32 - 170.3ms/batch - loss: 206.73142 - diff: 42.61mlTrain batch 31/32 - 171.2ms/batch - loss: 203.45559 - diff: 42.18mlTrain batch 32/32 - 52.9ms/batch - loss: 219.22411 - diff: 42.50mlTrain batch 32/32 - 10.5s 52.9ms/batch - loss: 219.22411 - diff: 42.50ml
Test 1.0s: val_loss: 187.22786 - diff: 41.79ml

Epoch 13: current best loss = 178.65438, at epoch 10
Train batch 1/32 - 170.9ms/batch - loss: 209.48149 - diff: 44.74mlTrain batch 2/32 - 171.3ms/batch - loss: 201.05725 - diff: 42.34mlTrain batch 3/32 - 171.1ms/batch - loss: 259.31416 - diff: 45.95mlTrain batch 4/32 - 171.7ms/batch - loss: 237.58017 - diff: 44.38mlTrain batch 5/32 - 171.3ms/batch - loss: 238.00139 - diff: 44.26mlTrain batch 6/32 - 171.6ms/batch - loss: 238.48017 - diff: 44.81mlTrain batch 7/32 - 171.2ms/batch - loss: 221.00923 - diff: 43.65mlTrain batch 8/32 - 171.9ms/batch - loss: 209.14044 - diff: 42.97mlTrain batch 9/32 - 171.2ms/batch - loss: 199.22782 - diff: 42.15mlTrain batch 10/32 - 171.1ms/batch - loss: 195.55224 - diff: 41.69mlTrain batch 11/32 - 171.4ms/batch - loss: 186.58423 - diff: 40.95mlTrain batch 12/32 - 171.4ms/batch - loss: 185.46899 - diff: 41.26mlTrain batch 13/32 - 171.1ms/batch - loss: 177.28143 - diff: 40.18mlTrain batch 14/32 - 171.4ms/batch - loss: 177.26300 - diff: 40.32mlTrain batch 15/32 - 170.9ms/batch - loss: 186.57679 - diff: 41.53mlTrain batch 16/32 - 171.5ms/batch - loss: 188.26940 - diff: 41.67mlTrain batch 17/32 - 171.1ms/batch - loss: 187.27625 - diff: 41.68mlTrain batch 18/32 - 171.5ms/batch - loss: 182.82842 - diff: 41.26mlTrain batch 19/32 - 171.1ms/batch - loss: 181.61625 - diff: 41.18mlTrain batch 20/32 - 171.3ms/batch - loss: 220.55730 - diff: 42.61mlTrain batch 21/32 - 170.9ms/batch - loss: 216.74283 - diff: 42.49mlTrain batch 22/32 - 171.5ms/batch - loss: 213.67045 - diff: 42.61mlTrain batch 23/32 - 171.2ms/batch - loss: 215.40615 - diff: 43.07mlTrain batch 24/32 - 171.8ms/batch - loss: 212.93047 - diff: 42.99mlTrain batch 25/32 - 171.3ms/batch - loss: 216.54906 - diff: 43.19mlTrain batch 26/32 - 170.6ms/batch - loss: 209.88498 - diff: 42.40mlTrain batch 27/32 - 171.1ms/batch - loss: 206.93351 - diff: 42.24mlTrain batch 28/32 - 170.5ms/batch - loss: 205.14824 - diff: 42.06mlTrain batch 29/32 - 171.3ms/batch - loss: 202.08716 - diff: 41.72mlTrain batch 30/32 - 171.5ms/batch - loss: 200.25867 - diff: 41.52mlTrain batch 31/32 - 171.3ms/batch - loss: 200.85065 - diff: 41.61mlTrain batch 32/32 - 53.6ms/batch - loss: 205.61313 - diff: 41.68mlTrain batch 32/32 - 10.5s 53.6ms/batch - loss: 205.61313 - diff: 41.68ml
Test 1.1s: val_loss: 175.26777 - diff: 40.38ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 14: current best loss = 175.26777, at epoch 13
Train batch 1/32 - 171.3ms/batch - loss: 163.05707 - diff: 38.44mlTrain batch 2/32 - 171.5ms/batch - loss: 129.65339 - diff: 36.27mlTrain batch 3/32 - 171.2ms/batch - loss: 105.12501 - diff: 31.63mlTrain batch 4/32 - 171.5ms/batch - loss: 155.77536 - diff: 38.98mlTrain batch 5/32 - 171.2ms/batch - loss: 136.78974 - diff: 35.92mlTrain batch 6/32 - 171.7ms/batch - loss: 142.43868 - diff: 36.67mlTrain batch 7/32 - 171.3ms/batch - loss: 174.47788 - diff: 39.05mlTrain batch 8/32 - 171.4ms/batch - loss: 177.89675 - diff: 39.09mlTrain batch 9/32 - 171.2ms/batch - loss: 170.62750 - diff: 38.55mlTrain batch 10/32 - 171.5ms/batch - loss: 173.78368 - diff: 39.33mlTrain batch 11/32 - 171.3ms/batch - loss: 166.60675 - diff: 38.57mlTrain batch 12/32 - 171.9ms/batch - loss: 170.86209 - diff: 38.82mlTrain batch 13/32 - 171.3ms/batch - loss: 178.12521 - diff: 39.76mlTrain batch 14/32 - 171.4ms/batch - loss: 174.53127 - diff: 39.45mlTrain batch 15/32 - 171.2ms/batch - loss: 169.70772 - diff: 39.05mlTrain batch 16/32 - 170.6ms/batch - loss: 209.97862 - diff: 40.84mlTrain batch 17/32 - 171.3ms/batch - loss: 203.73977 - diff: 40.16mlTrain batch 18/32 - 171.4ms/batch - loss: 202.78935 - diff: 40.19mlTrain batch 19/32 - 171.2ms/batch - loss: 200.53414 - diff: 40.02mlTrain batch 20/32 - 171.6ms/batch - loss: 196.25430 - diff: 39.63mlTrain batch 21/32 - 171.4ms/batch - loss: 194.97350 - diff: 39.83mlTrain batch 22/32 - 171.4ms/batch - loss: 200.92395 - diff: 40.79mlTrain batch 23/32 - 171.4ms/batch - loss: 196.79599 - diff: 40.55mlTrain batch 24/32 - 171.3ms/batch - loss: 193.60698 - diff: 40.41mlTrain batch 25/32 - 171.2ms/batch - loss: 193.70950 - diff: 40.42mlTrain batch 26/32 - 171.5ms/batch - loss: 195.18641 - diff: 40.53mlTrain batch 27/32 - 171.3ms/batch - loss: 197.17322 - diff: 40.99mlTrain batch 28/32 - 171.6ms/batch - loss: 195.32602 - diff: 41.09mlTrain batch 29/32 - 171.7ms/batch - loss: 196.30720 - diff: 41.26mlTrain batch 30/32 - 170.4ms/batch - loss: 195.53079 - diff: 41.49mlTrain batch 31/32 - 171.3ms/batch - loss: 200.81787 - diff: 41.73mlTrain batch 32/32 - 52.9ms/batch - loss: 210.63317 - diff: 41.81mlTrain batch 32/32 - 10.5s 52.9ms/batch - loss: 210.63317 - diff: 41.81ml
Test 1.1s: val_loss: 185.49772 - diff: 42.47ml

Epoch 15: current best loss = 175.26777, at epoch 13
Train batch 1/32 - 171.6ms/batch - loss: 205.29164 - diff: 39.54mlTrain batch 2/32 - 171.8ms/batch - loss: 142.82847 - diff: 34.41mlTrain batch 3/32 - 171.3ms/batch - loss: 193.98214 - diff: 41.71mlTrain batch 4/32 - 171.3ms/batch - loss: 186.09264 - diff: 42.04mlTrain batch 5/32 - 171.4ms/batch - loss: 180.20493 - diff: 41.98mlTrain batch 6/32 - 171.3ms/batch - loss: 171.45226 - diff: 39.97mlTrain batch 7/32 - 171.3ms/batch - loss: 170.50950 - diff: 39.28mlTrain batch 8/32 - 171.5ms/batch - loss: 167.21939 - diff: 38.96mlTrain batch 9/32 - 171.4ms/batch - loss: 161.71754 - diff: 38.66mlTrain batch 10/32 - 171.5ms/batch - loss: 166.10089 - diff: 38.78mlTrain batch 11/32 - 171.4ms/batch - loss: 180.67064 - diff: 40.42mlTrain batch 12/32 - 171.5ms/batch - loss: 174.94429 - diff: 40.02mlTrain batch 13/32 - 171.4ms/batch - loss: 167.69869 - diff: 39.12mlTrain batch 14/32 - 171.3ms/batch - loss: 166.24971 - diff: 39.04mlTrain batch 15/32 - 171.3ms/batch - loss: 166.18541 - diff: 39.20mlTrain batch 16/32 - 171.4ms/batch - loss: 166.11070 - diff: 39.51mlTrain batch 17/32 - 171.2ms/batch - loss: 165.47552 - diff: 39.58mlTrain batch 18/32 - 171.3ms/batch - loss: 164.82766 - diff: 39.34mlTrain batch 19/32 - 171.3ms/batch - loss: 160.18620 - diff: 38.67mlTrain batch 20/32 - 171.4ms/batch - loss: 168.06991 - diff: 39.51mlTrain batch 21/32 - 171.2ms/batch - loss: 169.26309 - diff: 39.50mlTrain batch 22/32 - 171.4ms/batch - loss: 174.55237 - diff: 40.03mlTrain batch 23/32 - 171.4ms/batch - loss: 177.18958 - diff: 40.36mlTrain batch 24/32 - 171.4ms/batch - loss: 177.85554 - diff: 40.34mlTrain batch 25/32 - 171.3ms/batch - loss: 211.17127 - diff: 42.18mlTrain batch 26/32 - 171.7ms/batch - loss: 208.41836 - diff: 42.08mlTrain batch 27/32 - 171.3ms/batch - loss: 203.90200 - diff: 41.63mlTrain batch 28/32 - 171.4ms/batch - loss: 201.48124 - diff: 41.52mlTrain batch 29/32 - 171.1ms/batch - loss: 200.00174 - diff: 41.45mlTrain batch 30/32 - 171.5ms/batch - loss: 199.59737 - diff: 41.39mlTrain batch 31/32 - 171.4ms/batch - loss: 197.85205 - diff: 41.27mlTrain batch 32/32 - 52.8ms/batch - loss: 203.15608 - diff: 41.39mlTrain batch 32/32 - 10.5s 52.8ms/batch - loss: 203.15608 - diff: 41.39ml
Test 1.1s: val_loss: 174.34351 - diff: 41.12ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 16: current best loss = 174.34351, at epoch 15
Train batch 1/32 - 171.2ms/batch - loss: 161.93652 - diff: 40.47mlTrain batch 2/32 - 171.8ms/batch - loss: 183.78341 - diff: 44.56mlTrain batch 3/32 - 171.2ms/batch - loss: 193.78230 - diff: 43.94mlTrain batch 4/32 - 171.6ms/batch - loss: 204.48661 - diff: 43.74mlTrain batch 5/32 - 171.3ms/batch - loss: 202.48889 - diff: 44.29mlTrain batch 6/32 - 172.1ms/batch - loss: 196.13978 - diff: 43.42mlTrain batch 7/32 - 171.2ms/batch - loss: 190.94263 - diff: 43.35mlTrain batch 8/32 - 170.4ms/batch - loss: 204.77709 - diff: 45.27mlTrain batch 9/32 - 171.3ms/batch - loss: 189.55842 - diff: 43.20mlTrain batch 10/32 - 170.8ms/batch - loss: 184.90125 - diff: 42.27mlTrain batch 11/32 - 171.4ms/batch - loss: 195.58832 - diff: 42.71mlTrain batch 12/32 - 171.4ms/batch - loss: 195.50909 - diff: 43.36mlTrain batch 13/32 - 171.3ms/batch - loss: 193.51048 - diff: 43.37mlTrain batch 14/32 - 171.5ms/batch - loss: 198.37595 - diff: 43.75mlTrain batch 15/32 - 171.2ms/batch - loss: 195.18548 - diff: 43.47mlTrain batch 16/32 - 171.6ms/batch - loss: 186.31609 - diff: 42.32mlTrain batch 17/32 - 171.2ms/batch - loss: 181.95362 - diff: 41.65mlTrain batch 18/32 - 171.8ms/batch - loss: 189.01255 - diff: 42.38mlTrain batch 19/32 - 171.3ms/batch - loss: 188.84582 - diff: 42.37mlTrain batch 20/32 - 171.9ms/batch - loss: 192.63683 - diff: 42.04mlTrain batch 21/32 - 171.3ms/batch - loss: 197.08115 - diff: 42.69mlTrain batch 22/32 - 172.1ms/batch - loss: 221.97415 - diff: 43.57mlTrain batch 23/32 - 171.3ms/batch - loss: 217.24586 - diff: 43.13mlTrain batch 24/32 - 171.5ms/batch - loss: 213.59682 - diff: 42.85mlTrain batch 25/32 - 170.7ms/batch - loss: 210.38700 - diff: 42.56mlTrain batch 26/32 - 171.7ms/batch - loss: 206.61312 - diff: 42.09mlTrain batch 27/32 - 171.3ms/batch - loss: 204.67932 - diff: 41.85mlTrain batch 28/32 - 171.6ms/batch - loss: 199.90130 - diff: 41.34mlTrain batch 29/32 - 171.3ms/batch - loss: 197.30403 - diff: 41.16mlTrain batch 30/32 - 171.7ms/batch - loss: 197.53824 - diff: 41.33mlTrain batch 31/32 - 171.4ms/batch - loss: 195.81383 - diff: 41.29mlTrain batch 32/32 - 52.9ms/batch - loss: 212.19522 - diff: 41.60mlTrain batch 32/32 - 10.5s 52.9ms/batch - loss: 212.19522 - diff: 41.60ml
Test 1.1s: val_loss: 174.66042 - diff: 40.74ml

Epoch 17: current best loss = 174.34351, at epoch 15
Train batch 1/32 - 171.3ms/batch - loss: 267.85037 - diff: 45.85mlTrain batch 2/32 - 171.7ms/batch - loss: 179.25828 - diff: 38.55mlTrain batch 3/32 - 171.2ms/batch - loss: 170.06796 - diff: 38.59mlTrain batch 4/32 - 171.9ms/batch - loss: 181.65393 - diff: 40.80mlTrain batch 5/32 - 171.4ms/batch - loss: 159.49294 - diff: 37.82mlTrain batch 6/32 - 171.8ms/batch - loss: 171.10826 - diff: 39.36mlTrain batch 7/32 - 171.5ms/batch - loss: 162.64133 - diff: 38.40mlTrain batch 8/32 - 171.8ms/batch - loss: 166.49364 - diff: 39.27mlTrain batch 9/32 - 171.3ms/batch - loss: 161.28343 - diff: 38.78mlTrain batch 10/32 - 170.7ms/batch - loss: 159.81859 - diff: 39.05mlTrain batch 11/32 - 171.4ms/batch - loss: 163.51423 - diff: 39.80mlTrain batch 12/32 - 171.7ms/batch - loss: 167.53346 - diff: 39.75mlTrain batch 13/32 - 171.5ms/batch - loss: 212.38524 - diff: 41.37mlTrain batch 14/32 - 171.6ms/batch - loss: 208.41082 - diff: 41.39mlTrain batch 15/32 - 171.3ms/batch - loss: 202.09072 - diff: 41.00mlTrain batch 16/32 - 171.7ms/batch - loss: 198.37035 - diff: 40.64mlTrain batch 17/32 - 171.4ms/batch - loss: 195.66427 - diff: 40.26mlTrain batch 18/32 - 172.0ms/batch - loss: 195.60479 - diff: 40.44mlTrain batch 19/32 - 171.6ms/batch - loss: 192.80632 - diff: 40.03mlTrain batch 20/32 - 171.6ms/batch - loss: 190.42892 - diff: 40.07mlTrain batch 21/32 - 171.3ms/batch - loss: 195.96252 - diff: 40.28mlTrain batch 22/32 - 170.6ms/batch - loss: 194.68768 - diff: 40.30mlTrain batch 23/32 - 171.3ms/batch - loss: 194.41806 - diff: 40.49mlTrain batch 24/32 - 171.3ms/batch - loss: 192.59224 - diff: 40.43mlTrain batch 25/32 - 171.3ms/batch - loss: 191.34812 - diff: 40.52mlTrain batch 26/32 - 171.8ms/batch - loss: 195.56279 - diff: 40.71mlTrain batch 27/32 - 171.4ms/batch - loss: 193.14484 - diff: 40.67mlTrain batch 28/32 - 171.7ms/batch - loss: 193.33268 - diff: 40.73mlTrain batch 29/32 - 171.4ms/batch - loss: 196.24908 - diff: 41.18mlTrain batch 30/32 - 171.5ms/batch - loss: 191.78559 - diff: 40.62mlTrain batch 31/32 - 171.3ms/batch - loss: 195.09611 - diff: 41.24mlTrain batch 32/32 - 52.7ms/batch - loss: 202.49536 - diff: 41.38mlTrain batch 32/32 - 10.5s 52.7ms/batch - loss: 202.49536 - diff: 41.38ml
Test 1.0s: val_loss: 171.02420 - diff: 40.27ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 18: current best loss = 171.02420, at epoch 17
Train batch 1/32 - 171.3ms/batch - loss: 152.86234 - diff: 41.02mlTrain batch 2/32 - 171.3ms/batch - loss: 160.42064 - diff: 39.01mlTrain batch 3/32 - 171.1ms/batch - loss: 164.44138 - diff: 40.40mlTrain batch 4/32 - 171.3ms/batch - loss: 128.76720 - diff: 33.81mlTrain batch 5/32 - 171.3ms/batch - loss: 143.59731 - diff: 36.10mlTrain batch 6/32 - 171.5ms/batch - loss: 144.72637 - diff: 36.77mlTrain batch 7/32 - 171.4ms/batch - loss: 232.45861 - diff: 40.56mlTrain batch 8/32 - 171.4ms/batch - loss: 210.93314 - diff: 38.71mlTrain batch 9/32 - 171.5ms/batch - loss: 217.14625 - diff: 39.26mlTrain batch 10/32 - 171.4ms/batch - loss: 214.40821 - diff: 39.50mlTrain batch 11/32 - 171.5ms/batch - loss: 203.33613 - diff: 39.07mlTrain batch 12/32 - 171.7ms/batch - loss: 196.36948 - diff: 38.62mlTrain batch 13/32 - 171.3ms/batch - loss: 193.56391 - diff: 38.81mlTrain batch 14/32 - 171.5ms/batch - loss: 193.10694 - diff: 39.31mlTrain batch 15/32 - 171.4ms/batch - loss: 187.40925 - diff: 38.99mlTrain batch 16/32 - 171.7ms/batch - loss: 187.10663 - diff: 39.09mlTrain batch 17/32 - 171.4ms/batch - loss: 191.84461 - diff: 39.65mlTrain batch 18/32 - 171.6ms/batch - loss: 199.32379 - diff: 40.31mlTrain batch 19/32 - 171.3ms/batch - loss: 193.67030 - diff: 39.56mlTrain batch 20/32 - 171.5ms/batch - loss: 191.65794 - diff: 39.47mlTrain batch 21/32 - 171.4ms/batch - loss: 187.45252 - diff: 39.25mlTrain batch 22/32 - 171.4ms/batch - loss: 180.94908 - diff: 38.51mlTrain batch 23/32 - 171.3ms/batch - loss: 182.39978 - diff: 38.66mlTrain batch 24/32 - 171.4ms/batch - loss: 184.63542 - diff: 39.00mlTrain batch 25/32 - 171.3ms/batch - loss: 186.78599 - diff: 39.01mlTrain batch 26/32 - 171.3ms/batch - loss: 185.48980 - diff: 38.90mlTrain batch 27/32 - 171.4ms/batch - loss: 181.66543 - diff: 38.65mlTrain batch 28/32 - 171.8ms/batch - loss: 185.48592 - diff: 39.24mlTrain batch 29/32 - 171.4ms/batch - loss: 186.00238 - diff: 39.55mlTrain batch 30/32 - 171.3ms/batch - loss: 184.38674 - diff: 39.56mlTrain batch 31/32 - 170.7ms/batch - loss: 183.39351 - diff: 39.61mlTrain batch 32/32 - 52.7ms/batch - loss: 199.09928 - diff: 39.96mlTrain batch 32/32 - 10.5s 52.7ms/batch - loss: 199.09928 - diff: 39.96ml
Test 1.1s: val_loss: 172.43827 - diff: 39.87ml

Epoch 19: current best loss = 171.02420, at epoch 17
Train batch 1/32 - 171.5ms/batch - loss: 84.05238 - diff: 31.18mlTrain batch 2/32 - 171.6ms/batch - loss: 164.62446 - diff: 38.37mlTrain batch 3/32 - 171.2ms/batch - loss: 157.80186 - diff: 40.07mlTrain batch 4/32 - 171.4ms/batch - loss: 152.76825 - diff: 40.59mlTrain batch 5/32 - 171.4ms/batch - loss: 145.68265 - diff: 39.57mlTrain batch 6/32 - 171.7ms/batch - loss: 144.11096 - diff: 40.25mlTrain batch 7/32 - 171.2ms/batch - loss: 156.10756 - diff: 40.67mlTrain batch 8/32 - 171.9ms/batch - loss: 165.78072 - diff: 40.19mlTrain batch 9/32 - 171.3ms/batch - loss: 160.95246 - diff: 39.46mlTrain batch 10/32 - 172.0ms/batch - loss: 161.48173 - diff: 39.57mlTrain batch 11/32 - 171.4ms/batch - loss: 155.39969 - diff: 38.48mlTrain batch 12/32 - 171.9ms/batch - loss: 161.62136 - diff: 39.29mlTrain batch 13/32 - 171.8ms/batch - loss: 163.54341 - diff: 39.67mlTrain batch 14/32 - 172.0ms/batch - loss: 166.85436 - diff: 40.03mlTrain batch 15/32 - 171.7ms/batch - loss: 170.08787 - diff: 40.47mlTrain batch 16/32 - 171.1ms/batch - loss: 170.86872 - diff: 40.87mlTrain batch 17/32 - 171.5ms/batch - loss: 171.21708 - diff: 41.07mlTrain batch 18/32 - 171.5ms/batch - loss: 183.01752 - diff: 42.22mlTrain batch 19/32 - 171.4ms/batch - loss: 176.84600 - diff: 41.40mlTrain batch 20/32 - 171.8ms/batch - loss: 171.99282 - diff: 40.58mlTrain batch 21/32 - 171.4ms/batch - loss: 171.60477 - diff: 40.81mlTrain batch 22/32 - 171.6ms/batch - loss: 170.06401 - diff: 40.68mlTrain batch 23/32 - 171.2ms/batch - loss: 168.81367 - diff: 40.29mlTrain batch 24/32 - 172.0ms/batch - loss: 167.48719 - diff: 39.86mlTrain batch 25/32 - 171.9ms/batch - loss: 165.20347 - diff: 39.49mlTrain batch 26/32 - 172.0ms/batch - loss: 201.67703 - diff: 41.31mlTrain batch 27/32 - 171.5ms/batch - loss: 201.79766 - diff: 41.57mlTrain batch 28/32 - 172.1ms/batch - loss: 200.79710 - diff: 41.55mlTrain batch 29/32 - 171.9ms/batch - loss: 196.41558 - diff: 41.12mlTrain batch 30/32 - 171.9ms/batch - loss: 194.46346 - diff: 41.12mlTrain batch 31/32 - 171.8ms/batch - loss: 194.33614 - diff: 41.11mlTrain batch 32/32 - 53.1ms/batch - loss: 196.58588 - diff: 41.09mlTrain batch 32/32 - 10.5s 53.1ms/batch - loss: 196.58588 - diff: 41.09ml
Test 1.1s: val_loss: 184.69937 - diff: 42.82ml

Epoch 20: current best loss = 171.02420, at epoch 17
Train batch 1/32 - 171.2ms/batch - loss: 114.62442 - diff: 29.23mlTrain batch 2/32 - 172.1ms/batch - loss: 169.93602 - diff: 34.36mlTrain batch 3/32 - 171.3ms/batch - loss: 220.10973 - diff: 37.39mlTrain batch 4/32 - 171.6ms/batch - loss: 232.14501 - diff: 39.95mlTrain batch 5/32 - 171.3ms/batch - loss: 201.38123 - diff: 37.11mlTrain batch 6/32 - 171.8ms/batch - loss: 191.42710 - diff: 36.79mlTrain batch 7/32 - 171.9ms/batch - loss: 190.64327 - diff: 37.59mlTrain batch 8/32 - 172.0ms/batch - loss: 174.19272 - diff: 35.99mlTrain batch 9/32 - 171.8ms/batch - loss: 243.60391 - diff: 40.00mlTrain batch 10/32 - 172.1ms/batch - loss: 232.34344 - diff: 39.96mlTrain batch 11/32 - 171.7ms/batch - loss: 229.98054 - diff: 40.26mlTrain batch 12/32 - 172.0ms/batch - loss: 223.46446 - diff: 40.06mlTrain batch 13/32 - 171.7ms/batch - loss: 214.46839 - diff: 39.57mlTrain batch 14/32 - 171.8ms/batch - loss: 214.18309 - diff: 39.70mlTrain batch 15/32 - 171.8ms/batch - loss: 204.98598 - diff: 38.85mlTrain batch 16/32 - 171.9ms/batch - loss: 199.52641 - diff: 38.51mlTrain batch 17/32 - 171.5ms/batch - loss: 205.69121 - diff: 39.89mlTrain batch 18/32 - 172.0ms/batch - loss: 202.91885 - diff: 39.78mlTrain batch 19/32 - 172.1ms/batch - loss: 198.16072 - diff: 39.47mlTrain batch 20/32 - 171.8ms/batch - loss: 198.36533 - diff: 39.84mlTrain batch 21/32 - 172.0ms/batch - loss: 199.94929 - diff: 40.35mlTrain batch 22/32 - 172.0ms/batch - loss: 194.95090 - diff: 39.90mlTrain batch 23/32 - 171.6ms/batch - loss: 191.33643 - diff: 39.67mlTrain batch 24/32 - 171.9ms/batch - loss: 192.76656 - diff: 40.01mlTrain batch 25/32 - 172.2ms/batch - loss: 197.18092 - diff: 40.82mlTrain batch 26/32 - 171.9ms/batch - loss: 195.41415 - diff: 40.84mlTrain batch 27/32 - 172.0ms/batch - loss: 190.46396 - diff: 40.25mlTrain batch 28/32 - 172.0ms/batch - loss: 190.18326 - diff: 40.38mlTrain batch 29/32 - 171.8ms/batch - loss: 189.27192 - diff: 40.52mlTrain batch 30/32 - 171.9ms/batch - loss: 191.64971 - diff: 40.94mlTrain batch 31/32 - 171.9ms/batch - loss: 189.81776 - diff: 40.81mlTrain batch 32/32 - 53.1ms/batch - loss: 191.00364 - diff: 40.76mlTrain batch 32/32 - 10.5s 53.1ms/batch - loss: 191.00364 - diff: 40.76ml
Test 1.1s: val_loss: 170.25424 - diff: 40.68ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 21: current best loss = 170.25424, at epoch 20
Train batch 1/32 - 171.3ms/batch - loss: 92.59553 - diff: 33.54mlTrain batch 2/32 - 171.7ms/batch - loss: 201.73882 - diff: 42.88mlTrain batch 3/32 - 171.3ms/batch - loss: 162.63788 - diff: 38.66mlTrain batch 4/32 - 171.9ms/batch - loss: 151.70411 - diff: 37.28mlTrain batch 5/32 - 171.3ms/batch - loss: 190.99846 - diff: 41.16mlTrain batch 6/32 - 171.7ms/batch - loss: 200.33330 - diff: 42.76mlTrain batch 7/32 - 171.7ms/batch - loss: 186.67850 - diff: 41.74mlTrain batch 8/32 - 172.1ms/batch - loss: 190.42689 - diff: 43.00mlTrain batch 9/32 - 171.8ms/batch - loss: 188.86392 - diff: 43.09mlTrain batch 10/32 - 172.1ms/batch - loss: 187.93556 - diff: 43.10mlTrain batch 11/32 - 171.3ms/batch - loss: 180.16288 - diff: 42.29mlTrain batch 12/32 - 171.7ms/batch - loss: 177.74724 - diff: 42.30mlTrain batch 13/32 - 171.4ms/batch - loss: 174.63682 - diff: 41.77mlTrain batch 14/32 - 171.7ms/batch - loss: 177.53934 - diff: 42.30mlTrain batch 15/32 - 171.4ms/batch - loss: 177.10988 - diff: 42.18mlTrain batch 16/32 - 171.7ms/batch - loss: 176.86205 - diff: 42.13mlTrain batch 17/32 - 171.4ms/batch - loss: 184.96999 - diff: 42.57mlTrain batch 18/32 - 171.8ms/batch - loss: 179.65888 - diff: 41.88mlTrain batch 19/32 - 171.3ms/batch - loss: 176.14819 - diff: 41.30mlTrain batch 20/32 - 171.9ms/batch - loss: 174.58658 - diff: 40.93mlTrain batch 21/32 - 172.1ms/batch - loss: 176.62785 - diff: 41.04mlTrain batch 22/32 - 171.9ms/batch - loss: 203.90428 - diff: 42.05mlTrain batch 23/32 - 172.0ms/batch - loss: 203.66965 - diff: 41.91mlTrain batch 24/32 - 172.1ms/batch - loss: 202.89331 - diff: 41.93mlTrain batch 25/32 - 171.8ms/batch - loss: 197.68190 - diff: 41.39mlTrain batch 26/32 - 171.7ms/batch - loss: 198.17536 - diff: 41.33mlTrain batch 27/32 - 171.3ms/batch - loss: 193.27788 - diff: 40.79mlTrain batch 28/32 - 171.8ms/batch - loss: 188.80482 - diff: 40.33mlTrain batch 29/32 - 171.5ms/batch - loss: 186.73983 - diff: 40.14mlTrain batch 30/32 - 171.8ms/batch - loss: 187.52332 - diff: 40.30mlTrain batch 31/32 - 171.7ms/batch - loss: 187.10158 - diff: 40.19mlTrain batch 32/32 - 53.0ms/batch - loss: 191.57978 - diff: 40.20mlTrain batch 32/32 - 10.5s 53.0ms/batch - loss: 191.57978 - diff: 40.20ml
Test 1.1s: val_loss: 160.38477 - diff: 40.09ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 22: current best loss = 160.38477, at epoch 21
Train batch 1/32 - 171.5ms/batch - loss: 71.90996 - diff: 28.57mlTrain batch 2/32 - 171.6ms/batch - loss: 105.62739 - diff: 34.28mlTrain batch 3/32 - 171.5ms/batch - loss: 111.97575 - diff: 35.82mlTrain batch 4/32 - 171.7ms/batch - loss: 144.10930 - diff: 35.67mlTrain batch 5/32 - 171.2ms/batch - loss: 171.29517 - diff: 38.42mlTrain batch 6/32 - 171.8ms/batch - loss: 195.01017 - diff: 42.24mlTrain batch 7/32 - 171.3ms/batch - loss: 188.30950 - diff: 41.35mlTrain batch 8/32 - 171.7ms/batch - loss: 175.99442 - diff: 40.37mlTrain batch 9/32 - 171.3ms/batch - loss: 167.24150 - diff: 39.43mlTrain batch 10/32 - 172.1ms/batch - loss: 167.14790 - diff: 39.62mlTrain batch 11/32 - 171.5ms/batch - loss: 160.40488 - diff: 38.79mlTrain batch 12/32 - 171.7ms/batch - loss: 158.27881 - diff: 38.54mlTrain batch 13/32 - 171.1ms/batch - loss: 164.70594 - diff: 39.06mlTrain batch 14/32 - 171.2ms/batch - loss: 165.26733 - diff: 39.23mlTrain batch 15/32 - 171.4ms/batch - loss: 162.66855 - diff: 39.22mlTrain batch 16/32 - 171.7ms/batch - loss: 168.01793 - diff: 39.44mlTrain batch 17/32 - 171.5ms/batch - loss: 169.32621 - diff: 39.76mlTrain batch 18/32 - 171.6ms/batch - loss: 174.63202 - diff: 40.34mlTrain batch 19/32 - 171.5ms/batch - loss: 174.21703 - diff: 40.81mlTrain batch 20/32 - 171.7ms/batch - loss: 172.90107 - diff: 40.84mlTrain batch 21/32 - 171.3ms/batch - loss: 201.09671 - diff: 41.80mlTrain batch 22/32 - 171.6ms/batch - loss: 205.32866 - diff: 42.40mlTrain batch 23/32 - 171.4ms/batch - loss: 213.19585 - diff: 43.31mlTrain batch 24/32 - 171.6ms/batch - loss: 211.57635 - diff: 43.11mlTrain batch 25/32 - 171.4ms/batch - loss: 206.97609 - diff: 42.71mlTrain batch 26/32 - 171.5ms/batch - loss: 208.97067 - diff: 42.81mlTrain batch 27/32 - 170.7ms/batch - loss: 203.27318 - diff: 42.11mlTrain batch 28/32 - 171.9ms/batch - loss: 197.68705 - diff: 41.42mlTrain batch 29/32 - 171.6ms/batch - loss: 195.91466 - diff: 41.19mlTrain batch 30/32 - 171.7ms/batch - loss: 191.85086 - diff: 40.72mlTrain batch 31/32 - 171.5ms/batch - loss: 190.67246 - diff: 40.79mlTrain batch 32/32 - 52.9ms/batch - loss: 194.57967 - diff: 40.73mlTrain batch 32/32 - 10.5s 52.9ms/batch - loss: 194.57967 - diff: 40.73ml
Test 1.1s: val_loss: 163.24933 - diff: 39.61ml

Epoch 23: current best loss = 160.38477, at epoch 21
Train batch 1/32 - 171.5ms/batch - loss: 243.20340 - diff: 40.16mlTrain batch 2/32 - 172.0ms/batch - loss: 246.31255 - diff: 45.32mlTrain batch 3/32 - 171.4ms/batch - loss: 207.62171 - diff: 42.47mlTrain batch 4/32 - 172.1ms/batch - loss: 221.60678 - diff: 44.93mlTrain batch 5/32 - 171.6ms/batch - loss: 203.19146 - diff: 43.82mlTrain batch 6/32 - 171.9ms/batch - loss: 193.04454 - diff: 43.06mlTrain batch 7/32 - 171.4ms/batch - loss: 180.71564 - diff: 41.64mlTrain batch 8/32 - 171.7ms/batch - loss: 169.32643 - diff: 40.51mlTrain batch 9/32 - 171.4ms/batch - loss: 160.84528 - diff: 39.51mlTrain batch 10/32 - 170.3ms/batch - loss: 177.76653 - diff: 41.42mlTrain batch 11/32 - 171.2ms/batch - loss: 184.86134 - diff: 42.03mlTrain batch 12/32 - 171.8ms/batch - loss: 197.74882 - diff: 42.93mlTrain batch 13/32 - 171.4ms/batch - loss: 193.51326 - diff: 42.21mlTrain batch 14/32 - 171.8ms/batch - loss: 185.91798 - diff: 41.30mlTrain batch 15/32 - 171.6ms/batch - loss: 181.16585 - diff: 40.91mlTrain batch 16/32 - 171.6ms/batch - loss: 180.90354 - diff: 41.10mlTrain batch 17/32 - 171.4ms/batch - loss: 175.10127 - diff: 40.46mlTrain batch 18/32 - 171.7ms/batch - loss: 206.94245 - diff: 41.29mlTrain batch 19/32 - 171.4ms/batch - loss: 203.99685 - diff: 40.92mlTrain batch 20/32 - 171.9ms/batch - loss: 207.95488 - diff: 41.50mlTrain batch 21/32 - 171.5ms/batch - loss: 202.03707 - diff: 40.91mlTrain batch 22/32 - 172.2ms/batch - loss: 199.54894 - diff: 40.84mlTrain batch 23/32 - 171.8ms/batch - loss: 193.67043 - diff: 40.17mlTrain batch 24/32 - 172.1ms/batch - loss: 195.52577 - diff: 40.61mlTrain batch 25/32 - 171.9ms/batch - loss: 193.79848 - diff: 40.57mlTrain batch 26/32 - 172.4ms/batch - loss: 189.25739 - diff: 39.98mlTrain batch 27/32 - 171.9ms/batch - loss: 191.20658 - diff: 40.20mlTrain batch 28/32 - 171.9ms/batch - loss: 186.07703 - diff: 39.56mlTrain batch 29/32 - 172.0ms/batch - loss: 187.36597 - diff: 39.90mlTrain batch 30/32 - 171.9ms/batch - loss: 185.26055 - diff: 39.65mlTrain batch 31/32 - 172.0ms/batch - loss: 183.96932 - diff: 39.46mlTrain batch 32/32 - 53.0ms/batch - loss: 187.19877 - diff: 39.49mlTrain batch 32/32 - 10.4s 53.0ms/batch - loss: 187.19877 - diff: 39.49ml
Test 1.1s: val_loss: 170.89479 - diff: 40.66ml

Epoch 24: current best loss = 160.38477, at epoch 21
Train batch 1/32 - 171.9ms/batch - loss: 717.69281 - diff: 51.60mlTrain batch 2/32 - 171.8ms/batch - loss: 470.07572 - diff: 49.42mlTrain batch 3/32 - 171.5ms/batch - loss: 416.39561 - diff: 51.27mlTrain batch 4/32 - 171.7ms/batch - loss: 355.53012 - diff: 47.97mlTrain batch 5/32 - 171.3ms/batch - loss: 339.76509 - diff: 49.44mlTrain batch 6/32 - 171.9ms/batch - loss: 334.73355 - diff: 51.06mlTrain batch 7/32 - 171.9ms/batch - loss: 306.18625 - diff: 48.88mlTrain batch 8/32 - 172.3ms/batch - loss: 291.94391 - diff: 48.43mlTrain batch 9/32 - 172.1ms/batch - loss: 276.50423 - diff: 47.32mlTrain batch 10/32 - 172.3ms/batch - loss: 253.99371 - diff: 44.91mlTrain batch 11/32 - 171.9ms/batch - loss: 250.08288 - diff: 45.07mlTrain batch 12/32 - 172.0ms/batch - loss: 235.65163 - diff: 43.88mlTrain batch 13/32 - 171.9ms/batch - loss: 225.15372 - diff: 43.13mlTrain batch 14/32 - 172.1ms/batch - loss: 219.58458 - diff: 42.72mlTrain batch 15/32 - 172.0ms/batch - loss: 212.32676 - diff: 42.27mlTrain batch 16/32 - 172.3ms/batch - loss: 207.02083 - diff: 41.92mlTrain batch 17/32 - 171.7ms/batch - loss: 201.36644 - diff: 41.42mlTrain batch 18/32 - 172.1ms/batch - loss: 202.79277 - diff: 41.46mlTrain batch 19/32 - 172.0ms/batch - loss: 200.84712 - diff: 41.38mlTrain batch 20/32 - 172.2ms/batch - loss: 197.09014 - diff: 41.09mlTrain batch 21/32 - 171.8ms/batch - loss: 197.44270 - diff: 41.25mlTrain batch 22/32 - 172.2ms/batch - loss: 193.73211 - diff: 40.78mlTrain batch 23/32 - 172.0ms/batch - loss: 201.23354 - diff: 41.44mlTrain batch 24/32 - 172.1ms/batch - loss: 196.74326 - diff: 40.98mlTrain batch 25/32 - 172.2ms/batch - loss: 193.65397 - diff: 40.75mlTrain batch 26/32 - 172.2ms/batch - loss: 200.99969 - diff: 41.37mlTrain batch 27/32 - 171.9ms/batch - loss: 198.23220 - diff: 41.07mlTrain batch 28/32 - 172.1ms/batch - loss: 196.09328 - diff: 40.92mlTrain batch 29/32 - 171.8ms/batch - loss: 193.80236 - diff: 40.68mlTrain batch 30/32 - 172.2ms/batch - loss: 189.81033 - diff: 40.01mlTrain batch 31/32 - 171.9ms/batch - loss: 186.41642 - diff: 39.74mlTrain batch 32/32 - 53.3ms/batch - loss: 187.50096 - diff: 39.66mlTrain batch 32/32 - 10.5s 53.3ms/batch - loss: 187.50096 - diff: 39.66ml
Test 1.1s: val_loss: 155.11605 - diff: 39.27ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 25: current best loss = 155.11605, at epoch 24
Train batch 1/32 - 171.5ms/batch - loss: 305.15442 - diff: 46.78mlTrain batch 2/32 - 171.9ms/batch - loss: 219.59772 - diff: 44.36mlTrain batch 3/32 - 171.6ms/batch - loss: 254.60380 - diff: 48.91mlTrain batch 4/32 - 172.2ms/batch - loss: 366.33891 - diff: 49.00mlTrain batch 5/32 - 172.1ms/batch - loss: 314.19555 - diff: 45.68mlTrain batch 6/32 - 172.2ms/batch - loss: 296.99286 - diff: 45.59mlTrain batch 7/32 - 171.2ms/batch - loss: 263.20940 - diff: 42.52mlTrain batch 8/32 - 170.9ms/batch - loss: 253.20355 - diff: 43.24mlTrain batch 9/32 - 171.8ms/batch - loss: 234.54767 - diff: 41.45mlTrain batch 10/32 - 171.8ms/batch - loss: 222.84484 - diff: 40.59mlTrain batch 11/32 - 171.4ms/batch - loss: 213.29729 - diff: 39.96mlTrain batch 12/32 - 172.2ms/batch - loss: 203.19175 - diff: 39.26mlTrain batch 13/32 - 171.4ms/batch - loss: 194.59356 - diff: 38.61mlTrain batch 14/32 - 172.0ms/batch - loss: 200.46814 - diff: 38.87mlTrain batch 15/32 - 171.9ms/batch - loss: 193.00756 - diff: 38.33mlTrain batch 16/32 - 172.0ms/batch - loss: 193.59535 - diff: 38.40mlTrain batch 17/32 - 171.8ms/batch - loss: 193.95225 - diff: 38.60mlTrain batch 18/32 - 172.4ms/batch - loss: 189.95023 - diff: 38.25mlTrain batch 19/32 - 172.0ms/batch - loss: 186.22552 - diff: 38.08mlTrain batch 20/32 - 172.1ms/batch - loss: 191.77038 - diff: 38.54mlTrain batch 21/32 - 171.1ms/batch - loss: 188.20432 - diff: 38.40mlTrain batch 22/32 - 171.0ms/batch - loss: 192.26744 - diff: 39.20mlTrain batch 23/32 - 171.8ms/batch - loss: 193.15256 - diff: 39.43mlTrain batch 24/32 - 171.8ms/batch - loss: 192.64703 - diff: 39.53mlTrain batch 25/32 - 171.6ms/batch - loss: 188.23337 - diff: 39.09mlTrain batch 26/32 - 171.9ms/batch - loss: 187.70739 - diff: 39.31mlTrain batch 27/32 - 171.4ms/batch - loss: 184.01073 - diff: 38.99mlTrain batch 28/32 - 171.9ms/batch - loss: 180.59645 - diff: 38.61mlTrain batch 29/32 - 171.6ms/batch - loss: 180.91788 - diff: 38.91mlTrain batch 30/32 - 171.8ms/batch - loss: 179.44372 - diff: 38.91mlTrain batch 31/32 - 172.0ms/batch - loss: 181.54237 - diff: 39.23mlTrain batch 32/32 - 53.2ms/batch - loss: 181.03798 - diff: 39.06mlTrain batch 32/32 - 10.5s 53.2ms/batch - loss: 181.03798 - diff: 39.06ml
Test 1.1s: val_loss: 162.32418 - diff: 40.03ml

Epoch 26: current best loss = 155.11605, at epoch 24
Train batch 1/32 - 171.8ms/batch - loss: 104.56653 - diff: 34.33mlTrain batch 2/32 - 171.1ms/batch - loss: 152.79951 - diff: 39.96mlTrain batch 3/32 - 171.1ms/batch - loss: 125.17694 - diff: 34.81mlTrain batch 4/32 - 171.9ms/batch - loss: 112.40337 - diff: 33.13mlTrain batch 5/32 - 171.5ms/batch - loss: 119.23078 - diff: 34.93mlTrain batch 6/32 - 171.9ms/batch - loss: 114.49785 - diff: 34.22mlTrain batch 7/32 - 171.6ms/batch - loss: 120.43976 - diff: 34.64mlTrain batch 8/32 - 171.7ms/batch - loss: 121.80694 - diff: 35.33mlTrain batch 9/32 - 171.9ms/batch - loss: 127.19232 - diff: 36.29mlTrain batch 10/32 - 171.7ms/batch - loss: 121.88457 - diff: 35.56mlTrain batch 11/32 - 171.7ms/batch - loss: 130.89391 - diff: 36.96mlTrain batch 12/32 - 172.1ms/batch - loss: 125.24182 - diff: 35.96mlTrain batch 13/32 - 171.4ms/batch - loss: 122.30627 - diff: 35.30mlTrain batch 14/32 - 172.3ms/batch - loss: 119.58416 - diff: 34.92mlTrain batch 15/32 - 172.2ms/batch - loss: 122.46648 - diff: 35.34mlTrain batch 16/32 - 172.1ms/batch - loss: 126.56401 - diff: 36.09mlTrain batch 17/32 - 171.9ms/batch - loss: 128.98470 - diff: 36.37mlTrain batch 18/32 - 172.6ms/batch - loss: 133.06383 - diff: 36.67mlTrain batch 19/32 - 172.1ms/batch - loss: 175.03373 - diff: 38.51mlTrain batch 20/32 - 172.7ms/batch - loss: 172.88027 - diff: 38.36mlTrain batch 21/32 - 172.4ms/batch - loss: 179.82824 - diff: 39.02mlTrain batch 22/32 - 172.3ms/batch - loss: 188.57985 - diff: 39.92mlTrain batch 23/32 - 172.1ms/batch - loss: 185.03690 - diff: 39.50mlTrain batch 24/32 - 172.5ms/batch - loss: 182.68930 - diff: 39.32mlTrain batch 25/32 - 172.1ms/batch - loss: 184.14280 - diff: 39.55mlTrain batch 26/32 - 172.6ms/batch - loss: 187.82128 - diff: 40.10mlTrain batch 27/32 - 172.0ms/batch - loss: 190.60636 - diff: 40.26mlTrain batch 28/32 - 172.5ms/batch - loss: 193.08903 - diff: 40.50mlTrain batch 29/32 - 171.9ms/batch - loss: 189.61614 - diff: 40.13mlTrain batch 30/32 - 171.4ms/batch - loss: 190.89780 - diff: 40.28mlTrain batch 31/32 - 171.3ms/batch - loss: 189.31509 - diff: 40.28mlTrain batch 32/32 - 53.1ms/batch - loss: 191.08505 - diff: 40.20mlTrain batch 32/32 - 10.5s 53.1ms/batch - loss: 191.08505 - diff: 40.20ml
Test 1.1s: val_loss: 166.34087 - diff: 40.41ml

Epoch 27: current best loss = 155.11605, at epoch 24
Train batch 1/32 - 171.7ms/batch - loss: 112.78781 - diff: 33.27mlTrain batch 2/32 - 172.0ms/batch - loss: 176.18847 - diff: 41.60mlTrain batch 3/32 - 171.8ms/batch - loss: 169.31448 - diff: 41.51mlTrain batch 4/32 - 171.8ms/batch - loss: 153.74458 - diff: 38.89mlTrain batch 5/32 - 171.7ms/batch - loss: 169.84973 - diff: 40.25mlTrain batch 6/32 - 172.2ms/batch - loss: 151.30795 - diff: 37.81mlTrain batch 7/32 - 171.6ms/batch - loss: 177.65384 - diff: 40.28mlTrain batch 8/32 - 171.7ms/batch - loss: 179.58961 - diff: 40.25mlTrain batch 9/32 - 172.0ms/batch - loss: 174.63095 - diff: 40.05mlTrain batch 10/32 - 172.7ms/batch - loss: 187.25761 - diff: 40.85mlTrain batch 11/32 - 172.2ms/batch - loss: 187.82670 - diff: 41.01mlTrain batch 12/32 - 172.5ms/batch - loss: 188.78996 - diff: 41.58mlTrain batch 13/32 - 171.9ms/batch - loss: 181.76534 - diff: 41.09mlTrain batch 14/32 - 170.8ms/batch - loss: 183.39651 - diff: 41.10mlTrain batch 15/32 - 171.5ms/batch - loss: 174.73772 - diff: 39.93mlTrain batch 16/32 - 171.6ms/batch - loss: 176.48171 - diff: 40.18mlTrain batch 17/32 - 172.1ms/batch - loss: 175.94626 - diff: 40.27mlTrain batch 18/32 - 172.6ms/batch - loss: 204.98239 - diff: 41.20mlTrain batch 19/32 - 172.0ms/batch - loss: 198.04025 - diff: 40.60mlTrain batch 20/32 - 172.5ms/batch - loss: 200.58414 - diff: 40.84mlTrain batch 21/32 - 171.8ms/batch - loss: 204.50833 - diff: 41.33mlTrain batch 22/32 - 172.8ms/batch - loss: 199.42174 - diff: 40.96mlTrain batch 23/32 - 172.3ms/batch - loss: 194.90798 - diff: 40.55mlTrain batch 24/32 - 172.7ms/batch - loss: 198.97092 - diff: 41.22mlTrain batch 25/32 - 172.2ms/batch - loss: 197.30538 - diff: 41.20mlTrain batch 26/32 - 172.5ms/batch - loss: 195.30751 - diff: 41.28mlTrain batch 27/32 - 172.4ms/batch - loss: 190.92086 - diff: 40.84mlTrain batch 28/32 - 172.5ms/batch - loss: 189.66254 - diff: 40.73mlTrain batch 29/32 - 171.7ms/batch - loss: 186.96743 - diff: 40.46mlTrain batch 30/32 - 171.1ms/batch - loss: 182.80966 - diff: 39.92mlTrain batch 31/32 - 172.3ms/batch - loss: 180.43641 - diff: 39.78mlTrain batch 32/32 - 53.2ms/batch - loss: 181.25648 - diff: 39.70mlTrain batch 32/32 - 10.5s 53.2ms/batch - loss: 181.25648 - diff: 39.70ml
Test 1.1s: val_loss: 153.13305 - diff: 37.18ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 28: current best loss = 153.13305, at epoch 27
Train batch 1/32 - 171.3ms/batch - loss: 836.45154 - diff: 67.86mlTrain batch 2/32 - 172.2ms/batch - loss: 567.38362 - diff: 60.43mlTrain batch 3/32 - 171.6ms/batch - loss: 436.43096 - diff: 51.72mlTrain batch 4/32 - 172.1ms/batch - loss: 364.45716 - diff: 48.66mlTrain batch 5/32 - 171.5ms/batch - loss: 345.05387 - diff: 48.42mlTrain batch 6/32 - 171.9ms/batch - loss: 334.43319 - diff: 48.65mlTrain batch 7/32 - 172.3ms/batch - loss: 302.87042 - diff: 46.91mlTrain batch 8/32 - 172.3ms/batch - loss: 289.21416 - diff: 46.36mlTrain batch 9/32 - 171.7ms/batch - loss: 269.51582 - diff: 45.21mlTrain batch 10/32 - 172.5ms/batch - loss: 247.74030 - diff: 42.95mlTrain batch 11/32 - 172.0ms/batch - loss: 235.09576 - diff: 42.12mlTrain batch 12/32 - 172.6ms/batch - loss: 235.70043 - diff: 41.96mlTrain batch 13/32 - 172.3ms/batch - loss: 227.55853 - diff: 41.59mlTrain batch 14/32 - 172.1ms/batch - loss: 223.36073 - diff: 41.56mlTrain batch 15/32 - 172.0ms/batch - loss: 219.42543 - diff: 41.31mlTrain batch 16/32 - 172.0ms/batch - loss: 211.54725 - diff: 40.71mlTrain batch 17/32 - 172.2ms/batch - loss: 210.86417 - diff: 41.05mlTrain batch 18/32 - 172.6ms/batch - loss: 203.14785 - diff: 40.35mlTrain batch 19/32 - 172.0ms/batch - loss: 203.92848 - diff: 40.54mlTrain batch 20/32 - 172.2ms/batch - loss: 204.84496 - diff: 40.80mlTrain batch 21/32 - 172.3ms/batch - loss: 201.81176 - diff: 40.87mlTrain batch 22/32 - 172.2ms/batch - loss: 200.45845 - diff: 40.65mlTrain batch 23/32 - 172.0ms/batch - loss: 199.32029 - diff: 40.71mlTrain batch 24/32 - 172.3ms/batch - loss: 198.84212 - diff: 41.04mlTrain batch 25/32 - 172.0ms/batch - loss: 195.84569 - diff: 40.72mlTrain batch 26/32 - 172.5ms/batch - loss: 194.09273 - diff: 40.70mlTrain batch 27/32 - 171.9ms/batch - loss: 191.20799 - diff: 40.59mlTrain batch 28/32 - 172.8ms/batch - loss: 188.53065 - diff: 40.29mlTrain batch 29/32 - 172.1ms/batch - loss: 187.37854 - diff: 40.26mlTrain batch 30/32 - 172.6ms/batch - loss: 185.43332 - diff: 39.97mlTrain batch 31/32 - 172.1ms/batch - loss: 184.33775 - diff: 39.88mlTrain batch 32/32 - 53.1ms/batch - loss: 187.28543 - diff: 39.86mlTrain batch 32/32 - 10.5s 53.1ms/batch - loss: 187.28543 - diff: 39.86ml
Test 1.1s: val_loss: 147.16273 - diff: 37.74ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 29: current best loss = 147.16273, at epoch 28
Train batch 1/32 - 171.7ms/batch - loss: 49.48917 - diff: 22.89mlTrain batch 2/32 - 171.7ms/batch - loss: 84.15617 - diff: 30.64mlTrain batch 3/32 - 171.9ms/batch - loss: 80.87567 - diff: 30.73mlTrain batch 4/32 - 171.8ms/batch - loss: 111.77287 - diff: 34.37mlTrain batch 5/32 - 171.9ms/batch - loss: 116.15336 - diff: 34.84mlTrain batch 6/32 - 172.6ms/batch - loss: 118.82249 - diff: 34.08mlTrain batch 7/32 - 172.1ms/batch - loss: 198.10332 - diff: 37.22mlTrain batch 8/32 - 171.7ms/batch - loss: 191.99771 - diff: 36.86mlTrain batch 9/32 - 172.3ms/batch - loss: 184.95559 - diff: 37.26mlTrain batch 10/32 - 172.6ms/batch - loss: 187.70574 - diff: 37.59mlTrain batch 11/32 - 172.1ms/batch - loss: 181.02823 - diff: 37.61mlTrain batch 12/32 - 172.7ms/batch - loss: 184.29558 - diff: 38.40mlTrain batch 13/32 - 172.2ms/batch - loss: 194.43184 - diff: 39.72mlTrain batch 14/32 - 172.3ms/batch - loss: 191.79292 - diff: 39.88mlTrain batch 15/32 - 172.4ms/batch - loss: 186.16707 - diff: 39.52mlTrain batch 16/32 - 172.3ms/batch - loss: 187.13746 - diff: 39.64mlTrain batch 17/32 - 172.2ms/batch - loss: 195.28719 - diff: 40.27mlTrain batch 18/32 - 172.6ms/batch - loss: 190.81258 - diff: 39.92mlTrain batch 19/32 - 172.5ms/batch - loss: 186.56409 - diff: 39.61mlTrain batch 20/32 - 172.3ms/batch - loss: 180.21048 - diff: 38.95mlTrain batch 21/32 - 172.6ms/batch - loss: 174.84357 - diff: 38.28mlTrain batch 22/32 - 172.3ms/batch - loss: 176.34980 - diff: 38.83mlTrain batch 23/32 - 172.0ms/batch - loss: 176.47594 - diff: 39.14mlTrain batch 24/32 - 172.6ms/batch - loss: 173.91082 - diff: 39.08mlTrain batch 25/32 - 171.7ms/batch - loss: 174.78854 - diff: 39.24mlTrain batch 26/32 - 171.0ms/batch - loss: 177.75101 - diff: 39.44mlTrain batch 27/32 - 172.5ms/batch - loss: 176.32798 - diff: 39.26mlTrain batch 28/32 - 172.1ms/batch - loss: 175.31187 - diff: 39.22mlTrain batch 29/32 - 172.2ms/batch - loss: 178.32374 - diff: 39.57mlTrain batch 30/32 - 172.1ms/batch - loss: 181.41506 - diff: 39.94mlTrain batch 31/32 - 172.2ms/batch - loss: 179.18010 - diff: 39.87mlTrain batch 32/32 - 53.2ms/batch - loss: 180.18722 - diff: 39.81mlTrain batch 32/32 - 10.4s 53.2ms/batch - loss: 180.18722 - diff: 39.81ml
Test 1.1s: val_loss: 161.66657 - diff: 40.39ml

Epoch 30: current best loss = 147.16273, at epoch 28
Train batch 1/32 - 171.9ms/batch - loss: 83.63245 - diff: 29.03mlTrain batch 2/32 - 172.6ms/batch - loss: 147.78220 - diff: 35.41mlTrain batch 3/32 - 172.0ms/batch - loss: 121.80715 - diff: 32.53mlTrain batch 4/32 - 172.2ms/batch - loss: 124.35346 - diff: 33.79mlTrain batch 5/32 - 172.1ms/batch - loss: 122.43995 - diff: 34.33mlTrain batch 6/32 - 172.6ms/batch - loss: 128.67059 - diff: 35.75mlTrain batch 7/32 - 172.4ms/batch - loss: 136.65064 - diff: 36.08mlTrain batch 8/32 - 172.3ms/batch - loss: 137.23435 - diff: 36.51mlTrain batch 9/32 - 171.6ms/batch - loss: 130.46910 - diff: 35.26mlTrain batch 10/32 - 171.7ms/batch - loss: 131.94071 - diff: 35.56mlTrain batch 11/32 - 172.4ms/batch - loss: 146.16214 - diff: 36.68mlTrain batch 12/32 - 172.4ms/batch - loss: 149.73041 - diff: 37.46mlTrain batch 13/32 - 172.1ms/batch - loss: 154.07293 - diff: 38.16mlTrain batch 14/32 - 172.7ms/batch - loss: 188.16975 - diff: 39.65mlTrain batch 15/32 - 172.4ms/batch - loss: 184.85196 - diff: 39.40mlTrain batch 16/32 - 172.4ms/batch - loss: 182.43472 - diff: 39.18mlTrain batch 17/32 - 172.2ms/batch - loss: 181.27075 - diff: 39.49mlTrain batch 18/32 - 172.1ms/batch - loss: 177.13053 - diff: 39.08mlTrain batch 19/32 - 172.5ms/batch - loss: 176.80325 - diff: 39.13mlTrain batch 20/32 - 172.4ms/batch - loss: 179.01850 - diff: 39.58mlTrain batch 21/32 - 171.9ms/batch - loss: 177.39718 - diff: 39.66mlTrain batch 22/32 - 172.5ms/batch - loss: 175.74761 - diff: 39.70mlTrain batch 23/32 - 172.1ms/batch - loss: 172.08419 - diff: 39.32mlTrain batch 24/32 - 172.7ms/batch - loss: 170.63016 - diff: 39.35mlTrain batch 25/32 - 172.2ms/batch - loss: 169.97270 - diff: 39.33mlTrain batch 26/32 - 172.4ms/batch - loss: 167.28041 - diff: 38.97mlTrain batch 27/32 - 171.9ms/batch - loss: 167.85968 - diff: 39.17mlTrain batch 28/32 - 172.4ms/batch - loss: 166.91179 - diff: 39.13mlTrain batch 29/32 - 172.3ms/batch - loss: 166.20748 - diff: 39.17mlTrain batch 30/32 - 172.9ms/batch - loss: 169.10237 - diff: 39.34mlTrain batch 31/32 - 172.2ms/batch - loss: 172.19887 - diff: 39.42mlTrain batch 32/32 - 53.2ms/batch - loss: 172.65404 - diff: 39.29mlTrain batch 32/32 - 10.5s 53.2ms/batch - loss: 172.65404 - diff: 39.29ml
Test 1.1s: val_loss: 153.29272 - diff: 39.53ml

Epoch 31: current best loss = 147.16273, at epoch 28
Train batch 1/32 - 172.4ms/batch - loss: 151.50208 - diff: 33.24mlTrain batch 2/32 - 171.8ms/batch - loss: 134.00426 - diff: 32.72mlTrain batch 3/32 - 172.1ms/batch - loss: 151.92206 - diff: 35.34mlTrain batch 4/32 - 172.5ms/batch - loss: 150.36081 - diff: 36.58mlTrain batch 5/32 - 172.0ms/batch - loss: 142.75289 - diff: 36.75mlTrain batch 6/32 - 172.4ms/batch - loss: 134.86784 - diff: 36.32mlTrain batch 7/32 - 172.1ms/batch - loss: 127.80264 - diff: 35.36mlTrain batch 8/32 - 172.6ms/batch - loss: 126.94763 - diff: 35.28mlTrain batch 9/32 - 172.1ms/batch - loss: 128.12001 - diff: 35.76mlTrain batch 10/32 - 172.8ms/batch - loss: 132.41769 - diff: 36.60mlTrain batch 11/32 - 172.3ms/batch - loss: 140.36536 - diff: 36.91mlTrain batch 12/32 - 172.4ms/batch - loss: 142.87260 - diff: 36.98mlTrain batch 13/32 - 171.9ms/batch - loss: 143.48610 - diff: 36.88mlTrain batch 14/32 - 172.5ms/batch - loss: 150.68521 - diff: 37.51mlTrain batch 15/32 - 171.9ms/batch - loss: 158.00800 - diff: 38.76mlTrain batch 16/32 - 171.4ms/batch - loss: 157.44077 - diff: 38.73mlTrain batch 17/32 - 172.3ms/batch - loss: 156.27068 - diff: 38.90mlTrain batch 18/32 - 172.5ms/batch - loss: 154.58404 - diff: 38.73mlTrain batch 19/32 - 172.3ms/batch - loss: 152.32080 - diff: 38.26mlTrain batch 20/32 - 172.5ms/batch - loss: 155.53312 - diff: 38.79mlTrain batch 21/32 - 172.4ms/batch - loss: 155.30704 - diff: 38.83mlTrain batch 22/32 - 172.6ms/batch - loss: 154.80167 - diff: 38.88mlTrain batch 23/32 - 172.4ms/batch - loss: 154.34432 - diff: 38.74mlTrain batch 24/32 - 172.9ms/batch - loss: 152.95265 - diff: 38.60mlTrain batch 25/32 - 172.2ms/batch - loss: 172.03688 - diff: 38.84mlTrain batch 26/32 - 172.2ms/batch - loss: 176.74358 - diff: 39.26mlTrain batch 27/32 - 171.7ms/batch - loss: 172.36103 - diff: 38.67mlTrain batch 28/32 - 172.6ms/batch - loss: 174.57258 - diff: 38.99mlTrain batch 29/32 - 172.2ms/batch - loss: 176.93636 - diff: 38.88mlTrain batch 30/32 - 172.3ms/batch - loss: 175.39373 - diff: 38.76mlTrain batch 31/32 - 172.4ms/batch - loss: 174.57910 - diff: 38.65mlTrain batch 32/32 - 53.1ms/batch - loss: 179.53336 - diff: 38.74mlTrain batch 32/32 - 10.6s 53.1ms/batch - loss: 179.53336 - diff: 38.74ml
Test 1.1s: val_loss: 170.49693 - diff: 40.85ml

Epoch 32: current best loss = 147.16273, at epoch 28
Train batch 1/32 - 172.0ms/batch - loss: 177.16267 - diff: 42.96mlTrain batch 2/32 - 172.3ms/batch - loss: 174.75593 - diff: 42.92mlTrain batch 3/32 - 172.1ms/batch - loss: 144.13041 - diff: 37.79mlTrain batch 4/32 - 172.4ms/batch - loss: 169.76065 - diff: 40.47mlTrain batch 5/32 - 171.9ms/batch - loss: 166.99578 - diff: 40.99mlTrain batch 6/32 - 171.9ms/batch - loss: 161.89489 - diff: 39.99mlTrain batch 7/32 - 172.3ms/batch - loss: 163.21777 - diff: 40.48mlTrain batch 8/32 - 172.6ms/batch - loss: 155.11237 - diff: 39.21mlTrain batch 9/32 - 172.2ms/batch - loss: 153.56411 - diff: 38.90mlTrain batch 10/32 - 172.4ms/batch - loss: 145.24954 - diff: 37.66mlTrain batch 11/32 - 171.9ms/batch - loss: 143.03759 - diff: 37.73mlTrain batch 12/32 - 172.5ms/batch - loss: 149.39627 - diff: 38.55mlTrain batch 13/32 - 172.1ms/batch - loss: 148.22543 - diff: 38.30mlTrain batch 14/32 - 172.7ms/batch - loss: 148.85623 - diff: 38.72mlTrain batch 15/32 - 172.1ms/batch - loss: 189.11012 - diff: 40.29mlTrain batch 16/32 - 172.3ms/batch - loss: 186.76455 - diff: 39.62mlTrain batch 17/32 - 172.3ms/batch - loss: 185.37592 - diff: 39.73mlTrain batch 18/32 - 172.8ms/batch - loss: 186.12323 - diff: 39.80mlTrain batch 19/32 - 171.9ms/batch - loss: 185.38843 - diff: 39.83mlTrain batch 20/32 - 172.5ms/batch - loss: 179.05188 - diff: 39.07mlTrain batch 21/32 - 172.4ms/batch - loss: 174.71130 - diff: 38.66mlTrain batch 22/32 - 171.5ms/batch - loss: 174.11528 - diff: 38.75mlTrain batch 23/32 - 172.4ms/batch - loss: 173.01333 - diff: 38.74mlTrain batch 24/32 - 172.3ms/batch - loss: 171.30698 - diff: 38.48mlTrain batch 25/32 - 171.9ms/batch - loss: 170.61005 - diff: 38.46mlTrain batch 26/32 - 172.5ms/batch - loss: 171.89507 - diff: 38.75mlTrain batch 27/32 - 172.4ms/batch - loss: 169.76783 - diff: 38.60mlTrain batch 28/32 - 172.5ms/batch - loss: 173.28053 - diff: 38.94mlTrain batch 29/32 - 172.4ms/batch - loss: 171.56454 - diff: 38.77mlTrain batch 30/32 - 172.3ms/batch - loss: 174.67431 - diff: 38.87mlTrain batch 31/32 - 172.4ms/batch - loss: 172.41941 - diff: 38.60mlTrain batch 32/32 - 53.1ms/batch - loss: 186.36031 - diff: 38.92mlTrain batch 32/32 - 10.5s 53.1ms/batch - loss: 186.36031 - diff: 38.92ml
Test 1.1s: val_loss: 157.06935 - diff: 40.56ml

Epoch 33: current best loss = 147.16273, at epoch 28
Train batch 1/32 - 172.0ms/batch - loss: 141.75777 - diff: 38.78mlTrain batch 2/32 - 171.5ms/batch - loss: 180.38627 - diff: 44.09mlTrain batch 3/32 - 172.2ms/batch - loss: 164.62579 - diff: 41.22mlTrain batch 4/32 - 171.8ms/batch - loss: 156.44547 - diff: 40.28mlTrain batch 5/32 - 171.9ms/batch - loss: 146.46336 - diff: 38.92mlTrain batch 6/32 - 172.4ms/batch - loss: 131.82978 - diff: 36.64mlTrain batch 7/32 - 171.8ms/batch - loss: 138.00443 - diff: 37.41mlTrain batch 8/32 - 172.6ms/batch - loss: 156.60237 - diff: 38.70mlTrain batch 9/32 - 172.1ms/batch - loss: 156.46462 - diff: 38.75mlTrain batch 10/32 - 172.6ms/batch - loss: 162.64304 - diff: 39.42mlTrain batch 11/32 - 172.2ms/batch - loss: 159.29336 - diff: 39.25mlTrain batch 12/32 - 172.5ms/batch - loss: 152.44629 - diff: 38.35mlTrain batch 13/32 - 171.9ms/batch - loss: 150.23283 - diff: 37.88mlTrain batch 14/32 - 172.3ms/batch - loss: 143.72138 - diff: 36.92mlTrain batch 15/32 - 172.1ms/batch - loss: 144.34107 - diff: 37.02mlTrain batch 16/32 - 172.5ms/batch - loss: 175.10202 - diff: 37.40mlTrain batch 17/32 - 171.5ms/batch - loss: 172.71752 - diff: 37.36mlTrain batch 18/32 - 171.6ms/batch - loss: 176.23419 - diff: 37.53mlTrain batch 19/32 - 172.2ms/batch - loss: 179.45587 - diff: 37.88mlTrain batch 20/32 - 172.5ms/batch - loss: 175.67339 - diff: 37.71mlTrain batch 21/32 - 172.2ms/batch - loss: 175.53698 - diff: 37.92mlTrain batch 22/32 - 172.6ms/batch - loss: 176.46351 - diff: 38.45mlTrain batch 23/32 - 171.9ms/batch - loss: 174.52052 - diff: 38.12mlTrain batch 24/32 - 172.5ms/batch - loss: 172.72708 - diff: 37.90mlTrain batch 25/32 - 172.2ms/batch - loss: 170.91254 - diff: 37.88mlTrain batch 26/32 - 172.6ms/batch - loss: 170.90976 - diff: 38.18mlTrain batch 27/32 - 171.9ms/batch - loss: 171.94026 - diff: 38.54mlTrain batch 28/32 - 172.7ms/batch - loss: 173.46798 - diff: 38.86mlTrain batch 29/32 - 172.2ms/batch - loss: 171.41963 - diff: 38.77mlTrain batch 30/32 - 172.8ms/batch - loss: 172.08510 - diff: 38.90mlTrain batch 31/32 - 172.4ms/batch - loss: 169.64352 - diff: 38.65mlTrain batch 32/32 - 53.1ms/batch - loss: 169.08372 - diff: 38.48mlTrain batch 32/32 - 10.5s 53.1ms/batch - loss: 169.08372 - diff: 38.48ml
Test 1.1s: val_loss: 145.63949 - diff: 37.63ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 34: current best loss = 145.63949, at epoch 33
Train batch 1/32 - 171.4ms/batch - loss: 132.77826 - diff: 39.04mlTrain batch 2/32 - 172.0ms/batch - loss: 151.21354 - diff: 39.17mlTrain batch 3/32 - 171.6ms/batch - loss: 145.53423 - diff: 39.56mlTrain batch 4/32 - 172.3ms/batch - loss: 172.55859 - diff: 42.48mlTrain batch 5/32 - 172.2ms/batch - loss: 178.44534 - diff: 43.66mlTrain batch 6/32 - 181.8ms/batch - loss: 176.98476 - diff: 42.70mlTrain batch 7/32 - 172.0ms/batch - loss: 167.64387 - diff: 41.11mlTrain batch 8/32 - 172.3ms/batch - loss: 161.20773 - diff: 40.30mlTrain batch 9/32 - 172.0ms/batch - loss: 157.77911 - diff: 39.82mlTrain batch 10/32 - 172.4ms/batch - loss: 156.92833 - diff: 39.50mlTrain batch 11/32 - 171.7ms/batch - loss: 149.21003 - diff: 38.11mlTrain batch 12/32 - 171.6ms/batch - loss: 146.25285 - diff: 37.74mlTrain batch 13/32 - 172.2ms/batch - loss: 145.77738 - diff: 37.83mlTrain batch 14/32 - 172.6ms/batch - loss: 140.27856 - diff: 37.22mlTrain batch 15/32 - 172.0ms/batch - loss: 145.05398 - diff: 37.50mlTrain batch 16/32 - 172.3ms/batch - loss: 139.54838 - diff: 36.69mlTrain batch 17/32 - 172.0ms/batch - loss: 140.27196 - diff: 36.63mlTrain batch 18/32 - 172.5ms/batch - loss: 145.56376 - diff: 37.11mlTrain batch 19/32 - 172.3ms/batch - loss: 156.81583 - diff: 38.10mlTrain batch 20/32 - 172.6ms/batch - loss: 152.62085 - diff: 37.65mlTrain batch 21/32 - 172.4ms/batch - loss: 152.34070 - diff: 37.76mlTrain batch 22/32 - 172.2ms/batch - loss: 151.62174 - diff: 37.86mlTrain batch 23/32 - 172.3ms/batch - loss: 150.03566 - diff: 37.40mlTrain batch 24/32 - 172.3ms/batch - loss: 150.20287 - diff: 37.37mlTrain batch 25/32 - 172.2ms/batch - loss: 148.50750 - diff: 37.20mlTrain batch 26/32 - 172.3ms/batch - loss: 150.96280 - diff: 37.50mlTrain batch 27/32 - 172.3ms/batch - loss: 147.05218 - diff: 36.92mlTrain batch 28/32 - 172.4ms/batch - loss: 145.64452 - diff: 36.76mlTrain batch 29/32 - 172.4ms/batch - loss: 163.49662 - diff: 37.62mlTrain batch 30/32 - 172.5ms/batch - loss: 166.20895 - diff: 37.80mlTrain batch 31/32 - 172.1ms/batch - loss: 163.86333 - diff: 37.45mlTrain batch 32/32 - 53.2ms/batch - loss: 165.86728 - diff: 37.44mlTrain batch 32/32 - 10.5s 53.2ms/batch - loss: 165.86728 - diff: 37.44ml
Test 1.1s: val_loss: 142.45341 - diff: 38.37ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 35: current best loss = 142.45341, at epoch 34
Train batch 1/32 - 171.7ms/batch - loss: 124.84349 - diff: 36.38mlTrain batch 2/32 - 172.1ms/batch - loss: 132.73939 - diff: 36.79mlTrain batch 3/32 - 171.8ms/batch - loss: 128.84454 - diff: 37.54mlTrain batch 4/32 - 172.0ms/batch - loss: 125.10974 - diff: 36.03mlTrain batch 5/32 - 171.9ms/batch - loss: 119.47619 - diff: 35.85mlTrain batch 6/32 - 172.2ms/batch - loss: 127.43320 - diff: 36.39mlTrain batch 7/32 - 171.7ms/batch - loss: 138.27618 - diff: 38.03mlTrain batch 8/32 - 172.0ms/batch - loss: 132.78602 - diff: 36.97mlTrain batch 9/32 - 171.7ms/batch - loss: 125.53877 - diff: 35.79mlTrain batch 10/32 - 172.2ms/batch - loss: 130.55501 - diff: 36.40mlTrain batch 11/32 - 172.2ms/batch - loss: 132.31418 - diff: 36.54mlTrain batch 12/32 - 173.0ms/batch - loss: 174.51449 - diff: 38.31mlTrain batch 13/32 - 172.3ms/batch - loss: 170.81181 - diff: 37.97mlTrain batch 14/32 - 172.3ms/batch - loss: 165.85002 - diff: 37.58mlTrain batch 15/32 - 172.2ms/batch - loss: 170.71427 - diff: 38.03mlTrain batch 16/32 - 172.4ms/batch - loss: 168.18942 - diff: 37.81mlTrain batch 17/32 - 171.8ms/batch - loss: 164.68451 - diff: 37.64mlTrain batch 18/32 - 171.5ms/batch - loss: 162.44094 - diff: 37.60mlTrain batch 19/32 - 172.2ms/batch - loss: 160.51761 - diff: 37.64mlTrain batch 20/32 - 172.5ms/batch - loss: 163.26805 - diff: 37.90mlTrain batch 21/32 - 171.9ms/batch - loss: 158.15359 - diff: 37.18mlTrain batch 22/32 - 172.5ms/batch - loss: 162.87400 - diff: 37.59mlTrain batch 23/32 - 172.1ms/batch - loss: 161.11817 - diff: 37.58mlTrain batch 24/32 - 172.4ms/batch - loss: 165.10106 - diff: 38.17mlTrain batch 25/32 - 172.2ms/batch - loss: 167.05552 - diff: 38.43mlTrain batch 26/32 - 172.6ms/batch - loss: 168.17768 - diff: 38.61mlTrain batch 27/32 - 172.3ms/batch - loss: 164.76459 - diff: 38.31mlTrain batch 28/32 - 172.5ms/batch - loss: 166.17829 - diff: 38.64mlTrain batch 29/32 - 172.2ms/batch - loss: 162.75001 - diff: 38.20mlTrain batch 30/32 - 172.4ms/batch - loss: 161.37633 - diff: 38.00mlTrain batch 31/32 - 172.4ms/batch - loss: 163.33014 - diff: 38.28mlTrain batch 32/32 - 53.1ms/batch - loss: 165.81749 - diff: 38.29mlTrain batch 32/32 - 10.5s 53.1ms/batch - loss: 165.81749 - diff: 38.29ml
Test 1.1s: val_loss: 136.82670 - diff: 36.33ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 36: current best loss = 136.82670, at epoch 35
Train batch 1/32 - 171.3ms/batch - loss: 194.16528 - diff: 47.31mlTrain batch 2/32 - 172.2ms/batch - loss: 222.87186 - diff: 47.80mlTrain batch 3/32 - 171.9ms/batch - loss: 193.78910 - diff: 45.04mlTrain batch 4/32 - 172.0ms/batch - loss: 184.09081 - diff: 43.34mlTrain batch 5/32 - 171.8ms/batch - loss: 188.06006 - diff: 43.56mlTrain batch 6/32 - 172.0ms/batch - loss: 177.69464 - diff: 41.94mlTrain batch 7/32 - 172.5ms/batch - loss: 185.67437 - diff: 43.20mlTrain batch 8/32 - 172.3ms/batch - loss: 197.28624 - diff: 43.33mlTrain batch 9/32 - 172.1ms/batch - loss: 186.64985 - diff: 42.21mlTrain batch 10/32 - 172.6ms/batch - loss: 180.92656 - diff: 41.63mlTrain batch 11/32 - 172.2ms/batch - loss: 173.12215 - diff: 40.42mlTrain batch 12/32 - 172.5ms/batch - loss: 174.21966 - diff: 40.38mlTrain batch 13/32 - 172.4ms/batch - loss: 177.45641 - diff: 40.77mlTrain batch 14/32 - 171.5ms/batch - loss: 173.53295 - diff: 40.57mlTrain batch 15/32 - 172.2ms/batch - loss: 168.95759 - diff: 40.03mlTrain batch 16/32 - 172.4ms/batch - loss: 166.75359 - diff: 39.84mlTrain batch 17/32 - 172.3ms/batch - loss: 162.54764 - diff: 39.28mlTrain batch 18/32 - 172.3ms/batch - loss: 164.20945 - diff: 39.22mlTrain batch 19/32 - 172.3ms/batch - loss: 158.98893 - diff: 38.66mlTrain batch 20/32 - 172.5ms/batch - loss: 160.03921 - diff: 38.74mlTrain batch 21/32 - 172.2ms/batch - loss: 155.53112 - diff: 38.19mlTrain batch 22/32 - 172.3ms/batch - loss: 177.80543 - diff: 38.75mlTrain batch 23/32 - 172.2ms/batch - loss: 175.35015 - diff: 38.56mlTrain batch 24/32 - 172.5ms/batch - loss: 172.15642 - diff: 38.29mlTrain batch 25/32 - 172.0ms/batch - loss: 169.13334 - diff: 38.01mlTrain batch 26/32 - 172.6ms/batch - loss: 173.14276 - diff: 38.59mlTrain batch 27/32 - 172.2ms/batch - loss: 171.10943 - diff: 38.38mlTrain batch 28/32 - 172.5ms/batch - loss: 172.38734 - diff: 38.50mlTrain batch 29/32 - 172.0ms/batch - loss: 169.50293 - diff: 38.06mlTrain batch 30/32 - 171.7ms/batch - loss: 166.97591 - diff: 37.86mlTrain batch 31/32 - 172.2ms/batch - loss: 165.90762 - diff: 37.88mlTrain batch 32/32 - 53.2ms/batch - loss: 173.60653 - diff: 37.97mlTrain batch 32/32 - 10.5s 53.2ms/batch - loss: 173.60653 - diff: 37.97ml
Test 1.1s: val_loss: 145.36010 - diff: 38.20ml

Epoch 37: current best loss = 136.82670, at epoch 35
Train batch 1/32 - 172.0ms/batch - loss: 166.51160 - diff: 43.37mlTrain batch 2/32 - 172.0ms/batch - loss: 215.55472 - diff: 42.60mlTrain batch 3/32 - 172.0ms/batch - loss: 221.65450 - diff: 43.93mlTrain batch 4/32 - 172.3ms/batch - loss: 196.61653 - diff: 42.00mlTrain batch 5/32 - 172.1ms/batch - loss: 186.12327 - diff: 39.96mlTrain batch 6/32 - 172.3ms/batch - loss: 170.95887 - diff: 38.50mlTrain batch 7/32 - 172.3ms/batch - loss: 241.36825 - diff: 41.20mlTrain batch 8/32 - 172.3ms/batch - loss: 220.89896 - diff: 40.04mlTrain batch 9/32 - 171.3ms/batch - loss: 220.27520 - diff: 39.87mlTrain batch 10/32 - 172.5ms/batch - loss: 213.19211 - diff: 39.42mlTrain batch 11/32 - 172.2ms/batch - loss: 204.29185 - diff: 39.13mlTrain batch 12/32 - 172.5ms/batch - loss: 193.15084 - diff: 37.89mlTrain batch 13/32 - 172.1ms/batch - loss: 197.78480 - diff: 39.17mlTrain batch 14/32 - 172.4ms/batch - loss: 193.75633 - diff: 39.36mlTrain batch 15/32 - 172.2ms/batch - loss: 191.99101 - diff: 39.59mlTrain batch 16/32 - 172.5ms/batch - loss: 188.17979 - diff: 39.23mlTrain batch 17/32 - 172.2ms/batch - loss: 183.37211 - diff: 38.99mlTrain batch 18/32 - 172.6ms/batch - loss: 185.62641 - diff: 39.06mlTrain batch 19/32 - 172.3ms/batch - loss: 183.53861 - diff: 39.20mlTrain batch 20/32 - 172.5ms/batch - loss: 181.59725 - diff: 38.95mlTrain batch 21/32 - 172.2ms/batch - loss: 179.96606 - diff: 39.02mlTrain batch 22/32 - 172.4ms/batch - loss: 177.86365 - diff: 39.05mlTrain batch 23/32 - 171.7ms/batch - loss: 177.19989 - diff: 39.05mlTrain batch 24/32 - 172.4ms/batch - loss: 176.54432 - diff: 39.09mlTrain batch 25/32 - 172.2ms/batch - loss: 176.54060 - diff: 39.02mlTrain batch 26/32 - 172.7ms/batch - loss: 176.61559 - diff: 39.30mlTrain batch 27/32 - 172.3ms/batch - loss: 176.68500 - diff: 39.20mlTrain batch 28/32 - 172.8ms/batch - loss: 172.40588 - diff: 38.62mlTrain batch 29/32 - 172.5ms/batch - loss: 169.14209 - diff: 38.41mlTrain batch 30/32 - 172.4ms/batch - loss: 166.38812 - diff: 38.13mlTrain batch 31/32 - 172.4ms/batch - loss: 165.16993 - diff: 38.18mlTrain batch 32/32 - 53.2ms/batch - loss: 167.76015 - diff: 38.14mlTrain batch 32/32 - 10.5s 53.2ms/batch - loss: 167.76015 - diff: 38.14ml
Test 1.1s: val_loss: 127.18598 - diff: 35.26ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 38: current best loss = 127.18598, at epoch 37
Train batch 1/32 - 172.2ms/batch - loss: 157.64864 - diff: 44.60mlTrain batch 2/32 - 172.2ms/batch - loss: 147.58395 - diff: 39.91mlTrain batch 3/32 - 171.9ms/batch - loss: 160.08364 - diff: 40.18mlTrain batch 4/32 - 172.3ms/batch - loss: 146.53165 - diff: 38.18mlTrain batch 5/32 - 172.0ms/batch - loss: 134.84656 - diff: 36.77mlTrain batch 6/32 - 172.2ms/batch - loss: 236.91457 - diff: 41.00mlTrain batch 7/32 - 172.2ms/batch - loss: 216.50715 - diff: 39.41mlTrain batch 8/32 - 172.7ms/batch - loss: 200.69357 - diff: 38.14mlTrain batch 9/32 - 172.3ms/batch - loss: 192.72717 - diff: 38.26mlTrain batch 10/32 - 172.4ms/batch - loss: 187.39630 - diff: 37.78mlTrain batch 11/32 - 172.2ms/batch - loss: 189.20833 - diff: 38.19mlTrain batch 12/32 - 172.3ms/batch - loss: 180.94376 - diff: 37.67mlTrain batch 13/32 - 171.9ms/batch - loss: 203.35718 - diff: 39.93mlTrain batch 14/32 - 171.9ms/batch - loss: 197.88120 - diff: 39.68mlTrain batch 15/32 - 172.4ms/batch - loss: 187.23736 - diff: 38.27mlTrain batch 16/32 - 172.5ms/batch - loss: 198.99802 - diff: 39.83mlTrain batch 17/32 - 172.4ms/batch - loss: 195.83035 - diff: 39.60mlTrain batch 18/32 - 172.5ms/batch - loss: 201.49022 - diff: 40.09mlTrain batch 19/32 - 172.3ms/batch - loss: 199.03917 - diff: 40.17mlTrain batch 20/32 - 172.7ms/batch - loss: 192.17099 - diff: 39.40mlTrain batch 21/32 - 172.1ms/batch - loss: 187.08035 - diff: 39.04mlTrain batch 22/32 - 172.9ms/batch - loss: 185.78866 - diff: 39.26mlTrain batch 23/32 - 172.2ms/batch - loss: 184.84063 - diff: 39.32mlTrain batch 24/32 - 172.7ms/batch - loss: 182.06565 - diff: 39.32mlTrain batch 25/32 - 172.3ms/batch - loss: 182.01410 - diff: 39.48mlTrain batch 26/32 - 172.5ms/batch - loss: 180.15752 - diff: 39.32mlTrain batch 27/32 - 172.4ms/batch - loss: 177.09293 - diff: 39.22mlTrain batch 28/32 - 172.3ms/batch - loss: 172.28328 - diff: 38.64mlTrain batch 29/32 - 171.6ms/batch - loss: 170.53512 - diff: 38.48mlTrain batch 30/32 - 171.2ms/batch - loss: 166.54302 - diff: 37.93mlTrain batch 31/32 - 172.4ms/batch - loss: 166.63490 - diff: 37.88mlTrain batch 32/32 - 53.2ms/batch - loss: 167.49815 - diff: 37.76mlTrain batch 32/32 - 10.5s 53.2ms/batch - loss: 167.49815 - diff: 37.76ml
Test 1.1s: val_loss: 131.06248 - diff: 34.60ml

Epoch 39: current best loss = 127.18598, at epoch 37
Train batch 1/32 - 172.0ms/batch - loss: 240.27066 - diff: 49.72mlTrain batch 2/32 - 172.4ms/batch - loss: 197.27333 - diff: 46.75mlTrain batch 3/32 - 172.4ms/batch - loss: 342.99726 - diff: 50.99mlTrain batch 4/32 - 172.4ms/batch - loss: 275.48323 - diff: 46.05mlTrain batch 5/32 - 172.2ms/batch - loss: 258.76415 - diff: 44.64mlTrain batch 6/32 - 172.7ms/batch - loss: 243.64531 - diff: 43.54mlTrain batch 7/32 - 172.1ms/batch - loss: 233.57699 - diff: 43.32mlTrain batch 8/32 - 172.4ms/batch - loss: 223.54127 - diff: 42.43mlTrain batch 9/32 - 172.1ms/batch - loss: 213.22598 - diff: 41.80mlTrain batch 10/32 - 172.6ms/batch - loss: 204.43652 - diff: 41.13mlTrain batch 11/32 - 172.1ms/batch - loss: 195.14702 - diff: 40.30mlTrain batch 12/32 - 172.5ms/batch - loss: 185.20630 - diff: 39.48mlTrain batch 13/32 - 172.1ms/batch - loss: 178.80103 - diff: 38.93mlTrain batch 14/32 - 172.6ms/batch - loss: 173.15824 - diff: 38.34mlTrain batch 15/32 - 171.8ms/batch - loss: 170.90537 - diff: 38.20mlTrain batch 16/32 - 171.5ms/batch - loss: 167.68410 - diff: 37.86mlTrain batch 17/32 - 172.1ms/batch - loss: 162.67462 - diff: 37.41mlTrain batch 18/32 - 172.5ms/batch - loss: 158.11991 - diff: 36.90mlTrain batch 19/32 - 172.3ms/batch - loss: 158.79138 - diff: 37.36mlTrain batch 20/32 - 172.5ms/batch - loss: 155.93035 - diff: 37.28mlTrain batch 21/32 - 172.0ms/batch - loss: 157.23926 - diff: 37.63mlTrain batch 22/32 - 172.7ms/batch - loss: 157.50198 - diff: 37.69mlTrain batch 23/32 - 172.1ms/batch - loss: 157.91067 - diff: 37.84mlTrain batch 24/32 - 172.6ms/batch - loss: 159.77235 - diff: 38.19mlTrain batch 25/32 - 172.3ms/batch - loss: 160.25286 - diff: 38.30mlTrain batch 26/32 - 172.5ms/batch - loss: 160.78024 - diff: 38.52mlTrain batch 27/32 - 172.2ms/batch - loss: 161.70256 - diff: 38.37mlTrain batch 28/32 - 172.6ms/batch - loss: 157.67645 - diff: 37.85mlTrain batch 29/32 - 172.4ms/batch - loss: 154.91907 - diff: 37.58mlTrain batch 30/32 - 172.6ms/batch - loss: 162.29226 - diff: 38.18mlTrain batch 31/32 - 172.3ms/batch - loss: 158.69272 - diff: 37.66mlTrain batch 32/32 - 53.3ms/batch - loss: 164.63365 - diff: 37.78mlTrain batch 32/32 - 10.5s 53.3ms/batch - loss: 164.63365 - diff: 37.78ml
Test 1.1s: val_loss: 141.24668 - diff: 37.10ml

Epoch 40: current best loss = 127.18598, at epoch 37
Train batch 1/32 - 172.3ms/batch - loss: 112.94899 - diff: 35.94mlTrain batch 2/32 - 172.6ms/batch - loss: 110.70041 - diff: 30.96mlTrain batch 3/32 - 172.2ms/batch - loss: 110.95356 - diff: 32.17mlTrain batch 4/32 - 172.6ms/batch - loss: 96.03265 - diff: 30.39mlTrain batch 5/32 - 172.2ms/batch - loss: 91.38347 - diff: 29.95mlTrain batch 6/32 - 172.6ms/batch - loss: 86.46739 - diff: 29.53mlTrain batch 7/32 - 172.1ms/batch - loss: 82.70320 - diff: 28.63mlTrain batch 8/32 - 172.4ms/batch - loss: 83.30963 - diff: 28.97mlTrain batch 9/32 - 172.3ms/batch - loss: 83.35931 - diff: 28.92mlTrain batch 10/32 - 172.6ms/batch - loss: 86.99737 - diff: 29.33mlTrain batch 11/32 - 172.5ms/batch - loss: 150.91971 - diff: 32.71mlTrain batch 12/32 - 171.6ms/batch - loss: 149.28618 - diff: 33.13mlTrain batch 13/32 - 172.4ms/batch - loss: 152.30238 - diff: 33.79mlTrain batch 14/32 - 172.4ms/batch - loss: 147.67910 - diff: 33.50mlTrain batch 15/32 - 172.3ms/batch - loss: 146.17442 - diff: 33.76mlTrain batch 16/32 - 172.5ms/batch - loss: 150.38991 - diff: 34.51mlTrain batch 17/32 - 172.3ms/batch - loss: 149.72535 - diff: 34.70mlTrain batch 18/32 - 172.6ms/batch - loss: 152.26698 - diff: 35.32mlTrain batch 19/32 - 172.2ms/batch - loss: 151.93252 - diff: 35.64mlTrain batch 20/32 - 172.5ms/batch - loss: 152.28661 - diff: 35.86mlTrain batch 21/32 - 172.2ms/batch - loss: 153.76795 - diff: 36.43mlTrain batch 22/32 - 172.4ms/batch - loss: 152.60182 - diff: 36.19mlTrain batch 23/32 - 172.0ms/batch - loss: 152.97123 - diff: 36.38mlTrain batch 24/32 - 172.5ms/batch - loss: 153.55994 - diff: 36.59mlTrain batch 25/32 - 172.1ms/batch - loss: 152.31060 - diff: 36.54mlTrain batch 26/32 - 172.4ms/batch - loss: 151.88996 - diff: 36.40mlTrain batch 27/32 - 172.3ms/batch - loss: 157.29667 - diff: 36.93mlTrain batch 28/32 - 172.7ms/batch - loss: 155.52943 - diff: 36.86mlTrain batch 29/32 - 172.2ms/batch - loss: 154.98171 - diff: 36.96mlTrain batch 30/32 - 172.6ms/batch - loss: 156.75130 - diff: 36.96mlTrain batch 31/32 - 172.1ms/batch - loss: 153.48480 - diff: 36.60mlTrain batch 32/32 - 53.2ms/batch - loss: 160.24599 - diff: 36.68mlTrain batch 32/32 - 10.5s 53.2ms/batch - loss: 160.24599 - diff: 36.68ml
Test 1.1s: val_loss: 124.93425 - diff: 35.16ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 41: current best loss = 124.93425, at epoch 40
Train batch 1/32 - 171.8ms/batch - loss: 100.19865 - diff: 33.06mlTrain batch 2/32 - 172.5ms/batch - loss: 137.82930 - diff: 37.33mlTrain batch 3/32 - 172.3ms/batch - loss: 132.40802 - diff: 37.78mlTrain batch 4/32 - 172.4ms/batch - loss: 125.36798 - diff: 36.24mlTrain batch 5/32 - 172.3ms/batch - loss: 125.61088 - diff: 34.90mlTrain batch 6/32 - 172.3ms/batch - loss: 131.06517 - diff: 35.63mlTrain batch 7/32 - 172.3ms/batch - loss: 145.40044 - diff: 37.01mlTrain batch 8/32 - 172.5ms/batch - loss: 144.77722 - diff: 37.14mlTrain batch 9/32 - 172.3ms/batch - loss: 138.97601 - diff: 36.45mlTrain batch 10/32 - 172.6ms/batch - loss: 139.30291 - diff: 36.93mlTrain batch 11/32 - 172.3ms/batch - loss: 135.20894 - diff: 36.65mlTrain batch 12/32 - 172.6ms/batch - loss: 130.48349 - diff: 36.14mlTrain batch 13/32 - 172.3ms/batch - loss: 126.23986 - diff: 35.50mlTrain batch 14/32 - 172.5ms/batch - loss: 124.42216 - diff: 35.21mlTrain batch 15/32 - 172.1ms/batch - loss: 125.29338 - diff: 35.50mlTrain batch 16/32 - 172.4ms/batch - loss: 126.83290 - diff: 35.63mlTrain batch 17/32 - 172.4ms/batch - loss: 140.76951 - diff: 36.59mlTrain batch 18/32 - 172.3ms/batch - loss: 138.30130 - diff: 36.31mlTrain batch 19/32 - 172.5ms/batch - loss: 139.41669 - diff: 36.40mlTrain batch 20/32 - 172.5ms/batch - loss: 136.17260 - diff: 35.85mlTrain batch 21/32 - 172.4ms/batch - loss: 133.45565 - diff: 35.39mlTrain batch 22/32 - 172.3ms/batch - loss: 133.92866 - diff: 35.57mlTrain batch 23/32 - 171.9ms/batch - loss: 131.07591 - diff: 35.07mlTrain batch 24/32 - 172.5ms/batch - loss: 132.74560 - diff: 35.30mlTrain batch 25/32 - 172.2ms/batch - loss: 132.84070 - diff: 35.33mlTrain batch 26/32 - 172.5ms/batch - loss: 134.41949 - diff: 35.25mlTrain batch 27/32 - 172.2ms/batch - loss: 140.08846 - diff: 36.05mlTrain batch 28/32 - 172.7ms/batch - loss: 139.47801 - diff: 36.01mlTrain batch 29/32 - 172.2ms/batch - loss: 141.04576 - diff: 36.30mlTrain batch 30/32 - 172.7ms/batch - loss: 141.69932 - diff: 36.42mlTrain batch 31/32 - 172.3ms/batch - loss: 155.95404 - diff: 36.97mlTrain batch 32/32 - 53.2ms/batch - loss: 160.66965 - diff: 37.06mlTrain batch 32/32 - 10.5s 53.2ms/batch - loss: 160.66965 - diff: 37.06ml
Test 1.1s: val_loss: 137.59590 - diff: 36.92ml

Epoch 42: current best loss = 124.93425, at epoch 40
Train batch 1/32 - 172.3ms/batch - loss: 104.77261 - diff: 30.97mlTrain batch 2/32 - 171.2ms/batch - loss: 153.86090 - diff: 39.73mlTrain batch 3/32 - 172.5ms/batch - loss: 144.09344 - diff: 38.22mlTrain batch 4/32 - 172.4ms/batch - loss: 145.67621 - diff: 38.49mlTrain batch 5/32 - 172.4ms/batch - loss: 148.57780 - diff: 37.80mlTrain batch 6/32 - 172.4ms/batch - loss: 136.56883 - diff: 35.74mlTrain batch 7/32 - 172.3ms/batch - loss: 127.47266 - diff: 34.49mlTrain batch 8/32 - 172.7ms/batch - loss: 131.14429 - diff: 35.51mlTrain batch 9/32 - 172.5ms/batch - loss: 134.04306 - diff: 35.87mlTrain batch 10/32 - 172.5ms/batch - loss: 127.46074 - diff: 35.05mlTrain batch 11/32 - 172.2ms/batch - loss: 132.70042 - diff: 35.30mlTrain batch 12/32 - 172.9ms/batch - loss: 128.83629 - diff: 34.71mlTrain batch 13/32 - 172.4ms/batch - loss: 128.82710 - diff: 34.78mlTrain batch 14/32 - 172.7ms/batch - loss: 135.34112 - diff: 35.57mlTrain batch 15/32 - 172.3ms/batch - loss: 138.74420 - diff: 35.82mlTrain batch 16/32 - 172.5ms/batch - loss: 151.08636 - diff: 36.86mlTrain batch 17/32 - 172.3ms/batch - loss: 148.74805 - diff: 36.83mlTrain batch 18/32 - 172.5ms/batch - loss: 144.65860 - diff: 36.15mlTrain batch 19/32 - 172.1ms/batch - loss: 140.65200 - diff: 35.37mlTrain batch 20/32 - 172.8ms/batch - loss: 170.35309 - diff: 36.51mlTrain batch 21/32 - 172.2ms/batch - loss: 168.74630 - diff: 36.51mlTrain batch 22/32 - 172.5ms/batch - loss: 165.48464 - diff: 36.22mlTrain batch 23/32 - 172.3ms/batch - loss: 171.35306 - diff: 36.80mlTrain batch 24/32 - 172.6ms/batch - loss: 170.42169 - diff: 36.78mlTrain batch 25/32 - 172.4ms/batch - loss: 170.91339 - diff: 37.13mlTrain batch 26/32 - 172.4ms/batch - loss: 169.27059 - diff: 37.15mlTrain batch 27/32 - 172.5ms/batch - loss: 165.57279 - diff: 36.80mlTrain batch 28/32 - 172.6ms/batch - loss: 161.40833 - diff: 36.31mlTrain batch 29/32 - 172.3ms/batch - loss: 161.81751 - diff: 36.62mlTrain batch 30/32 - 172.6ms/batch - loss: 158.39054 - diff: 36.27mlTrain batch 31/32 - 172.5ms/batch - loss: 155.90042 - diff: 36.10mlTrain batch 32/32 - 53.2ms/batch - loss: 155.67817 - diff: 35.95mlTrain batch 32/32 - 10.5s 53.2ms/batch - loss: 155.67817 - diff: 35.95ml
Test 1.1s: val_loss: 127.38535 - diff: 35.66ml

Epoch 43: current best loss = 124.93425, at epoch 40
Train batch 1/32 - 172.3ms/batch - loss: 163.49617 - diff: 40.59mlTrain batch 2/32 - 172.5ms/batch - loss: 403.55341 - diff: 48.31mlTrain batch 3/32 - 172.4ms/batch - loss: 319.27188 - diff: 44.28mlTrain batch 4/32 - 172.5ms/batch - loss: 272.65912 - diff: 42.31mlTrain batch 5/32 - 172.4ms/batch - loss: 249.65223 - diff: 40.93mlTrain batch 6/32 - 172.6ms/batch - loss: 248.91098 - diff: 42.18mlTrain batch 7/32 - 172.4ms/batch - loss: 220.67559 - diff: 39.47mlTrain batch 8/32 - 172.5ms/batch - loss: 210.95733 - diff: 39.74mlTrain batch 9/32 - 172.2ms/batch - loss: 210.69475 - diff: 40.71mlTrain batch 10/32 - 172.8ms/batch - loss: 203.27326 - diff: 40.47mlTrain batch 11/32 - 172.4ms/batch - loss: 190.61538 - diff: 39.35mlTrain batch 12/32 - 172.3ms/batch - loss: 183.46492 - diff: 39.01mlTrain batch 13/32 - 172.0ms/batch - loss: 174.96105 - diff: 38.38mlTrain batch 14/32 - 172.6ms/batch - loss: 171.72536 - diff: 38.45mlTrain batch 15/32 - 172.3ms/batch - loss: 168.53366 - diff: 38.18mlTrain batch 16/32 - 172.5ms/batch - loss: 165.61132 - diff: 38.11mlTrain batch 17/32 - 172.3ms/batch - loss: 161.68724 - diff: 37.89mlTrain batch 18/32 - 172.6ms/batch - loss: 164.43202 - diff: 38.07mlTrain batch 19/32 - 172.3ms/batch - loss: 164.26167 - diff: 37.98mlTrain batch 20/32 - 172.6ms/batch - loss: 162.81644 - diff: 37.78mlTrain batch 21/32 - 172.4ms/batch - loss: 158.35690 - diff: 37.16mlTrain batch 22/32 - 172.5ms/batch - loss: 157.19521 - diff: 37.25mlTrain batch 23/32 - 172.4ms/batch - loss: 158.13514 - diff: 37.29mlTrain batch 24/32 - 172.6ms/batch - loss: 163.19659 - diff: 37.50mlTrain batch 25/32 - 172.2ms/batch - loss: 159.57123 - diff: 37.15mlTrain batch 26/32 - 172.8ms/batch - loss: 157.12281 - diff: 36.90mlTrain batch 27/32 - 172.3ms/batch - loss: 159.32975 - diff: 37.34mlTrain batch 28/32 - 172.6ms/batch - loss: 158.49017 - diff: 37.28mlTrain batch 29/32 - 172.2ms/batch - loss: 156.64088 - diff: 37.20mlTrain batch 30/32 - 172.7ms/batch - loss: 154.56436 - diff: 36.96mlTrain batch 31/32 - 172.5ms/batch - loss: 151.29334 - diff: 36.50mlTrain batch 32/32 - 53.2ms/batch - loss: 153.46487 - diff: 36.50mlTrain batch 32/32 - 10.5s 53.2ms/batch - loss: 153.46487 - diff: 36.50ml
Test 1.1s: val_loss: 124.33494 - diff: 34.26ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 44: current best loss = 124.33494, at epoch 43
Train batch 1/32 - 171.9ms/batch - loss: 108.78285 - diff: 30.20mlTrain batch 2/32 - 172.3ms/batch - loss: 162.15195 - diff: 37.85mlTrain batch 3/32 - 171.6ms/batch - loss: 138.29438 - diff: 35.03mlTrain batch 4/32 - 172.2ms/batch - loss: 135.83728 - diff: 34.70mlTrain batch 5/32 - 171.8ms/batch - loss: 129.05812 - diff: 33.74mlTrain batch 6/32 - 172.3ms/batch - loss: 128.09045 - diff: 34.08mlTrain batch 7/32 - 171.9ms/batch - loss: 129.81843 - diff: 34.72mlTrain batch 8/32 - 172.6ms/batch - loss: 127.73693 - diff: 34.97mlTrain batch 9/32 - 172.2ms/batch - loss: 124.94934 - diff: 34.50mlTrain batch 10/32 - 172.6ms/batch - loss: 126.45738 - diff: 34.30mlTrain batch 11/32 - 172.2ms/batch - loss: 120.82544 - diff: 33.56mlTrain batch 12/32 - 172.4ms/batch - loss: 118.83655 - diff: 33.20mlTrain batch 13/32 - 172.2ms/batch - loss: 132.77521 - diff: 34.34mlTrain batch 14/32 - 172.5ms/batch - loss: 136.02592 - diff: 34.91mlTrain batch 15/32 - 172.2ms/batch - loss: 135.03447 - diff: 34.80mlTrain batch 16/32 - 172.4ms/batch - loss: 140.36271 - diff: 35.57mlTrain batch 17/32 - 172.3ms/batch - loss: 139.42641 - diff: 35.21mlTrain batch 18/32 - 172.4ms/batch - loss: 137.88719 - diff: 35.20mlTrain batch 19/32 - 172.1ms/batch - loss: 134.00352 - diff: 34.72mlTrain batch 20/32 - 172.4ms/batch - loss: 135.38926 - diff: 34.85mlTrain batch 21/32 - 172.4ms/batch - loss: 137.43300 - diff: 35.37mlTrain batch 22/32 - 172.4ms/batch - loss: 139.97377 - diff: 35.81mlTrain batch 23/32 - 172.1ms/batch - loss: 139.49365 - diff: 35.89mlTrain batch 24/32 - 172.7ms/batch - loss: 136.90540 - diff: 35.64mlTrain batch 25/32 - 172.3ms/batch - loss: 136.29540 - diff: 35.57mlTrain batch 26/32 - 172.5ms/batch - loss: 137.43248 - diff: 35.64mlTrain batch 27/32 - 171.5ms/batch - loss: 136.26703 - diff: 35.59mlTrain batch 28/32 - 171.9ms/batch - loss: 134.53229 - diff: 35.34mlTrain batch 29/32 - 172.2ms/batch - loss: 139.35017 - diff: 35.66mlTrain batch 30/32 - 172.4ms/batch - loss: 136.96352 - diff: 35.39mlTrain batch 31/32 - 172.4ms/batch - loss: 154.06525 - diff: 36.26mlTrain batch 32/32 - 53.2ms/batch - loss: 162.80018 - diff: 36.53mlTrain batch 32/32 - 10.5s 53.2ms/batch - loss: 162.80018 - diff: 36.53ml
Test 1.1s: val_loss: 121.60858 - diff: 35.20ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 45: current best loss = 121.60858, at epoch 44
Going to unfreeze the pretrained weights
Train batch 1/32 - 233.9ms/batch - loss: 115.20588 - diff: 35.89mlTrain batch 2/32 - 235.8ms/batch - loss: 177.72165 - diff: 43.17mlTrain batch 3/32 - 234.3ms/batch - loss: 230.42934 - diff: 43.78mlTrain batch 4/32 - 236.1ms/batch - loss: 217.75314 - diff: 43.90mlTrain batch 5/32 - 235.9ms/batch - loss: 207.59266 - diff: 42.86mlTrain batch 6/32 - 236.4ms/batch - loss: 198.55725 - diff: 42.63mlTrain batch 7/32 - 236.0ms/batch - loss: 257.86155 - diff: 43.18mlTrain batch 8/32 - 236.3ms/batch - loss: 231.79235 - diff: 40.48mlTrain batch 9/32 - 236.2ms/batch - loss: 214.68739 - diff: 39.35mlTrain batch 10/32 - 236.1ms/batch - loss: 203.31234 - diff: 38.76mlTrain batch 11/32 - 235.9ms/batch - loss: 191.91513 - diff: 37.79mlTrain batch 12/32 - 236.0ms/batch - loss: 219.33046 - diff: 40.38mlTrain batch 13/32 - 236.1ms/batch - loss: 207.73333 - diff: 39.32mlTrain batch 14/32 - 236.0ms/batch - loss: 209.72741 - diff: 39.91mlTrain batch 15/32 - 235.9ms/batch - loss: 201.08241 - diff: 39.29mlTrain batch 16/32 - 236.4ms/batch - loss: 197.49418 - diff: 39.34mlTrain batch 17/32 - 236.0ms/batch - loss: 195.96749 - diff: 39.49mlTrain batch 18/32 - 236.3ms/batch - loss: 191.29277 - diff: 39.24mlTrain batch 19/32 - 236.0ms/batch - loss: 190.14882 - diff: 39.16mlTrain batch 20/32 - 236.2ms/batch - loss: 190.08994 - diff: 39.14mlTrain batch 21/32 - 236.1ms/batch - loss: 183.81731 - diff: 38.44mlTrain batch 22/32 - 236.3ms/batch - loss: 180.75310 - diff: 38.25mlTrain batch 23/32 - 236.1ms/batch - loss: 177.01198 - diff: 37.99mlTrain batch 24/32 - 236.4ms/batch - loss: 178.56912 - diff: 38.23mlTrain batch 25/32 - 236.1ms/batch - loss: 181.09567 - diff: 38.58mlTrain batch 26/32 - 236.7ms/batch - loss: 177.83093 - diff: 38.27mlTrain batch 27/32 - 235.7ms/batch - loss: 173.30923 - diff: 37.81mlTrain batch 28/32 - 237.0ms/batch - loss: 170.13499 - diff: 37.52mlTrain batch 29/32 - 235.9ms/batch - loss: 167.66927 - diff: 37.22mlTrain batch 30/32 - 236.4ms/batch - loss: 167.87480 - diff: 37.39mlTrain batch 31/32 - 236.4ms/batch - loss: 164.60805 - diff: 37.09mlTrain batch 32/32 - 76.6ms/batch - loss: 165.81541 - diff: 37.06mlTrain batch 32/32 - 10.5s 76.6ms/batch - loss: 165.81541 - diff: 37.06ml
Test 1.1s: val_loss: 161.76082 - diff: 39.71ml

Epoch 46: current best loss = 121.60858, at epoch 44
Train batch 1/32 - 236.1ms/batch - loss: 142.65210 - diff: 31.72mlTrain batch 2/32 - 235.8ms/batch - loss: 121.79001 - diff: 31.15mlTrain batch 3/32 - 236.1ms/batch - loss: 123.16578 - diff: 34.44mlTrain batch 4/32 - 236.4ms/batch - loss: 107.24758 - diff: 32.49mlTrain batch 5/32 - 235.7ms/batch - loss: 139.36413 - diff: 36.11mlTrain batch 6/32 - 236.3ms/batch - loss: 147.21254 - diff: 37.37mlTrain batch 7/32 - 235.9ms/batch - loss: 140.08529 - diff: 36.73mlTrain batch 8/32 - 236.6ms/batch - loss: 136.24923 - diff: 35.80mlTrain batch 9/32 - 235.8ms/batch - loss: 126.24022 - diff: 34.29mlTrain batch 10/32 - 236.8ms/batch - loss: 139.89415 - diff: 34.86mlTrain batch 11/32 - 236.1ms/batch - loss: 146.71536 - diff: 36.07mlTrain batch 12/32 - 236.3ms/batch - loss: 155.06689 - diff: 37.07mlTrain batch 13/32 - 236.1ms/batch - loss: 147.86988 - diff: 36.15mlTrain batch 14/32 - 236.6ms/batch - loss: 145.08514 - diff: 36.02mlTrain batch 15/32 - 235.9ms/batch - loss: 149.13706 - diff: 36.95mlTrain batch 16/32 - 236.8ms/batch - loss: 146.94869 - diff: 36.70mlTrain batch 17/32 - 236.1ms/batch - loss: 140.43805 - diff: 35.80mlTrain batch 18/32 - 236.2ms/batch - loss: 135.76831 - diff: 35.22mlTrain batch 19/32 - 236.6ms/batch - loss: 136.32147 - diff: 35.20mlTrain batch 20/32 - 236.5ms/batch - loss: 132.52802 - diff: 34.58mlTrain batch 21/32 - 236.3ms/batch - loss: 130.32512 - diff: 34.29mlTrain batch 22/32 - 236.5ms/batch - loss: 126.63152 - diff: 33.84mlTrain batch 23/32 - 236.2ms/batch - loss: 125.45781 - diff: 33.76mlTrain batch 24/32 - 236.3ms/batch - loss: 123.24403 - diff: 33.51mlTrain batch 25/32 - 236.3ms/batch - loss: 122.81541 - diff: 33.54mlTrain batch 26/32 - 237.2ms/batch - loss: 138.50395 - diff: 34.24mlTrain batch 27/32 - 236.1ms/batch - loss: 135.72470 - diff: 33.92mlTrain batch 28/32 - 236.5ms/batch - loss: 134.91073 - diff: 33.78mlTrain batch 29/32 - 235.7ms/batch - loss: 136.85384 - diff: 34.23mlTrain batch 30/32 - 236.9ms/batch - loss: 138.03102 - diff: 34.46mlTrain batch 31/32 - 236.4ms/batch - loss: 140.06305 - diff: 34.70mlTrain batch 32/32 - 77.7ms/batch - loss: 145.84792 - diff: 34.85mlTrain batch 32/32 - 10.6s 77.7ms/batch - loss: 145.84792 - diff: 34.85ml
Test 1.1s: val_loss: 292.91264 - diff: 54.45ml

Epoch 47: current best loss = 121.60858, at epoch 44
Train batch 1/32 - 235.8ms/batch - loss: 132.01016 - diff: 33.62mlTrain batch 2/32 - 237.0ms/batch - loss: 115.37012 - diff: 31.99mlTrain batch 3/32 - 236.2ms/batch - loss: 102.24998 - diff: 31.39mlTrain batch 4/32 - 237.1ms/batch - loss: 221.68431 - diff: 37.88mlTrain batch 5/32 - 236.5ms/batch - loss: 212.50502 - diff: 38.78mlTrain batch 6/32 - 237.2ms/batch - loss: 194.30299 - diff: 37.20mlTrain batch 7/32 - 236.4ms/batch - loss: 171.21456 - diff: 34.42mlTrain batch 8/32 - 237.1ms/batch - loss: 160.62601 - diff: 34.20mlTrain batch 9/32 - 236.0ms/batch - loss: 157.60931 - diff: 34.30mlTrain batch 10/32 - 236.8ms/batch - loss: 157.38194 - diff: 34.36mlTrain batch 11/32 - 236.0ms/batch - loss: 151.37922 - diff: 34.06mlTrain batch 12/32 - 236.7ms/batch - loss: 146.92656 - diff: 34.03mlTrain batch 13/32 - 236.3ms/batch - loss: 139.48010 - diff: 33.01mlTrain batch 14/32 - 237.1ms/batch - loss: 135.28859 - diff: 32.83mlTrain batch 15/32 - 236.3ms/batch - loss: 143.83198 - diff: 32.90mlTrain batch 16/32 - 236.0ms/batch - loss: 144.08524 - diff: 33.05mlTrain batch 17/32 - 236.2ms/batch - loss: 141.99261 - diff: 33.19mlTrain batch 18/32 - 236.2ms/batch - loss: 138.54123 - diff: 32.90mlTrain batch 19/32 - 236.2ms/batch - loss: 139.38881 - diff: 33.16mlTrain batch 20/32 - 237.6ms/batch - loss: 133.69627 - diff: 32.42mlTrain batch 21/32 - 236.2ms/batch - loss: 136.23671 - diff: 32.98mlTrain batch 22/32 - 237.4ms/batch - loss: 132.96935 - diff: 32.63mlTrain batch 23/32 - 236.3ms/batch - loss: 130.35097 - diff: 32.40mlTrain batch 24/32 - 237.5ms/batch - loss: 127.65375 - diff: 32.28mlTrain batch 25/32 - 236.1ms/batch - loss: 125.89179 - diff: 32.25mlTrain batch 26/32 - 237.5ms/batch - loss: 123.54981 - diff: 31.86mlTrain batch 27/32 - 236.5ms/batch - loss: 123.80394 - diff: 32.12mlTrain batch 28/32 - 237.8ms/batch - loss: 123.80071 - diff: 32.33mlTrain batch 29/32 - 236.4ms/batch - loss: 123.67629 - diff: 32.44mlTrain batch 30/32 - 237.2ms/batch - loss: 120.78300 - diff: 32.09mlTrain batch 31/32 - 236.2ms/batch - loss: 118.41496 - diff: 31.71mlTrain batch 32/32 - 78.0ms/batch - loss: 130.34331 - diff: 31.90mlTrain batch 32/32 - 10.5s 78.0ms/batch - loss: 130.34331 - diff: 31.90ml
Test 1.1s: val_loss: 295.94178 - diff: 60.44ml

Epoch 48: current best loss = 121.60858, at epoch 44
Train batch 1/32 - 236.9ms/batch - loss: 137.97565 - diff: 33.36mlTrain batch 2/32 - 237.2ms/batch - loss: 128.46131 - diff: 34.81mlTrain batch 3/32 - 236.9ms/batch - loss: 103.20931 - diff: 29.70mlTrain batch 4/32 - 236.7ms/batch - loss: 114.27956 - diff: 31.81mlTrain batch 5/32 - 236.0ms/batch - loss: 123.92375 - diff: 33.70mlTrain batch 6/32 - 237.5ms/batch - loss: 122.02241 - diff: 34.22mlTrain batch 7/32 - 236.3ms/batch - loss: 118.95301 - diff: 34.07mlTrain batch 8/32 - 237.4ms/batch - loss: 117.95341 - diff: 33.98mlTrain batch 9/32 - 236.0ms/batch - loss: 116.37641 - diff: 33.99mlTrain batch 10/32 - 237.2ms/batch - loss: 114.08894 - diff: 33.63mlTrain batch 11/32 - 236.5ms/batch - loss: 117.08566 - diff: 34.11mlTrain batch 12/32 - 237.7ms/batch - loss: 116.08365 - diff: 34.17mlTrain batch 13/32 - 236.6ms/batch - loss: 112.99157 - diff: 33.49mlTrain batch 14/32 - 237.6ms/batch - loss: 109.10067 - diff: 32.95mlTrain batch 15/32 - 236.5ms/batch - loss: 109.96775 - diff: 33.12mlTrain batch 16/32 - 237.5ms/batch - loss: 108.22866 - diff: 33.01mlTrain batch 17/32 - 236.7ms/batch - loss: 108.29915 - diff: 33.09mlTrain batch 18/32 - 237.2ms/batch - loss: 104.31518 - diff: 32.35mlTrain batch 19/32 - 236.5ms/batch - loss: 101.35711 - diff: 31.82mlTrain batch 20/32 - 237.1ms/batch - loss: 102.26710 - diff: 31.95mlTrain batch 21/32 - 236.2ms/batch - loss: 106.05020 - diff: 32.43mlTrain batch 22/32 - 237.3ms/batch - loss: 103.77895 - diff: 32.00mlTrain batch 23/32 - 236.6ms/batch - loss: 112.98706 - diff: 32.43mlTrain batch 24/32 - 237.2ms/batch - loss: 110.21920 - diff: 31.91mlTrain batch 25/32 - 236.3ms/batch - loss: 108.80470 - diff: 31.70mlTrain batch 26/32 - 237.3ms/batch - loss: 107.42830 - diff: 31.50mlTrain batch 27/32 - 236.3ms/batch - loss: 106.23286 - diff: 31.39mlTrain batch 28/32 - 237.6ms/batch - loss: 116.00349 - diff: 31.68mlTrain batch 29/32 - 236.5ms/batch - loss: 116.62865 - diff: 31.82mlTrain batch 30/32 - 237.8ms/batch - loss: 117.29547 - diff: 32.07mlTrain batch 31/32 - 236.1ms/batch - loss: 122.60829 - diff: 32.84mlTrain batch 32/32 - 78.2ms/batch - loss: 123.36805 - diff: 32.77mlTrain batch 32/32 - 10.6s 78.2ms/batch - loss: 123.36805 - diff: 32.77ml
Test 1.1s: val_loss: 76.18849 - diff: 26.27ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 49: current best loss = 76.18849, at epoch 48
Train batch 1/32 - 235.9ms/batch - loss: 282.41754 - diff: 45.86mlTrain batch 2/32 - 237.3ms/batch - loss: 207.91670 - diff: 43.03mlTrain batch 3/32 - 236.1ms/batch - loss: 190.96132 - diff: 40.02mlTrain batch 4/32 - 237.1ms/batch - loss: 253.88587 - diff: 43.09mlTrain batch 5/32 - 236.4ms/batch - loss: 228.84219 - diff: 41.36mlTrain batch 6/32 - 237.2ms/batch - loss: 215.43088 - diff: 41.30mlTrain batch 7/32 - 236.4ms/batch - loss: 201.82301 - diff: 40.28mlTrain batch 8/32 - 237.3ms/batch - loss: 185.39430 - diff: 38.67mlTrain batch 9/32 - 236.2ms/batch - loss: 176.45423 - diff: 37.75mlTrain batch 10/32 - 237.7ms/batch - loss: 164.96506 - diff: 36.46mlTrain batch 11/32 - 236.5ms/batch - loss: 158.51262 - diff: 36.00mlTrain batch 12/32 - 237.5ms/batch - loss: 155.48232 - diff: 35.81mlTrain batch 13/32 - 236.3ms/batch - loss: 149.37410 - diff: 35.19mlTrain batch 14/32 - 237.2ms/batch - loss: 145.87030 - diff: 35.23mlTrain batch 15/32 - 236.4ms/batch - loss: 138.55533 - diff: 33.94mlTrain batch 16/32 - 237.8ms/batch - loss: 135.78651 - diff: 33.88mlTrain batch 17/32 - 236.6ms/batch - loss: 130.79144 - diff: 33.09mlTrain batch 18/32 - 237.5ms/batch - loss: 133.50852 - diff: 33.87mlTrain batch 19/32 - 236.5ms/batch - loss: 139.26313 - diff: 34.45mlTrain batch 20/32 - 237.5ms/batch - loss: 137.48067 - diff: 34.23mlTrain batch 21/32 - 236.1ms/batch - loss: 134.93465 - diff: 34.09mlTrain batch 22/32 - 237.3ms/batch - loss: 132.86322 - diff: 33.80mlTrain batch 23/32 - 236.2ms/batch - loss: 129.72920 - diff: 33.50mlTrain batch 24/32 - 236.9ms/batch - loss: 126.90853 - diff: 33.29mlTrain batch 25/32 - 236.9ms/batch - loss: 127.72077 - diff: 33.50mlTrain batch 26/32 - 237.7ms/batch - loss: 125.22780 - diff: 33.14mlTrain batch 27/32 - 236.1ms/batch - loss: 123.93229 - diff: 33.16mlTrain batch 28/32 - 237.4ms/batch - loss: 121.24738 - diff: 32.84mlTrain batch 29/32 - 236.4ms/batch - loss: 120.67366 - diff: 32.95mlTrain batch 30/32 - 237.2ms/batch - loss: 120.14071 - diff: 32.95mlTrain batch 31/32 - 237.4ms/batch - loss: 120.03531 - diff: 32.99mlTrain batch 32/32 - 77.9ms/batch - loss: 123.48769 - diff: 33.04mlTrain batch 32/32 - 10.6s 77.9ms/batch - loss: 123.48769 - diff: 33.04ml
Test 1.1s: val_loss: 102.85288 - diff: 29.74ml

Epoch 50: current best loss = 76.18849, at epoch 48
Train batch 1/32 - 236.4ms/batch - loss: 77.26807 - diff: 28.16mlTrain batch 2/32 - 236.9ms/batch - loss: 83.70230 - diff: 29.04mlTrain batch 3/32 - 236.4ms/batch - loss: 91.72575 - diff: 30.28mlTrain batch 4/32 - 237.3ms/batch - loss: 79.09514 - diff: 27.78mlTrain batch 5/32 - 236.7ms/batch - loss: 81.57064 - diff: 28.71mlTrain batch 6/32 - 237.0ms/batch - loss: 137.01625 - diff: 31.82mlTrain batch 7/32 - 236.5ms/batch - loss: 125.67488 - diff: 30.53mlTrain batch 8/32 - 237.6ms/batch - loss: 122.64902 - diff: 30.75mlTrain batch 9/32 - 236.7ms/batch - loss: 127.08167 - diff: 31.10mlTrain batch 10/32 - 237.7ms/batch - loss: 119.30198 - diff: 30.25mlTrain batch 11/32 - 236.7ms/batch - loss: 115.00717 - diff: 29.73mlTrain batch 12/32 - 237.6ms/batch - loss: 108.42036 - diff: 28.98mlTrain batch 13/32 - 236.7ms/batch - loss: 109.79464 - diff: 29.66mlTrain batch 14/32 - 237.4ms/batch - loss: 106.55169 - diff: 29.37mlTrain batch 15/32 - 236.6ms/batch - loss: 103.68364 - diff: 29.12mlTrain batch 16/32 - 237.8ms/batch - loss: 104.18446 - diff: 29.47mlTrain batch 17/32 - 236.9ms/batch - loss: 102.35860 - diff: 29.35mlTrain batch 18/32 - 237.4ms/batch - loss: 98.76665 - diff: 28.82mlTrain batch 19/32 - 236.7ms/batch - loss: 97.47795 - diff: 28.73mlTrain batch 20/32 - 237.2ms/batch - loss: 96.74907 - diff: 28.66mlTrain batch 21/32 - 236.6ms/batch - loss: 97.73102 - diff: 29.00mlTrain batch 22/32 - 237.6ms/batch - loss: 96.82894 - diff: 28.87mlTrain batch 23/32 - 236.8ms/batch - loss: 96.46231 - diff: 28.92mlTrain batch 24/32 - 237.5ms/batch - loss: 94.33893 - diff: 28.53mlTrain batch 25/32 - 237.4ms/batch - loss: 94.36022 - diff: 28.53mlTrain batch 26/32 - 237.6ms/batch - loss: 94.79393 - diff: 28.58mlTrain batch 27/32 - 237.4ms/batch - loss: 94.62152 - diff: 28.66mlTrain batch 28/32 - 237.6ms/batch - loss: 95.45896 - diff: 28.82mlTrain batch 29/32 - 236.4ms/batch - loss: 94.16310 - diff: 28.64mlTrain batch 30/32 - 237.4ms/batch - loss: 94.24216 - diff: 28.59mlTrain batch 31/32 - 236.9ms/batch - loss: 104.21105 - diff: 28.93mlTrain batch 32/32 - 78.4ms/batch - loss: 104.98280 - diff: 28.90mlTrain batch 32/32 - 10.6s 78.4ms/batch - loss: 104.98280 - diff: 28.90ml
Test 1.1s: val_loss: 170.79465 - diff: 41.16ml

Epoch 51: current best loss = 76.18849, at epoch 48
Train batch 1/32 - 236.5ms/batch - loss: 39.42596 - diff: 18.62mlTrain batch 2/32 - 237.5ms/batch - loss: 77.07580 - diff: 25.70mlTrain batch 3/32 - 236.6ms/batch - loss: 78.32392 - diff: 26.53mlTrain batch 4/32 - 237.5ms/batch - loss: 76.90926 - diff: 26.63mlTrain batch 5/32 - 237.3ms/batch - loss: 66.75078 - diff: 24.42mlTrain batch 6/32 - 237.0ms/batch - loss: 64.38627 - diff: 23.95mlTrain batch 7/32 - 237.3ms/batch - loss: 75.52241 - diff: 25.53mlTrain batch 8/32 - 237.3ms/batch - loss: 73.22483 - diff: 25.32mlTrain batch 9/32 - 237.0ms/batch - loss: 80.69844 - diff: 26.42mlTrain batch 10/32 - 238.0ms/batch - loss: 80.98903 - diff: 26.55mlTrain batch 11/32 - 236.5ms/batch - loss: 77.00066 - diff: 25.96mlTrain batch 12/32 - 237.7ms/batch - loss: 78.45822 - diff: 26.48mlTrain batch 13/32 - 236.9ms/batch - loss: 76.05020 - diff: 25.99mlTrain batch 14/32 - 237.8ms/batch - loss: 78.15786 - diff: 26.28mlTrain batch 15/32 - 236.4ms/batch - loss: 79.85877 - diff: 26.94mlTrain batch 16/32 - 237.5ms/batch - loss: 96.09081 - diff: 27.94mlTrain batch 17/32 - 236.8ms/batch - loss: 96.60973 - diff: 28.01mlTrain batch 18/32 - 238.2ms/batch - loss: 95.56733 - diff: 28.10mlTrain batch 19/32 - 237.4ms/batch - loss: 96.93769 - diff: 28.26mlTrain batch 20/32 - 238.0ms/batch - loss: 101.30865 - diff: 29.24mlTrain batch 21/32 - 236.7ms/batch - loss: 99.67963 - diff: 29.03mlTrain batch 22/32 - 238.5ms/batch - loss: 100.82012 - diff: 29.48mlTrain batch 23/32 - 236.9ms/batch - loss: 102.46026 - diff: 29.86mlTrain batch 24/32 - 238.1ms/batch - loss: 102.62629 - diff: 29.92mlTrain batch 25/32 - 236.6ms/batch - loss: 102.02525 - diff: 29.93mlTrain batch 26/32 - 238.8ms/batch - loss: 108.85040 - diff: 30.69mlTrain batch 27/32 - 236.4ms/batch - loss: 109.05463 - diff: 30.87mlTrain batch 28/32 - 238.8ms/batch - loss: 107.68339 - diff: 30.73mlTrain batch 29/32 - 236.6ms/batch - loss: 106.37560 - diff: 30.59mlTrain batch 30/32 - 237.2ms/batch - loss: 106.07007 - diff: 30.57mlTrain batch 31/32 - 237.3ms/batch - loss: 108.70436 - diff: 30.98mlTrain batch 32/32 - 78.0ms/batch - loss: 108.59644 - diff: 30.86mlTrain batch 32/32 - 10.6s 78.0ms/batch - loss: 108.59644 - diff: 30.86ml
Test 1.1s: val_loss: 65.02328 - diff: 24.18ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 52: current best loss = 65.02328, at epoch 51
Train batch 1/32 - 236.2ms/batch - loss: 78.66968 - diff: 29.89mlTrain batch 2/32 - 237.5ms/batch - loss: 72.69991 - diff: 27.40mlTrain batch 3/32 - 236.8ms/batch - loss: 96.44140 - diff: 30.90mlTrain batch 4/32 - 240.8ms/batch - loss: 86.41903 - diff: 28.84mlTrain batch 5/32 - 236.5ms/batch - loss: 84.60308 - diff: 28.98mlTrain batch 6/32 - 237.6ms/batch - loss: 81.16450 - diff: 28.78mlTrain batch 7/32 - 236.4ms/batch - loss: 75.99120 - diff: 27.73mlTrain batch 8/32 - 237.5ms/batch - loss: 75.63641 - diff: 28.11mlTrain batch 9/32 - 236.3ms/batch - loss: 70.41696 - diff: 26.90mlTrain batch 10/32 - 237.5ms/batch - loss: 80.10686 - diff: 27.87mlTrain batch 11/32 - 236.3ms/batch - loss: 82.71904 - diff: 28.47mlTrain batch 12/32 - 237.7ms/batch - loss: 85.16465 - diff: 29.15mlTrain batch 13/32 - 236.4ms/batch - loss: 82.71459 - diff: 28.52mlTrain batch 14/32 - 237.8ms/batch - loss: 80.33487 - diff: 28.08mlTrain batch 15/32 - 236.6ms/batch - loss: 78.99419 - diff: 28.09mlTrain batch 16/32 - 237.6ms/batch - loss: 78.37152 - diff: 28.01mlTrain batch 17/32 - 236.5ms/batch - loss: 81.17877 - diff: 28.74mlTrain batch 18/32 - 237.8ms/batch - loss: 80.21371 - diff: 28.56mlTrain batch 19/32 - 236.5ms/batch - loss: 79.27519 - diff: 28.23mlTrain batch 20/32 - 237.4ms/batch - loss: 78.61702 - diff: 28.25mlTrain batch 21/32 - 236.3ms/batch - loss: 77.83625 - diff: 28.07mlTrain batch 22/32 - 238.9ms/batch - loss: 79.10912 - diff: 28.19mlTrain batch 23/32 - 237.2ms/batch - loss: 93.80533 - diff: 29.11mlTrain batch 24/32 - 237.5ms/batch - loss: 93.22762 - diff: 29.14mlTrain batch 25/32 - 237.2ms/batch - loss: 93.92477 - diff: 29.24mlTrain batch 26/32 - 237.2ms/batch - loss: 93.65865 - diff: 29.28mlTrain batch 27/32 - 237.2ms/batch - loss: 95.74679 - diff: 29.72mlTrain batch 28/32 - 237.9ms/batch - loss: 94.46374 - diff: 29.59mlTrain batch 29/32 - 236.4ms/batch - loss: 97.84585 - diff: 30.31mlTrain batch 30/32 - 237.8ms/batch - loss: 101.69560 - diff: 30.92mlTrain batch 31/32 - 236.4ms/batch - loss: 106.83571 - diff: 31.22mlTrain batch 32/32 - 78.7ms/batch - loss: 108.55728 - diff: 31.24mlTrain batch 32/32 - 10.6s 78.7ms/batch - loss: 108.55728 - diff: 31.24ml
Test 1.1s: val_loss: 111.26011 - diff: 32.78ml

Epoch 53: current best loss = 65.02328, at epoch 51
Train batch 1/32 - 236.3ms/batch - loss: 328.26273 - diff: 40.81mlTrain batch 2/32 - 237.9ms/batch - loss: 213.71456 - diff: 36.29mlTrain batch 3/32 - 236.7ms/batch - loss: 169.90376 - diff: 34.42mlTrain batch 4/32 - 237.8ms/batch - loss: 145.88664 - diff: 32.63mlTrain batch 5/32 - 237.2ms/batch - loss: 133.74552 - diff: 32.42mlTrain batch 6/32 - 237.8ms/batch - loss: 123.10761 - diff: 31.43mlTrain batch 7/32 - 237.4ms/batch - loss: 130.39942 - diff: 33.67mlTrain batch 8/32 - 237.3ms/batch - loss: 152.31279 - diff: 35.31mlTrain batch 9/32 - 236.6ms/batch - loss: 146.98731 - diff: 35.14mlTrain batch 10/32 - 237.8ms/batch - loss: 138.31700 - diff: 33.98mlTrain batch 11/32 - 236.6ms/batch - loss: 133.63091 - diff: 33.21mlTrain batch 12/32 - 238.1ms/batch - loss: 128.98097 - diff: 32.70mlTrain batch 13/32 - 236.4ms/batch - loss: 125.02312 - diff: 32.48mlTrain batch 14/32 - 238.9ms/batch - loss: 121.53613 - diff: 32.24mlTrain batch 15/32 - 236.3ms/batch - loss: 119.17583 - diff: 32.27mlTrain batch 16/32 - 238.2ms/batch - loss: 132.27253 - diff: 32.93mlTrain batch 17/32 - 237.0ms/batch - loss: 129.24127 - diff: 32.64mlTrain batch 18/32 - 238.4ms/batch - loss: 128.01299 - diff: 32.46mlTrain batch 19/32 - 236.5ms/batch - loss: 126.11795 - diff: 32.49mlTrain batch 20/32 - 238.6ms/batch - loss: 123.31651 - diff: 32.25mlTrain batch 21/32 - 236.6ms/batch - loss: 122.60631 - diff: 32.26mlTrain batch 22/32 - 238.8ms/batch - loss: 120.14530 - diff: 32.12mlTrain batch 23/32 - 236.5ms/batch - loss: 126.84951 - diff: 32.63mlTrain batch 24/32 - 238.7ms/batch - loss: 123.41756 - diff: 32.17mlTrain batch 25/32 - 237.7ms/batch - loss: 123.35650 - diff: 32.22mlTrain batch 26/32 - 237.6ms/batch - loss: 124.45376 - diff: 32.61mlTrain batch 27/32 - 237.1ms/batch - loss: 123.11606 - diff: 32.52mlTrain batch 28/32 - 237.5ms/batch - loss: 123.09828 - diff: 32.61mlTrain batch 29/32 - 237.0ms/batch - loss: 126.56193 - diff: 33.09mlTrain batch 30/32 - 237.6ms/batch - loss: 125.42183 - diff: 32.79mlTrain batch 31/32 - 237.0ms/batch - loss: 123.32382 - diff: 32.55mlTrain batch 32/32 - 78.4ms/batch - loss: 124.84056 - diff: 32.54mlTrain batch 32/32 - 10.6s 78.4ms/batch - loss: 124.84056 - diff: 32.54ml
Test 1.1s: val_loss: 297.90043 - diff: 54.76ml

Epoch 54: current best loss = 65.02328, at epoch 51
Train batch 1/32 - 236.7ms/batch - loss: 64.94957 - diff: 22.46mlTrain batch 2/32 - 237.5ms/batch - loss: 60.99511 - diff: 24.04mlTrain batch 3/32 - 236.9ms/batch - loss: 78.56014 - diff: 27.41mlTrain batch 4/32 - 236.2ms/batch - loss: 83.86039 - diff: 28.67mlTrain batch 5/32 - 237.3ms/batch - loss: 103.07680 - diff: 31.42mlTrain batch 6/32 - 237.8ms/batch - loss: 102.57867 - diff: 31.79mlTrain batch 7/32 - 237.2ms/batch - loss: 100.72816 - diff: 31.18mlTrain batch 8/32 - 237.5ms/batch - loss: 99.46467 - diff: 31.31mlTrain batch 9/32 - 237.4ms/batch - loss: 96.18131 - diff: 30.79mlTrain batch 10/32 - 238.3ms/batch - loss: 95.94136 - diff: 31.14mlTrain batch 11/32 - 236.8ms/batch - loss: 94.37750 - diff: 30.75mlTrain batch 12/32 - 238.2ms/batch - loss: 94.70774 - diff: 31.02mlTrain batch 13/32 - 236.6ms/batch - loss: 92.90682 - diff: 30.65mlTrain batch 14/32 - 238.3ms/batch - loss: 88.41964 - diff: 29.69mlTrain batch 15/32 - 236.8ms/batch - loss: 85.65770 - diff: 29.27mlTrain batch 16/32 - 237.9ms/batch - loss: 84.50477 - diff: 29.22mlTrain batch 17/32 - 236.8ms/batch - loss: 84.36665 - diff: 29.13mlTrain batch 18/32 - 239.5ms/batch - loss: 90.34889 - diff: 30.09mlTrain batch 19/32 - 236.9ms/batch - loss: 89.51165 - diff: 29.82mlTrain batch 20/32 - 239.1ms/batch - loss: 89.87052 - diff: 29.95mlTrain batch 21/32 - 236.7ms/batch - loss: 96.90574 - diff: 30.26mlTrain batch 22/32 - 239.5ms/batch - loss: 94.29957 - diff: 29.84mlTrain batch 23/32 - 236.8ms/batch - loss: 93.76476 - diff: 29.89mlTrain batch 24/32 - 238.4ms/batch - loss: 94.09975 - diff: 29.95mlTrain batch 25/32 - 237.5ms/batch - loss: 96.98708 - diff: 30.52mlTrain batch 26/32 - 238.9ms/batch - loss: 96.27008 - diff: 30.42mlTrain batch 27/32 - 237.3ms/batch - loss: 95.82656 - diff: 30.31mlTrain batch 28/32 - 237.5ms/batch - loss: 100.19457 - diff: 30.80mlTrain batch 29/32 - 237.0ms/batch - loss: 99.86320 - diff: 30.66mlTrain batch 30/32 - 237.8ms/batch - loss: 98.85212 - diff: 30.46mlTrain batch 31/32 - 237.4ms/batch - loss: 100.95879 - diff: 30.83mlTrain batch 32/32 - 78.6ms/batch - loss: 101.78231 - diff: 30.72mlTrain batch 32/32 - 10.6s 78.6ms/batch - loss: 101.78231 - diff: 30.72ml
Test 1.1s: val_loss: 85.44130 - diff: 28.45ml

Epoch 55: current best loss = 65.02328, at epoch 51
Train batch 1/32 - 236.7ms/batch - loss: 99.63442 - diff: 28.75mlTrain batch 2/32 - 238.4ms/batch - loss: 110.65474 - diff: 32.51mlTrain batch 3/32 - 236.6ms/batch - loss: 88.49736 - diff: 29.12mlTrain batch 4/32 - 239.3ms/batch - loss: 98.42575 - diff: 30.80mlTrain batch 5/32 - 237.1ms/batch - loss: 92.94968 - diff: 30.04mlTrain batch 6/32 - 238.8ms/batch - loss: 88.74121 - diff: 29.10mlTrain batch 7/32 - 237.4ms/batch - loss: 86.50321 - diff: 28.95mlTrain batch 8/32 - 237.6ms/batch - loss: 78.39690 - diff: 27.13mlTrain batch 9/32 - 237.3ms/batch - loss: 83.47736 - diff: 27.68mlTrain batch 10/32 - 237.7ms/batch - loss: 80.60728 - diff: 27.06mlTrain batch 11/32 - 238.2ms/batch - loss: 81.89095 - diff: 27.33mlTrain batch 12/32 - 237.7ms/batch - loss: 82.68812 - diff: 27.56mlTrain batch 13/32 - 236.6ms/batch - loss: 79.37345 - diff: 26.95mlTrain batch 14/32 - 237.6ms/batch - loss: 78.46275 - diff: 26.80mlTrain batch 15/32 - 236.4ms/batch - loss: 79.19240 - diff: 27.13mlTrain batch 16/32 - 239.0ms/batch - loss: 78.62581 - diff: 27.19mlTrain batch 17/32 - 236.9ms/batch - loss: 78.50722 - diff: 27.13mlTrain batch 18/32 - 238.3ms/batch - loss: 75.76020 - diff: 26.67mlTrain batch 19/32 - 237.1ms/batch - loss: 76.06122 - diff: 26.88mlTrain batch 20/32 - 239.2ms/batch - loss: 81.62694 - diff: 27.33mlTrain batch 21/32 - 236.9ms/batch - loss: 82.82905 - diff: 27.58mlTrain batch 22/32 - 239.3ms/batch - loss: 84.81307 - diff: 28.03mlTrain batch 23/32 - 237.2ms/batch - loss: 83.67773 - diff: 27.82mlTrain batch 24/32 - 238.8ms/batch - loss: 83.26517 - diff: 27.77mlTrain batch 25/32 - 237.0ms/batch - loss: 81.91319 - diff: 27.56mlTrain batch 26/32 - 239.4ms/batch - loss: 97.01062 - diff: 28.40mlTrain batch 27/32 - 237.1ms/batch - loss: 95.66592 - diff: 28.31mlTrain batch 28/32 - 239.9ms/batch - loss: 95.16534 - diff: 28.34mlTrain batch 29/32 - 236.6ms/batch - loss: 94.52423 - diff: 28.39mlTrain batch 30/32 - 239.0ms/batch - loss: 92.81415 - diff: 28.19mlTrain batch 31/32 - 237.5ms/batch - loss: 93.43064 - diff: 28.43mlTrain batch 32/32 - 78.4ms/batch - loss: 95.75624 - diff: 28.49mlTrain batch 32/32 - 10.5s 78.4ms/batch - loss: 95.75624 - diff: 28.49ml
Test 1.1s: val_loss: 164.02469 - diff: 41.29ml

Epoch 56: current best loss = 65.02328, at epoch 51
Train batch 1/32 - 236.6ms/batch - loss: 153.43663 - diff: 38.10mlTrain batch 2/32 - 238.2ms/batch - loss: 96.22051 - diff: 29.18mlTrain batch 3/32 - 236.8ms/batch - loss: 92.27518 - diff: 29.02mlTrain batch 4/32 - 239.2ms/batch - loss: 93.52704 - diff: 30.12mlTrain batch 5/32 - 236.4ms/batch - loss: 83.69561 - diff: 28.32mlTrain batch 6/32 - 239.0ms/batch - loss: 79.68524 - diff: 27.81mlTrain batch 7/32 - 236.7ms/batch - loss: 86.04297 - diff: 29.03mlTrain batch 8/32 - 239.1ms/batch - loss: 86.09201 - diff: 28.95mlTrain batch 9/32 - 237.5ms/batch - loss: 93.42359 - diff: 28.77mlTrain batch 10/32 - 237.5ms/batch - loss: 92.77084 - diff: 28.34mlTrain batch 11/32 - 237.3ms/batch - loss: 89.60792 - diff: 28.12mlTrain batch 12/32 - 237.7ms/batch - loss: 97.76353 - diff: 28.74mlTrain batch 13/32 - 236.2ms/batch - loss: 97.07467 - diff: 28.87mlTrain batch 14/32 - 237.6ms/batch - loss: 94.18483 - diff: 28.63mlTrain batch 15/32 - 236.7ms/batch - loss: 91.72962 - diff: 28.46mlTrain batch 16/32 - 239.8ms/batch - loss: 91.47696 - diff: 28.72mlTrain batch 17/32 - 237.4ms/batch - loss: 91.33172 - diff: 28.72mlTrain batch 18/32 - 239.5ms/batch - loss: 97.73368 - diff: 29.56mlTrain batch 19/32 - 237.1ms/batch - loss: 95.36309 - diff: 29.23mlTrain batch 20/32 - 239.1ms/batch - loss: 95.12545 - diff: 29.21mlTrain batch 21/32 - 236.8ms/batch - loss: 93.66267 - diff: 29.00mlTrain batch 22/32 - 240.0ms/batch - loss: 92.18244 - diff: 28.89mlTrain batch 23/32 - 237.0ms/batch - loss: 90.76411 - diff: 28.79mlTrain batch 24/32 - 239.7ms/batch - loss: 90.33018 - diff: 28.77mlTrain batch 25/32 - 237.8ms/batch - loss: 90.75754 - diff: 28.88mlTrain batch 26/32 - 239.2ms/batch - loss: 91.80284 - diff: 29.13mlTrain batch 27/32 - 237.5ms/batch - loss: 90.96293 - diff: 28.99mlTrain batch 28/32 - 239.8ms/batch - loss: 91.89412 - diff: 29.29mlTrain batch 29/32 - 237.4ms/batch - loss: 91.48689 - diff: 29.22mlTrain batch 30/32 - 240.0ms/batch - loss: 92.98482 - diff: 29.49mlTrain batch 31/32 - 237.5ms/batch - loss: 92.45075 - diff: 29.47mlTrain batch 32/32 - 78.3ms/batch - loss: 102.33496 - diff: 29.69mlTrain batch 32/32 - 10.6s 78.3ms/batch - loss: 102.33496 - diff: 29.69ml
Test 1.1s: val_loss: 127.69262 - diff: 36.48ml

Epoch 57: current best loss = 65.02328, at epoch 51
Train batch 1/32 - 236.5ms/batch - loss: 77.69461 - diff: 30.27mlTrain batch 2/32 - 239.0ms/batch - loss: 69.61229 - diff: 27.15mlTrain batch 3/32 - 237.3ms/batch - loss: 83.48266 - diff: 29.92mlTrain batch 4/32 - 239.1ms/batch - loss: 84.95615 - diff: 29.67mlTrain batch 5/32 - 236.6ms/batch - loss: 80.31058 - diff: 29.14mlTrain batch 6/32 - 240.0ms/batch - loss: 81.09385 - diff: 28.92mlTrain batch 7/32 - 236.6ms/batch - loss: 85.51598 - diff: 29.69mlTrain batch 8/32 - 239.5ms/batch - loss: 86.33955 - diff: 29.42mlTrain batch 9/32 - 237.5ms/batch - loss: 98.23022 - diff: 30.50mlTrain batch 10/32 - 238.8ms/batch - loss: 98.40092 - diff: 30.37mlTrain batch 11/32 - 237.2ms/batch - loss: 98.44555 - diff: 30.60mlTrain batch 12/32 - 237.6ms/batch - loss: 95.71612 - diff: 30.45mlTrain batch 13/32 - 237.2ms/batch - loss: 92.60934 - diff: 30.23mlTrain batch 14/32 - 237.6ms/batch - loss: 92.96304 - diff: 30.20mlTrain batch 15/32 - 236.7ms/batch - loss: 90.15978 - diff: 29.90mlTrain batch 16/32 - 238.0ms/batch - loss: 87.06380 - diff: 29.33mlTrain batch 17/32 - 236.4ms/batch - loss: 88.35656 - diff: 29.56mlTrain batch 18/32 - 239.1ms/batch - loss: 86.42209 - diff: 29.18mlTrain batch 19/32 - 236.5ms/batch - loss: 85.03002 - diff: 28.97mlTrain batch 20/32 - 240.6ms/batch - loss: 83.26067 - diff: 28.73mlTrain batch 21/32 - 237.2ms/batch - loss: 83.35412 - diff: 28.79mlTrain batch 22/32 - 239.9ms/batch - loss: 84.22378 - diff: 29.03mlTrain batch 23/32 - 237.4ms/batch - loss: 85.26269 - diff: 29.18mlTrain batch 24/32 - 239.3ms/batch - loss: 84.09835 - diff: 28.83mlTrain batch 25/32 - 237.3ms/batch - loss: 96.37527 - diff: 29.33mlTrain batch 26/32 - 239.2ms/batch - loss: 96.11297 - diff: 29.28mlTrain batch 27/32 - 237.5ms/batch - loss: 94.12411 - diff: 29.01mlTrain batch 28/32 - 239.8ms/batch - loss: 92.51195 - diff: 28.78mlTrain batch 29/32 - 236.7ms/batch - loss: 92.04541 - diff: 28.77mlTrain batch 30/32 - 239.3ms/batch - loss: 91.22329 - diff: 28.64mlTrain batch 31/32 - 237.8ms/batch - loss: 92.08788 - diff: 28.55mlTrain batch 32/32 - 78.5ms/batch - loss: 92.91040 - diff: 28.52mlTrain batch 32/32 - 10.6s 78.5ms/batch - loss: 92.91040 - diff: 28.52ml
Test 1.1s: val_loss: 473.29307 - diff: 76.58ml

Epoch 58: current best loss = 65.02328, at epoch 51
Train batch 1/32 - 236.7ms/batch - loss: 95.93469 - diff: 29.68mlTrain batch 2/32 - 238.6ms/batch - loss: 75.28435 - diff: 27.00mlTrain batch 3/32 - 236.6ms/batch - loss: 69.16591 - diff: 26.29mlTrain batch 4/32 - 238.8ms/batch - loss: 95.22847 - diff: 28.37mlTrain batch 5/32 - 236.7ms/batch - loss: 102.09599 - diff: 30.15mlTrain batch 6/32 - 238.8ms/batch - loss: 93.81244 - diff: 29.09mlTrain batch 7/32 - 237.6ms/batch - loss: 87.69198 - diff: 28.25mlTrain batch 8/32 - 237.6ms/batch - loss: 87.06590 - diff: 27.94mlTrain batch 9/32 - 237.0ms/batch - loss: 83.91340 - diff: 27.65mlTrain batch 10/32 - 237.7ms/batch - loss: 81.29707 - diff: 27.54mlTrain batch 11/32 - 236.9ms/batch - loss: 79.47805 - diff: 27.45mlTrain batch 12/32 - 238.1ms/batch - loss: 80.64892 - diff: 27.79mlTrain batch 13/32 - 236.4ms/batch - loss: 83.12131 - diff: 28.09mlTrain batch 14/32 - 239.0ms/batch - loss: 81.66952 - diff: 27.92mlTrain batch 15/32 - 236.8ms/batch - loss: 84.43200 - diff: 28.40mlTrain batch 16/32 - 238.0ms/batch - loss: 87.92240 - diff: 28.79mlTrain batch 17/32 - 236.8ms/batch - loss: 91.20888 - diff: 29.26mlTrain batch 18/32 - 239.6ms/batch - loss: 89.40386 - diff: 28.92mlTrain batch 19/32 - 236.9ms/batch - loss: 89.26690 - diff: 28.90mlTrain batch 20/32 - 239.3ms/batch - loss: 87.94457 - diff: 28.76mlTrain batch 21/32 - 237.0ms/batch - loss: 86.93708 - diff: 28.53mlTrain batch 22/32 - 239.6ms/batch - loss: 85.24003 - diff: 28.35mlTrain batch 23/32 - 237.2ms/batch - loss: 83.63219 - diff: 28.02mlTrain batch 24/32 - 239.2ms/batch - loss: 82.73174 - diff: 28.04mlTrain batch 25/32 - 237.2ms/batch - loss: 83.29462 - diff: 28.16mlTrain batch 26/32 - 237.6ms/batch - loss: 83.89275 - diff: 28.33mlTrain batch 27/32 - 237.1ms/batch - loss: 82.22945 - diff: 28.01mlTrain batch 28/32 - 237.7ms/batch - loss: 82.86561 - diff: 28.08mlTrain batch 29/32 - 237.3ms/batch - loss: 95.12323 - diff: 28.96mlTrain batch 30/32 - 238.0ms/batch - loss: 93.42157 - diff: 28.70mlTrain batch 31/32 - 236.9ms/batch - loss: 91.55607 - diff: 28.41mlTrain batch 32/32 - 79.6ms/batch - loss: 92.60635 - diff: 28.41mlTrain batch 32/32 - 10.6s 79.6ms/batch - loss: 92.60635 - diff: 28.41ml
Test 1.1s: val_loss: 67.94357 - diff: 24.69ml

Epoch 59: current best loss = 65.02328, at epoch 51
Train batch 1/32 - 236.9ms/batch - loss: 120.46979 - diff: 34.05mlTrain batch 2/32 - 238.4ms/batch - loss: 92.83736 - diff: 30.83mlTrain batch 3/32 - 237.5ms/batch - loss: 69.27603 - diff: 25.82mlTrain batch 4/32 - 237.7ms/batch - loss: 69.58188 - diff: 25.24mlTrain batch 5/32 - 237.3ms/batch - loss: 75.54054 - diff: 27.48mlTrain batch 6/32 - 237.8ms/batch - loss: 74.54603 - diff: 26.71mlTrain batch 7/32 - 236.7ms/batch - loss: 72.75831 - diff: 26.19mlTrain batch 8/32 - 238.1ms/batch - loss: 75.69991 - diff: 26.52mlTrain batch 9/32 - 236.5ms/batch - loss: 74.89239 - diff: 26.45mlTrain batch 10/32 - 239.0ms/batch - loss: 76.02100 - diff: 26.75mlTrain batch 11/32 - 236.4ms/batch - loss: 75.47372 - diff: 26.87mlTrain batch 12/32 - 240.1ms/batch - loss: 75.55723 - diff: 26.98mlTrain batch 13/32 - 237.0ms/batch - loss: 72.43900 - diff: 26.22mlTrain batch 14/32 - 239.2ms/batch - loss: 69.47326 - diff: 25.63mlTrain batch 15/32 - 237.0ms/batch - loss: 67.77988 - diff: 25.38mlTrain batch 16/32 - 239.2ms/batch - loss: 68.02954 - diff: 25.38mlTrain batch 17/32 - 236.8ms/batch - loss: 71.84537 - diff: 26.10mlTrain batch 18/32 - 240.4ms/batch - loss: 72.10938 - diff: 26.28mlTrain batch 19/32 - 237.0ms/batch - loss: 75.55726 - diff: 26.63mlTrain batch 20/32 - 239.6ms/batch - loss: 74.31783 - diff: 26.48mlTrain batch 21/32 - 236.7ms/batch - loss: 73.04300 - diff: 26.21mlTrain batch 22/32 - 237.2ms/batch - loss: 85.78774 - diff: 26.85mlTrain batch 23/32 - 237.3ms/batch - loss: 84.58849 - diff: 26.80mlTrain batch 24/32 - 238.9ms/batch - loss: 86.87573 - diff: 27.21mlTrain batch 25/32 - 237.3ms/batch - loss: 91.74760 - diff: 27.71mlTrain batch 26/32 - 237.8ms/batch - loss: 91.95341 - diff: 27.92mlTrain batch 27/32 - 237.3ms/batch - loss: 92.57611 - diff: 28.03mlTrain batch 28/32 - 237.9ms/batch - loss: 91.62905 - diff: 27.93mlTrain batch 29/32 - 236.8ms/batch - loss: 90.40086 - diff: 27.75mlTrain batch 30/32 - 239.6ms/batch - loss: 91.20459 - diff: 28.12mlTrain batch 31/32 - 236.4ms/batch - loss: 91.05715 - diff: 28.14mlTrain batch 32/32 - 78.1ms/batch - loss: 93.07374 - diff: 28.15mlTrain batch 32/32 - 10.6s 78.1ms/batch - loss: 93.07374 - diff: 28.15ml
Test 1.1s: val_loss: 75.75540 - diff: 27.09ml

Epoch 60: current best loss = 65.02328, at epoch 51
Train batch 1/32 - 236.8ms/batch - loss: 64.53113 - diff: 27.23mlTrain batch 2/32 - 237.7ms/batch - loss: 133.30759 - diff: 36.89mlTrain batch 3/32 - 236.7ms/batch - loss: 110.31725 - diff: 32.96mlTrain batch 4/32 - 237.8ms/batch - loss: 102.00519 - diff: 31.89mlTrain batch 5/32 - 236.9ms/batch - loss: 91.42297 - diff: 29.91mlTrain batch 6/32 - 237.9ms/batch - loss: 82.32989 - diff: 28.62mlTrain batch 7/32 - 236.5ms/batch - loss: 85.42043 - diff: 28.81mlTrain batch 8/32 - 239.0ms/batch - loss: 84.30373 - diff: 28.29mlTrain batch 9/32 - 236.8ms/batch - loss: 81.36917 - diff: 27.51mlTrain batch 10/32 - 239.1ms/batch - loss: 80.82695 - diff: 27.46mlTrain batch 11/32 - 237.2ms/batch - loss: 85.47331 - diff: 28.24mlTrain batch 12/32 - 239.3ms/batch - loss: 84.35588 - diff: 27.90mlTrain batch 13/32 - 236.6ms/batch - loss: 86.72496 - diff: 28.22mlTrain batch 14/32 - 239.5ms/batch - loss: 86.33159 - diff: 27.99mlTrain batch 15/32 - 236.9ms/batch - loss: 85.45551 - diff: 27.82mlTrain batch 16/32 - 240.1ms/batch - loss: 88.28455 - diff: 28.35mlTrain batch 17/32 - 237.4ms/batch - loss: 85.42211 - diff: 27.95mlTrain batch 18/32 - 237.7ms/batch - loss: 86.52616 - diff: 28.27mlTrain batch 19/32 - 237.9ms/batch - loss: 86.39125 - diff: 28.28mlTrain batch 20/32 - 237.7ms/batch - loss: 85.61044 - diff: 28.24mlTrain batch 21/32 - 236.9ms/batch - loss: 83.79844 - diff: 28.00mlTrain batch 22/32 - 237.9ms/batch - loss: 83.57512 - diff: 28.06mlTrain batch 23/32 - 236.7ms/batch - loss: 104.97755 - diff: 29.30mlTrain batch 24/32 - 239.4ms/batch - loss: 105.87547 - diff: 29.77mlTrain batch 25/32 - 237.3ms/batch - loss: 103.56848 - diff: 29.51mlTrain batch 26/32 - 240.3ms/batch - loss: 106.93828 - diff: 29.87mlTrain batch 27/32 - 237.2ms/batch - loss: 104.16707 - diff: 29.48mlTrain batch 28/32 - 240.5ms/batch - loss: 106.35100 - diff: 29.74mlTrain batch 29/32 - 236.8ms/batch - loss: 104.12638 - diff: 29.45mlTrain batch 30/32 - 239.9ms/batch - loss: 103.93302 - diff: 29.66mlTrain batch 31/32 - 237.5ms/batch - loss: 102.47251 - diff: 29.50mlTrain batch 32/32 - 78.1ms/batch - loss: 103.78462 - diff: 29.48mlTrain batch 32/32 - 10.6s 78.1ms/batch - loss: 103.78462 - diff: 29.48ml
Test 1.1s: val_loss: 641.63528 - diff: 92.99ml

Epoch 61: current best loss = 65.02328, at epoch 51
Train batch 1/32 - 236.6ms/batch - loss: 32.21144 - diff: 18.21mlTrain batch 2/32 - 238.4ms/batch - loss: 59.72019 - diff: 24.70mlTrain batch 3/32 - 236.6ms/batch - loss: 60.46061 - diff: 24.60mlTrain batch 4/32 - 238.3ms/batch - loss: 70.93730 - diff: 27.10mlTrain batch 5/32 - 236.7ms/batch - loss: 65.28469 - diff: 25.93mlTrain batch 6/32 - 238.9ms/batch - loss: 61.03890 - diff: 24.92mlTrain batch 7/32 - 237.3ms/batch - loss: 86.94711 - diff: 29.49mlTrain batch 8/32 - 239.9ms/batch - loss: 88.64328 - diff: 30.05mlTrain batch 9/32 - 236.8ms/batch - loss: 83.11037 - diff: 28.82mlTrain batch 10/32 - 240.3ms/batch - loss: 84.10159 - diff: 29.11mlTrain batch 11/32 - 236.9ms/batch - loss: 84.59369 - diff: 28.72mlTrain batch 12/32 - 240.0ms/batch - loss: 86.54998 - diff: 29.19mlTrain batch 13/32 - 237.1ms/batch - loss: 100.50517 - diff: 29.92mlTrain batch 14/32 - 239.4ms/batch - loss: 98.02619 - diff: 29.58mlTrain batch 15/32 - 236.8ms/batch - loss: 95.67645 - diff: 29.36mlTrain batch 16/32 - 239.5ms/batch - loss: 104.23834 - diff: 30.59mlTrain batch 17/32 - 236.7ms/batch - loss: 101.55155 - diff: 30.36mlTrain batch 18/32 - 241.8ms/batch - loss: 101.54352 - diff: 30.63mlTrain batch 19/32 - 237.4ms/batch - loss: 99.25465 - diff: 30.32mlTrain batch 20/32 - 239.5ms/batch - loss: 107.89028 - diff: 31.10mlTrain batch 21/32 - 237.3ms/batch - loss: 106.96038 - diff: 31.08mlTrain batch 22/32 - 237.7ms/batch - loss: 105.03131 - diff: 30.83mlTrain batch 23/32 - 236.7ms/batch - loss: 102.38675 - diff: 30.47mlTrain batch 24/32 - 238.7ms/batch - loss: 99.59670 - diff: 30.00mlTrain batch 25/32 - 236.6ms/batch - loss: 98.47535 - diff: 29.86mlTrain batch 26/32 - 239.7ms/batch - loss: 99.03111 - diff: 29.88mlTrain batch 27/32 - 236.7ms/batch - loss: 96.96621 - diff: 29.59mlTrain batch 28/32 - 239.9ms/batch - loss: 99.18830 - diff: 30.01mlTrain batch 29/32 - 236.9ms/batch - loss: 98.87787 - diff: 30.02mlTrain batch 30/32 - 239.8ms/batch - loss: 100.70325 - diff: 30.22mlTrain batch 31/32 - 236.9ms/batch - loss: 99.26688 - diff: 30.03mlTrain batch 32/32 - 77.8ms/batch - loss: 99.87924 - diff: 29.96mlTrain batch 32/32 - 10.6s 77.8ms/batch - loss: 99.87924 - diff: 29.96ml
Test 1.1s: val_loss: 89.97446 - diff: 30.14ml

Epoch 62: current best loss = 65.02328, at epoch 51
Train batch 1/32 - 236.7ms/batch - loss: 129.13684 - diff: 36.29mlTrain batch 2/32 - 240.3ms/batch - loss: 102.64957 - diff: 31.66mlTrain batch 3/32 - 236.5ms/batch - loss: 102.62247 - diff: 32.01mlTrain batch 4/32 - 238.5ms/batch - loss: 116.49097 - diff: 34.52mlTrain batch 5/32 - 236.3ms/batch - loss: 116.32868 - diff: 35.24mlTrain batch 6/32 - 239.1ms/batch - loss: 114.58353 - diff: 35.43mlTrain batch 7/32 - 237.3ms/batch - loss: 112.11390 - diff: 34.95mlTrain batch 8/32 - 240.4ms/batch - loss: 103.54285 - diff: 33.26mlTrain batch 9/32 - 236.9ms/batch - loss: 95.64308 - diff: 31.46mlTrain batch 10/32 - 239.2ms/batch - loss: 93.52888 - diff: 30.65mlTrain batch 11/32 - 237.0ms/batch - loss: 89.29418 - diff: 30.04mlTrain batch 12/32 - 239.3ms/batch - loss: 85.77085 - diff: 29.44mlTrain batch 13/32 - 236.6ms/batch - loss: 87.41672 - diff: 29.53mlTrain batch 14/32 - 239.9ms/batch - loss: 82.34706 - diff: 28.37mlTrain batch 15/32 - 237.1ms/batch - loss: 82.86424 - diff: 28.17mlTrain batch 16/32 - 239.3ms/batch - loss: 84.74838 - diff: 28.89mlTrain batch 17/32 - 236.5ms/batch - loss: 82.81223 - diff: 28.52mlTrain batch 18/32 - 238.1ms/batch - loss: 80.58291 - diff: 28.13mlTrain batch 19/32 - 237.7ms/batch - loss: 79.09798 - diff: 27.74mlTrain batch 20/32 - 237.9ms/batch - loss: 85.43674 - diff: 28.67mlTrain batch 21/32 - 237.5ms/batch - loss: 84.28320 - diff: 28.45mlTrain batch 22/32 - 237.7ms/batch - loss: 83.08824 - diff: 28.18mlTrain batch 23/32 - 237.6ms/batch - loss: 93.53638 - diff: 28.56mlTrain batch 24/32 - 237.6ms/batch - loss: 90.30454 - diff: 27.95mlTrain batch 25/32 - 237.4ms/batch - loss: 90.89068 - diff: 27.98mlTrain batch 26/32 - 238.3ms/batch - loss: 88.91857 - diff: 27.65mlTrain batch 27/32 - 236.9ms/batch - loss: 88.68214 - diff: 27.74mlTrain batch 28/32 - 239.1ms/batch - loss: 88.10920 - diff: 27.75mlTrain batch 29/32 - 236.4ms/batch - loss: 92.39250 - diff: 28.21mlTrain batch 30/32 - 239.6ms/batch - loss: 91.31041 - diff: 28.08mlTrain batch 31/32 - 236.7ms/batch - loss: 89.65177 - diff: 27.87mlTrain batch 32/32 - 78.7ms/batch - loss: 91.88967 - diff: 27.92mlTrain batch 32/32 - 10.6s 78.7ms/batch - loss: 91.88967 - diff: 27.92ml
Test 1.1s: val_loss: 200.58479 - diff: 46.21ml
Epoch    63: reducing learning rate of group 0 to 5.0000e-04.

Epoch 63: current best loss = 65.02328, at epoch 51
Train batch 1/32 - 236.0ms/batch - loss: 99.98575 - diff: 30.88mlTrain batch 2/32 - 239.4ms/batch - loss: 74.21883 - diff: 25.80mlTrain batch 3/32 - 237.7ms/batch - loss: 87.21088 - diff: 29.32mlTrain batch 4/32 - 237.9ms/batch - loss: 85.44466 - diff: 28.97mlTrain batch 5/32 - 236.6ms/batch - loss: 106.13338 - diff: 30.52mlTrain batch 6/32 - 238.7ms/batch - loss: 106.87422 - diff: 31.40mlTrain batch 7/32 - 236.5ms/batch - loss: 101.04915 - diff: 30.52mlTrain batch 8/32 - 238.5ms/batch - loss: 95.00045 - diff: 29.64mlTrain batch 9/32 - 237.0ms/batch - loss: 91.36308 - diff: 29.09mlTrain batch 10/32 - 240.1ms/batch - loss: 92.22552 - diff: 29.31mlTrain batch 11/32 - 236.5ms/batch - loss: 91.78756 - diff: 29.49mlTrain batch 12/32 - 238.9ms/batch - loss: 87.91515 - diff: 28.63mlTrain batch 13/32 - 236.6ms/batch - loss: 84.92503 - diff: 27.98mlTrain batch 14/32 - 239.4ms/batch - loss: 82.30527 - diff: 27.25mlTrain batch 15/32 - 236.8ms/batch - loss: 78.60045 - diff: 26.53mlTrain batch 16/32 - 239.7ms/batch - loss: 77.23672 - diff: 26.47mlTrain batch 17/32 - 237.1ms/batch - loss: 75.63118 - diff: 26.01mlTrain batch 18/32 - 239.7ms/batch - loss: 74.54341 - diff: 25.83mlTrain batch 19/32 - 237.4ms/batch - loss: 74.72643 - diff: 25.99mlTrain batch 20/32 - 237.9ms/batch - loss: 74.27983 - diff: 26.04mlTrain batch 21/32 - 237.2ms/batch - loss: 72.76911 - diff: 25.70mlTrain batch 22/32 - 237.7ms/batch - loss: 72.21297 - diff: 25.72mlTrain batch 23/32 - 237.5ms/batch - loss: 71.10759 - diff: 25.52mlTrain batch 24/32 - 237.6ms/batch - loss: 80.27549 - diff: 26.21mlTrain batch 25/32 - 236.4ms/batch - loss: 80.22463 - diff: 26.31mlTrain batch 26/32 - 239.1ms/batch - loss: 80.74547 - diff: 26.43mlTrain batch 27/32 - 237.4ms/batch - loss: 80.13951 - diff: 26.37mlTrain batch 28/32 - 240.1ms/batch - loss: 79.02346 - diff: 26.19mlTrain batch 29/32 - 237.0ms/batch - loss: 79.12124 - diff: 26.16mlTrain batch 30/32 - 240.0ms/batch - loss: 77.76670 - diff: 25.90mlTrain batch 31/32 - 237.1ms/batch - loss: 77.18073 - diff: 25.90mlTrain batch 32/32 - 77.9ms/batch - loss: 80.35217 - diff: 26.03mlTrain batch 32/32 - 10.6s 77.9ms/batch - loss: 80.35217 - diff: 26.03ml
Test 1.1s: val_loss: 66.66790 - diff: 24.86ml

Epoch 64: current best loss = 65.02328, at epoch 51
Train batch 1/32 - 236.4ms/batch - loss: 53.78471 - diff: 24.47mlTrain batch 2/32 - 237.7ms/batch - loss: 60.97409 - diff: 25.95mlTrain batch 3/32 - 236.7ms/batch - loss: 112.38561 - diff: 29.99mlTrain batch 4/32 - 239.6ms/batch - loss: 95.62939 - diff: 26.96mlTrain batch 5/32 - 237.0ms/batch - loss: 83.10541 - diff: 25.21mlTrain batch 6/32 - 239.7ms/batch - loss: 81.06708 - diff: 25.81mlTrain batch 7/32 - 236.7ms/batch - loss: 73.07211 - diff: 24.50mlTrain batch 8/32 - 239.2ms/batch - loss: 71.08123 - diff: 24.25mlTrain batch 9/32 - 236.6ms/batch - loss: 71.12782 - diff: 24.72mlTrain batch 10/32 - 239.5ms/batch - loss: 70.02839 - diff: 25.10mlTrain batch 11/32 - 236.2ms/batch - loss: 72.42705 - diff: 25.35mlTrain batch 12/32 - 240.8ms/batch - loss: 70.05140 - diff: 25.09mlTrain batch 13/32 - 237.6ms/batch - loss: 69.49098 - diff: 25.07mlTrain batch 14/32 - 237.6ms/batch - loss: 76.60689 - diff: 26.33mlTrain batch 15/32 - 237.2ms/batch - loss: 74.52929 - diff: 25.97mlTrain batch 16/32 - 237.7ms/batch - loss: 72.02629 - diff: 25.39mlTrain batch 17/32 - 236.8ms/batch - loss: 70.89727 - diff: 25.29mlTrain batch 18/32 - 238.0ms/batch - loss: 70.19147 - diff: 25.28mlTrain batch 19/32 - 236.4ms/batch - loss: 71.32684 - diff: 25.65mlTrain batch 20/32 - 238.6ms/batch - loss: 79.21377 - diff: 26.24mlTrain batch 21/32 - 236.4ms/batch - loss: 77.83824 - diff: 26.01mlTrain batch 22/32 - 240.0ms/batch - loss: 77.61502 - diff: 25.84mlTrain batch 23/32 - 237.2ms/batch - loss: 75.91566 - diff: 25.66mlTrain batch 24/32 - 240.0ms/batch - loss: 74.87677 - diff: 25.58mlTrain batch 25/32 - 237.4ms/batch - loss: 73.22845 - diff: 25.28mlTrain batch 26/32 - 239.4ms/batch - loss: 71.95463 - diff: 24.97mlTrain batch 27/32 - 237.2ms/batch - loss: 70.92928 - diff: 24.92mlTrain batch 28/32 - 238.9ms/batch - loss: 70.22755 - diff: 24.89mlTrain batch 29/32 - 237.7ms/batch - loss: 69.11570 - diff: 24.69mlTrain batch 30/32 - 239.1ms/batch - loss: 68.85319 - diff: 24.77mlTrain batch 31/32 - 237.1ms/batch - loss: 69.83239 - diff: 25.02mlTrain batch 32/32 - 78.6ms/batch - loss: 70.21363 - diff: 24.97mlTrain batch 32/32 - 10.6s 78.6ms/batch - loss: 70.21363 - diff: 24.97ml
Test 1.1s: val_loss: 84.48490 - diff: 28.29ml

Epoch 65: current best loss = 65.02328, at epoch 51
Train batch 1/32 - 236.5ms/batch - loss: 57.46558 - diff: 24.73mlTrain batch 2/32 - 237.5ms/batch - loss: 55.36788 - diff: 24.46mlTrain batch 3/32 - 236.9ms/batch - loss: 60.46353 - diff: 26.04mlTrain batch 4/32 - 238.9ms/batch - loss: 63.65777 - diff: 26.72mlTrain batch 5/32 - 237.0ms/batch - loss: 67.42675 - diff: 26.38mlTrain batch 6/32 - 239.3ms/batch - loss: 67.16132 - diff: 26.46mlTrain batch 7/32 - 236.9ms/batch - loss: 73.08014 - diff: 26.31mlTrain batch 8/32 - 239.5ms/batch - loss: 72.05650 - diff: 26.01mlTrain batch 9/32 - 236.9ms/batch - loss: 72.88637 - diff: 25.82mlTrain batch 10/32 - 239.2ms/batch - loss: 70.40284 - diff: 25.54mlTrain batch 11/32 - 236.3ms/batch - loss: 70.80730 - diff: 25.63mlTrain batch 12/32 - 237.4ms/batch - loss: 70.39398 - diff: 25.54mlTrain batch 13/32 - 237.4ms/batch - loss: 68.51115 - diff: 25.29mlTrain batch 14/32 - 237.6ms/batch - loss: 69.25943 - diff: 25.44mlTrain batch 15/32 - 237.1ms/batch - loss: 71.68904 - diff: 25.96mlTrain batch 16/32 - 237.7ms/batch - loss: 70.46718 - diff: 25.61mlTrain batch 17/32 - 236.5ms/batch - loss: 67.85788 - diff: 24.99mlTrain batch 18/32 - 238.1ms/batch - loss: 66.33424 - diff: 24.76mlTrain batch 19/32 - 236.2ms/batch - loss: 70.68849 - diff: 25.40mlTrain batch 20/32 - 239.6ms/batch - loss: 69.67152 - diff: 25.15mlTrain batch 21/32 - 236.8ms/batch - loss: 67.31541 - diff: 24.68mlTrain batch 22/32 - 240.1ms/batch - loss: 68.47330 - diff: 24.96mlTrain batch 23/32 - 237.3ms/batch - loss: 67.90665 - diff: 24.94mlTrain batch 24/32 - 240.0ms/batch - loss: 67.05924 - diff: 24.82mlTrain batch 25/32 - 236.9ms/batch - loss: 69.20724 - diff: 24.89mlTrain batch 26/32 - 239.7ms/batch - loss: 70.33255 - diff: 25.12mlTrain batch 27/32 - 237.0ms/batch - loss: 69.25063 - diff: 24.94mlTrain batch 28/32 - 239.1ms/batch - loss: 73.88215 - diff: 25.52mlTrain batch 29/32 - 237.2ms/batch - loss: 74.36141 - diff: 25.71mlTrain batch 30/32 - 240.3ms/batch - loss: 74.10272 - diff: 25.64mlTrain batch 31/32 - 237.1ms/batch - loss: 72.73634 - diff: 25.41mlTrain batch 32/32 - 77.8ms/batch - loss: 72.72144 - diff: 25.32mlTrain batch 32/32 - 10.6s 77.8ms/batch - loss: 72.72144 - diff: 25.32ml
Test 1.1s: val_loss: 262.43952 - diff: 51.27ml

Epoch 66: current best loss = 65.02328, at epoch 51
Train batch 1/32 - 236.5ms/batch - loss: 48.89721 - diff: 22.90mlTrain batch 2/32 - 238.1ms/batch - loss: 61.55888 - diff: 25.27mlTrain batch 3/32 - 236.4ms/batch - loss: 90.74194 - diff: 29.54mlTrain batch 4/32 - 238.9ms/batch - loss: 81.11886 - diff: 27.43mlTrain batch 5/32 - 236.7ms/batch - loss: 77.32331 - diff: 26.80mlTrain batch 6/32 - 239.1ms/batch - loss: 81.70579 - diff: 27.52mlTrain batch 7/32 - 236.4ms/batch - loss: 74.80997 - diff: 26.33mlTrain batch 8/32 - 239.2ms/batch - loss: 78.94601 - diff: 26.97mlTrain batch 9/32 - 236.7ms/batch - loss: 77.46521 - diff: 26.89mlTrain batch 10/32 - 240.5ms/batch - loss: 79.51821 - diff: 26.73mlTrain batch 11/32 - 236.9ms/batch - loss: 73.94200 - diff: 25.55mlTrain batch 12/32 - 237.7ms/batch - loss: 73.49188 - diff: 25.29mlTrain batch 13/32 - 236.5ms/batch - loss: 71.03286 - diff: 25.02mlTrain batch 14/32 - 238.7ms/batch - loss: 71.28519 - diff: 25.29mlTrain batch 15/32 - 236.8ms/batch - loss: 72.55888 - diff: 25.43mlTrain batch 16/32 - 240.4ms/batch - loss: 71.31891 - diff: 25.38mlTrain batch 17/32 - 237.1ms/batch - loss: 69.57469 - diff: 25.18mlTrain batch 18/32 - 239.6ms/batch - loss: 69.84724 - diff: 25.31mlTrain batch 19/32 - 236.8ms/batch - loss: 68.63562 - diff: 25.03mlTrain batch 20/32 - 239.2ms/batch - loss: 68.51385 - diff: 25.04mlTrain batch 21/32 - 237.0ms/batch - loss: 68.58021 - diff: 25.05mlTrain batch 22/32 - 239.4ms/batch - loss: 66.78873 - diff: 24.79mlTrain batch 23/32 - 236.6ms/batch - loss: 67.11478 - diff: 24.89mlTrain batch 24/32 - 239.1ms/batch - loss: 66.01718 - diff: 24.63mlTrain batch 25/32 - 236.4ms/batch - loss: 67.56748 - diff: 24.77mlTrain batch 26/32 - 239.3ms/batch - loss: 67.51144 - diff: 24.86mlTrain batch 27/32 - 237.2ms/batch - loss: 66.83099 - diff: 24.77mlTrain batch 28/32 - 239.0ms/batch - loss: 67.69006 - diff: 24.89mlTrain batch 29/32 - 237.4ms/batch - loss: 66.55235 - diff: 24.75mlTrain batch 30/32 - 237.7ms/batch - loss: 65.81766 - diff: 24.66mlTrain batch 31/32 - 237.3ms/batch - loss: 68.59983 - diff: 24.89mlTrain batch 32/32 - 78.7ms/batch - loss: 68.60514 - diff: 24.81mlTrain batch 32/32 - 10.7s 78.7ms/batch - loss: 68.60514 - diff: 24.81ml
Test 1.1s: val_loss: 98.75617 - diff: 31.22ml

Epoch 67: current best loss = 65.02328, at epoch 51
Train batch 1/32 - 236.6ms/batch - loss: 70.51036 - diff: 26.94mlTrain batch 2/32 - 238.3ms/batch - loss: 108.88181 - diff: 29.37mlTrain batch 3/32 - 236.4ms/batch - loss: 86.43602 - diff: 25.60mlTrain batch 4/32 - 249.5ms/batch - loss: 91.83085 - diff: 26.78mlTrain batch 5/32 - 236.8ms/batch - loss: 90.86219 - diff: 27.50mlTrain batch 6/32 - 237.6ms/batch - loss: 84.59750 - diff: 26.23mlTrain batch 7/32 - 236.6ms/batch - loss: 83.47571 - diff: 26.22mlTrain batch 8/32 - 238.6ms/batch - loss: 86.97521 - diff: 26.98mlTrain batch 9/32 - 236.7ms/batch - loss: 97.01320 - diff: 27.33mlTrain batch 10/32 - 238.8ms/batch - loss: 90.43922 - diff: 26.36mlTrain batch 11/32 - 237.1ms/batch - loss: 88.04997 - diff: 26.25mlTrain batch 12/32 - 239.8ms/batch - loss: 85.39928 - diff: 25.84mlTrain batch 13/32 - 237.3ms/batch - loss: 80.86335 - diff: 25.06mlTrain batch 14/32 - 239.5ms/batch - loss: 79.27390 - diff: 24.92mlTrain batch 15/32 - 236.9ms/batch - loss: 81.21049 - diff: 25.62mlTrain batch 16/32 - 239.5ms/batch - loss: 82.11272 - diff: 26.01mlTrain batch 17/32 - 237.8ms/batch - loss: 80.22605 - diff: 25.77mlTrain batch 18/32 - 239.1ms/batch - loss: 79.04455 - diff: 25.56mlTrain batch 19/32 - 236.9ms/batch - loss: 79.49384 - diff: 25.72mlTrain batch 20/32 - 239.5ms/batch - loss: 77.58583 - diff: 25.47mlTrain batch 21/32 - 236.6ms/batch - loss: 76.86036 - diff: 25.52mlTrain batch 22/32 - 239.8ms/batch - loss: 75.01964 - diff: 25.15mlTrain batch 23/32 - 237.6ms/batch - loss: 72.89736 - diff: 24.66mlTrain batch 24/32 - 239.6ms/batch - loss: 71.95334 - diff: 24.57mlTrain batch 25/32 - 237.2ms/batch - loss: 72.00746 - diff: 24.66mlTrain batch 26/32 - 237.4ms/batch - loss: 73.47814 - diff: 24.94mlTrain batch 27/32 - 237.3ms/batch - loss: 73.92683 - diff: 25.24mlTrain batch 28/32 - 237.7ms/batch - loss: 73.34179 - diff: 25.12mlTrain batch 29/32 - 236.9ms/batch - loss: 77.02131 - diff: 25.90mlTrain batch 30/32 - 237.8ms/batch - loss: 77.18541 - diff: 26.05mlTrain batch 31/32 - 236.8ms/batch - loss: 75.87295 - diff: 25.87mlTrain batch 32/32 - 77.8ms/batch - loss: 75.99989 - diff: 25.80mlTrain batch 32/32 - 10.6s 77.8ms/batch - loss: 75.99989 - diff: 25.80ml
Test 1.1s: val_loss: 89.48467 - diff: 27.31ml

Epoch 68: current best loss = 65.02328, at epoch 51
Train batch 1/32 - 236.4ms/batch - loss: 40.03956 - diff: 19.29mlTrain batch 2/32 - 238.3ms/batch - loss: 32.87717 - diff: 17.58mlTrain batch 3/32 - 236.5ms/batch - loss: 54.96126 - diff: 20.85mlTrain batch 4/32 - 240.3ms/batch - loss: 58.74430 - diff: 22.17mlTrain batch 5/32 - 237.1ms/batch - loss: 72.15461 - diff: 25.08mlTrain batch 6/32 - 237.1ms/batch - loss: 70.86072 - diff: 25.21mlTrain batch 7/32 - 237.4ms/batch - loss: 75.89972 - diff: 26.55mlTrain batch 8/32 - 237.8ms/batch - loss: 68.56100 - diff: 24.88mlTrain batch 9/32 - 236.6ms/batch - loss: 67.74702 - diff: 24.90mlTrain batch 10/32 - 238.0ms/batch - loss: 66.10294 - diff: 24.80mlTrain batch 11/32 - 236.5ms/batch - loss: 66.06727 - diff: 24.93mlTrain batch 12/32 - 238.8ms/batch - loss: 67.70903 - diff: 25.33mlTrain batch 13/32 - 237.1ms/batch - loss: 70.19292 - diff: 25.69mlTrain batch 14/32 - 239.2ms/batch - loss: 69.86054 - diff: 25.71mlTrain batch 15/32 - 236.8ms/batch - loss: 69.03869 - diff: 25.55mlTrain batch 16/32 - 239.2ms/batch - loss: 67.00107 - diff: 25.24mlTrain batch 17/32 - 237.1ms/batch - loss: 83.66944 - diff: 26.36mlTrain batch 18/32 - 239.5ms/batch - loss: 82.18116 - diff: 26.28mlTrain batch 19/32 - 237.0ms/batch - loss: 80.93984 - diff: 26.08mlTrain batch 20/32 - 239.9ms/batch - loss: 83.20822 - diff: 26.63mlTrain batch 21/32 - 237.8ms/batch - loss: 80.94305 - diff: 26.24mlTrain batch 22/32 - 238.7ms/batch - loss: 80.46189 - diff: 26.22mlTrain batch 23/32 - 236.9ms/batch - loss: 80.27170 - diff: 26.38mlTrain batch 24/32 - 239.4ms/batch - loss: 80.64429 - diff: 26.50mlTrain batch 25/32 - 237.1ms/batch - loss: 78.78384 - diff: 26.17mlTrain batch 26/32 - 239.6ms/batch - loss: 78.39923 - diff: 26.20mlTrain batch 27/32 - 237.7ms/batch - loss: 77.09289 - diff: 26.01mlTrain batch 28/32 - 237.9ms/batch - loss: 75.69984 - diff: 25.75mlTrain batch 29/32 - 237.5ms/batch - loss: 75.74215 - diff: 25.80mlTrain batch 30/32 - 237.9ms/batch - loss: 75.72261 - diff: 25.85mlTrain batch 31/32 - 237.2ms/batch - loss: 75.72171 - diff: 25.86mlTrain batch 32/32 - 78.4ms/batch - loss: 75.84354 - diff: 25.80mlTrain batch 32/32 - 10.6s 78.4ms/batch - loss: 75.84354 - diff: 25.80ml
Test 1.1s: val_loss: 77.28688 - diff: 26.91ml

Epoch 69: current best loss = 65.02328, at epoch 51
Train batch 1/32 - 236.8ms/batch - loss: 74.59488 - diff: 29.44mlTrain batch 2/32 - 238.1ms/batch - loss: 62.27744 - diff: 25.83mlTrain batch 3/32 - 236.4ms/batch - loss: 56.90266 - diff: 24.80mlTrain batch 4/32 - 238.9ms/batch - loss: 51.02244 - diff: 23.33mlTrain batch 5/32 - 236.6ms/batch - loss: 49.47205 - diff: 23.08mlTrain batch 6/32 - 239.2ms/batch - loss: 48.75105 - diff: 22.40mlTrain batch 7/32 - 237.3ms/batch - loss: 59.44979 - diff: 23.51mlTrain batch 8/32 - 237.7ms/batch - loss: 61.06489 - diff: 23.85mlTrain batch 9/32 - 237.1ms/batch - loss: 63.23890 - diff: 24.13mlTrain batch 10/32 - 238.0ms/batch - loss: 63.28898 - diff: 24.34mlTrain batch 11/32 - 236.4ms/batch - loss: 66.06610 - diff: 24.68mlTrain batch 12/32 - 239.1ms/batch - loss: 67.57286 - diff: 24.84mlTrain batch 13/32 - 236.2ms/batch - loss: 65.40229 - diff: 24.33mlTrain batch 14/32 - 238.8ms/batch - loss: 67.52529 - diff: 24.49mlTrain batch 15/32 - 237.2ms/batch - loss: 65.67279 - diff: 24.11mlTrain batch 16/32 - 240.2ms/batch - loss: 64.45213 - diff: 23.91mlTrain batch 17/32 - 237.2ms/batch - loss: 65.06625 - diff: 24.29mlTrain batch 18/32 - 240.3ms/batch - loss: 65.33636 - diff: 24.38mlTrain batch 19/32 - 237.3ms/batch - loss: 66.12670 - diff: 24.71mlTrain batch 20/32 - 238.9ms/batch - loss: 64.61065 - diff: 24.41mlTrain batch 21/32 - 237.9ms/batch - loss: 63.93629 - diff: 24.23mlTrain batch 22/32 - 239.0ms/batch - loss: 64.79873 - diff: 24.41mlTrain batch 23/32 - 237.6ms/batch - loss: 66.87003 - diff: 24.85mlTrain batch 24/32 - 239.7ms/batch - loss: 65.54603 - diff: 24.55mlTrain batch 25/32 - 237.1ms/batch - loss: 65.73114 - diff: 24.61mlTrain batch 26/32 - 240.4ms/batch - loss: 67.38154 - diff: 24.87mlTrain batch 27/32 - 236.2ms/batch - loss: 65.76682 - diff: 24.58mlTrain batch 28/32 - 242.1ms/batch - loss: 66.92900 - diff: 24.87mlTrain batch 29/32 - 237.5ms/batch - loss: 66.63647 - diff: 24.78mlTrain batch 30/32 - 237.8ms/batch - loss: 65.78939 - diff: 24.64mlTrain batch 31/32 - 237.1ms/batch - loss: 65.52875 - diff: 24.62mlTrain batch 32/32 - 78.3ms/batch - loss: 67.12432 - diff: 24.64mlTrain batch 32/32 - 10.6s 78.3ms/batch - loss: 67.12432 - diff: 24.64ml
Test 1.1s: val_loss: 94.14518 - diff: 29.94ml

Epoch 70: current best loss = 65.02328, at epoch 51
Train batch 1/32 - 236.9ms/batch - loss: 46.61710 - diff: 23.93mlTrain batch 2/32 - 239.1ms/batch - loss: 100.76661 - diff: 27.90mlTrain batch 3/32 - 236.9ms/batch - loss: 89.57270 - diff: 26.93mlTrain batch 4/32 - 239.1ms/batch - loss: 92.92274 - diff: 28.53mlTrain batch 5/32 - 236.4ms/batch - loss: 89.40463 - diff: 28.06mlTrain batch 6/32 - 238.7ms/batch - loss: 78.75711 - diff: 26.03mlTrain batch 7/32 - 237.6ms/batch - loss: 77.18771 - diff: 26.30mlTrain batch 8/32 - 237.8ms/batch - loss: 71.98207 - diff: 25.38mlTrain batch 9/32 - 237.2ms/batch - loss: 74.84100 - diff: 26.20mlTrain batch 10/32 - 237.7ms/batch - loss: 76.21080 - diff: 26.28mlTrain batch 11/32 - 236.4ms/batch - loss: 71.40743 - diff: 25.28mlTrain batch 12/32 - 237.6ms/batch - loss: 80.66041 - diff: 26.01mlTrain batch 13/32 - 237.0ms/batch - loss: 81.99751 - diff: 26.40mlTrain batch 14/32 - 238.9ms/batch - loss: 79.51033 - diff: 26.14mlTrain batch 15/32 - 237.0ms/batch - loss: 80.69024 - diff: 26.77mlTrain batch 16/32 - 239.1ms/batch - loss: 79.79159 - diff: 26.69mlTrain batch 17/32 - 236.5ms/batch - loss: 83.01884 - diff: 27.18mlTrain batch 18/32 - 239.3ms/batch - loss: 84.54691 - diff: 27.56mlTrain batch 19/32 - 237.3ms/batch - loss: 83.46733 - diff: 27.40mlTrain batch 20/32 - 239.3ms/batch - loss: 83.16964 - diff: 27.50mlTrain batch 21/32 - 236.8ms/batch - loss: 80.50407 - diff: 26.92mlTrain batch 22/32 - 239.3ms/batch - loss: 81.06644 - diff: 26.96mlTrain batch 23/32 - 236.8ms/batch - loss: 81.14798 - diff: 26.97mlTrain batch 24/32 - 238.9ms/batch - loss: 81.34004 - diff: 27.16mlTrain batch 25/32 - 237.9ms/batch - loss: 85.51812 - diff: 27.81mlTrain batch 26/32 - 237.2ms/batch - loss: 84.75493 - diff: 27.83mlTrain batch 27/32 - 236.9ms/batch - loss: 84.30175 - diff: 27.68mlTrain batch 28/32 - 237.9ms/batch - loss: 84.66144 - diff: 27.75mlTrain batch 29/32 - 237.8ms/batch - loss: 85.07307 - diff: 28.01mlTrain batch 30/32 - 237.6ms/batch - loss: 85.08578 - diff: 28.09mlTrain batch 31/32 - 236.3ms/batch - loss: 84.31844 - diff: 28.05mlTrain batch 32/32 - 79.7ms/batch - loss: 86.45111 - diff: 28.11mlTrain batch 32/32 - 10.6s 79.7ms/batch - loss: 86.45111 - diff: 28.11ml
Test 1.1s: val_loss: 86.58506 - diff: 28.74ml

Epoch 71: current best loss = 65.02328, at epoch 51
Train batch 1/32 - 236.4ms/batch - loss: 81.52315 - diff: 27.06mlTrain batch 2/32 - 238.7ms/batch - loss: 95.41829 - diff: 31.43mlTrain batch 3/32 - 236.5ms/batch - loss: 91.38401 - diff: 31.23mlTrain batch 4/32 - 238.6ms/batch - loss: 151.05834 - diff: 33.43mlTrain batch 5/32 - 237.8ms/batch - loss: 152.84382 - diff: 35.31mlTrain batch 6/32 - 237.9ms/batch - loss: 135.48765 - diff: 33.39mlTrain batch 7/32 - 237.4ms/batch - loss: 125.69346 - diff: 32.27mlTrain batch 8/32 - 237.7ms/batch - loss: 123.90434 - diff: 32.63mlTrain batch 9/32 - 237.4ms/batch - loss: 114.85720 - diff: 30.99mlTrain batch 10/32 - 237.8ms/batch - loss: 113.58988 - diff: 30.84mlTrain batch 11/32 - 236.5ms/batch - loss: 108.98225 - diff: 30.28mlTrain batch 12/32 - 237.6ms/batch - loss: 106.73251 - diff: 30.49mlTrain batch 13/32 - 236.2ms/batch - loss: 106.63580 - diff: 30.93mlTrain batch 14/32 - 238.6ms/batch - loss: 101.07607 - diff: 29.90mlTrain batch 15/32 - 237.0ms/batch - loss: 100.46240 - diff: 29.87mlTrain batch 16/32 - 238.4ms/batch - loss: 99.13671 - diff: 29.86mlTrain batch 17/32 - 237.0ms/batch - loss: 97.66558 - diff: 29.91mlTrain batch 18/32 - 238.9ms/batch - loss: 94.54305 - diff: 29.42mlTrain batch 19/32 - 237.0ms/batch - loss: 93.55772 - diff: 29.51mlTrain batch 20/32 - 239.5ms/batch - loss: 91.54647 - diff: 29.17mlTrain batch 21/32 - 236.8ms/batch - loss: 89.79497 - diff: 28.87mlTrain batch 22/32 - 239.9ms/batch - loss: 87.42940 - diff: 28.49mlTrain batch 23/32 - 237.6ms/batch - loss: 86.22627 - diff: 28.41mlTrain batch 24/32 - 239.7ms/batch - loss: 92.72938 - diff: 29.19mlTrain batch 25/32 - 236.9ms/batch - loss: 92.03595 - diff: 29.21mlTrain batch 26/32 - 239.8ms/batch - loss: 93.98690 - diff: 29.60mlTrain batch 27/32 - 236.8ms/batch - loss: 91.94600 - diff: 29.11mlTrain batch 28/32 - 239.4ms/batch - loss: 91.78359 - diff: 29.08mlTrain batch 29/32 - 237.5ms/batch - loss: 94.33613 - diff: 29.46mlTrain batch 30/32 - 237.6ms/batch - loss: 92.34293 - diff: 29.13mlTrain batch 31/32 - 237.4ms/batch - loss: 90.76190 - diff: 28.82mlTrain batch 32/32 - 78.0ms/batch - loss: 90.58082 - diff: 28.69mlTrain batch 32/32 - 10.6s 78.0ms/batch - loss: 90.58082 - diff: 28.69ml
Test 1.1s: val_loss: 62.45142 - diff: 23.67ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 72: current best loss = 62.45142, at epoch 71
Train batch 1/32 - 236.4ms/batch - loss: 45.31304 - diff: 22.43mlTrain batch 2/32 - 237.2ms/batch - loss: 37.92999 - diff: 18.85mlTrain batch 3/32 - 236.3ms/batch - loss: 43.01449 - diff: 20.81mlTrain batch 4/32 - 237.6ms/batch - loss: 49.30736 - diff: 22.08mlTrain batch 5/32 - 236.9ms/batch - loss: 57.90449 - diff: 23.75mlTrain batch 6/32 - 237.7ms/batch - loss: 53.08039 - diff: 22.77mlTrain batch 7/32 - 236.7ms/batch - loss: 53.07476 - diff: 22.63mlTrain batch 8/32 - 238.0ms/batch - loss: 55.90488 - diff: 23.09mlTrain batch 9/32 - 236.2ms/batch - loss: 55.38267 - diff: 22.69mlTrain batch 10/32 - 237.6ms/batch - loss: 53.10102 - diff: 22.35mlTrain batch 11/32 - 236.5ms/batch - loss: 64.22083 - diff: 24.08mlTrain batch 12/32 - 238.2ms/batch - loss: 70.93283 - diff: 24.76mlTrain batch 13/32 - 236.5ms/batch - loss: 68.56686 - diff: 24.45mlTrain batch 14/32 - 237.7ms/batch - loss: 68.22840 - diff: 24.58mlTrain batch 15/32 - 237.3ms/batch - loss: 65.50084 - diff: 24.01mlTrain batch 16/32 - 237.3ms/batch - loss: 64.46040 - diff: 24.01mlTrain batch 17/32 - 237.3ms/batch - loss: 64.46210 - diff: 24.12mlTrain batch 18/32 - 237.6ms/batch - loss: 64.60007 - diff: 24.17mlTrain batch 19/32 - 236.4ms/batch - loss: 62.41635 - diff: 23.79mlTrain batch 20/32 - 237.5ms/batch - loss: 61.24788 - diff: 23.62mlTrain batch 21/32 - 236.6ms/batch - loss: 59.99096 - diff: 23.41mlTrain batch 22/32 - 238.0ms/batch - loss: 66.06159 - diff: 24.34mlTrain batch 23/32 - 236.5ms/batch - loss: 64.90286 - diff: 24.15mlTrain batch 24/32 - 237.9ms/batch - loss: 65.39024 - diff: 24.23mlTrain batch 25/32 - 236.5ms/batch - loss: 64.30763 - diff: 24.00mlTrain batch 26/32 - 238.2ms/batch - loss: 64.08779 - diff: 23.94mlTrain batch 27/32 - 236.7ms/batch - loss: 64.12776 - diff: 24.02mlTrain batch 28/32 - 238.4ms/batch - loss: 64.34132 - diff: 24.17mlTrain batch 29/32 - 236.6ms/batch - loss: 72.73797 - diff: 24.93mlTrain batch 30/32 - 238.8ms/batch - loss: 72.00583 - diff: 24.87mlTrain batch 31/32 - 236.6ms/batch - loss: 73.52170 - diff: 25.17mlTrain batch 32/32 - 77.9ms/batch - loss: 77.91301 - diff: 25.31mlTrain batch 32/32 - 10.6s 77.9ms/batch - loss: 77.91301 - diff: 25.31ml
Test 1.1s: val_loss: 101.31723 - diff: 31.50ml

Epoch 73: current best loss = 62.45142, at epoch 71
Train batch 1/32 - 236.7ms/batch - loss: 64.49667 - diff: 28.04mlTrain batch 2/32 - 237.8ms/batch - loss: 67.12065 - diff: 28.78mlTrain batch 3/32 - 236.5ms/batch - loss: 65.55172 - diff: 27.41mlTrain batch 4/32 - 237.3ms/batch - loss: 64.45119 - diff: 26.60mlTrain batch 5/32 - 236.7ms/batch - loss: 59.42626 - diff: 24.89mlTrain batch 6/32 - 238.3ms/batch - loss: 56.29039 - diff: 23.71mlTrain batch 7/32 - 235.9ms/batch - loss: 53.92059 - diff: 22.98mlTrain batch 8/32 - 237.7ms/batch - loss: 60.75860 - diff: 23.94mlTrain batch 9/32 - 236.7ms/batch - loss: 60.19611 - diff: 23.89mlTrain batch 10/32 - 237.4ms/batch - loss: 60.99500 - diff: 24.00mlTrain batch 11/32 - 236.8ms/batch - loss: 60.27448 - diff: 23.74mlTrain batch 12/32 - 238.1ms/batch - loss: 63.25218 - diff: 24.38mlTrain batch 13/32 - 236.4ms/batch - loss: 64.75339 - diff: 24.71mlTrain batch 14/32 - 238.1ms/batch - loss: 63.21515 - diff: 24.44mlTrain batch 15/32 - 236.5ms/batch - loss: 63.63961 - diff: 24.71mlTrain batch 16/32 - 237.7ms/batch - loss: 63.01783 - diff: 24.79mlTrain batch 17/32 - 236.5ms/batch - loss: 64.54336 - diff: 25.25mlTrain batch 18/32 - 238.5ms/batch - loss: 64.34806 - diff: 25.29mlTrain batch 19/32 - 236.6ms/batch - loss: 63.43487 - diff: 25.05mlTrain batch 20/32 - 238.7ms/batch - loss: 63.47732 - diff: 25.09mlTrain batch 21/32 - 236.6ms/batch - loss: 64.25520 - diff: 25.37mlTrain batch 22/32 - 238.8ms/batch - loss: 65.10412 - diff: 25.51mlTrain batch 23/32 - 237.7ms/batch - loss: 64.20875 - diff: 25.31mlTrain batch 24/32 - 238.2ms/batch - loss: 63.74490 - diff: 25.21mlTrain batch 25/32 - 237.6ms/batch - loss: 63.08354 - diff: 25.11mlTrain batch 26/32 - 237.7ms/batch - loss: 64.49157 - diff: 25.45mlTrain batch 27/32 - 237.3ms/batch - loss: 66.21992 - diff: 25.81mlTrain batch 28/32 - 237.5ms/batch - loss: 66.90444 - diff: 25.86mlTrain batch 29/32 - 236.5ms/batch - loss: 72.20133 - diff: 26.40mlTrain batch 30/32 - 238.3ms/batch - loss: 72.51610 - diff: 26.46mlTrain batch 31/32 - 236.3ms/batch - loss: 78.38466 - diff: 26.62mlTrain batch 32/32 - 84.2ms/batch - loss: 83.04843 - diff: 26.76mlTrain batch 32/32 - 10.7s 84.2ms/batch - loss: 83.04843 - diff: 26.76ml
Test 1.1s: val_loss: 95.51431 - diff: 30.33ml

Epoch 74: current best loss = 62.45142, at epoch 71
Train batch 1/32 - 236.2ms/batch - loss: 58.29226 - diff: 26.17mlTrain batch 2/32 - 237.6ms/batch - loss: 43.82278 - diff: 21.65mlTrain batch 3/32 - 236.3ms/batch - loss: 54.71152 - diff: 22.82mlTrain batch 4/32 - 241.0ms/batch - loss: 61.21273 - diff: 23.76mlTrain batch 5/32 - 236.8ms/batch - loss: 60.98380 - diff: 24.09mlTrain batch 6/32 - 238.0ms/batch - loss: 64.59854 - diff: 25.23mlTrain batch 7/32 - 236.6ms/batch - loss: 67.94845 - diff: 26.06mlTrain batch 8/32 - 238.5ms/batch - loss: 65.92870 - diff: 25.77mlTrain batch 9/32 - 236.5ms/batch - loss: 68.46703 - diff: 26.36mlTrain batch 10/32 - 238.3ms/batch - loss: 69.59815 - diff: 26.53mlTrain batch 11/32 - 236.8ms/batch - loss: 70.87496 - diff: 26.77mlTrain batch 12/32 - 238.3ms/batch - loss: 72.65582 - diff: 26.98mlTrain batch 13/32 - 236.8ms/batch - loss: 70.56590 - diff: 26.46mlTrain batch 14/32 - 238.6ms/batch - loss: 68.06252 - diff: 25.84mlTrain batch 15/32 - 237.0ms/batch - loss: 66.95943 - diff: 25.58mlTrain batch 16/32 - 239.0ms/batch - loss: 65.36622 - diff: 25.28mlTrain batch 17/32 - 236.7ms/batch - loss: 68.50758 - diff: 26.02mlTrain batch 18/32 - 236.9ms/batch - loss: 69.92910 - diff: 26.12mlTrain batch 19/32 - 237.7ms/batch - loss: 69.59394 - diff: 26.18mlTrain batch 20/32 - 238.8ms/batch - loss: 68.57028 - diff: 26.05mlTrain batch 21/32 - 237.5ms/batch - loss: 71.18391 - diff: 26.59mlTrain batch 22/32 - 237.7ms/batch - loss: 70.53269 - diff: 26.28mlTrain batch 23/32 - 236.8ms/batch - loss: 68.89796 - diff: 26.05mlTrain batch 24/32 - 237.5ms/batch - loss: 67.40605 - diff: 25.70mlTrain batch 25/32 - 236.7ms/batch - loss: 79.69254 - diff: 26.44mlTrain batch 26/32 - 238.0ms/batch - loss: 80.19958 - diff: 26.60mlTrain batch 27/32 - 236.4ms/batch - loss: 81.18484 - diff: 26.54mlTrain batch 28/32 - 237.9ms/batch - loss: 80.94388 - diff: 26.57mlTrain batch 29/32 - 237.0ms/batch - loss: 80.39047 - diff: 26.51mlTrain batch 30/32 - 238.6ms/batch - loss: 80.72002 - diff: 26.60mlTrain batch 31/32 - 236.5ms/batch - loss: 79.95387 - diff: 26.52mlTrain batch 32/32 - 78.1ms/batch - loss: 82.31834 - diff: 26.57mlTrain batch 32/32 - 10.6s 78.1ms/batch - loss: 82.31834 - diff: 26.57ml
Test 1.1s: val_loss: 99.69336 - diff: 30.88ml

Epoch 75: current best loss = 62.45142, at epoch 71
Train batch 1/32 - 236.5ms/batch - loss: 79.79343 - diff: 31.10mlTrain batch 2/32 - 238.6ms/batch - loss: 155.14279 - diff: 34.10mlTrain batch 3/32 - 236.8ms/batch - loss: 116.58107 - diff: 29.34mlTrain batch 4/32 - 237.8ms/batch - loss: 104.95968 - diff: 29.01mlTrain batch 5/32 - 237.8ms/batch - loss: 104.67209 - diff: 29.08mlTrain batch 6/32 - 238.9ms/batch - loss: 94.17196 - diff: 27.43mlTrain batch 7/32 - 237.1ms/batch - loss: 88.25732 - diff: 26.86mlTrain batch 8/32 - 239.7ms/batch - loss: 82.68704 - diff: 26.25mlTrain batch 9/32 - 236.6ms/batch - loss: 79.05160 - diff: 26.02mlTrain batch 10/32 - 238.3ms/batch - loss: 78.95218 - diff: 26.08mlTrain batch 11/32 - 237.1ms/batch - loss: 79.42247 - diff: 26.31mlTrain batch 12/32 - 238.4ms/batch - loss: 78.02976 - diff: 26.25mlTrain batch 13/32 - 236.4ms/batch - loss: 79.20807 - diff: 26.66mlTrain batch 14/32 - 239.2ms/batch - loss: 75.30361 - diff: 25.94mlTrain batch 15/32 - 236.7ms/batch - loss: 73.71911 - diff: 25.77mlTrain batch 16/32 - 239.4ms/batch - loss: 73.68002 - diff: 26.05mlTrain batch 17/32 - 237.2ms/batch - loss: 75.14027 - diff: 26.53mlTrain batch 18/32 - 237.8ms/batch - loss: 80.09448 - diff: 26.89mlTrain batch 19/32 - 237.6ms/batch - loss: 82.99232 - diff: 27.35mlTrain batch 20/32 - 237.6ms/batch - loss: 80.46226 - diff: 26.90mlTrain batch 21/32 - 237.2ms/batch - loss: 80.50930 - diff: 27.04mlTrain batch 22/32 - 237.7ms/batch - loss: 78.12763 - diff: 26.47mlTrain batch 23/32 - 236.7ms/batch - loss: 77.56778 - diff: 26.28mlTrain batch 24/32 - 238.0ms/batch - loss: 76.94036 - diff: 26.26mlTrain batch 25/32 - 236.5ms/batch - loss: 75.25260 - diff: 25.96mlTrain batch 26/32 - 239.4ms/batch - loss: 74.34187 - diff: 25.94mlTrain batch 27/32 - 236.7ms/batch - loss: 74.61417 - diff: 25.91mlTrain batch 28/32 - 239.1ms/batch - loss: 74.73835 - diff: 26.07mlTrain batch 29/32 - 237.1ms/batch - loss: 74.28176 - diff: 25.94mlTrain batch 30/32 - 239.5ms/batch - loss: 73.62643 - diff: 25.78mlTrain batch 31/32 - 236.8ms/batch - loss: 73.85656 - diff: 25.91mlTrain batch 32/32 - 77.8ms/batch - loss: 73.84001 - diff: 25.81mlTrain batch 32/32 - 10.6s 77.8ms/batch - loss: 73.84001 - diff: 25.81ml
Test 1.1s: val_loss: 109.55714 - diff: 33.15ml

Epoch 76: current best loss = 62.45142, at epoch 71
Train batch 1/32 - 236.3ms/batch - loss: 71.88972 - diff: 29.97mlTrain batch 2/32 - 238.0ms/batch - loss: 45.95511 - diff: 22.77mlTrain batch 3/32 - 236.7ms/batch - loss: 52.22093 - diff: 22.97mlTrain batch 4/32 - 239.1ms/batch - loss: 52.08569 - diff: 22.83mlTrain batch 5/32 - 236.6ms/batch - loss: 51.54009 - diff: 22.83mlTrain batch 6/32 - 238.7ms/batch - loss: 57.56001 - diff: 24.28mlTrain batch 7/32 - 237.0ms/batch - loss: 55.98717 - diff: 24.15mlTrain batch 8/32 - 239.5ms/batch - loss: 58.59480 - diff: 24.49mlTrain batch 9/32 - 236.8ms/batch - loss: 64.66307 - diff: 25.30mlTrain batch 10/32 - 238.9ms/batch - loss: 67.18497 - diff: 25.85mlTrain batch 11/32 - 236.9ms/batch - loss: 65.18544 - diff: 25.62mlTrain batch 12/32 - 239.5ms/batch - loss: 64.29162 - diff: 25.35mlTrain batch 13/32 - 236.7ms/batch - loss: 64.09978 - diff: 25.54mlTrain batch 14/32 - 238.8ms/batch - loss: 70.09540 - diff: 25.99mlTrain batch 15/32 - 237.4ms/batch - loss: 72.16238 - diff: 26.46mlTrain batch 16/32 - 237.7ms/batch - loss: 70.66770 - diff: 26.23mlTrain batch 17/32 - 237.2ms/batch - loss: 72.53592 - diff: 26.64mlTrain batch 18/32 - 237.9ms/batch - loss: 72.35009 - diff: 26.83mlTrain batch 19/32 - 236.7ms/batch - loss: 70.60544 - diff: 26.49mlTrain batch 20/32 - 239.1ms/batch - loss: 70.12797 - diff: 26.49mlTrain batch 21/32 - 236.5ms/batch - loss: 71.78549 - diff: 26.84mlTrain batch 22/32 - 239.7ms/batch - loss: 72.90291 - diff: 27.12mlTrain batch 23/32 - 237.4ms/batch - loss: 74.56623 - diff: 27.33mlTrain batch 24/32 - 240.0ms/batch - loss: 73.67483 - diff: 27.05mlTrain batch 25/32 - 236.9ms/batch - loss: 72.28975 - diff: 26.76mlTrain batch 26/32 - 239.4ms/batch - loss: 73.53815 - diff: 26.81mlTrain batch 27/32 - 237.4ms/batch - loss: 72.44938 - diff: 26.67mlTrain batch 28/32 - 239.0ms/batch - loss: 72.53678 - diff: 26.61mlTrain batch 29/32 - 237.2ms/batch - loss: 72.02273 - diff: 26.53mlTrain batch 30/32 - 239.6ms/batch - loss: 70.88374 - diff: 26.30mlTrain batch 31/32 - 237.4ms/batch - loss: 70.25784 - diff: 26.16mlTrain batch 32/32 - 78.3ms/batch - loss: 71.26772 - diff: 26.16mlTrain batch 32/32 - 10.6s 78.3ms/batch - loss: 71.26772 - diff: 26.16ml
Test 1.1s: val_loss: 81.60342 - diff: 27.58ml

Epoch 77: current best loss = 62.45142, at epoch 71
Train batch 1/32 - 236.8ms/batch - loss: 27.47177 - diff: 17.98mlTrain batch 2/32 - 237.7ms/batch - loss: 39.60382 - diff: 19.62mlTrain batch 3/32 - 236.6ms/batch - loss: 54.70800 - diff: 22.45mlTrain batch 4/32 - 238.9ms/batch - loss: 56.00133 - diff: 22.14mlTrain batch 5/32 - 237.3ms/batch - loss: 93.24984 - diff: 26.99mlTrain batch 6/32 - 238.9ms/batch - loss: 87.97538 - diff: 26.81mlTrain batch 7/32 - 237.2ms/batch - loss: 79.85988 - diff: 25.56mlTrain batch 8/32 - 239.5ms/batch - loss: 75.91249 - diff: 25.08mlTrain batch 9/32 - 236.9ms/batch - loss: 73.84946 - diff: 24.81mlTrain batch 10/32 - 238.6ms/batch - loss: 79.36498 - diff: 26.14mlTrain batch 11/32 - 237.2ms/batch - loss: 77.23994 - diff: 26.03mlTrain batch 12/32 - 237.7ms/batch - loss: 79.24165 - diff: 26.28mlTrain batch 13/32 - 237.7ms/batch - loss: 75.66593 - diff: 25.51mlTrain batch 14/32 - 237.9ms/batch - loss: 74.58033 - diff: 25.39mlTrain batch 15/32 - 236.7ms/batch - loss: 72.47406 - diff: 25.00mlTrain batch 16/32 - 238.3ms/batch - loss: 72.19189 - diff: 25.22mlTrain batch 17/32 - 237.0ms/batch - loss: 73.17251 - diff: 25.39mlTrain batch 18/32 - 239.6ms/batch - loss: 71.14204 - diff: 25.11mlTrain batch 19/32 - 236.9ms/batch - loss: 70.40813 - diff: 25.12mlTrain batch 20/32 - 240.8ms/batch - loss: 72.47265 - diff: 25.55mlTrain batch 21/32 - 237.4ms/batch - loss: 72.83109 - diff: 25.70mlTrain batch 22/32 - 240.3ms/batch - loss: 73.70865 - diff: 25.85mlTrain batch 23/32 - 237.0ms/batch - loss: 72.38041 - diff: 25.74mlTrain batch 24/32 - 239.9ms/batch - loss: 72.56973 - diff: 25.94mlTrain batch 25/32 - 238.4ms/batch - loss: 71.60224 - diff: 25.81mlTrain batch 26/32 - 239.9ms/batch - loss: 73.49962 - diff: 26.16mlTrain batch 27/32 - 237.5ms/batch - loss: 74.04646 - diff: 26.27mlTrain batch 28/32 - 240.7ms/batch - loss: 73.21510 - diff: 26.03mlTrain batch 29/32 - 237.4ms/batch - loss: 73.26468 - diff: 26.00mlTrain batch 30/32 - 240.0ms/batch - loss: 73.99773 - diff: 26.23mlTrain batch 31/32 - 237.6ms/batch - loss: 73.64619 - diff: 26.20mlTrain batch 32/32 - 78.4ms/batch - loss: 80.35755 - diff: 26.39mlTrain batch 32/32 - 10.6s 78.4ms/batch - loss: 80.35755 - diff: 26.39ml
Test 1.1s: val_loss: 78.07546 - diff: 25.27ml

Epoch 78: current best loss = 62.45142, at epoch 71
Train batch 1/32 - 236.6ms/batch - loss: 42.14400 - diff: 20.57mlTrain batch 2/32 - 238.8ms/batch - loss: 34.50399 - diff: 18.76mlTrain batch 3/32 - 236.7ms/batch - loss: 38.89663 - diff: 19.57mlTrain batch 4/32 - 239.8ms/batch - loss: 53.95682 - diff: 23.09mlTrain batch 5/32 - 237.3ms/batch - loss: 49.73546 - diff: 22.33mlTrain batch 6/32 - 239.2ms/batch - loss: 47.82045 - diff: 21.80mlTrain batch 7/32 - 237.8ms/batch - loss: 51.93270 - diff: 22.54mlTrain batch 8/32 - 240.1ms/batch - loss: 48.72205 - diff: 21.70mlTrain batch 9/32 - 237.3ms/batch - loss: 53.07330 - diff: 22.58mlTrain batch 10/32 - 239.9ms/batch - loss: 58.17082 - diff: 23.85mlTrain batch 11/32 - 237.2ms/batch - loss: 58.31350 - diff: 23.84mlTrain batch 12/32 - 237.5ms/batch - loss: 74.49010 - diff: 25.00mlTrain batch 13/32 - 237.3ms/batch - loss: 73.20717 - diff: 25.15mlTrain batch 14/32 - 239.3ms/batch - loss: 69.36410 - diff: 24.28mlTrain batch 15/32 - 236.5ms/batch - loss: 70.59058 - diff: 24.71mlTrain batch 16/32 - 239.3ms/batch - loss: 70.19608 - diff: 24.72mlTrain batch 17/32 - 236.9ms/batch - loss: 69.02288 - diff: 24.72mlTrain batch 18/32 - 239.9ms/batch - loss: 67.15101 - diff: 24.38mlTrain batch 19/32 - 237.2ms/batch - loss: 66.75390 - diff: 24.23mlTrain batch 20/32 - 241.1ms/batch - loss: 65.71759 - diff: 24.11mlTrain batch 21/32 - 237.0ms/batch - loss: 64.16457 - diff: 23.83mlTrain batch 22/32 - 240.6ms/batch - loss: 64.23096 - diff: 23.82mlTrain batch 23/32 - 236.6ms/batch - loss: 67.52692 - diff: 24.51mlTrain batch 24/32 - 240.4ms/batch - loss: 75.68681 - diff: 25.19mlTrain batch 25/32 - 236.9ms/batch - loss: 78.56847 - diff: 25.95mlTrain batch 26/32 - 240.4ms/batch - loss: 77.70670 - diff: 25.90mlTrain batch 27/32 - 237.7ms/batch - loss: 79.37134 - diff: 26.21mlTrain batch 28/32 - 240.7ms/batch - loss: 79.34584 - diff: 26.28mlTrain batch 29/32 - 237.0ms/batch - loss: 79.48957 - diff: 26.40mlTrain batch 30/32 - 236.8ms/batch - loss: 78.88438 - diff: 26.36mlTrain batch 31/32 - 237.6ms/batch - loss: 77.75807 - diff: 26.14mlTrain batch 32/32 - 78.2ms/batch - loss: 78.30484 - diff: 26.09mlTrain batch 32/32 - 10.6s 78.2ms/batch - loss: 78.30484 - diff: 26.09ml
Test 1.1s: val_loss: 85.41387 - diff: 28.42ml

Epoch 79: current best loss = 62.45142, at epoch 71
Train batch 1/32 - 236.3ms/batch - loss: 73.66032 - diff: 29.02mlTrain batch 2/32 - 237.5ms/batch - loss: 55.07733 - diff: 24.26mlTrain batch 3/32 - 236.9ms/batch - loss: 59.06401 - diff: 25.76mlTrain batch 4/32 - 238.8ms/batch - loss: 95.22248 - diff: 29.44mlTrain batch 5/32 - 237.7ms/batch - loss: 88.16771 - diff: 28.69mlTrain batch 6/32 - 239.5ms/batch - loss: 115.24270 - diff: 30.84mlTrain batch 7/32 - 236.9ms/batch - loss: 105.43182 - diff: 29.82mlTrain batch 8/32 - 240.2ms/batch - loss: 97.08849 - diff: 28.74mlTrain batch 9/32 - 236.6ms/batch - loss: 96.84198 - diff: 28.69mlTrain batch 10/32 - 240.8ms/batch - loss: 90.64082 - diff: 27.62mlTrain batch 11/32 - 237.5ms/batch - loss: 92.24736 - diff: 28.26mlTrain batch 12/32 - 238.8ms/batch - loss: 89.84999 - diff: 27.99mlTrain batch 13/32 - 237.8ms/batch - loss: 91.25480 - diff: 28.67mlTrain batch 14/32 - 238.0ms/batch - loss: 90.33972 - diff: 28.42mlTrain batch 15/32 - 236.2ms/batch - loss: 88.55191 - diff: 28.11mlTrain batch 16/32 - 238.1ms/batch - loss: 85.31809 - diff: 27.63mlTrain batch 17/32 - 236.7ms/batch - loss: 85.43056 - diff: 27.70mlTrain batch 18/32 - 239.5ms/batch - loss: 85.39751 - diff: 27.92mlTrain batch 19/32 - 237.1ms/batch - loss: 86.37455 - diff: 28.21mlTrain batch 20/32 - 241.4ms/batch - loss: 85.08037 - diff: 28.17mlTrain batch 21/32 - 237.0ms/batch - loss: 84.12018 - diff: 28.19mlTrain batch 22/32 - 239.9ms/batch - loss: 81.34399 - diff: 27.54mlTrain batch 23/32 - 236.9ms/batch - loss: 82.16561 - diff: 27.70mlTrain batch 24/32 - 239.9ms/batch - loss: 80.44239 - diff: 27.45mlTrain batch 25/32 - 236.9ms/batch - loss: 82.34701 - diff: 27.84mlTrain batch 26/32 - 239.9ms/batch - loss: 80.25482 - diff: 27.38mlTrain batch 27/32 - 237.7ms/batch - loss: 78.72226 - diff: 27.19mlTrain batch 28/32 - 239.5ms/batch - loss: 77.86714 - diff: 27.13mlTrain batch 29/32 - 237.7ms/batch - loss: 76.68476 - diff: 26.84mlTrain batch 30/32 - 239.9ms/batch - loss: 75.53073 - diff: 26.67mlTrain batch 31/32 - 236.9ms/batch - loss: 75.51349 - diff: 26.71mlTrain batch 32/32 - 78.0ms/batch - loss: 75.93680 - diff: 26.65mlTrain batch 32/32 - 10.6s 78.0ms/batch - loss: 75.93680 - diff: 26.65ml
Test 1.1s: val_loss: 67.75370 - diff: 24.54ml

Epoch 80: current best loss = 62.45142, at epoch 71
Train batch 1/32 - 236.7ms/batch - loss: 91.46790 - diff: 25.82mlTrain batch 2/32 - 238.6ms/batch - loss: 97.82985 - diff: 28.56mlTrain batch 3/32 - 236.5ms/batch - loss: 111.19992 - diff: 31.59mlTrain batch 4/32 - 239.7ms/batch - loss: 104.14362 - diff: 30.57mlTrain batch 5/32 - 236.7ms/batch - loss: 89.98178 - diff: 28.29mlTrain batch 6/32 - 239.8ms/batch - loss: 89.26965 - diff: 28.54mlTrain batch 7/32 - 237.7ms/batch - loss: 85.33623 - diff: 27.74mlTrain batch 8/32 - 238.9ms/batch - loss: 83.59897 - diff: 27.23mlTrain batch 9/32 - 237.4ms/batch - loss: 79.28573 - diff: 26.74mlTrain batch 10/32 - 237.7ms/batch - loss: 78.72956 - diff: 27.14mlTrain batch 11/32 - 237.3ms/batch - loss: 76.94473 - diff: 27.00mlTrain batch 12/32 - 237.5ms/batch - loss: 77.45176 - diff: 26.90mlTrain batch 13/32 - 237.3ms/batch - loss: 75.57504 - diff: 26.75mlTrain batch 14/32 - 237.5ms/batch - loss: 88.85143 - diff: 27.69mlTrain batch 15/32 - 236.4ms/batch - loss: 86.90950 - diff: 27.50mlTrain batch 16/32 - 240.1ms/batch - loss: 93.77218 - diff: 28.37mlTrain batch 17/32 - 236.9ms/batch - loss: 95.18706 - diff: 28.94mlTrain batch 18/32 - 239.8ms/batch - loss: 91.85431 - diff: 28.28mlTrain batch 19/32 - 237.5ms/batch - loss: 90.21631 - diff: 28.06mlTrain batch 20/32 - 240.8ms/batch - loss: 88.63878 - diff: 27.77mlTrain batch 21/32 - 237.1ms/batch - loss: 87.36690 - diff: 27.63mlTrain batch 22/32 - 240.4ms/batch - loss: 86.62250 - diff: 27.44mlTrain batch 23/32 - 236.9ms/batch - loss: 83.74602 - diff: 26.88mlTrain batch 24/32 - 239.8ms/batch - loss: 81.64229 - diff: 26.60mlTrain batch 25/32 - 237.1ms/batch - loss: 80.08641 - diff: 26.40mlTrain batch 26/32 - 240.4ms/batch - loss: 78.60644 - diff: 26.25mlTrain batch 27/32 - 237.5ms/batch - loss: 78.24651 - diff: 26.21mlTrain batch 28/32 - 239.9ms/batch - loss: 77.31299 - diff: 26.13mlTrain batch 29/32 - 237.6ms/batch - loss: 76.02137 - diff: 25.91mlTrain batch 30/32 - 240.5ms/batch - loss: 75.28324 - diff: 25.79mlTrain batch 31/32 - 237.6ms/batch - loss: 74.16928 - diff: 25.61mlTrain batch 32/32 - 78.2ms/batch - loss: 76.88456 - diff: 25.67mlTrain batch 32/32 - 10.6s 78.2ms/batch - loss: 76.88456 - diff: 25.67ml
Test 1.1s: val_loss: 110.08233 - diff: 33.29ml

Epoch 81: current best loss = 62.45142, at epoch 71
Train batch 1/32 - 236.8ms/batch - loss: 35.98487 - diff: 19.96mlTrain batch 2/32 - 237.7ms/batch - loss: 37.06342 - diff: 20.35mlTrain batch 3/32 - 236.7ms/batch - loss: 31.90080 - diff: 18.51mlTrain batch 4/32 - 239.2ms/batch - loss: 34.42852 - diff: 18.59mlTrain batch 5/32 - 237.1ms/batch - loss: 33.50235 - diff: 18.21mlTrain batch 6/32 - 239.7ms/batch - loss: 41.67422 - diff: 19.90mlTrain batch 7/32 - 236.8ms/batch - loss: 40.26220 - diff: 19.54mlTrain batch 8/32 - 239.5ms/batch - loss: 41.76419 - diff: 19.74mlTrain batch 9/32 - 237.7ms/batch - loss: 42.59185 - diff: 19.82mlTrain batch 10/32 - 237.9ms/batch - loss: 42.81061 - diff: 20.11mlTrain batch 11/32 - 237.2ms/batch - loss: 45.90048 - diff: 20.61mlTrain batch 12/32 - 237.3ms/batch - loss: 47.94782 - diff: 21.26mlTrain batch 13/32 - 236.4ms/batch - loss: 51.60331 - diff: 21.75mlTrain batch 14/32 - 239.6ms/batch - loss: 54.50987 - diff: 22.27mlTrain batch 15/32 - 237.3ms/batch - loss: 55.02252 - diff: 22.41mlTrain batch 16/32 - 241.2ms/batch - loss: 54.55459 - diff: 22.16mlTrain batch 17/32 - 237.0ms/batch - loss: 55.52829 - diff: 22.34mlTrain batch 18/32 - 240.4ms/batch - loss: 54.26601 - diff: 22.04mlTrain batch 19/32 - 237.3ms/batch - loss: 56.00471 - diff: 22.50mlTrain batch 20/32 - 239.9ms/batch - loss: 56.76047 - diff: 22.76mlTrain batch 21/32 - 237.0ms/batch - loss: 55.99699 - diff: 22.63mlTrain batch 22/32 - 239.3ms/batch - loss: 56.58402 - diff: 22.65mlTrain batch 23/32 - 237.2ms/batch - loss: 56.15775 - diff: 22.64mlTrain batch 24/32 - 239.8ms/batch - loss: 56.47354 - diff: 22.78mlTrain batch 25/32 - 237.4ms/batch - loss: 55.68962 - diff: 22.63mlTrain batch 26/32 - 240.6ms/batch - loss: 57.51141 - diff: 22.93mlTrain batch 27/32 - 237.6ms/batch - loss: 57.38173 - diff: 23.01mlTrain batch 28/32 - 238.0ms/batch - loss: 57.08931 - diff: 22.93mlTrain batch 29/32 - 237.2ms/batch - loss: 56.99764 - diff: 22.89mlTrain batch 30/32 - 237.6ms/batch - loss: 56.96499 - diff: 22.99mlTrain batch 31/32 - 236.8ms/batch - loss: 56.90423 - diff: 22.96mlTrain batch 32/32 - 79.1ms/batch - loss: 59.15986 - diff: 23.04mlTrain batch 32/32 - 10.6s 79.1ms/batch - loss: 59.15986 - diff: 23.04ml
Test 1.1s: val_loss: 56.70134 - diff: 22.84ml
Saving new best model in models/checkpoints/2CH_preproc1_150x150_bySlices_dataset_allViews_Diastole_WideResNet50_0_Adam-0.001_MSE_DA3_best

Epoch 82: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.0ms/batch - loss: 63.65036 - diff: 24.72mlTrain batch 2/32 - 237.8ms/batch - loss: 60.35743 - diff: 25.09mlTrain batch 3/32 - 236.8ms/batch - loss: 61.57479 - diff: 24.54mlTrain batch 4/32 - 238.3ms/batch - loss: 52.92908 - diff: 22.15mlTrain batch 5/32 - 236.4ms/batch - loss: 55.41347 - diff: 22.99mlTrain batch 6/32 - 238.1ms/batch - loss: 55.42996 - diff: 23.44mlTrain batch 7/32 - 236.5ms/batch - loss: 56.73615 - diff: 23.74mlTrain batch 8/32 - 239.0ms/batch - loss: 53.61995 - diff: 23.20mlTrain batch 9/32 - 236.8ms/batch - loss: 49.43566 - diff: 22.04mlTrain batch 10/32 - 239.0ms/batch - loss: 47.89018 - diff: 21.57mlTrain batch 11/32 - 236.9ms/batch - loss: 45.77100 - diff: 20.96mlTrain batch 12/32 - 239.5ms/batch - loss: 47.68138 - diff: 21.52mlTrain batch 13/32 - 236.4ms/batch - loss: 46.46854 - diff: 21.24mlTrain batch 14/32 - 239.2ms/batch - loss: 50.40021 - diff: 22.11mlTrain batch 15/32 - 237.7ms/batch - loss: 51.19101 - diff: 22.43mlTrain batch 16/32 - 238.1ms/batch - loss: 53.18104 - diff: 22.67mlTrain batch 17/32 - 237.6ms/batch - loss: 55.68140 - diff: 23.23mlTrain batch 18/32 - 237.8ms/batch - loss: 64.65427 - diff: 23.98mlTrain batch 19/32 - 236.3ms/batch - loss: 71.32875 - diff: 24.81mlTrain batch 20/32 - 237.7ms/batch - loss: 71.31516 - diff: 24.64mlTrain batch 21/32 - 236.3ms/batch - loss: 72.09226 - diff: 24.99mlTrain batch 22/32 - 238.0ms/batch - loss: 71.08444 - diff: 24.91mlTrain batch 23/32 - 236.9ms/batch - loss: 69.17489 - diff: 24.53mlTrain batch 24/32 - 239.9ms/batch - loss: 67.28101 - diff: 24.16mlTrain batch 25/32 - 237.1ms/batch - loss: 66.72402 - diff: 24.16mlTrain batch 26/32 - 239.2ms/batch - loss: 65.76745 - diff: 24.06mlTrain batch 27/32 - 237.4ms/batch - loss: 64.59711 - diff: 23.90mlTrain batch 28/32 - 239.3ms/batch - loss: 65.47224 - diff: 24.10mlTrain batch 29/32 - 236.8ms/batch - loss: 64.69612 - diff: 24.02mlTrain batch 30/32 - 239.3ms/batch - loss: 64.96130 - diff: 24.16mlTrain batch 31/32 - 236.6ms/batch - loss: 64.14804 - diff: 24.04mlTrain batch 32/32 - 77.1ms/batch - loss: 64.50957 - diff: 23.96mlTrain batch 32/32 - 10.6s 77.1ms/batch - loss: 64.50957 - diff: 23.96ml
Test 1.1s: val_loss: 96.50287 - diff: 27.49ml

Epoch 83: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.8ms/batch - loss: 111.75967 - diff: 27.88mlTrain batch 2/32 - 238.1ms/batch - loss: 69.65907 - diff: 22.91mlTrain batch 3/32 - 236.7ms/batch - loss: 60.46140 - diff: 22.64mlTrain batch 4/32 - 239.0ms/batch - loss: 61.27332 - diff: 23.02mlTrain batch 5/32 - 237.0ms/batch - loss: 59.36209 - diff: 23.05mlTrain batch 6/32 - 239.0ms/batch - loss: 59.56132 - diff: 23.37mlTrain batch 7/32 - 237.0ms/batch - loss: 59.01257 - diff: 23.53mlTrain batch 8/32 - 239.1ms/batch - loss: 63.18660 - diff: 24.22mlTrain batch 9/32 - 237.3ms/batch - loss: 66.95883 - diff: 24.94mlTrain batch 10/32 - 237.7ms/batch - loss: 67.56566 - diff: 25.01mlTrain batch 11/32 - 237.1ms/batch - loss: 64.23484 - diff: 24.37mlTrain batch 12/32 - 237.7ms/batch - loss: 62.98171 - diff: 24.07mlTrain batch 13/32 - 237.1ms/batch - loss: 60.38763 - diff: 23.60mlTrain batch 14/32 - 237.4ms/batch - loss: 64.54176 - diff: 24.06mlTrain batch 15/32 - 236.4ms/batch - loss: 62.79557 - diff: 23.90mlTrain batch 16/32 - 239.1ms/batch - loss: 61.09030 - diff: 23.76mlTrain batch 17/32 - 237.1ms/batch - loss: 58.78258 - diff: 23.21mlTrain batch 18/32 - 238.7ms/batch - loss: 59.12572 - diff: 23.48mlTrain batch 19/32 - 236.8ms/batch - loss: 57.96640 - diff: 23.20mlTrain batch 20/32 - 240.1ms/batch - loss: 61.73165 - diff: 23.56mlTrain batch 21/32 - 237.2ms/batch - loss: 61.61106 - diff: 23.60mlTrain batch 22/32 - 240.3ms/batch - loss: 60.39153 - diff: 23.38mlTrain batch 23/32 - 237.2ms/batch - loss: 59.71812 - diff: 23.39mlTrain batch 24/32 - 238.9ms/batch - loss: 58.36389 - diff: 23.15mlTrain batch 25/32 - 237.8ms/batch - loss: 59.52233 - diff: 23.36mlTrain batch 26/32 - 239.5ms/batch - loss: 61.21589 - diff: 23.72mlTrain batch 27/32 - 237.7ms/batch - loss: 61.16463 - diff: 23.75mlTrain batch 28/32 - 239.0ms/batch - loss: 59.68697 - diff: 23.44mlTrain batch 29/32 - 237.0ms/batch - loss: 59.81073 - diff: 23.49mlTrain batch 30/32 - 239.5ms/batch - loss: 59.92692 - diff: 23.52mlTrain batch 31/32 - 236.6ms/batch - loss: 60.78359 - diff: 23.61mlTrain batch 32/32 - 77.9ms/batch - loss: 64.26225 - diff: 23.68mlTrain batch 32/32 - 10.6s 77.9ms/batch - loss: 64.26225 - diff: 23.68ml
Test 1.1s: val_loss: 167.94335 - diff: 38.56ml

Epoch 84: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.7ms/batch - loss: 83.99418 - diff: 27.36mlTrain batch 2/32 - 238.5ms/batch - loss: 62.11145 - diff: 23.51mlTrain batch 3/32 - 236.8ms/batch - loss: 53.06208 - diff: 21.26mlTrain batch 4/32 - 238.9ms/batch - loss: 64.44680 - diff: 22.46mlTrain batch 5/32 - 237.1ms/batch - loss: 61.13109 - diff: 22.71mlTrain batch 6/32 - 238.7ms/batch - loss: 70.81710 - diff: 23.30mlTrain batch 7/32 - 236.4ms/batch - loss: 72.38594 - diff: 24.10mlTrain batch 8/32 - 240.8ms/batch - loss: 77.03376 - diff: 24.70mlTrain batch 9/32 - 237.0ms/batch - loss: 73.38233 - diff: 24.37mlTrain batch 10/32 - 237.6ms/batch - loss: 74.66828 - diff: 24.34mlTrain batch 11/32 - 237.2ms/batch - loss: 75.73035 - diff: 24.74mlTrain batch 12/32 - 237.7ms/batch - loss: 71.68025 - diff: 24.12mlTrain batch 13/32 - 236.5ms/batch - loss: 70.04981 - diff: 23.98mlTrain batch 14/32 - 237.8ms/batch - loss: 68.35894 - diff: 23.78mlTrain batch 15/32 - 236.9ms/batch - loss: 70.00200 - diff: 24.47mlTrain batch 16/32 - 238.6ms/batch - loss: 69.85560 - diff: 24.47mlTrain batch 17/32 - 237.3ms/batch - loss: 70.60294 - diff: 24.76mlTrain batch 18/32 - 240.4ms/batch - loss: 68.61310 - diff: 24.50mlTrain batch 19/32 - 236.9ms/batch - loss: 69.74224 - diff: 24.75mlTrain batch 20/32 - 239.7ms/batch - loss: 69.51930 - diff: 24.80mlTrain batch 21/32 - 237.3ms/batch - loss: 72.73908 - diff: 25.47mlTrain batch 22/32 - 240.0ms/batch - loss: 71.03523 - diff: 25.18mlTrain batch 23/32 - 236.8ms/batch - loss: 69.38319 - diff: 24.82mlTrain batch 24/32 - 239.0ms/batch - loss: 68.10519 - diff: 24.68mlTrain batch 25/32 - 237.3ms/batch - loss: 69.80156 - diff: 25.01mlTrain batch 26/32 - 239.3ms/batch - loss: 69.58078 - diff: 25.00mlTrain batch 27/32 - 237.2ms/batch - loss: 68.66360 - diff: 24.88mlTrain batch 28/32 - 239.2ms/batch - loss: 67.41941 - diff: 24.68mlTrain batch 29/32 - 237.0ms/batch - loss: 67.13312 - diff: 24.65mlTrain batch 30/32 - 239.6ms/batch - loss: 65.38381 - diff: 24.26mlTrain batch 31/32 - 236.9ms/batch - loss: 64.55314 - diff: 24.16mlTrain batch 32/32 - 77.6ms/batch - loss: 68.92447 - diff: 24.30mlTrain batch 32/32 - 10.6s 77.6ms/batch - loss: 68.92447 - diff: 24.30ml
Test 1.1s: val_loss: 76.82138 - diff: 26.03ml

Epoch 85: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.7ms/batch - loss: 32.27800 - diff: 19.03mlTrain batch 2/32 - 238.2ms/batch - loss: 93.55764 - diff: 25.36mlTrain batch 3/32 - 236.7ms/batch - loss: 79.73759 - diff: 24.18mlTrain batch 4/32 - 238.5ms/batch - loss: 64.61349 - diff: 22.08mlTrain batch 5/32 - 237.0ms/batch - loss: 62.65879 - diff: 22.21mlTrain batch 6/32 - 238.1ms/batch - loss: 58.83655 - diff: 21.49mlTrain batch 7/32 - 236.5ms/batch - loss: 57.68118 - diff: 21.61mlTrain batch 8/32 - 239.5ms/batch - loss: 61.57925 - diff: 22.83mlTrain batch 9/32 - 237.7ms/batch - loss: 64.40970 - diff: 23.52mlTrain batch 10/32 - 238.3ms/batch - loss: 66.12314 - diff: 24.00mlTrain batch 11/32 - 237.6ms/batch - loss: 82.06360 - diff: 25.94mlTrain batch 12/32 - 237.6ms/batch - loss: 78.85200 - diff: 25.55mlTrain batch 13/32 - 237.7ms/batch - loss: 75.68959 - diff: 25.11mlTrain batch 14/32 - 237.6ms/batch - loss: 78.55367 - diff: 25.88mlTrain batch 15/32 - 237.5ms/batch - loss: 77.05880 - diff: 25.55mlTrain batch 16/32 - 237.4ms/batch - loss: 75.39425 - diff: 25.48mlTrain batch 17/32 - 237.2ms/batch - loss: 75.44227 - diff: 25.48mlTrain batch 18/32 - 237.8ms/batch - loss: 72.85816 - diff: 25.05mlTrain batch 19/32 - 236.5ms/batch - loss: 73.62453 - diff: 25.12mlTrain batch 20/32 - 237.6ms/batch - loss: 73.60516 - diff: 25.26mlTrain batch 21/32 - 237.0ms/batch - loss: 71.36866 - diff: 24.71mlTrain batch 22/32 - 238.9ms/batch - loss: 72.95737 - diff: 24.98mlTrain batch 23/32 - 237.3ms/batch - loss: 72.10266 - diff: 24.91mlTrain batch 24/32 - 240.4ms/batch - loss: 70.41698 - diff: 24.63mlTrain batch 25/32 - 237.1ms/batch - loss: 69.21867 - diff: 24.51mlTrain batch 26/32 - 239.3ms/batch - loss: 68.44252 - diff: 24.48mlTrain batch 27/32 - 237.3ms/batch - loss: 67.49234 - diff: 24.42mlTrain batch 28/32 - 239.1ms/batch - loss: 66.25205 - diff: 24.18mlTrain batch 29/32 - 236.9ms/batch - loss: 64.78028 - diff: 23.88mlTrain batch 30/32 - 239.4ms/batch - loss: 63.41850 - diff: 23.59mlTrain batch 31/32 - 236.8ms/batch - loss: 62.12921 - diff: 23.37mlTrain batch 32/32 - 78.7ms/batch - loss: 62.34865 - diff: 23.31mlTrain batch 32/32 - 10.6s 78.7ms/batch - loss: 62.34865 - diff: 23.31ml
Test 1.1s: val_loss: 66.99934 - diff: 23.45ml

Epoch 86: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 237.0ms/batch - loss: 54.54102 - diff: 23.45mlTrain batch 2/32 - 237.8ms/batch - loss: 51.33489 - diff: 22.91mlTrain batch 3/32 - 236.8ms/batch - loss: 53.35719 - diff: 22.65mlTrain batch 4/32 - 238.9ms/batch - loss: 65.68862 - diff: 25.05mlTrain batch 5/32 - 236.5ms/batch - loss: 65.46814 - diff: 24.71mlTrain batch 6/32 - 239.4ms/batch - loss: 59.29668 - diff: 23.29mlTrain batch 7/32 - 236.4ms/batch - loss: 55.63566 - diff: 22.87mlTrain batch 8/32 - 239.3ms/batch - loss: 51.19223 - diff: 21.68mlTrain batch 9/32 - 237.5ms/batch - loss: 49.60089 - diff: 21.58mlTrain batch 10/32 - 237.7ms/batch - loss: 49.30112 - diff: 21.64mlTrain batch 11/32 - 237.0ms/batch - loss: 51.06185 - diff: 21.86mlTrain batch 12/32 - 237.8ms/batch - loss: 50.33795 - diff: 21.58mlTrain batch 13/32 - 236.7ms/batch - loss: 48.39864 - diff: 21.12mlTrain batch 14/32 - 238.1ms/batch - loss: 53.22215 - diff: 22.10mlTrain batch 15/32 - 236.8ms/batch - loss: 57.35797 - diff: 22.80mlTrain batch 16/32 - 238.3ms/batch - loss: 55.83819 - diff: 22.52mlTrain batch 17/32 - 236.1ms/batch - loss: 56.00076 - diff: 22.64mlTrain batch 18/32 - 239.7ms/batch - loss: 55.81051 - diff: 22.58mlTrain batch 19/32 - 237.2ms/batch - loss: 56.73492 - diff: 22.88mlTrain batch 20/32 - 239.9ms/batch - loss: 56.31104 - diff: 22.80mlTrain batch 21/32 - 237.2ms/batch - loss: 55.71484 - diff: 22.70mlTrain batch 22/32 - 239.9ms/batch - loss: 56.86019 - diff: 22.96mlTrain batch 23/32 - 236.6ms/batch - loss: 57.06219 - diff: 23.00mlTrain batch 24/32 - 239.3ms/batch - loss: 56.69326 - diff: 22.88mlTrain batch 25/32 - 237.5ms/batch - loss: 57.62338 - diff: 23.12mlTrain batch 26/32 - 239.8ms/batch - loss: 56.96279 - diff: 22.97mlTrain batch 27/32 - 237.1ms/batch - loss: 56.84994 - diff: 22.99mlTrain batch 28/32 - 239.3ms/batch - loss: 59.59459 - diff: 23.51mlTrain batch 29/32 - 237.1ms/batch - loss: 59.04373 - diff: 23.43mlTrain batch 30/32 - 239.7ms/batch - loss: 58.34762 - diff: 23.24mlTrain batch 31/32 - 237.1ms/batch - loss: 57.34528 - diff: 23.09mlTrain batch 32/32 - 78.5ms/batch - loss: 57.53648 - diff: 23.01mlTrain batch 32/32 - 10.6s 78.5ms/batch - loss: 57.53648 - diff: 23.01ml
Test 1.1s: val_loss: 73.62477 - diff: 25.79ml

Epoch 87: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 240.9ms/batch - loss: 89.41229 - diff: 25.87mlTrain batch 2/32 - 237.9ms/batch - loss: 73.09451 - diff: 26.29mlTrain batch 3/32 - 238.3ms/batch - loss: 93.15447 - diff: 27.07mlTrain batch 4/32 - 237.7ms/batch - loss: 90.87017 - diff: 27.81mlTrain batch 5/32 - 237.1ms/batch - loss: 81.94280 - diff: 26.34mlTrain batch 6/32 - 237.9ms/batch - loss: 73.06680 - diff: 25.24mlTrain batch 7/32 - 236.9ms/batch - loss: 80.48312 - diff: 26.70mlTrain batch 8/32 - 238.2ms/batch - loss: 77.21784 - diff: 26.37mlTrain batch 9/32 - 236.2ms/batch - loss: 72.23484 - diff: 25.45mlTrain batch 10/32 - 239.8ms/batch - loss: 68.87732 - diff: 25.02mlTrain batch 11/32 - 237.2ms/batch - loss: 66.69666 - diff: 24.85mlTrain batch 12/32 - 239.3ms/batch - loss: 64.34632 - diff: 24.52mlTrain batch 13/32 - 236.9ms/batch - loss: 62.67083 - diff: 24.24mlTrain batch 14/32 - 239.9ms/batch - loss: 59.44117 - diff: 23.52mlTrain batch 15/32 - 237.0ms/batch - loss: 60.60420 - diff: 23.83mlTrain batch 16/32 - 240.0ms/batch - loss: 61.14121 - diff: 23.98mlTrain batch 17/32 - 237.1ms/batch - loss: 60.43845 - diff: 23.63mlTrain batch 18/32 - 239.6ms/batch - loss: 59.98471 - diff: 23.67mlTrain batch 19/32 - 236.7ms/batch - loss: 59.19251 - diff: 23.45mlTrain batch 20/32 - 240.4ms/batch - loss: 58.59281 - diff: 23.32mlTrain batch 21/32 - 236.4ms/batch - loss: 59.37078 - diff: 23.33mlTrain batch 22/32 - 241.7ms/batch - loss: 63.12578 - diff: 24.11mlTrain batch 23/32 - 237.4ms/batch - loss: 64.20582 - diff: 24.49mlTrain batch 24/32 - 237.6ms/batch - loss: 65.11470 - diff: 24.62mlTrain batch 25/32 - 236.7ms/batch - loss: 63.58890 - diff: 24.30mlTrain batch 26/32 - 237.7ms/batch - loss: 63.36773 - diff: 24.26mlTrain batch 27/32 - 236.9ms/batch - loss: 63.10981 - diff: 24.30mlTrain batch 28/32 - 238.6ms/batch - loss: 62.57703 - diff: 24.26mlTrain batch 29/32 - 236.5ms/batch - loss: 62.52670 - diff: 24.25mlTrain batch 30/32 - 239.6ms/batch - loss: 64.45003 - diff: 24.61mlTrain batch 31/32 - 236.1ms/batch - loss: 64.30857 - diff: 24.62mlTrain batch 32/32 - 78.3ms/batch - loss: 65.89388 - diff: 24.65mlTrain batch 32/32 - 10.6s 78.3ms/batch - loss: 65.89388 - diff: 24.65ml
Test 1.1s: val_loss: 87.57948 - diff: 29.10ml

Epoch 88: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.7ms/batch - loss: 31.03632 - diff: 18.91mlTrain batch 2/32 - 237.7ms/batch - loss: 37.12762 - diff: 19.24mlTrain batch 3/32 - 236.4ms/batch - loss: 45.89850 - diff: 21.88mlTrain batch 4/32 - 238.2ms/batch - loss: 51.58835 - diff: 22.95mlTrain batch 5/32 - 237.0ms/batch - loss: 55.23644 - diff: 23.54mlTrain batch 6/32 - 238.7ms/batch - loss: 49.92640 - diff: 22.32mlTrain batch 7/32 - 236.5ms/batch - loss: 51.81334 - diff: 22.57mlTrain batch 8/32 - 238.9ms/batch - loss: 54.34805 - diff: 23.33mlTrain batch 9/32 - 237.1ms/batch - loss: 66.61101 - diff: 24.20mlTrain batch 10/32 - 239.6ms/batch - loss: 64.73780 - diff: 24.04mlTrain batch 11/32 - 236.7ms/batch - loss: 62.66652 - diff: 24.00mlTrain batch 12/32 - 239.1ms/batch - loss: 63.44139 - diff: 24.06mlTrain batch 13/32 - 236.1ms/batch - loss: 61.17472 - diff: 23.73mlTrain batch 14/32 - 239.5ms/batch - loss: 61.51169 - diff: 23.88mlTrain batch 15/32 - 236.6ms/batch - loss: 63.34249 - diff: 24.17mlTrain batch 16/32 - 238.6ms/batch - loss: 63.76107 - diff: 24.27mlTrain batch 17/32 - 237.7ms/batch - loss: 63.94576 - diff: 24.50mlTrain batch 18/32 - 239.0ms/batch - loss: 62.86994 - diff: 24.27mlTrain batch 19/32 - 237.3ms/batch - loss: 63.50132 - diff: 24.41mlTrain batch 20/32 - 237.5ms/batch - loss: 62.77188 - diff: 24.31mlTrain batch 21/32 - 237.4ms/batch - loss: 61.51999 - diff: 24.11mlTrain batch 22/32 - 237.9ms/batch - loss: 61.07020 - diff: 24.19mlTrain batch 23/32 - 236.5ms/batch - loss: 65.66948 - diff: 24.73mlTrain batch 24/32 - 238.9ms/batch - loss: 64.95424 - diff: 24.56mlTrain batch 25/32 - 236.2ms/batch - loss: 65.15798 - diff: 24.48mlTrain batch 26/32 - 240.0ms/batch - loss: 64.97781 - diff: 24.54mlTrain batch 27/32 - 236.9ms/batch - loss: 64.24408 - diff: 24.31mlTrain batch 28/32 - 239.5ms/batch - loss: 64.01741 - diff: 24.27mlTrain batch 29/32 - 236.9ms/batch - loss: 64.57421 - diff: 24.40mlTrain batch 30/32 - 240.2ms/batch - loss: 63.29560 - diff: 24.12mlTrain batch 31/32 - 236.8ms/batch - loss: 63.27214 - diff: 24.08mlTrain batch 32/32 - 77.3ms/batch - loss: 63.43064 - diff: 24.01mlTrain batch 32/32 - 10.6s 77.3ms/batch - loss: 63.43064 - diff: 24.01ml
Test 1.1s: val_loss: 160.16824 - diff: 41.01ml

Epoch 89: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.4ms/batch - loss: 86.99161 - diff: 29.01mlTrain batch 2/32 - 237.9ms/batch - loss: 53.71079 - diff: 21.79mlTrain batch 3/32 - 236.5ms/batch - loss: 48.15151 - diff: 21.51mlTrain batch 4/32 - 238.1ms/batch - loss: 75.17559 - diff: 26.54mlTrain batch 5/32 - 237.1ms/batch - loss: 79.35300 - diff: 26.96mlTrain batch 6/32 - 238.9ms/batch - loss: 77.78628 - diff: 26.74mlTrain batch 7/32 - 236.6ms/batch - loss: 72.36551 - diff: 25.73mlTrain batch 8/32 - 239.4ms/batch - loss: 70.88711 - diff: 25.62mlTrain batch 9/32 - 236.8ms/batch - loss: 68.50475 - diff: 25.25mlTrain batch 10/32 - 239.2ms/batch - loss: 70.60215 - diff: 26.05mlTrain batch 11/32 - 236.5ms/batch - loss: 69.85580 - diff: 25.62mlTrain batch 12/32 - 239.2ms/batch - loss: 65.98056 - diff: 24.78mlTrain batch 13/32 - 236.8ms/batch - loss: 79.39520 - diff: 25.86mlTrain batch 14/32 - 239.4ms/batch - loss: 77.83439 - diff: 25.85mlTrain batch 15/32 - 237.1ms/batch - loss: 75.16394 - diff: 25.39mlTrain batch 16/32 - 239.1ms/batch - loss: 73.48174 - diff: 25.18mlTrain batch 17/32 - 237.1ms/batch - loss: 71.10206 - diff: 24.72mlTrain batch 18/32 - 237.3ms/batch - loss: 69.27893 - diff: 24.53mlTrain batch 19/32 - 237.1ms/batch - loss: 68.22381 - diff: 24.35mlTrain batch 20/32 - 237.6ms/batch - loss: 68.25362 - diff: 24.53mlTrain batch 21/32 - 236.9ms/batch - loss: 69.99224 - diff: 24.67mlTrain batch 22/32 - 237.7ms/batch - loss: 69.57382 - diff: 24.73mlTrain batch 23/32 - 236.9ms/batch - loss: 68.35845 - diff: 24.58mlTrain batch 24/32 - 238.9ms/batch - loss: 67.74665 - diff: 24.55mlTrain batch 25/32 - 236.4ms/batch - loss: 69.24609 - diff: 24.75mlTrain batch 26/32 - 238.7ms/batch - loss: 68.70214 - diff: 24.71mlTrain batch 27/32 - 237.3ms/batch - loss: 67.49049 - diff: 24.56mlTrain batch 28/32 - 240.3ms/batch - loss: 66.63906 - diff: 24.51mlTrain batch 29/32 - 237.1ms/batch - loss: 66.51590 - diff: 24.44mlTrain batch 30/32 - 239.9ms/batch - loss: 65.06622 - diff: 24.01mlTrain batch 31/32 - 236.9ms/batch - loss: 63.62919 - diff: 23.66mlTrain batch 32/32 - 77.5ms/batch - loss: 64.45945 - diff: 23.64mlTrain batch 32/32 - 10.6s 77.5ms/batch - loss: 64.45945 - diff: 23.64ml
Test 1.1s: val_loss: 85.28198 - diff: 28.13ml

Epoch 90: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.4ms/batch - loss: 63.78627 - diff: 20.71mlTrain batch 2/32 - 243.6ms/batch - loss: 38.34832 - diff: 16.25mlTrain batch 3/32 - 236.9ms/batch - loss: 50.17044 - diff: 20.54mlTrain batch 4/32 - 238.6ms/batch - loss: 58.12956 - diff: 21.81mlTrain batch 5/32 - 236.3ms/batch - loss: 53.55066 - diff: 21.27mlTrain batch 6/32 - 239.4ms/batch - loss: 54.17181 - diff: 21.46mlTrain batch 7/32 - 236.7ms/batch - loss: 49.43449 - diff: 20.55mlTrain batch 8/32 - 238.9ms/batch - loss: 51.11468 - diff: 21.37mlTrain batch 9/32 - 237.0ms/batch - loss: 58.69427 - diff: 22.67mlTrain batch 10/32 - 239.3ms/batch - loss: 61.28289 - diff: 23.12mlTrain batch 11/32 - 236.7ms/batch - loss: 60.74274 - diff: 23.33mlTrain batch 12/32 - 239.4ms/batch - loss: 59.50507 - diff: 22.96mlTrain batch 13/32 - 236.8ms/batch - loss: 60.03989 - diff: 23.22mlTrain batch 14/32 - 237.6ms/batch - loss: 60.33220 - diff: 23.39mlTrain batch 15/32 - 236.9ms/batch - loss: 60.69847 - diff: 23.26mlTrain batch 16/32 - 238.4ms/batch - loss: 60.76494 - diff: 23.48mlTrain batch 17/32 - 236.8ms/batch - loss: 59.41672 - diff: 23.22mlTrain batch 18/32 - 239.4ms/batch - loss: 58.53200 - diff: 23.18mlTrain batch 19/32 - 237.2ms/batch - loss: 57.99953 - diff: 23.07mlTrain batch 20/32 - 239.0ms/batch - loss: 57.55065 - diff: 22.89mlTrain batch 21/32 - 236.9ms/batch - loss: 57.59844 - diff: 22.88mlTrain batch 22/32 - 239.6ms/batch - loss: 56.74729 - diff: 22.81mlTrain batch 23/32 - 237.7ms/batch - loss: 58.35351 - diff: 23.00mlTrain batch 24/32 - 237.5ms/batch - loss: 60.87529 - diff: 23.54mlTrain batch 25/32 - 236.6ms/batch - loss: 60.28773 - diff: 23.49mlTrain batch 26/32 - 239.1ms/batch - loss: 59.93284 - diff: 23.50mlTrain batch 27/32 - 237.1ms/batch - loss: 66.64850 - diff: 23.91mlTrain batch 28/32 - 240.3ms/batch - loss: 66.88844 - diff: 24.01mlTrain batch 29/32 - 236.7ms/batch - loss: 66.03071 - diff: 23.81mlTrain batch 30/32 - 239.5ms/batch - loss: 65.01353 - diff: 23.67mlTrain batch 31/32 - 236.7ms/batch - loss: 64.60739 - diff: 23.57mlTrain batch 32/32 - 77.0ms/batch - loss: 65.36323 - diff: 23.57mlTrain batch 32/32 - 10.9s 77.0ms/batch - loss: 65.36323 - diff: 23.57ml
Test 1.1s: val_loss: 67.25715 - diff: 24.66ml

Epoch 91: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.4ms/batch - loss: 31.36592 - diff: 17.57mlTrain batch 2/32 - 238.2ms/batch - loss: 32.29158 - diff: 18.62mlTrain batch 3/32 - 236.5ms/batch - loss: 45.88625 - diff: 20.48mlTrain batch 4/32 - 238.8ms/batch - loss: 70.17686 - diff: 24.34mlTrain batch 5/32 - 237.5ms/batch - loss: 60.08653 - diff: 22.47mlTrain batch 6/32 - 239.5ms/batch - loss: 56.28170 - diff: 22.21mlTrain batch 7/32 - 237.3ms/batch - loss: 54.86795 - diff: 22.21mlTrain batch 8/32 - 237.6ms/batch - loss: 58.84525 - diff: 23.02mlTrain batch 9/32 - 237.2ms/batch - loss: 60.84253 - diff: 23.50mlTrain batch 10/32 - 237.4ms/batch - loss: 60.94425 - diff: 23.67mlTrain batch 11/32 - 237.3ms/batch - loss: 58.61704 - diff: 23.27mlTrain batch 12/32 - 237.8ms/batch - loss: 55.28046 - diff: 22.42mlTrain batch 13/32 - 236.5ms/batch - loss: 56.16131 - diff: 22.69mlTrain batch 14/32 - 237.8ms/batch - loss: 56.30775 - diff: 22.66mlTrain batch 15/32 - 236.6ms/batch - loss: 61.14607 - diff: 23.61mlTrain batch 16/32 - 238.2ms/batch - loss: 58.68347 - diff: 23.01mlTrain batch 17/32 - 236.6ms/batch - loss: 60.26555 - diff: 23.39mlTrain batch 18/32 - 239.2ms/batch - loss: 59.89046 - diff: 23.29mlTrain batch 19/32 - 236.9ms/batch - loss: 62.22703 - diff: 23.58mlTrain batch 20/32 - 239.8ms/batch - loss: 62.34237 - diff: 23.58mlTrain batch 21/32 - 236.8ms/batch - loss: 61.59433 - diff: 23.50mlTrain batch 22/32 - 239.2ms/batch - loss: 60.19510 - diff: 23.20mlTrain batch 23/32 - 237.5ms/batch - loss: 60.19501 - diff: 23.25mlTrain batch 24/32 - 239.3ms/batch - loss: 59.61660 - diff: 23.21mlTrain batch 25/32 - 236.7ms/batch - loss: 61.31145 - diff: 23.42mlTrain batch 26/32 - 240.3ms/batch - loss: 61.50869 - diff: 23.52mlTrain batch 27/32 - 236.3ms/batch - loss: 61.15081 - diff: 23.46mlTrain batch 28/32 - 240.5ms/batch - loss: 60.64244 - diff: 23.37mlTrain batch 29/32 - 237.8ms/batch - loss: 60.35633 - diff: 23.31mlTrain batch 30/32 - 237.8ms/batch - loss: 60.45830 - diff: 23.32mlTrain batch 31/32 - 237.7ms/batch - loss: 60.57824 - diff: 23.36mlTrain batch 32/32 - 78.4ms/batch - loss: 60.75948 - diff: 23.31mlTrain batch 32/32 - 10.6s 78.4ms/batch - loss: 60.75948 - diff: 23.31ml
Test 1.1s: val_loss: 453.18931 - diff: 63.21ml

Epoch 92: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.6ms/batch - loss: 64.35088 - diff: 27.02mlTrain batch 2/32 - 238.5ms/batch - loss: 77.93752 - diff: 29.85mlTrain batch 3/32 - 236.2ms/batch - loss: 69.06748 - diff: 27.55mlTrain batch 4/32 - 239.7ms/batch - loss: 61.81041 - diff: 25.24mlTrain batch 5/32 - 237.3ms/batch - loss: 54.66344 - diff: 23.52mlTrain batch 6/32 - 237.6ms/batch - loss: 50.52468 - diff: 22.82mlTrain batch 7/32 - 236.9ms/batch - loss: 49.39420 - diff: 22.69mlTrain batch 8/32 - 237.5ms/batch - loss: 48.06863 - diff: 21.98mlTrain batch 9/32 - 236.6ms/batch - loss: 46.08283 - diff: 21.62mlTrain batch 10/32 - 238.6ms/batch - loss: 56.75030 - diff: 24.06mlTrain batch 11/32 - 236.9ms/batch - loss: 55.69803 - diff: 23.83mlTrain batch 12/32 - 239.0ms/batch - loss: 57.46657 - diff: 24.09mlTrain batch 13/32 - 237.0ms/batch - loss: 59.97426 - diff: 24.52mlTrain batch 14/32 - 240.2ms/batch - loss: 60.43601 - diff: 24.52mlTrain batch 15/32 - 237.0ms/batch - loss: 59.91490 - diff: 24.50mlTrain batch 16/32 - 239.0ms/batch - loss: 58.54543 - diff: 24.20mlTrain batch 17/32 - 237.5ms/batch - loss: 57.44810 - diff: 23.93mlTrain batch 18/32 - 239.6ms/batch - loss: 56.36954 - diff: 23.57mlTrain batch 19/32 - 237.2ms/batch - loss: 56.01606 - diff: 23.56mlTrain batch 20/32 - 239.7ms/batch - loss: 55.45704 - diff: 23.44mlTrain batch 21/32 - 236.4ms/batch - loss: 55.61214 - diff: 23.49mlTrain batch 22/32 - 239.0ms/batch - loss: 54.48021 - diff: 23.25mlTrain batch 23/32 - 237.7ms/batch - loss: 54.09489 - diff: 23.18mlTrain batch 24/32 - 239.1ms/batch - loss: 53.14676 - diff: 23.02mlTrain batch 25/32 - 237.6ms/batch - loss: 52.24871 - diff: 22.71mlTrain batch 26/32 - 237.7ms/batch - loss: 52.02197 - diff: 22.64mlTrain batch 27/32 - 236.5ms/batch - loss: 51.81203 - diff: 22.68mlTrain batch 28/32 - 238.5ms/batch - loss: 53.71112 - diff: 23.02mlTrain batch 29/32 - 236.5ms/batch - loss: 57.37503 - diff: 23.50mlTrain batch 30/32 - 238.3ms/batch - loss: 58.14518 - diff: 23.67mlTrain batch 31/32 - 236.3ms/batch - loss: 58.94863 - diff: 23.75mlTrain batch 32/32 - 78.3ms/batch - loss: 59.78428 - diff: 23.75mlTrain batch 32/32 - 10.6s 78.3ms/batch - loss: 59.78428 - diff: 23.75ml
Test 1.1s: val_loss: 94.43679 - diff: 29.53ml
Epoch    93: reducing learning rate of group 0 to 2.5000e-04.

Epoch 93: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.4ms/batch - loss: 52.07469 - diff: 21.77mlTrain batch 2/32 - 237.7ms/batch - loss: 41.77944 - diff: 19.90mlTrain batch 3/32 - 236.7ms/batch - loss: 45.10480 - diff: 20.30mlTrain batch 4/32 - 238.2ms/batch - loss: 43.95423 - diff: 20.69mlTrain batch 5/32 - 236.7ms/batch - loss: 44.57132 - diff: 21.64mlTrain batch 6/32 - 238.9ms/batch - loss: 45.16319 - diff: 21.75mlTrain batch 7/32 - 237.3ms/batch - loss: 42.47593 - diff: 21.05mlTrain batch 8/32 - 239.2ms/batch - loss: 39.43783 - diff: 20.34mlTrain batch 9/32 - 237.1ms/batch - loss: 39.41603 - diff: 20.33mlTrain batch 10/32 - 238.5ms/batch - loss: 43.00416 - diff: 21.08mlTrain batch 11/32 - 237.4ms/batch - loss: 45.10964 - diff: 21.11mlTrain batch 12/32 - 239.1ms/batch - loss: 45.69662 - diff: 21.37mlTrain batch 13/32 - 237.5ms/batch - loss: 46.36587 - diff: 21.55mlTrain batch 14/32 - 239.6ms/batch - loss: 45.39074 - diff: 21.41mlTrain batch 15/32 - 237.4ms/batch - loss: 45.07020 - diff: 21.31mlTrain batch 16/32 - 238.9ms/batch - loss: 44.36548 - diff: 21.16mlTrain batch 17/32 - 237.3ms/batch - loss: 51.62199 - diff: 22.54mlTrain batch 18/32 - 239.9ms/batch - loss: 52.81932 - diff: 22.82mlTrain batch 19/32 - 236.4ms/batch - loss: 51.65085 - diff: 22.48mlTrain batch 20/32 - 238.9ms/batch - loss: 52.44757 - diff: 22.76mlTrain batch 21/32 - 237.2ms/batch - loss: 51.97510 - diff: 22.69mlTrain batch 22/32 - 237.8ms/batch - loss: 53.96432 - diff: 23.09mlTrain batch 23/32 - 237.6ms/batch - loss: 52.85982 - diff: 22.92mlTrain batch 24/32 - 237.7ms/batch - loss: 53.25973 - diff: 23.07mlTrain batch 25/32 - 237.7ms/batch - loss: 52.96000 - diff: 23.09mlTrain batch 26/32 - 237.8ms/batch - loss: 52.80689 - diff: 23.07mlTrain batch 27/32 - 236.6ms/batch - loss: 52.82664 - diff: 22.98mlTrain batch 28/32 - 239.0ms/batch - loss: 53.86202 - diff: 23.15mlTrain batch 29/32 - 236.9ms/batch - loss: 52.93377 - diff: 22.87mlTrain batch 30/32 - 239.4ms/batch - loss: 52.53814 - diff: 22.80mlTrain batch 31/32 - 237.3ms/batch - loss: 52.77609 - diff: 22.88mlTrain batch 32/32 - 80.0ms/batch - loss: 52.78205 - diff: 22.80mlTrain batch 32/32 - 10.6s 80.0ms/batch - loss: 52.78205 - diff: 22.80ml
Test 1.1s: val_loss: 70.54507 - diff: 23.60ml

Epoch 94: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.2ms/batch - loss: 52.23611 - diff: 20.63mlTrain batch 2/32 - 239.5ms/batch - loss: 35.52876 - diff: 18.06mlTrain batch 3/32 - 237.2ms/batch - loss: 35.62302 - diff: 18.13mlTrain batch 4/32 - 237.8ms/batch - loss: 44.06623 - diff: 20.19mlTrain batch 5/32 - 237.5ms/batch - loss: 45.31191 - diff: 20.49mlTrain batch 6/32 - 237.5ms/batch - loss: 46.27452 - diff: 21.00mlTrain batch 7/32 - 236.4ms/batch - loss: 45.45678 - diff: 20.46mlTrain batch 8/32 - 239.5ms/batch - loss: 44.28303 - diff: 20.20mlTrain batch 9/32 - 236.4ms/batch - loss: 48.54085 - diff: 20.97mlTrain batch 10/32 - 240.0ms/batch - loss: 80.35567 - diff: 24.24mlTrain batch 11/32 - 237.4ms/batch - loss: 77.78738 - diff: 24.20mlTrain batch 12/32 - 240.2ms/batch - loss: 73.85254 - diff: 23.66mlTrain batch 13/32 - 237.6ms/batch - loss: 72.79114 - diff: 23.81mlTrain batch 14/32 - 239.8ms/batch - loss: 70.65092 - diff: 23.73mlTrain batch 15/32 - 237.0ms/batch - loss: 70.82057 - diff: 23.84mlTrain batch 16/32 - 239.2ms/batch - loss: 70.67659 - diff: 23.98mlTrain batch 17/32 - 238.3ms/batch - loss: 69.76633 - diff: 24.07mlTrain batch 18/32 - 239.5ms/batch - loss: 68.29038 - diff: 23.88mlTrain batch 19/32 - 237.4ms/batch - loss: 66.50026 - diff: 23.56mlTrain batch 20/32 - 240.6ms/batch - loss: 65.82281 - diff: 23.57mlTrain batch 21/32 - 236.9ms/batch - loss: 66.17746 - diff: 23.47mlTrain batch 22/32 - 237.9ms/batch - loss: 64.97674 - diff: 23.37mlTrain batch 23/32 - 236.6ms/batch - loss: 64.01694 - diff: 23.19mlTrain batch 24/32 - 238.8ms/batch - loss: 62.78706 - diff: 22.99mlTrain batch 25/32 - 236.8ms/batch - loss: 63.90564 - diff: 23.23mlTrain batch 26/32 - 239.9ms/batch - loss: 64.05354 - diff: 23.32mlTrain batch 27/32 - 237.5ms/batch - loss: 63.21921 - diff: 23.28mlTrain batch 28/32 - 241.8ms/batch - loss: 63.85904 - diff: 23.59mlTrain batch 29/32 - 237.4ms/batch - loss: 63.74937 - diff: 23.64mlTrain batch 30/32 - 240.2ms/batch - loss: 63.11115 - diff: 23.58mlTrain batch 31/32 - 237.3ms/batch - loss: 65.99558 - diff: 24.10mlTrain batch 32/32 - 77.9ms/batch - loss: 66.51104 - diff: 24.06mlTrain batch 32/32 - 10.6s 77.9ms/batch - loss: 66.51104 - diff: 24.06ml
Test 1.1s: val_loss: 89.44809 - diff: 26.59ml

Epoch 95: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.4ms/batch - loss: 127.06680 - diff: 28.37mlTrain batch 2/32 - 238.1ms/batch - loss: 84.74552 - diff: 23.88mlTrain batch 3/32 - 236.8ms/batch - loss: 65.92642 - diff: 21.12mlTrain batch 4/32 - 239.2ms/batch - loss: 54.63903 - diff: 19.72mlTrain batch 5/32 - 237.0ms/batch - loss: 49.50281 - diff: 19.44mlTrain batch 6/32 - 239.2ms/batch - loss: 53.00660 - diff: 20.95mlTrain batch 7/32 - 236.6ms/batch - loss: 55.26938 - diff: 21.48mlTrain batch 8/32 - 239.5ms/batch - loss: 60.80932 - diff: 23.26mlTrain batch 9/32 - 236.4ms/batch - loss: 57.85705 - diff: 22.80mlTrain batch 10/32 - 239.1ms/batch - loss: 59.27221 - diff: 23.01mlTrain batch 11/32 - 237.4ms/batch - loss: 57.16096 - diff: 22.86mlTrain batch 12/32 - 237.7ms/batch - loss: 55.25056 - diff: 22.60mlTrain batch 13/32 - 237.3ms/batch - loss: 54.70501 - diff: 22.65mlTrain batch 14/32 - 237.7ms/batch - loss: 53.83537 - diff: 22.66mlTrain batch 15/32 - 236.5ms/batch - loss: 55.60366 - diff: 23.14mlTrain batch 16/32 - 239.6ms/batch - loss: 54.70071 - diff: 22.99mlTrain batch 17/32 - 237.1ms/batch - loss: 53.40261 - diff: 22.60mlTrain batch 18/32 - 239.8ms/batch - loss: 51.97058 - diff: 22.21mlTrain batch 19/32 - 237.3ms/batch - loss: 50.93980 - diff: 21.97mlTrain batch 20/32 - 239.9ms/batch - loss: 51.24813 - diff: 22.11mlTrain batch 21/32 - 237.6ms/batch - loss: 49.97219 - diff: 21.83mlTrain batch 22/32 - 239.7ms/batch - loss: 49.20999 - diff: 21.64mlTrain batch 23/32 - 236.9ms/batch - loss: 48.90053 - diff: 21.73mlTrain batch 24/32 - 239.3ms/batch - loss: 48.91958 - diff: 21.82mlTrain batch 25/32 - 237.1ms/batch - loss: 50.39053 - diff: 22.26mlTrain batch 26/32 - 239.5ms/batch - loss: 52.69263 - diff: 22.59mlTrain batch 27/32 - 237.3ms/batch - loss: 54.82745 - diff: 22.85mlTrain batch 28/32 - 239.6ms/batch - loss: 55.98017 - diff: 23.05mlTrain batch 29/32 - 237.8ms/batch - loss: 55.80916 - diff: 23.01mlTrain batch 30/32 - 237.5ms/batch - loss: 56.17507 - diff: 23.16mlTrain batch 31/32 - 236.9ms/batch - loss: 56.11934 - diff: 23.08mlTrain batch 32/32 - 79.1ms/batch - loss: 57.48047 - diff: 23.12mlTrain batch 32/32 - 10.6s 79.1ms/batch - loss: 57.48047 - diff: 23.12ml
Test 1.1s: val_loss: 148.49213 - diff: 35.80ml

Epoch 96: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.4ms/batch - loss: 58.93495 - diff: 23.19mlTrain batch 2/32 - 238.7ms/batch - loss: 60.24623 - diff: 24.25mlTrain batch 3/32 - 236.8ms/batch - loss: 53.52782 - diff: 23.20mlTrain batch 4/32 - 238.8ms/batch - loss: 52.26193 - diff: 23.22mlTrain batch 5/32 - 236.6ms/batch - loss: 50.52785 - diff: 22.90mlTrain batch 6/32 - 239.6ms/batch - loss: 46.97107 - diff: 22.17mlTrain batch 7/32 - 237.4ms/batch - loss: 43.94370 - diff: 21.29mlTrain batch 8/32 - 239.6ms/batch - loss: 43.54474 - diff: 21.39mlTrain batch 9/32 - 237.1ms/batch - loss: 50.02120 - diff: 22.45mlTrain batch 10/32 - 237.7ms/batch - loss: 54.18718 - diff: 22.72mlTrain batch 11/32 - 237.5ms/batch - loss: 52.63285 - diff: 22.34mlTrain batch 12/32 - 237.5ms/batch - loss: 49.92171 - diff: 21.82mlTrain batch 13/32 - 237.3ms/batch - loss: 50.27303 - diff: 21.84mlTrain batch 14/32 - 237.6ms/batch - loss: 48.42037 - diff: 21.37mlTrain batch 15/32 - 236.1ms/batch - loss: 48.93146 - diff: 21.67mlTrain batch 16/32 - 240.4ms/batch - loss: 49.22547 - diff: 21.67mlTrain batch 17/32 - 236.4ms/batch - loss: 52.38947 - diff: 22.03mlTrain batch 18/32 - 239.5ms/batch - loss: 52.41634 - diff: 21.97mlTrain batch 19/32 - 237.3ms/batch - loss: 53.43527 - diff: 22.16mlTrain batch 20/32 - 240.1ms/batch - loss: 53.20820 - diff: 22.23mlTrain batch 21/32 - 236.7ms/batch - loss: 52.69228 - diff: 22.05mlTrain batch 22/32 - 239.8ms/batch - loss: 52.49387 - diff: 22.05mlTrain batch 23/32 - 236.7ms/batch - loss: 52.59197 - diff: 21.95mlTrain batch 24/32 - 240.4ms/batch - loss: 52.71094 - diff: 22.07mlTrain batch 25/32 - 237.8ms/batch - loss: 53.15825 - diff: 22.28mlTrain batch 26/32 - 239.7ms/batch - loss: 52.40443 - diff: 22.16mlTrain batch 27/32 - 237.5ms/batch - loss: 54.17761 - diff: 22.59mlTrain batch 28/32 - 240.2ms/batch - loss: 54.55732 - diff: 22.76mlTrain batch 29/32 - 237.7ms/batch - loss: 53.91773 - diff: 22.65mlTrain batch 30/32 - 240.5ms/batch - loss: 53.02512 - diff: 22.46mlTrain batch 31/32 - 237.2ms/batch - loss: 52.24164 - diff: 22.25mlTrain batch 32/32 - 79.1ms/batch - loss: 52.57199 - diff: 22.21mlTrain batch 32/32 - 10.6s 79.1ms/batch - loss: 52.57199 - diff: 22.21ml
Test 1.1s: val_loss: 86.57735 - diff: 28.80ml

Epoch 97: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.5ms/batch - loss: 22.66758 - diff: 15.31mlTrain batch 2/32 - 239.4ms/batch - loss: 34.24291 - diff: 18.46mlTrain batch 3/32 - 236.8ms/batch - loss: 41.28027 - diff: 19.67mlTrain batch 4/32 - 240.4ms/batch - loss: 50.83491 - diff: 21.35mlTrain batch 5/32 - 237.7ms/batch - loss: 47.94891 - diff: 20.73mlTrain batch 6/32 - 237.4ms/batch - loss: 45.24494 - diff: 20.33mlTrain batch 7/32 - 237.0ms/batch - loss: 40.95764 - diff: 19.30mlTrain batch 8/32 - 237.3ms/batch - loss: 56.34836 - diff: 21.73mlTrain batch 9/32 - 236.7ms/batch - loss: 54.57503 - diff: 21.57mlTrain batch 10/32 - 239.8ms/batch - loss: 54.12955 - diff: 21.51mlTrain batch 11/32 - 236.6ms/batch - loss: 51.45348 - diff: 20.90mlTrain batch 12/32 - 239.2ms/batch - loss: 49.32133 - diff: 20.51mlTrain batch 13/32 - 237.5ms/batch - loss: 49.61905 - diff: 20.84mlTrain batch 14/32 - 239.1ms/batch - loss: 47.61418 - diff: 20.46mlTrain batch 15/32 - 238.1ms/batch - loss: 47.65649 - diff: 20.60mlTrain batch 16/32 - 239.7ms/batch - loss: 47.81373 - diff: 20.58mlTrain batch 17/32 - 237.6ms/batch - loss: 48.84928 - diff: 21.04mlTrain batch 18/32 - 239.5ms/batch - loss: 48.96372 - diff: 21.11mlTrain batch 19/32 - 236.7ms/batch - loss: 50.40975 - diff: 21.57mlTrain batch 20/32 - 239.1ms/batch - loss: 50.99462 - diff: 21.62mlTrain batch 21/32 - 237.6ms/batch - loss: 51.71946 - diff: 21.74mlTrain batch 22/32 - 239.8ms/batch - loss: 51.36500 - diff: 21.78mlTrain batch 23/32 - 237.3ms/batch - loss: 52.16838 - diff: 21.91mlTrain batch 24/32 - 237.8ms/batch - loss: 53.55292 - diff: 22.17mlTrain batch 25/32 - 236.7ms/batch - loss: 52.40105 - diff: 21.94mlTrain batch 26/32 - 239.3ms/batch - loss: 51.46771 - diff: 21.80mlTrain batch 27/32 - 236.7ms/batch - loss: 52.23992 - diff: 21.91mlTrain batch 28/32 - 240.6ms/batch - loss: 51.79671 - diff: 21.89mlTrain batch 29/32 - 237.6ms/batch - loss: 51.23540 - diff: 21.79mlTrain batch 30/32 - 239.6ms/batch - loss: 51.50940 - diff: 21.87mlTrain batch 31/32 - 237.1ms/batch - loss: 51.60296 - diff: 21.98mlTrain batch 32/32 - 78.2ms/batch - loss: 52.06716 - diff: 21.94mlTrain batch 32/32 - 10.7s 78.2ms/batch - loss: 52.06716 - diff: 21.94ml
Test 1.1s: val_loss: 88.74615 - diff: 28.20ml

Epoch 98: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.2ms/batch - loss: 92.93179 - diff: 26.51mlTrain batch 2/32 - 237.6ms/batch - loss: 57.95580 - diff: 21.82mlTrain batch 3/32 - 236.7ms/batch - loss: 48.28065 - diff: 20.28mlTrain batch 4/32 - 238.8ms/batch - loss: 48.47865 - diff: 20.54mlTrain batch 5/32 - 237.1ms/batch - loss: 44.67845 - diff: 20.15mlTrain batch 6/32 - 239.4ms/batch - loss: 45.83666 - diff: 20.82mlTrain batch 7/32 - 236.9ms/batch - loss: 46.30997 - diff: 21.04mlTrain batch 8/32 - 239.4ms/batch - loss: 60.50760 - diff: 23.25mlTrain batch 9/32 - 237.1ms/batch - loss: 61.07156 - diff: 23.20mlTrain batch 10/32 - 239.8ms/batch - loss: 60.93804 - diff: 23.42mlTrain batch 11/32 - 236.7ms/batch - loss: 56.06990 - diff: 22.09mlTrain batch 12/32 - 239.4ms/batch - loss: 55.21166 - diff: 22.07mlTrain batch 13/32 - 237.7ms/batch - loss: 54.52080 - diff: 22.13mlTrain batch 14/32 - 238.4ms/batch - loss: 54.61359 - diff: 21.93mlTrain batch 15/32 - 237.3ms/batch - loss: 54.78712 - diff: 22.08mlTrain batch 16/32 - 237.7ms/batch - loss: 52.83051 - diff: 21.62mlTrain batch 17/32 - 236.6ms/batch - loss: 54.16401 - diff: 21.72mlTrain batch 18/32 - 239.8ms/batch - loss: 52.29826 - diff: 21.26mlTrain batch 19/32 - 236.3ms/batch - loss: 52.75734 - diff: 21.45mlTrain batch 20/32 - 239.7ms/batch - loss: 54.41015 - diff: 21.86mlTrain batch 21/32 - 236.4ms/batch - loss: 52.99401 - diff: 21.60mlTrain batch 22/32 - 240.0ms/batch - loss: 53.10146 - diff: 21.81mlTrain batch 23/32 - 237.0ms/batch - loss: 51.56391 - diff: 21.47mlTrain batch 24/32 - 240.1ms/batch - loss: 52.68361 - diff: 21.67mlTrain batch 25/32 - 237.6ms/batch - loss: 51.77717 - diff: 21.55mlTrain batch 26/32 - 240.4ms/batch - loss: 51.26015 - diff: 21.45mlTrain batch 27/32 - 236.8ms/batch - loss: 50.75316 - diff: 21.34mlTrain batch 28/32 - 240.7ms/batch - loss: 50.64538 - diff: 21.41mlTrain batch 29/32 - 237.1ms/batch - loss: 50.90422 - diff: 21.49mlTrain batch 30/32 - 240.3ms/batch - loss: 50.43969 - diff: 21.48mlTrain batch 31/32 - 237.4ms/batch - loss: 50.08208 - diff: 21.40mlTrain batch 32/32 - 78.5ms/batch - loss: 50.61347 - diff: 21.35mlTrain batch 32/32 - 10.6s 78.5ms/batch - loss: 50.61347 - diff: 21.35ml
Test 1.1s: val_loss: 74.57844 - diff: 23.58ml

Epoch 99: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.5ms/batch - loss: 22.21062 - diff: 15.99mlTrain batch 2/32 - 237.8ms/batch - loss: 66.49704 - diff: 22.51mlTrain batch 3/32 - 236.7ms/batch - loss: 53.33634 - diff: 21.22mlTrain batch 4/32 - 239.7ms/batch - loss: 46.91462 - diff: 19.77mlTrain batch 5/32 - 237.5ms/batch - loss: 48.17393 - diff: 20.25mlTrain batch 6/32 - 239.0ms/batch - loss: 49.43283 - diff: 20.85mlTrain batch 7/32 - 236.7ms/batch - loss: 44.22827 - diff: 19.38mlTrain batch 8/32 - 239.5ms/batch - loss: 46.87309 - diff: 19.99mlTrain batch 9/32 - 236.4ms/batch - loss: 47.58784 - diff: 20.80mlTrain batch 10/32 - 237.2ms/batch - loss: 49.04691 - diff: 21.14mlTrain batch 11/32 - 237.5ms/batch - loss: 46.84797 - diff: 20.62mlTrain batch 12/32 - 237.6ms/batch - loss: 44.85364 - diff: 20.24mlTrain batch 13/32 - 237.6ms/batch - loss: 44.70169 - diff: 20.31mlTrain batch 14/32 - 237.7ms/batch - loss: 45.85579 - diff: 20.67mlTrain batch 15/32 - 236.6ms/batch - loss: 46.02100 - diff: 20.92mlTrain batch 16/32 - 238.7ms/batch - loss: 49.08077 - diff: 21.76mlTrain batch 17/32 - 236.7ms/batch - loss: 50.35898 - diff: 22.16mlTrain batch 18/32 - 239.7ms/batch - loss: 50.30482 - diff: 22.36mlTrain batch 19/32 - 237.1ms/batch - loss: 48.38436 - diff: 21.76mlTrain batch 20/32 - 240.7ms/batch - loss: 48.69547 - diff: 21.98mlTrain batch 21/32 - 237.4ms/batch - loss: 47.53740 - diff: 21.72mlTrain batch 22/32 - 239.8ms/batch - loss: 46.80816 - diff: 21.51mlTrain batch 23/32 - 237.4ms/batch - loss: 46.82266 - diff: 21.52mlTrain batch 24/32 - 239.8ms/batch - loss: 46.79507 - diff: 21.57mlTrain batch 25/32 - 236.5ms/batch - loss: 46.32786 - diff: 21.47mlTrain batch 26/32 - 238.5ms/batch - loss: 45.47464 - diff: 21.25mlTrain batch 27/32 - 237.5ms/batch - loss: 44.37395 - diff: 20.95mlTrain batch 28/32 - 240.4ms/batch - loss: 45.96476 - diff: 21.29mlTrain batch 29/32 - 237.4ms/batch - loss: 45.71419 - diff: 21.24mlTrain batch 30/32 - 237.8ms/batch - loss: 47.38700 - diff: 21.48mlTrain batch 31/32 - 237.3ms/batch - loss: 53.23760 - diff: 22.36mlTrain batch 32/32 - 79.2ms/batch - loss: 54.19436 - diff: 22.35mlTrain batch 32/32 - 10.7s 79.2ms/batch - loss: 54.19436 - diff: 22.35ml
Test 1.1s: val_loss: 114.43054 - diff: 29.67ml

Epoch 100: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 272.8ms/batch - loss: 35.31936 - diff: 20.33mlTrain batch 2/32 - 243.8ms/batch - loss: 49.91091 - diff: 22.89mlTrain batch 3/32 - 237.4ms/batch - loss: 58.25952 - diff: 24.32mlTrain batch 4/32 - 238.9ms/batch - loss: 52.86966 - diff: 23.53mlTrain batch 5/32 - 237.6ms/batch - loss: 48.00513 - diff: 22.64mlTrain batch 6/32 - 237.3ms/batch - loss: 42.68998 - diff: 20.75mlTrain batch 7/32 - 237.2ms/batch - loss: 47.31735 - diff: 21.41mlTrain batch 8/32 - 237.8ms/batch - loss: 46.21495 - diff: 21.35mlTrain batch 9/32 - 236.8ms/batch - loss: 43.24905 - diff: 20.57mlTrain batch 10/32 - 237.7ms/batch - loss: 45.04334 - diff: 21.10mlTrain batch 11/32 - 236.1ms/batch - loss: 44.74143 - diff: 21.19mlTrain batch 12/32 - 239.9ms/batch - loss: 47.81473 - diff: 22.00mlTrain batch 13/32 - 236.8ms/batch - loss: 49.01295 - diff: 22.09mlTrain batch 14/32 - 240.0ms/batch - loss: 51.06914 - diff: 22.52mlTrain batch 15/32 - 236.8ms/batch - loss: 53.62750 - diff: 22.77mlTrain batch 16/32 - 240.1ms/batch - loss: 55.06463 - diff: 22.98mlTrain batch 17/32 - 238.2ms/batch - loss: 53.95725 - diff: 22.74mlTrain batch 18/32 - 239.4ms/batch - loss: 52.55432 - diff: 22.47mlTrain batch 19/32 - 237.5ms/batch - loss: 52.41966 - diff: 22.42mlTrain batch 20/32 - 239.5ms/batch - loss: 50.72731 - diff: 22.00mlTrain batch 21/32 - 236.6ms/batch - loss: 50.05313 - diff: 21.87mlTrain batch 22/32 - 241.5ms/batch - loss: 48.93216 - diff: 21.61mlTrain batch 23/32 - 237.4ms/batch - loss: 50.19439 - diff: 21.86mlTrain batch 24/32 - 237.5ms/batch - loss: 51.22496 - diff: 22.05mlTrain batch 25/32 - 237.5ms/batch - loss: 51.67184 - diff: 22.27mlTrain batch 26/32 - 237.7ms/batch - loss: 50.64075 - diff: 22.03mlTrain batch 27/32 - 236.4ms/batch - loss: 50.11171 - diff: 21.83mlTrain batch 28/32 - 240.2ms/batch - loss: 49.64888 - diff: 21.75mlTrain batch 29/32 - 237.4ms/batch - loss: 48.85281 - diff: 21.59mlTrain batch 30/32 - 241.3ms/batch - loss: 47.90552 - diff: 21.38mlTrain batch 31/32 - 237.1ms/batch - loss: 47.18038 - diff: 21.17mlTrain batch 32/32 - 79.3ms/batch - loss: 50.67823 - diff: 21.32mlTrain batch 32/32 - 10.6s 79.3ms/batch - loss: 50.67823 - diff: 21.32ml
Test 1.1s: val_loss: 95.05211 - diff: 30.88ml

Epoch 101: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.2ms/batch - loss: 47.11690 - diff: 22.89mlTrain batch 2/32 - 245.3ms/batch - loss: 63.12978 - diff: 26.52mlTrain batch 3/32 - 236.3ms/batch - loss: 56.74742 - diff: 24.65mlTrain batch 4/32 - 237.8ms/batch - loss: 53.05574 - diff: 23.72mlTrain batch 5/32 - 236.5ms/batch - loss: 55.41294 - diff: 23.97mlTrain batch 6/32 - 238.8ms/batch - loss: 84.37791 - diff: 27.19mlTrain batch 7/32 - 237.2ms/batch - loss: 75.94985 - diff: 25.69mlTrain batch 8/32 - 240.1ms/batch - loss: 72.97831 - diff: 25.27mlTrain batch 9/32 - 236.2ms/batch - loss: 73.45286 - diff: 25.44mlTrain batch 10/32 - 239.5ms/batch - loss: 68.40286 - diff: 24.44mlTrain batch 11/32 - 237.9ms/batch - loss: 70.21811 - diff: 24.99mlTrain batch 12/32 - 239.2ms/batch - loss: 66.70313 - diff: 24.25mlTrain batch 13/32 - 237.1ms/batch - loss: 64.47349 - diff: 23.80mlTrain batch 14/32 - 240.4ms/batch - loss: 62.78827 - diff: 23.55mlTrain batch 15/32 - 237.1ms/batch - loss: 63.13119 - diff: 23.26mlTrain batch 16/32 - 239.9ms/batch - loss: 61.41098 - diff: 22.94mlTrain batch 17/32 - 237.4ms/batch - loss: 59.72441 - diff: 22.41mlTrain batch 18/32 - 239.0ms/batch - loss: 58.88252 - diff: 22.38mlTrain batch 19/32 - 237.1ms/batch - loss: 59.34339 - diff: 22.47mlTrain batch 20/32 - 237.7ms/batch - loss: 57.31740 - diff: 21.98mlTrain batch 21/32 - 237.4ms/batch - loss: 56.44690 - diff: 21.93mlTrain batch 22/32 - 237.2ms/batch - loss: 58.98524 - diff: 22.00mlTrain batch 23/32 - 237.5ms/batch - loss: 58.01093 - diff: 21.86mlTrain batch 24/32 - 237.9ms/batch - loss: 56.82893 - diff: 21.68mlTrain batch 25/32 - 236.3ms/batch - loss: 55.24856 - diff: 21.30mlTrain batch 26/32 - 240.2ms/batch - loss: 54.10260 - diff: 21.10mlTrain batch 27/32 - 236.6ms/batch - loss: 53.20978 - diff: 21.02mlTrain batch 28/32 - 239.6ms/batch - loss: 52.30629 - diff: 20.91mlTrain batch 29/32 - 236.5ms/batch - loss: 51.69978 - diff: 20.89mlTrain batch 30/32 - 240.3ms/batch - loss: 52.40234 - diff: 21.04mlTrain batch 31/32 - 237.4ms/batch - loss: 52.63573 - diff: 21.08mlTrain batch 32/32 - 79.5ms/batch - loss: 54.83501 - diff: 21.14mlTrain batch 32/32 - 10.6s 79.5ms/batch - loss: 54.83501 - diff: 21.14ml
Test 1.1s: val_loss: 61.01505 - diff: 23.91ml

Epoch 102: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.7ms/batch - loss: 55.31239 - diff: 22.65mlTrain batch 2/32 - 238.1ms/batch - loss: 34.45466 - diff: 17.79mlTrain batch 3/32 - 236.6ms/batch - loss: 47.63132 - diff: 19.37mlTrain batch 4/32 - 239.2ms/batch - loss: 48.78343 - diff: 20.39mlTrain batch 5/32 - 236.6ms/batch - loss: 49.42426 - diff: 20.77mlTrain batch 6/32 - 239.5ms/batch - loss: 58.39771 - diff: 22.67mlTrain batch 7/32 - 237.4ms/batch - loss: 54.76307 - diff: 21.97mlTrain batch 8/32 - 241.0ms/batch - loss: 65.47987 - diff: 23.08mlTrain batch 9/32 - 237.0ms/batch - loss: 61.04202 - diff: 22.10mlTrain batch 10/32 - 239.1ms/batch - loss: 58.95947 - diff: 21.85mlTrain batch 11/32 - 237.5ms/batch - loss: 57.12960 - diff: 21.82mlTrain batch 12/32 - 239.5ms/batch - loss: 55.18761 - diff: 21.57mlTrain batch 13/32 - 236.4ms/batch - loss: 53.44393 - diff: 21.35mlTrain batch 14/32 - 239.5ms/batch - loss: 53.94660 - diff: 21.72mlTrain batch 15/32 - 237.5ms/batch - loss: 51.69131 - diff: 21.25mlTrain batch 16/32 - 237.8ms/batch - loss: 51.42095 - diff: 21.32mlTrain batch 17/32 - 237.4ms/batch - loss: 49.21906 - diff: 20.75mlTrain batch 18/32 - 237.8ms/batch - loss: 48.41075 - diff: 20.63mlTrain batch 19/32 - 237.4ms/batch - loss: 48.39975 - diff: 20.61mlTrain batch 20/32 - 238.1ms/batch - loss: 47.06065 - diff: 20.30mlTrain batch 21/32 - 236.8ms/batch - loss: 46.88903 - diff: 20.35mlTrain batch 22/32 - 240.8ms/batch - loss: 46.86648 - diff: 20.41mlTrain batch 23/32 - 236.8ms/batch - loss: 45.63237 - diff: 20.14mlTrain batch 24/32 - 240.5ms/batch - loss: 46.23800 - diff: 20.24mlTrain batch 25/32 - 237.2ms/batch - loss: 45.74592 - diff: 20.15mlTrain batch 26/32 - 240.6ms/batch - loss: 46.22777 - diff: 20.28mlTrain batch 27/32 - 237.1ms/batch - loss: 47.05103 - diff: 20.51mlTrain batch 28/32 - 240.9ms/batch - loss: 46.99492 - diff: 20.62mlTrain batch 29/32 - 236.8ms/batch - loss: 47.38646 - diff: 20.63mlTrain batch 30/32 - 240.5ms/batch - loss: 46.55061 - diff: 20.46mlTrain batch 31/32 - 238.2ms/batch - loss: 46.94787 - diff: 20.55mlTrain batch 32/32 - 78.8ms/batch - loss: 50.09262 - diff: 20.64mlTrain batch 32/32 - 10.6s 78.8ms/batch - loss: 50.09262 - diff: 20.64ml
Test 1.1s: val_loss: 85.33596 - diff: 25.53ml

Epoch 103: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.9ms/batch - loss: 34.35401 - diff: 19.52mlTrain batch 2/32 - 239.5ms/batch - loss: 35.10205 - diff: 17.67mlTrain batch 3/32 - 237.1ms/batch - loss: 49.19373 - diff: 21.49mlTrain batch 4/32 - 238.5ms/batch - loss: 44.50233 - diff: 20.75mlTrain batch 5/32 - 237.2ms/batch - loss: 46.14699 - diff: 21.07mlTrain batch 6/32 - 239.0ms/batch - loss: 55.06820 - diff: 22.83mlTrain batch 7/32 - 237.5ms/batch - loss: 53.00336 - diff: 22.52mlTrain batch 8/32 - 239.7ms/batch - loss: 50.18576 - diff: 21.66mlTrain batch 9/32 - 237.9ms/batch - loss: 50.40606 - diff: 21.98mlTrain batch 10/32 - 239.5ms/batch - loss: 54.37738 - diff: 23.06mlTrain batch 11/32 - 237.1ms/batch - loss: 53.43893 - diff: 23.02mlTrain batch 12/32 - 239.8ms/batch - loss: 52.63964 - diff: 22.72mlTrain batch 13/32 - 236.7ms/batch - loss: 50.32673 - diff: 22.27mlTrain batch 14/32 - 240.5ms/batch - loss: 52.00877 - diff: 22.70mlTrain batch 15/32 - 237.5ms/batch - loss: 51.56421 - diff: 22.82mlTrain batch 16/32 - 238.9ms/batch - loss: 50.82606 - diff: 22.67mlTrain batch 17/32 - 237.2ms/batch - loss: 50.40410 - diff: 22.57mlTrain batch 18/32 - 237.9ms/batch - loss: 51.08643 - diff: 22.68mlTrain batch 19/32 - 237.4ms/batch - loss: 49.88106 - diff: 22.42mlTrain batch 20/32 - 237.8ms/batch - loss: 49.78695 - diff: 22.37mlTrain batch 21/32 - 236.5ms/batch - loss: 49.15338 - diff: 22.21mlTrain batch 22/32 - 240.6ms/batch - loss: 47.65807 - diff: 21.78mlTrain batch 23/32 - 236.8ms/batch - loss: 50.78531 - diff: 22.46mlTrain batch 24/32 - 239.6ms/batch - loss: 51.33925 - diff: 22.53mlTrain batch 25/32 - 237.3ms/batch - loss: 50.73881 - diff: 22.42mlTrain batch 26/32 - 239.8ms/batch - loss: 49.62584 - diff: 22.20mlTrain batch 27/32 - 237.4ms/batch - loss: 48.96034 - diff: 21.95mlTrain batch 28/32 - 240.1ms/batch - loss: 52.31756 - diff: 22.46mlTrain batch 29/32 - 236.8ms/batch - loss: 53.60036 - diff: 22.71mlTrain batch 30/32 - 240.5ms/batch - loss: 52.44297 - diff: 22.40mlTrain batch 31/32 - 237.3ms/batch - loss: 52.33015 - diff: 22.40mlTrain batch 32/32 - 78.7ms/batch - loss: 53.17300 - diff: 22.41mlTrain batch 32/32 - 10.6s 78.7ms/batch - loss: 53.17300 - diff: 22.41ml
Test 1.1s: val_loss: 104.28965 - diff: 30.93ml
Epoch   104: reducing learning rate of group 0 to 1.2500e-04.

Epoch 104: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.5ms/batch - loss: 49.99725 - diff: 24.41mlTrain batch 2/32 - 237.8ms/batch - loss: 47.59342 - diff: 22.82mlTrain batch 3/32 - 237.0ms/batch - loss: 51.89275 - diff: 24.53mlTrain batch 4/32 - 239.2ms/batch - loss: 46.04027 - diff: 22.72mlTrain batch 5/32 - 236.8ms/batch - loss: 45.42590 - diff: 22.71mlTrain batch 6/32 - 238.2ms/batch - loss: 44.79621 - diff: 22.20mlTrain batch 7/32 - 236.8ms/batch - loss: 48.45364 - diff: 22.91mlTrain batch 8/32 - 239.5ms/batch - loss: 44.31438 - diff: 21.37mlTrain batch 9/32 - 237.6ms/batch - loss: 43.81817 - diff: 21.26mlTrain batch 10/32 - 239.7ms/batch - loss: 45.20625 - diff: 21.38mlTrain batch 11/32 - 237.2ms/batch - loss: 47.18027 - diff: 21.96mlTrain batch 12/32 - 239.1ms/batch - loss: 46.56071 - diff: 21.79mlTrain batch 13/32 - 237.3ms/batch - loss: 45.59103 - diff: 21.46mlTrain batch 14/32 - 239.7ms/batch - loss: 44.67591 - diff: 21.21mlTrain batch 15/32 - 236.8ms/batch - loss: 45.02849 - diff: 21.28mlTrain batch 16/32 - 239.2ms/batch - loss: 43.62144 - diff: 20.90mlTrain batch 17/32 - 237.2ms/batch - loss: 43.45510 - diff: 20.84mlTrain batch 18/32 - 239.2ms/batch - loss: 41.57419 - diff: 20.27mlTrain batch 19/32 - 237.3ms/batch - loss: 40.46196 - diff: 19.94mlTrain batch 20/32 - 238.0ms/batch - loss: 41.89024 - diff: 20.37mlTrain batch 21/32 - 237.7ms/batch - loss: 42.28511 - diff: 20.62mlTrain batch 22/32 - 237.8ms/batch - loss: 41.99033 - diff: 20.51mlTrain batch 23/32 - 236.2ms/batch - loss: 40.95981 - diff: 20.24mlTrain batch 24/32 - 239.1ms/batch - loss: 41.46112 - diff: 20.40mlTrain batch 25/32 - 236.9ms/batch - loss: 51.57408 - diff: 21.76mlTrain batch 26/32 - 239.5ms/batch - loss: 51.66849 - diff: 21.86mlTrain batch 27/32 - 236.9ms/batch - loss: 51.08262 - diff: 21.72mlTrain batch 28/32 - 240.7ms/batch - loss: 51.21982 - diff: 21.82mlTrain batch 29/32 - 237.3ms/batch - loss: 50.34768 - diff: 21.63mlTrain batch 30/32 - 239.9ms/batch - loss: 49.82085 - diff: 21.58mlTrain batch 31/32 - 236.9ms/batch - loss: 49.88290 - diff: 21.57mlTrain batch 32/32 - 77.7ms/batch - loss: 53.24283 - diff: 21.69mlTrain batch 32/32 - 10.6s 77.7ms/batch - loss: 53.24283 - diff: 21.69ml
Test 1.1s: val_loss: 90.15111 - diff: 26.29ml

Epoch 105: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.4ms/batch - loss: 29.50629 - diff: 17.05mlTrain batch 2/32 - 238.8ms/batch - loss: 45.18632 - diff: 21.24mlTrain batch 3/32 - 237.0ms/batch - loss: 43.30391 - diff: 20.93mlTrain batch 4/32 - 239.2ms/batch - loss: 41.58674 - diff: 20.56mlTrain batch 5/32 - 237.4ms/batch - loss: 42.61263 - diff: 20.64mlTrain batch 6/32 - 239.2ms/batch - loss: 43.53724 - diff: 21.29mlTrain batch 7/32 - 237.5ms/batch - loss: 43.92409 - diff: 21.65mlTrain batch 8/32 - 239.6ms/batch - loss: 47.74744 - diff: 22.73mlTrain batch 9/32 - 237.4ms/batch - loss: 45.82291 - diff: 22.26mlTrain batch 10/32 - 238.9ms/batch - loss: 46.76093 - diff: 22.31mlTrain batch 11/32 - 237.6ms/batch - loss: 46.80157 - diff: 22.07mlTrain batch 12/32 - 239.9ms/batch - loss: 44.24944 - diff: 21.28mlTrain batch 13/32 - 237.8ms/batch - loss: 44.04649 - diff: 21.16mlTrain batch 14/32 - 239.0ms/batch - loss: 44.50865 - diff: 21.23mlTrain batch 15/32 - 237.1ms/batch - loss: 45.87355 - diff: 21.55mlTrain batch 16/32 - 237.6ms/batch - loss: 44.64134 - diff: 21.13mlTrain batch 17/32 - 237.3ms/batch - loss: 44.16275 - diff: 20.94mlTrain batch 18/32 - 237.7ms/batch - loss: 43.71079 - diff: 20.74mlTrain batch 19/32 - 236.6ms/batch - loss: 43.64940 - diff: 20.87mlTrain batch 20/32 - 239.9ms/batch - loss: 50.06706 - diff: 21.79mlTrain batch 21/32 - 236.2ms/batch - loss: 48.85787 - diff: 21.53mlTrain batch 22/32 - 239.6ms/batch - loss: 49.40888 - diff: 21.74mlTrain batch 23/32 - 237.2ms/batch - loss: 48.36480 - diff: 21.56mlTrain batch 24/32 - 240.0ms/batch - loss: 48.92667 - diff: 21.74mlTrain batch 25/32 - 237.3ms/batch - loss: 49.31325 - diff: 21.86mlTrain batch 26/32 - 240.2ms/batch - loss: 49.61651 - diff: 21.79mlTrain batch 27/32 - 237.3ms/batch - loss: 49.32606 - diff: 21.78mlTrain batch 28/32 - 239.3ms/batch - loss: 49.03684 - diff: 21.76mlTrain batch 29/32 - 237.7ms/batch - loss: 48.52660 - diff: 21.66mlTrain batch 30/32 - 240.2ms/batch - loss: 47.61065 - diff: 21.48mlTrain batch 31/32 - 237.3ms/batch - loss: 46.97758 - diff: 21.34mlTrain batch 32/32 - 78.8ms/batch - loss: 46.83990 - diff: 21.25mlTrain batch 32/32 - 10.6s 78.8ms/batch - loss: 46.83990 - diff: 21.25ml
Test 1.1s: val_loss: 69.66238 - diff: 24.07ml

Epoch 106: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.6ms/batch - loss: 28.20950 - diff: 17.53mlTrain batch 2/32 - 237.9ms/batch - loss: 18.38165 - diff: 13.14mlTrain batch 3/32 - 236.6ms/batch - loss: 29.12320 - diff: 15.76mlTrain batch 4/32 - 239.3ms/batch - loss: 40.52239 - diff: 18.25mlTrain batch 5/32 - 236.7ms/batch - loss: 84.75887 - diff: 24.79mlTrain batch 6/32 - 239.7ms/batch - loss: 77.11765 - diff: 24.41mlTrain batch 7/32 - 236.7ms/batch - loss: 69.52229 - diff: 23.37mlTrain batch 8/32 - 236.8ms/batch - loss: 70.77959 - diff: 24.25mlTrain batch 9/32 - 237.3ms/batch - loss: 67.21743 - diff: 23.95mlTrain batch 10/32 - 237.7ms/batch - loss: 62.24577 - diff: 23.04mlTrain batch 11/32 - 237.4ms/batch - loss: 59.38730 - diff: 22.53mlTrain batch 12/32 - 237.9ms/batch - loss: 58.48369 - diff: 22.54mlTrain batch 13/32 - 245.9ms/batch - loss: 57.30333 - diff: 22.51mlTrain batch 14/32 - 238.1ms/batch - loss: 55.75620 - diff: 22.18mlTrain batch 15/32 - 237.2ms/batch - loss: 55.77643 - diff: 22.36mlTrain batch 16/32 - 241.2ms/batch - loss: 54.00652 - diff: 22.04mlTrain batch 17/32 - 237.2ms/batch - loss: 53.35339 - diff: 21.90mlTrain batch 18/32 - 239.1ms/batch - loss: 54.07491 - diff: 21.92mlTrain batch 19/32 - 237.8ms/batch - loss: 55.30322 - diff: 22.22mlTrain batch 20/32 - 239.4ms/batch - loss: 53.96531 - diff: 21.87mlTrain batch 21/32 - 236.7ms/batch - loss: 51.92766 - diff: 21.37mlTrain batch 22/32 - 240.4ms/batch - loss: 50.47528 - diff: 20.97mlTrain batch 23/32 - 237.8ms/batch - loss: 48.93613 - diff: 20.51mlTrain batch 24/32 - 237.9ms/batch - loss: 48.05227 - diff: 20.36mlTrain batch 25/32 - 237.2ms/batch - loss: 48.05760 - diff: 20.53mlTrain batch 26/32 - 238.1ms/batch - loss: 47.09939 - diff: 20.38mlTrain batch 27/32 - 236.7ms/batch - loss: 46.99635 - diff: 20.34mlTrain batch 28/32 - 239.7ms/batch - loss: 46.58270 - diff: 20.32mlTrain batch 29/32 - 237.6ms/batch - loss: 45.94662 - diff: 20.18mlTrain batch 30/32 - 240.7ms/batch - loss: 45.54696 - diff: 20.10mlTrain batch 31/32 - 237.5ms/batch - loss: 45.44034 - diff: 20.17mlTrain batch 32/32 - 79.4ms/batch - loss: 47.28457 - diff: 20.20mlTrain batch 32/32 - 10.7s 79.4ms/batch - loss: 47.28457 - diff: 20.20ml
Test 1.1s: val_loss: 67.86975 - diff: 24.61ml

Epoch 107: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 237.1ms/batch - loss: 35.78256 - diff: 19.26mlTrain batch 2/32 - 237.7ms/batch - loss: 87.21290 - diff: 28.09mlTrain batch 3/32 - 236.8ms/batch - loss: 61.28257 - diff: 22.07mlTrain batch 4/32 - 239.0ms/batch - loss: 65.73566 - diff: 23.32mlTrain batch 5/32 - 237.7ms/batch - loss: 66.28025 - diff: 24.19mlTrain batch 6/32 - 241.0ms/batch - loss: 59.79064 - diff: 22.96mlTrain batch 7/32 - 236.9ms/batch - loss: 58.68934 - diff: 22.95mlTrain batch 8/32 - 239.6ms/batch - loss: 54.69879 - diff: 22.14mlTrain batch 9/32 - 237.5ms/batch - loss: 57.07647 - diff: 22.83mlTrain batch 10/32 - 239.9ms/batch - loss: 53.67449 - diff: 21.98mlTrain batch 11/32 - 236.8ms/batch - loss: 49.79358 - diff: 20.94mlTrain batch 12/32 - 239.4ms/batch - loss: 48.22032 - diff: 20.71mlTrain batch 13/32 - 237.7ms/batch - loss: 49.36453 - diff: 21.09mlTrain batch 14/32 - 239.7ms/batch - loss: 47.67207 - diff: 20.68mlTrain batch 15/32 - 237.6ms/batch - loss: 49.10973 - diff: 21.17mlTrain batch 16/32 - 239.9ms/batch - loss: 49.07518 - diff: 21.24mlTrain batch 17/32 - 237.2ms/batch - loss: 48.16156 - diff: 21.04mlTrain batch 18/32 - 239.3ms/batch - loss: 47.38198 - diff: 20.82mlTrain batch 19/32 - 237.9ms/batch - loss: 46.90515 - diff: 20.83mlTrain batch 20/32 - 239.1ms/batch - loss: 45.64532 - diff: 20.48mlTrain batch 21/32 - 237.4ms/batch - loss: 45.09495 - diff: 20.31mlTrain batch 22/32 - 238.3ms/batch - loss: 44.88575 - diff: 20.20mlTrain batch 23/32 - 237.3ms/batch - loss: 43.73955 - diff: 19.96mlTrain batch 24/32 - 237.8ms/batch - loss: 43.08714 - diff: 19.85mlTrain batch 25/32 - 237.4ms/batch - loss: 42.59831 - diff: 19.72mlTrain batch 26/32 - 238.2ms/batch - loss: 42.41363 - diff: 19.76mlTrain batch 27/32 - 237.0ms/batch - loss: 42.23930 - diff: 19.66mlTrain batch 28/32 - 240.1ms/batch - loss: 44.25627 - diff: 19.82mlTrain batch 29/32 - 237.3ms/batch - loss: 43.58749 - diff: 19.63mlTrain batch 30/32 - 241.9ms/batch - loss: 43.68142 - diff: 19.67mlTrain batch 31/32 - 237.4ms/batch - loss: 43.49311 - diff: 19.74mlTrain batch 32/32 - 79.8ms/batch - loss: 46.92791 - diff: 19.91mlTrain batch 32/32 - 10.6s 79.8ms/batch - loss: 46.92791 - diff: 19.91ml
Test 1.1s: val_loss: 62.63148 - diff: 23.51ml

Epoch 108: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.7ms/batch - loss: 22.79424 - diff: 16.34mlTrain batch 2/32 - 237.6ms/batch - loss: 23.27529 - diff: 15.68mlTrain batch 3/32 - 237.2ms/batch - loss: 23.74901 - diff: 16.12mlTrain batch 4/32 - 237.6ms/batch - loss: 21.06386 - diff: 15.15mlTrain batch 5/32 - 236.2ms/batch - loss: 25.07159 - diff: 16.42mlTrain batch 6/32 - 239.2ms/batch - loss: 28.27686 - diff: 17.29mlTrain batch 7/32 - 236.7ms/batch - loss: 33.96046 - diff: 18.93mlTrain batch 8/32 - 239.9ms/batch - loss: 37.26849 - diff: 19.62mlTrain batch 9/32 - 237.3ms/batch - loss: 36.25395 - diff: 19.38mlTrain batch 10/32 - 241.1ms/batch - loss: 36.77309 - diff: 19.55mlTrain batch 11/32 - 237.4ms/batch - loss: 35.23254 - diff: 18.97mlTrain batch 12/32 - 239.8ms/batch - loss: 34.85507 - diff: 18.90mlTrain batch 13/32 - 237.3ms/batch - loss: 36.01776 - diff: 19.02mlTrain batch 14/32 - 240.5ms/batch - loss: 37.01813 - diff: 19.26mlTrain batch 15/32 - 237.7ms/batch - loss: 40.43616 - diff: 19.87mlTrain batch 16/32 - 239.0ms/batch - loss: 42.42105 - diff: 20.44mlTrain batch 17/32 - 237.3ms/batch - loss: 41.86252 - diff: 20.22mlTrain batch 18/32 - 237.8ms/batch - loss: 41.24200 - diff: 20.12mlTrain batch 19/32 - 237.4ms/batch - loss: 41.67273 - diff: 20.37mlTrain batch 20/32 - 238.0ms/batch - loss: 41.60676 - diff: 20.41mlTrain batch 21/32 - 236.8ms/batch - loss: 40.82479 - diff: 20.16mlTrain batch 22/32 - 240.0ms/batch - loss: 40.14775 - diff: 20.00mlTrain batch 23/32 - 237.6ms/batch - loss: 40.14836 - diff: 20.03mlTrain batch 24/32 - 241.1ms/batch - loss: 39.81453 - diff: 19.96mlTrain batch 25/32 - 237.2ms/batch - loss: 40.28643 - diff: 20.05mlTrain batch 26/32 - 239.9ms/batch - loss: 39.92894 - diff: 19.93mlTrain batch 27/32 - 238.0ms/batch - loss: 39.09842 - diff: 19.71mlTrain batch 28/32 - 239.8ms/batch - loss: 38.50916 - diff: 19.61mlTrain batch 29/32 - 237.5ms/batch - loss: 38.05252 - diff: 19.45mlTrain batch 30/32 - 240.7ms/batch - loss: 37.19266 - diff: 19.21mlTrain batch 31/32 - 237.8ms/batch - loss: 37.28958 - diff: 19.25mlTrain batch 32/32 - 78.3ms/batch - loss: 40.52280 - diff: 19.43mlTrain batch 32/32 - 10.7s 78.3ms/batch - loss: 40.52280 - diff: 19.43ml
Test 1.1s: val_loss: 74.45187 - diff: 26.84ml

Epoch 109: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 237.5ms/batch - loss: 20.86922 - diff: 14.25mlTrain batch 2/32 - 237.7ms/batch - loss: 14.57142 - diff: 12.31mlTrain batch 3/32 - 236.8ms/batch - loss: 21.24151 - diff: 14.87mlTrain batch 4/32 - 239.4ms/batch - loss: 25.33482 - diff: 16.21mlTrain batch 5/32 - 236.6ms/batch - loss: 29.36468 - diff: 17.35mlTrain batch 6/32 - 240.4ms/batch - loss: 31.14880 - diff: 17.91mlTrain batch 7/32 - 237.4ms/batch - loss: 31.32603 - diff: 18.19mlTrain batch 8/32 - 239.8ms/batch - loss: 34.98440 - diff: 18.47mlTrain batch 9/32 - 236.7ms/batch - loss: 36.46047 - diff: 18.74mlTrain batch 10/32 - 237.3ms/batch - loss: 36.30310 - diff: 18.88mlTrain batch 11/32 - 237.8ms/batch - loss: 35.79899 - diff: 18.80mlTrain batch 12/32 - 237.7ms/batch - loss: 36.47167 - diff: 19.31mlTrain batch 13/32 - 237.5ms/batch - loss: 36.11556 - diff: 19.18mlTrain batch 14/32 - 237.7ms/batch - loss: 35.45209 - diff: 19.08mlTrain batch 15/32 - 237.3ms/batch - loss: 34.90116 - diff: 18.87mlTrain batch 16/32 - 237.8ms/batch - loss: 35.89413 - diff: 19.11mlTrain batch 17/32 - 236.7ms/batch - loss: 38.57967 - diff: 19.78mlTrain batch 18/32 - 240.1ms/batch - loss: 38.41026 - diff: 19.80mlTrain batch 19/32 - 236.7ms/batch - loss: 39.48631 - diff: 20.14mlTrain batch 20/32 - 240.3ms/batch - loss: 39.79133 - diff: 20.04mlTrain batch 21/32 - 237.5ms/batch - loss: 41.13397 - diff: 20.41mlTrain batch 22/32 - 241.0ms/batch - loss: 40.59173 - diff: 20.35mlTrain batch 23/32 - 237.7ms/batch - loss: 40.82342 - diff: 20.30mlTrain batch 24/32 - 240.2ms/batch - loss: 40.94351 - diff: 20.27mlTrain batch 25/32 - 237.6ms/batch - loss: 39.87410 - diff: 19.99mlTrain batch 26/32 - 239.9ms/batch - loss: 40.05668 - diff: 19.98mlTrain batch 27/32 - 238.1ms/batch - loss: 41.46357 - diff: 20.08mlTrain batch 28/32 - 239.7ms/batch - loss: 41.02835 - diff: 20.07mlTrain batch 29/32 - 237.5ms/batch - loss: 41.39581 - diff: 20.12mlTrain batch 30/32 - 240.4ms/batch - loss: 41.06161 - diff: 20.05mlTrain batch 31/32 - 237.2ms/batch - loss: 40.89293 - diff: 19.98mlTrain batch 32/32 - 78.3ms/batch - loss: 42.90044 - diff: 20.08mlTrain batch 32/32 - 10.6s 78.3ms/batch - loss: 42.90044 - diff: 20.08ml
Test 1.1s: val_loss: 78.33773 - diff: 25.08ml

Epoch 110: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.5ms/batch - loss: 36.80962 - diff: 19.10mlTrain batch 2/32 - 238.4ms/batch - loss: 36.41866 - diff: 18.57mlTrain batch 3/32 - 237.5ms/batch - loss: 31.92197 - diff: 17.42mlTrain batch 4/32 - 239.5ms/batch - loss: 41.05261 - diff: 18.92mlTrain batch 5/32 - 236.9ms/batch - loss: 42.20365 - diff: 19.39mlTrain batch 6/32 - 240.4ms/batch - loss: 41.91117 - diff: 19.32mlTrain batch 7/32 - 236.3ms/batch - loss: 40.15180 - diff: 19.22mlTrain batch 8/32 - 240.8ms/batch - loss: 44.34331 - diff: 20.24mlTrain batch 9/32 - 237.5ms/batch - loss: 42.58825 - diff: 19.90mlTrain batch 10/32 - 238.0ms/batch - loss: 42.90574 - diff: 19.96mlTrain batch 11/32 - 237.5ms/batch - loss: 41.44002 - diff: 19.72mlTrain batch 12/32 - 237.7ms/batch - loss: 40.72624 - diff: 19.54mlTrain batch 13/32 - 236.9ms/batch - loss: 48.41511 - diff: 21.00mlTrain batch 14/32 - 239.9ms/batch - loss: 50.36092 - diff: 21.27mlTrain batch 15/32 - 236.6ms/batch - loss: 51.58069 - diff: 21.47mlTrain batch 16/32 - 239.7ms/batch - loss: 50.91532 - diff: 21.36mlTrain batch 17/32 - 237.8ms/batch - loss: 49.13218 - diff: 20.89mlTrain batch 18/32 - 241.1ms/batch - loss: 47.98656 - diff: 20.67mlTrain batch 19/32 - 237.1ms/batch - loss: 47.20755 - diff: 20.53mlTrain batch 20/32 - 240.7ms/batch - loss: 45.51593 - diff: 20.10mlTrain batch 21/32 - 238.0ms/batch - loss: 46.06737 - diff: 20.17mlTrain batch 22/32 - 239.7ms/batch - loss: 45.19378 - diff: 20.11mlTrain batch 23/32 - 237.2ms/batch - loss: 45.44429 - diff: 20.26mlTrain batch 24/32 - 240.6ms/batch - loss: 44.72574 - diff: 20.19mlTrain batch 25/32 - 237.5ms/batch - loss: 44.11661 - diff: 20.09mlTrain batch 26/32 - 240.6ms/batch - loss: 44.53337 - diff: 20.22mlTrain batch 27/32 - 237.4ms/batch - loss: 44.40350 - diff: 20.15mlTrain batch 28/32 - 237.8ms/batch - loss: 44.34852 - diff: 20.13mlTrain batch 29/32 - 237.4ms/batch - loss: 43.61208 - diff: 19.95mlTrain batch 30/32 - 237.8ms/batch - loss: 43.11299 - diff: 19.90mlTrain batch 31/32 - 237.1ms/batch - loss: 43.64585 - diff: 20.10mlTrain batch 32/32 - 79.3ms/batch - loss: 44.62686 - diff: 20.11mlTrain batch 32/32 - 10.6s 79.3ms/batch - loss: 44.62686 - diff: 20.11ml
Test 1.1s: val_loss: 58.35006 - diff: 23.01ml

Epoch 111: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.4ms/batch - loss: 50.46286 - diff: 20.86mlTrain batch 2/32 - 237.0ms/batch - loss: 37.25313 - diff: 18.74mlTrain batch 3/32 - 237.1ms/batch - loss: 46.21644 - diff: 20.54mlTrain batch 4/32 - 237.5ms/batch - loss: 43.87465 - diff: 20.26mlTrain batch 5/32 - 237.5ms/batch - loss: 41.65091 - diff: 19.49mlTrain batch 6/32 - 237.7ms/batch - loss: 37.77795 - diff: 18.37mlTrain batch 7/32 - 236.7ms/batch - loss: 36.58120 - diff: 18.50mlTrain batch 8/32 - 238.8ms/batch - loss: 38.75515 - diff: 19.01mlTrain batch 9/32 - 237.2ms/batch - loss: 36.30115 - diff: 18.25mlTrain batch 10/32 - 240.8ms/batch - loss: 38.08474 - diff: 18.93mlTrain batch 11/32 - 236.7ms/batch - loss: 37.92587 - diff: 18.79mlTrain batch 12/32 - 239.5ms/batch - loss: 35.74533 - diff: 18.12mlTrain batch 13/32 - 237.4ms/batch - loss: 35.32813 - diff: 18.10mlTrain batch 14/32 - 239.4ms/batch - loss: 34.24235 - diff: 17.88mlTrain batch 15/32 - 237.4ms/batch - loss: 34.15346 - diff: 17.90mlTrain batch 16/32 - 239.6ms/batch - loss: 36.33299 - diff: 18.60mlTrain batch 17/32 - 237.0ms/batch - loss: 37.83247 - diff: 18.90mlTrain batch 18/32 - 239.7ms/batch - loss: 41.88379 - diff: 19.68mlTrain batch 19/32 - 237.4ms/batch - loss: 41.12798 - diff: 19.54mlTrain batch 20/32 - 240.4ms/batch - loss: 41.06698 - diff: 19.64mlTrain batch 21/32 - 237.4ms/batch - loss: 40.17315 - diff: 19.43mlTrain batch 22/32 - 237.7ms/batch - loss: 40.02709 - diff: 19.34mlTrain batch 23/32 - 237.3ms/batch - loss: 39.77614 - diff: 19.27mlTrain batch 24/32 - 237.9ms/batch - loss: 40.44010 - diff: 19.48mlTrain batch 25/32 - 236.7ms/batch - loss: 40.67979 - diff: 19.49mlTrain batch 26/32 - 238.6ms/batch - loss: 39.90373 - diff: 19.26mlTrain batch 27/32 - 236.5ms/batch - loss: 40.82874 - diff: 19.56mlTrain batch 28/32 - 240.3ms/batch - loss: 40.63392 - diff: 19.48mlTrain batch 29/32 - 236.9ms/batch - loss: 40.77468 - diff: 19.49mlTrain batch 30/32 - 240.1ms/batch - loss: 41.43588 - diff: 19.70mlTrain batch 31/32 - 237.7ms/batch - loss: 41.71205 - diff: 19.63mlTrain batch 32/32 - 79.0ms/batch - loss: 42.17025 - diff: 19.59mlTrain batch 32/32 - 10.6s 79.0ms/batch - loss: 42.17025 - diff: 19.59ml
Test 1.1s: val_loss: 66.90293 - diff: 23.78ml

Epoch 112: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.8ms/batch - loss: 70.28324 - diff: 25.55mlTrain batch 2/32 - 237.9ms/batch - loss: 48.95168 - diff: 20.72mlTrain batch 3/32 - 236.8ms/batch - loss: 41.28410 - diff: 19.62mlTrain batch 4/32 - 239.2ms/batch - loss: 45.26685 - diff: 20.81mlTrain batch 5/32 - 237.3ms/batch - loss: 46.25201 - diff: 21.59mlTrain batch 6/32 - 239.1ms/batch - loss: 45.91628 - diff: 21.79mlTrain batch 7/32 - 237.2ms/batch - loss: 43.55395 - diff: 21.31mlTrain batch 8/32 - 239.4ms/batch - loss: 41.77836 - diff: 20.55mlTrain batch 9/32 - 237.0ms/batch - loss: 39.84556 - diff: 19.96mlTrain batch 10/32 - 239.6ms/batch - loss: 38.88645 - diff: 19.79mlTrain batch 11/32 - 237.2ms/batch - loss: 38.81240 - diff: 19.69mlTrain batch 12/32 - 240.1ms/batch - loss: 38.12365 - diff: 19.47mlTrain batch 13/32 - 236.9ms/batch - loss: 39.55830 - diff: 19.71mlTrain batch 14/32 - 239.8ms/batch - loss: 38.11621 - diff: 19.20mlTrain batch 15/32 - 237.7ms/batch - loss: 38.59831 - diff: 19.34mlTrain batch 16/32 - 237.9ms/batch - loss: 38.16225 - diff: 19.22mlTrain batch 17/32 - 237.5ms/batch - loss: 39.09148 - diff: 19.62mlTrain batch 18/32 - 237.7ms/batch - loss: 37.98482 - diff: 19.30mlTrain batch 19/32 - 236.4ms/batch - loss: 39.11011 - diff: 19.66mlTrain batch 20/32 - 240.3ms/batch - loss: 38.33663 - diff: 19.36mlTrain batch 21/32 - 237.3ms/batch - loss: 38.11211 - diff: 19.40mlTrain batch 22/32 - 240.0ms/batch - loss: 37.39032 - diff: 19.24mlTrain batch 23/32 - 237.1ms/batch - loss: 37.20669 - diff: 19.21mlTrain batch 24/32 - 240.0ms/batch - loss: 37.15124 - diff: 19.13mlTrain batch 25/32 - 237.8ms/batch - loss: 37.22814 - diff: 19.15mlTrain batch 26/32 - 239.6ms/batch - loss: 37.96483 - diff: 19.42mlTrain batch 27/32 - 237.1ms/batch - loss: 37.89083 - diff: 19.45mlTrain batch 28/32 - 239.4ms/batch - loss: 37.17591 - diff: 19.23mlTrain batch 29/32 - 237.2ms/batch - loss: 38.57829 - diff: 19.56mlTrain batch 30/32 - 240.2ms/batch - loss: 38.84072 - diff: 19.65mlTrain batch 31/32 - 237.3ms/batch - loss: 39.23341 - diff: 19.65mlTrain batch 32/32 - 78.7ms/batch - loss: 44.23749 - diff: 19.88mlTrain batch 32/32 - 10.7s 78.7ms/batch - loss: 44.23749 - diff: 19.88ml
Test 1.1s: val_loss: 80.88101 - diff: 26.99ml

Epoch 113: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.5ms/batch - loss: 33.05249 - diff: 19.87mlTrain batch 2/32 - 239.3ms/batch - loss: 46.10258 - diff: 22.74mlTrain batch 3/32 - 236.3ms/batch - loss: 46.43813 - diff: 22.10mlTrain batch 4/32 - 237.4ms/batch - loss: 42.29027 - diff: 20.93mlTrain batch 5/32 - 237.5ms/batch - loss: 37.35522 - diff: 19.45mlTrain batch 6/32 - 237.6ms/batch - loss: 38.64849 - diff: 19.78mlTrain batch 7/32 - 237.6ms/batch - loss: 34.64626 - diff: 18.62mlTrain batch 8/32 - 238.2ms/batch - loss: 33.15021 - diff: 18.15mlTrain batch 9/32 - 236.6ms/batch - loss: 35.18928 - diff: 18.55mlTrain batch 10/32 - 238.8ms/batch - loss: 35.74383 - diff: 18.72mlTrain batch 11/32 - 237.2ms/batch - loss: 34.20116 - diff: 18.30mlTrain batch 12/32 - 241.0ms/batch - loss: 35.23510 - diff: 18.53mlTrain batch 13/32 - 237.2ms/batch - loss: 40.15040 - diff: 19.38mlTrain batch 14/32 - 239.9ms/batch - loss: 39.68721 - diff: 19.35mlTrain batch 15/32 - 237.8ms/batch - loss: 39.35265 - diff: 19.21mlTrain batch 16/32 - 239.9ms/batch - loss: 38.67114 - diff: 19.19mlTrain batch 17/32 - 237.4ms/batch - loss: 37.23126 - diff: 18.79mlTrain batch 18/32 - 239.6ms/batch - loss: 36.76790 - diff: 18.73mlTrain batch 19/32 - 237.8ms/batch - loss: 36.42346 - diff: 18.66mlTrain batch 20/32 - 238.9ms/batch - loss: 35.91104 - diff: 18.55mlTrain batch 21/32 - 237.3ms/batch - loss: 36.17935 - diff: 18.74mlTrain batch 22/32 - 237.5ms/batch - loss: 37.01395 - diff: 19.04mlTrain batch 23/32 - 237.1ms/batch - loss: 37.51536 - diff: 19.12mlTrain batch 24/32 - 238.9ms/batch - loss: 44.69307 - diff: 20.15mlTrain batch 25/32 - 236.8ms/batch - loss: 45.96583 - diff: 20.37mlTrain batch 26/32 - 239.8ms/batch - loss: 46.64982 - diff: 20.48mlTrain batch 27/32 - 236.6ms/batch - loss: 45.47782 - diff: 20.15mlTrain batch 28/32 - 239.3ms/batch - loss: 44.60104 - diff: 19.97mlTrain batch 29/32 - 237.2ms/batch - loss: 44.82979 - diff: 20.12mlTrain batch 30/32 - 241.2ms/batch - loss: 45.06928 - diff: 20.25mlTrain batch 31/32 - 237.3ms/batch - loss: 45.66043 - diff: 20.43mlTrain batch 32/32 - 79.4ms/batch - loss: 46.85533 - diff: 20.46mlTrain batch 32/32 - 10.7s 79.4ms/batch - loss: 46.85533 - diff: 20.46ml
Test 1.1s: val_loss: 149.57331 - diff: 36.70ml

Epoch 114: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 237.0ms/batch - loss: 35.01937 - diff: 17.51mlTrain batch 2/32 - 237.8ms/batch - loss: 30.84868 - diff: 17.66mlTrain batch 3/32 - 236.4ms/batch - loss: 39.92527 - diff: 20.28mlTrain batch 4/32 - 238.4ms/batch - loss: 34.10913 - diff: 18.03mlTrain batch 5/32 - 237.2ms/batch - loss: 41.66078 - diff: 19.12mlTrain batch 6/32 - 240.1ms/batch - loss: 51.15680 - diff: 21.81mlTrain batch 7/32 - 237.2ms/batch - loss: 55.53729 - diff: 23.15mlTrain batch 8/32 - 238.9ms/batch - loss: 53.81445 - diff: 22.71mlTrain batch 9/32 - 237.5ms/batch - loss: 51.65168 - diff: 22.26mlTrain batch 10/32 - 239.5ms/batch - loss: 51.78253 - diff: 22.19mlTrain batch 11/32 - 237.4ms/batch - loss: 51.29356 - diff: 21.93mlTrain batch 12/32 - 239.9ms/batch - loss: 49.51237 - diff: 21.65mlTrain batch 13/32 - 236.6ms/batch - loss: 50.16234 - diff: 21.85mlTrain batch 14/32 - 240.7ms/batch - loss: 49.06097 - diff: 21.62mlTrain batch 15/32 - 237.3ms/batch - loss: 47.41870 - diff: 21.21mlTrain batch 16/32 - 237.6ms/batch - loss: 53.26371 - diff: 22.36mlTrain batch 17/32 - 237.4ms/batch - loss: 56.36099 - diff: 23.12mlTrain batch 18/32 - 237.7ms/batch - loss: 55.62561 - diff: 23.03mlTrain batch 19/32 - 236.3ms/batch - loss: 53.94841 - diff: 22.67mlTrain batch 20/32 - 239.2ms/batch - loss: 52.23421 - diff: 22.22mlTrain batch 21/32 - 237.2ms/batch - loss: 54.87358 - diff: 23.04mlTrain batch 22/32 - 240.2ms/batch - loss: 54.95454 - diff: 23.17mlTrain batch 23/32 - 236.7ms/batch - loss: 53.17014 - diff: 22.68mlTrain batch 24/32 - 239.8ms/batch - loss: 51.99171 - diff: 22.41mlTrain batch 25/32 - 237.5ms/batch - loss: 50.77524 - diff: 22.16mlTrain batch 26/32 - 239.6ms/batch - loss: 50.06213 - diff: 22.03mlTrain batch 27/32 - 237.2ms/batch - loss: 48.92783 - diff: 21.71mlTrain batch 28/32 - 239.9ms/batch - loss: 49.57111 - diff: 21.93mlTrain batch 29/32 - 236.6ms/batch - loss: 48.11579 - diff: 21.50mlTrain batch 30/32 - 239.3ms/batch - loss: 47.41230 - diff: 21.31mlTrain batch 31/32 - 237.5ms/batch - loss: 46.61912 - diff: 21.06mlTrain batch 32/32 - 78.5ms/batch - loss: 47.12918 - diff: 21.03mlTrain batch 32/32 - 10.7s 78.5ms/batch - loss: 47.12918 - diff: 21.03ml
Test 1.1s: val_loss: 88.05024 - diff: 26.16ml
Epoch   115: reducing learning rate of group 0 to 6.2500e-05.

Epoch 115: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.6ms/batch - loss: 22.38716 - diff: 16.50mlTrain batch 2/32 - 238.6ms/batch - loss: 28.94957 - diff: 18.40mlTrain batch 3/32 - 236.5ms/batch - loss: 31.31298 - diff: 17.55mlTrain batch 4/32 - 241.4ms/batch - loss: 32.60502 - diff: 18.03mlTrain batch 5/32 - 237.2ms/batch - loss: 35.61317 - diff: 19.02mlTrain batch 6/32 - 237.8ms/batch - loss: 35.00476 - diff: 18.94mlTrain batch 7/32 - 237.1ms/batch - loss: 34.45778 - diff: 18.95mlTrain batch 8/32 - 238.0ms/batch - loss: 33.18263 - diff: 18.63mlTrain batch 9/32 - 236.4ms/batch - loss: 34.59890 - diff: 19.00mlTrain batch 10/32 - 239.6ms/batch - loss: 35.43476 - diff: 18.85mlTrain batch 11/32 - 237.0ms/batch - loss: 36.54120 - diff: 19.20mlTrain batch 12/32 - 239.1ms/batch - loss: 37.16228 - diff: 19.41mlTrain batch 13/32 - 237.3ms/batch - loss: 36.23290 - diff: 19.24mlTrain batch 14/32 - 239.2ms/batch - loss: 35.84039 - diff: 19.13mlTrain batch 15/32 - 237.0ms/batch - loss: 36.23415 - diff: 19.07mlTrain batch 16/32 - 239.5ms/batch - loss: 36.86944 - diff: 19.24mlTrain batch 17/32 - 237.8ms/batch - loss: 36.00652 - diff: 18.89mlTrain batch 18/32 - 239.9ms/batch - loss: 40.66603 - diff: 19.68mlTrain batch 19/32 - 237.5ms/batch - loss: 39.77519 - diff: 19.40mlTrain batch 20/32 - 240.1ms/batch - loss: 38.71089 - diff: 19.13mlTrain batch 21/32 - 236.4ms/batch - loss: 38.74290 - diff: 19.17mlTrain batch 22/32 - 239.2ms/batch - loss: 38.38759 - diff: 19.10mlTrain batch 23/32 - 237.0ms/batch - loss: 38.77417 - diff: 19.26mlTrain batch 24/32 - 237.5ms/batch - loss: 38.76681 - diff: 19.29mlTrain batch 25/32 - 237.4ms/batch - loss: 39.52812 - diff: 19.58mlTrain batch 26/32 - 237.4ms/batch - loss: 40.32162 - diff: 19.80mlTrain batch 27/32 - 236.5ms/batch - loss: 46.95515 - diff: 20.66mlTrain batch 28/32 - 240.6ms/batch - loss: 47.11579 - diff: 20.64mlTrain batch 29/32 - 236.7ms/batch - loss: 46.87270 - diff: 20.59mlTrain batch 30/32 - 240.0ms/batch - loss: 45.71143 - diff: 20.26mlTrain batch 31/32 - 237.4ms/batch - loss: 44.94295 - diff: 20.10mlTrain batch 32/32 - 79.5ms/batch - loss: 44.71317 - diff: 20.00mlTrain batch 32/32 - 10.7s 79.5ms/batch - loss: 44.71317 - diff: 20.00ml
Test 1.1s: val_loss: 68.16215 - diff: 23.96ml

Epoch 116: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.1ms/batch - loss: 111.17670 - diff: 30.66mlTrain batch 2/32 - 238.3ms/batch - loss: 93.85870 - diff: 30.47mlTrain batch 3/32 - 236.4ms/batch - loss: 73.51086 - diff: 26.37mlTrain batch 4/32 - 238.8ms/batch - loss: 62.30050 - diff: 24.18mlTrain batch 5/32 - 236.7ms/batch - loss: 55.16564 - diff: 22.82mlTrain batch 6/32 - 238.9ms/batch - loss: 50.91236 - diff: 22.13mlTrain batch 7/32 - 236.9ms/batch - loss: 49.34279 - diff: 21.67mlTrain batch 8/32 - 239.3ms/batch - loss: 48.43532 - diff: 21.46mlTrain batch 9/32 - 237.2ms/batch - loss: 47.69026 - diff: 21.28mlTrain batch 10/32 - 239.5ms/batch - loss: 46.26980 - diff: 21.16mlTrain batch 11/32 - 237.0ms/batch - loss: 48.59082 - diff: 21.77mlTrain batch 12/32 - 239.5ms/batch - loss: 46.94346 - diff: 21.35mlTrain batch 13/32 - 237.6ms/batch - loss: 50.10810 - diff: 21.91mlTrain batch 14/32 - 237.9ms/batch - loss: 52.49282 - diff: 22.25mlTrain batch 15/32 - 236.7ms/batch - loss: 49.61184 - diff: 21.30mlTrain batch 16/32 - 237.7ms/batch - loss: 47.64513 - diff: 20.84mlTrain batch 17/32 - 236.8ms/batch - loss: 52.27145 - diff: 21.76mlTrain batch 18/32 - 239.6ms/batch - loss: 53.78030 - diff: 22.17mlTrain batch 19/32 - 237.3ms/batch - loss: 53.29454 - diff: 22.09mlTrain batch 20/32 - 240.4ms/batch - loss: 52.56231 - diff: 21.94mlTrain batch 21/32 - 237.7ms/batch - loss: 51.67387 - diff: 21.80mlTrain batch 22/32 - 239.5ms/batch - loss: 50.47210 - diff: 21.50mlTrain batch 23/32 - 237.5ms/batch - loss: 48.99297 - diff: 21.08mlTrain batch 24/32 - 239.6ms/batch - loss: 48.34817 - diff: 20.96mlTrain batch 25/32 - 236.4ms/batch - loss: 50.42295 - diff: 21.49mlTrain batch 26/32 - 239.2ms/batch - loss: 49.85707 - diff: 21.43mlTrain batch 27/32 - 237.7ms/batch - loss: 48.81291 - diff: 21.17mlTrain batch 28/32 - 239.1ms/batch - loss: 48.40988 - diff: 21.19mlTrain batch 29/32 - 237.2ms/batch - loss: 47.54590 - diff: 21.00mlTrain batch 30/32 - 237.5ms/batch - loss: 46.63962 - diff: 20.81mlTrain batch 31/32 - 237.2ms/batch - loss: 46.21185 - diff: 20.80mlTrain batch 32/32 - 78.9ms/batch - loss: 46.56379 - diff: 20.76mlTrain batch 32/32 - 10.7s 78.9ms/batch - loss: 46.56379 - diff: 20.76ml
Test 1.1s: val_loss: 61.40948 - diff: 23.38ml

Epoch 117: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.5ms/batch - loss: 13.68066 - diff: 12.44mlTrain batch 2/32 - 238.6ms/batch - loss: 21.05728 - diff: 14.95mlTrain batch 3/32 - 237.3ms/batch - loss: 25.06228 - diff: 16.28mlTrain batch 4/32 - 237.6ms/batch - loss: 25.26068 - diff: 16.20mlTrain batch 5/32 - 236.4ms/batch - loss: 25.71552 - diff: 16.82mlTrain batch 6/32 - 238.8ms/batch - loss: 30.46572 - diff: 18.12mlTrain batch 7/32 - 237.1ms/batch - loss: 30.11085 - diff: 17.73mlTrain batch 8/32 - 240.1ms/batch - loss: 31.31485 - diff: 18.07mlTrain batch 9/32 - 236.6ms/batch - loss: 32.45763 - diff: 18.34mlTrain batch 10/32 - 239.4ms/batch - loss: 32.16045 - diff: 18.28mlTrain batch 11/32 - 237.2ms/batch - loss: 31.44201 - diff: 18.20mlTrain batch 12/32 - 239.6ms/batch - loss: 31.32533 - diff: 18.10mlTrain batch 13/32 - 237.0ms/batch - loss: 32.81986 - diff: 18.20mlTrain batch 14/32 - 239.5ms/batch - loss: 33.38664 - diff: 18.21mlTrain batch 15/32 - 237.2ms/batch - loss: 33.36312 - diff: 18.18mlTrain batch 16/32 - 239.5ms/batch - loss: 33.76242 - diff: 18.34mlTrain batch 17/32 - 237.0ms/batch - loss: 32.92831 - diff: 18.07mlTrain batch 18/32 - 240.5ms/batch - loss: 35.93619 - diff: 18.69mlTrain batch 19/32 - 237.1ms/batch - loss: 35.98178 - diff: 18.68mlTrain batch 20/32 - 237.8ms/batch - loss: 35.59230 - diff: 18.56mlTrain batch 21/32 - 237.5ms/batch - loss: 38.64524 - diff: 19.08mlTrain batch 22/32 - 237.6ms/batch - loss: 37.83946 - diff: 18.87mlTrain batch 23/32 - 236.4ms/batch - loss: 38.62666 - diff: 19.01mlTrain batch 24/32 - 239.7ms/batch - loss: 37.89223 - diff: 18.79mlTrain batch 25/32 - 237.0ms/batch - loss: 38.34668 - diff: 19.06mlTrain batch 26/32 - 239.4ms/batch - loss: 37.56585 - diff: 18.86mlTrain batch 27/32 - 237.1ms/batch - loss: 37.66807 - diff: 18.92mlTrain batch 28/32 - 240.3ms/batch - loss: 37.15329 - diff: 18.80mlTrain batch 29/32 - 237.1ms/batch - loss: 36.91748 - diff: 18.67mlTrain batch 30/32 - 239.4ms/batch - loss: 40.27148 - diff: 19.44mlTrain batch 31/32 - 238.0ms/batch - loss: 39.70173 - diff: 19.29mlTrain batch 32/32 - 79.1ms/batch - loss: 41.03080 - diff: 19.32mlTrain batch 32/32 - 10.7s 79.1ms/batch - loss: 41.03080 - diff: 19.32ml
Test 1.1s: val_loss: 61.29544 - diff: 23.56ml

Epoch 118: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.5ms/batch - loss: 18.13031 - diff: 14.05mlTrain batch 2/32 - 237.9ms/batch - loss: 27.00362 - diff: 16.20mlTrain batch 3/32 - 236.8ms/batch - loss: 35.22133 - diff: 16.01mlTrain batch 4/32 - 239.0ms/batch - loss: 37.59065 - diff: 17.47mlTrain batch 5/32 - 237.4ms/batch - loss: 34.40733 - diff: 17.24mlTrain batch 6/32 - 238.7ms/batch - loss: 47.17262 - diff: 20.55mlTrain batch 7/32 - 236.5ms/batch - loss: 43.03454 - diff: 19.73mlTrain batch 8/32 - 240.1ms/batch - loss: 42.11453 - diff: 19.74mlTrain batch 9/32 - 236.7ms/batch - loss: 41.86415 - diff: 19.81mlTrain batch 10/32 - 239.2ms/batch - loss: 41.21840 - diff: 19.79mlTrain batch 11/32 - 236.8ms/batch - loss: 38.97028 - diff: 19.29mlTrain batch 12/32 - 237.5ms/batch - loss: 36.95655 - diff: 18.80mlTrain batch 13/32 - 236.6ms/batch - loss: 36.05513 - diff: 18.63mlTrain batch 14/32 - 237.9ms/batch - loss: 36.16075 - diff: 18.85mlTrain batch 15/32 - 236.9ms/batch - loss: 35.33912 - diff: 18.57mlTrain batch 16/32 - 239.2ms/batch - loss: 37.05265 - diff: 19.08mlTrain batch 17/32 - 237.0ms/batch - loss: 37.12888 - diff: 19.13mlTrain batch 18/32 - 239.9ms/batch - loss: 36.30953 - diff: 18.86mlTrain batch 19/32 - 236.9ms/batch - loss: 35.35386 - diff: 18.60mlTrain batch 20/32 - 239.1ms/batch - loss: 35.31688 - diff: 18.62mlTrain batch 21/32 - 237.0ms/batch - loss: 34.37629 - diff: 18.35mlTrain batch 22/32 - 239.3ms/batch - loss: 33.76509 - diff: 18.13mlTrain batch 23/32 - 237.2ms/batch - loss: 33.32539 - diff: 18.09mlTrain batch 24/32 - 239.8ms/batch - loss: 33.30320 - diff: 18.04mlTrain batch 25/32 - 237.0ms/batch - loss: 33.95929 - diff: 18.27mlTrain batch 26/32 - 240.4ms/batch - loss: 35.23573 - diff: 18.56mlTrain batch 27/32 - 237.1ms/batch - loss: 34.88203 - diff: 18.48mlTrain batch 28/32 - 239.0ms/batch - loss: 34.58642 - diff: 18.44mlTrain batch 29/32 - 237.4ms/batch - loss: 34.96374 - diff: 18.60mlTrain batch 30/32 - 237.7ms/batch - loss: 34.77954 - diff: 18.62mlTrain batch 31/32 - 237.6ms/batch - loss: 34.55378 - diff: 18.61mlTrain batch 32/32 - 78.6ms/batch - loss: 36.01123 - diff: 18.65mlTrain batch 32/32 - 10.7s 78.6ms/batch - loss: 36.01123 - diff: 18.65ml
Test 1.1s: val_loss: 63.04471 - diff: 23.65ml

Epoch 119: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.5ms/batch - loss: 18.95772 - diff: 14.10mlTrain batch 2/32 - 238.8ms/batch - loss: 22.50797 - diff: 14.09mlTrain batch 3/32 - 236.5ms/batch - loss: 18.74189 - diff: 12.56mlTrain batch 4/32 - 239.4ms/batch - loss: 22.15995 - diff: 13.76mlTrain batch 5/32 - 237.3ms/batch - loss: 23.96990 - diff: 14.69mlTrain batch 6/32 - 237.7ms/batch - loss: 26.36516 - diff: 15.46mlTrain batch 7/32 - 236.9ms/batch - loss: 26.05369 - diff: 15.61mlTrain batch 8/32 - 238.1ms/batch - loss: 25.25086 - diff: 15.45mlTrain batch 9/32 - 236.5ms/batch - loss: 36.18587 - diff: 17.21mlTrain batch 10/32 - 239.1ms/batch - loss: 37.92897 - diff: 17.85mlTrain batch 11/32 - 237.1ms/batch - loss: 37.55644 - diff: 17.75mlTrain batch 12/32 - 238.6ms/batch - loss: 36.35970 - diff: 17.56mlTrain batch 13/32 - 236.6ms/batch - loss: 35.92770 - diff: 17.61mlTrain batch 14/32 - 239.1ms/batch - loss: 35.01706 - diff: 17.48mlTrain batch 15/32 - 237.5ms/batch - loss: 33.66332 - diff: 17.16mlTrain batch 16/32 - 239.3ms/batch - loss: 33.07446 - diff: 17.02mlTrain batch 17/32 - 236.7ms/batch - loss: 32.11229 - diff: 16.72mlTrain batch 18/32 - 237.1ms/batch - loss: 31.67251 - diff: 16.67mlTrain batch 19/32 - 237.8ms/batch - loss: 31.60345 - diff: 16.78mlTrain batch 20/32 - 237.8ms/batch - loss: 32.38920 - diff: 17.10mlTrain batch 21/32 - 237.5ms/batch - loss: 31.57979 - diff: 16.85mlTrain batch 22/32 - 237.8ms/batch - loss: 31.28827 - diff: 16.88mlTrain batch 23/32 - 236.9ms/batch - loss: 30.62186 - diff: 16.76mlTrain batch 24/32 - 237.9ms/batch - loss: 31.77438 - diff: 16.97mlTrain batch 25/32 - 236.6ms/batch - loss: 31.51208 - diff: 16.88mlTrain batch 26/32 - 239.0ms/batch - loss: 31.78173 - diff: 16.97mlTrain batch 27/32 - 236.7ms/batch - loss: 31.70336 - diff: 17.02mlTrain batch 28/32 - 239.9ms/batch - loss: 31.26558 - diff: 16.92mlTrain batch 29/32 - 237.6ms/batch - loss: 32.41000 - diff: 17.27mlTrain batch 30/32 - 239.6ms/batch - loss: 31.92530 - diff: 17.12mlTrain batch 31/32 - 236.8ms/batch - loss: 32.84887 - diff: 17.40mlTrain batch 32/32 - 77.5ms/batch - loss: 32.64446 - diff: 17.30mlTrain batch 32/32 - 10.7s 77.5ms/batch - loss: 32.64446 - diff: 17.30ml
Test 1.1s: val_loss: 60.38581 - diff: 23.22ml

Epoch 120: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.5ms/batch - loss: 40.83830 - diff: 21.45mlTrain batch 2/32 - 238.2ms/batch - loss: 32.26276 - diff: 18.65mlTrain batch 3/32 - 236.1ms/batch - loss: 40.06931 - diff: 20.35mlTrain batch 4/32 - 239.1ms/batch - loss: 36.54652 - diff: 19.45mlTrain batch 5/32 - 236.8ms/batch - loss: 36.71637 - diff: 19.26mlTrain batch 6/32 - 238.5ms/batch - loss: 34.64248 - diff: 18.91mlTrain batch 7/32 - 237.7ms/batch - loss: 33.83921 - diff: 18.89mlTrain batch 8/32 - 237.3ms/batch - loss: 35.29515 - diff: 19.06mlTrain batch 9/32 - 237.4ms/batch - loss: 32.66924 - diff: 18.19mlTrain batch 10/32 - 237.7ms/batch - loss: 31.97277 - diff: 18.05mlTrain batch 11/32 - 237.3ms/batch - loss: 33.27974 - diff: 18.45mlTrain batch 12/32 - 237.9ms/batch - loss: 35.30271 - diff: 18.77mlTrain batch 13/32 - 236.2ms/batch - loss: 34.98847 - diff: 18.71mlTrain batch 14/32 - 238.9ms/batch - loss: 33.77193 - diff: 18.45mlTrain batch 15/32 - 236.9ms/batch - loss: 33.27206 - diff: 18.36mlTrain batch 16/32 - 239.0ms/batch - loss: 35.34305 - diff: 18.82mlTrain batch 17/32 - 237.1ms/batch - loss: 36.55797 - diff: 19.11mlTrain batch 18/32 - 240.2ms/batch - loss: 42.22957 - diff: 20.39mlTrain batch 19/32 - 236.9ms/batch - loss: 42.50729 - diff: 20.54mlTrain batch 20/32 - 239.8ms/batch - loss: 43.46099 - diff: 20.73mlTrain batch 21/32 - 236.8ms/batch - loss: 42.59571 - diff: 20.58mlTrain batch 22/32 - 239.9ms/batch - loss: 42.31984 - diff: 20.55mlTrain batch 23/32 - 237.3ms/batch - loss: 41.69867 - diff: 20.42mlTrain batch 24/32 - 240.0ms/batch - loss: 41.92491 - diff: 20.47mlTrain batch 25/32 - 237.1ms/batch - loss: 41.89736 - diff: 20.47mlTrain batch 26/32 - 240.0ms/batch - loss: 41.52089 - diff: 20.41mlTrain batch 27/32 - 237.6ms/batch - loss: 42.04461 - diff: 20.66mlTrain batch 28/32 - 239.8ms/batch - loss: 41.82716 - diff: 20.59mlTrain batch 29/32 - 237.3ms/batch - loss: 40.97621 - diff: 20.33mlTrain batch 30/32 - 237.6ms/batch - loss: 40.29131 - diff: 20.15mlTrain batch 31/32 - 237.2ms/batch - loss: 39.78222 - diff: 20.05mlTrain batch 32/32 - 78.6ms/batch - loss: 41.45652 - diff: 20.13mlTrain batch 32/32 - 10.6s 78.6ms/batch - loss: 41.45652 - diff: 20.13ml
Test 1.1s: val_loss: 69.08426 - diff: 23.35ml

Epoch 121: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.8ms/batch - loss: 28.66834 - diff: 18.03mlTrain batch 2/32 - 238.8ms/batch - loss: 30.50102 - diff: 17.65mlTrain batch 3/32 - 237.7ms/batch - loss: 27.21825 - diff: 16.67mlTrain batch 4/32 - 239.2ms/batch - loss: 25.99729 - diff: 16.12mlTrain batch 5/32 - 237.5ms/batch - loss: 27.48522 - diff: 16.24mlTrain batch 6/32 - 237.7ms/batch - loss: 27.38954 - diff: 16.12mlTrain batch 7/32 - 237.4ms/batch - loss: 29.97842 - diff: 16.63mlTrain batch 8/32 - 237.8ms/batch - loss: 32.50897 - diff: 17.29mlTrain batch 9/32 - 236.5ms/batch - loss: 33.22010 - diff: 17.52mlTrain batch 10/32 - 239.1ms/batch - loss: 32.19978 - diff: 17.37mlTrain batch 11/32 - 236.3ms/batch - loss: 32.94056 - diff: 17.42mlTrain batch 12/32 - 239.7ms/batch - loss: 31.27503 - diff: 16.90mlTrain batch 13/32 - 236.9ms/batch - loss: 32.80967 - diff: 17.43mlTrain batch 14/32 - 240.1ms/batch - loss: 32.74406 - diff: 17.46mlTrain batch 15/32 - 236.9ms/batch - loss: 33.91423 - diff: 17.68mlTrain batch 16/32 - 240.2ms/batch - loss: 32.46302 - diff: 17.29mlTrain batch 17/32 - 237.0ms/batch - loss: 33.27465 - diff: 17.63mlTrain batch 18/32 - 240.0ms/batch - loss: 32.96355 - diff: 17.64mlTrain batch 19/32 - 237.5ms/batch - loss: 32.82266 - diff: 17.59mlTrain batch 20/32 - 239.8ms/batch - loss: 32.55955 - diff: 17.56mlTrain batch 21/32 - 236.8ms/batch - loss: 32.56447 - diff: 17.61mlTrain batch 22/32 - 240.6ms/batch - loss: 33.64171 - diff: 17.92mlTrain batch 23/32 - 237.9ms/batch - loss: 33.07045 - diff: 17.79mlTrain batch 24/32 - 239.0ms/batch - loss: 33.07913 - diff: 17.79mlTrain batch 25/32 - 237.6ms/batch - loss: 33.17171 - diff: 17.76mlTrain batch 26/32 - 237.7ms/batch - loss: 32.68445 - diff: 17.63mlTrain batch 27/32 - 237.3ms/batch - loss: 33.14760 - diff: 17.78mlTrain batch 28/32 - 237.9ms/batch - loss: 34.83873 - diff: 18.28mlTrain batch 29/32 - 236.4ms/batch - loss: 36.60779 - diff: 18.71mlTrain batch 30/32 - 239.4ms/batch - loss: 36.09785 - diff: 18.56mlTrain batch 31/32 - 236.6ms/batch - loss: 36.67030 - diff: 18.63mlTrain batch 32/32 - 78.1ms/batch - loss: 38.90368 - diff: 18.74mlTrain batch 32/32 - 10.6s 78.1ms/batch - loss: 38.90368 - diff: 18.74ml
Test 1.1s: val_loss: 64.95252 - diff: 23.57ml

Epoch 122: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.2ms/batch - loss: 41.11647 - diff: 18.35mlTrain batch 2/32 - 238.3ms/batch - loss: 28.95344 - diff: 16.01mlTrain batch 3/32 - 236.7ms/batch - loss: 34.40247 - diff: 17.87mlTrain batch 4/32 - 239.7ms/batch - loss: 31.47274 - diff: 17.11mlTrain batch 5/32 - 237.1ms/batch - loss: 36.29515 - diff: 18.21mlTrain batch 6/32 - 240.3ms/batch - loss: 35.66685 - diff: 18.13mlTrain batch 7/32 - 237.2ms/batch - loss: 35.45501 - diff: 18.03mlTrain batch 8/32 - 239.4ms/batch - loss: 34.08677 - diff: 17.72mlTrain batch 9/32 - 237.7ms/batch - loss: 34.09474 - diff: 17.87mlTrain batch 10/32 - 238.9ms/batch - loss: 34.85815 - diff: 17.99mlTrain batch 11/32 - 236.8ms/batch - loss: 32.73494 - diff: 17.18mlTrain batch 12/32 - 239.6ms/batch - loss: 32.88159 - diff: 17.26mlTrain batch 13/32 - 237.0ms/batch - loss: 32.10810 - diff: 17.23mlTrain batch 14/32 - 239.1ms/batch - loss: 31.95961 - diff: 17.34mlTrain batch 15/32 - 237.4ms/batch - loss: 33.30178 - diff: 17.75mlTrain batch 16/32 - 237.7ms/batch - loss: 33.16099 - diff: 17.85mlTrain batch 17/32 - 237.5ms/batch - loss: 33.59487 - diff: 17.94mlTrain batch 18/32 - 238.3ms/batch - loss: 33.97428 - diff: 18.02mlTrain batch 19/32 - 237.7ms/batch - loss: 33.46756 - diff: 17.93mlTrain batch 20/32 - 239.2ms/batch - loss: 33.94203 - diff: 17.99mlTrain batch 21/32 - 237.4ms/batch - loss: 33.12417 - diff: 17.81mlTrain batch 22/32 - 240.2ms/batch - loss: 33.68989 - diff: 18.05mlTrain batch 23/32 - 236.9ms/batch - loss: 33.67068 - diff: 18.12mlTrain batch 24/32 - 239.8ms/batch - loss: 34.60139 - diff: 18.24mlTrain batch 25/32 - 237.5ms/batch - loss: 34.65432 - diff: 18.15mlTrain batch 26/32 - 239.1ms/batch - loss: 34.58063 - diff: 18.23mlTrain batch 27/32 - 236.9ms/batch - loss: 34.08949 - diff: 18.12mlTrain batch 28/32 - 239.1ms/batch - loss: 33.70650 - diff: 18.07mlTrain batch 29/32 - 237.4ms/batch - loss: 33.78132 - diff: 18.16mlTrain batch 30/32 - 239.7ms/batch - loss: 33.17639 - diff: 17.96mlTrain batch 31/32 - 237.2ms/batch - loss: 32.89044 - diff: 17.89mlTrain batch 32/32 - 78.3ms/batch - loss: 44.59102 - diff: 18.25mlTrain batch 32/32 - 10.7s 78.3ms/batch - loss: 44.59102 - diff: 18.25ml
Test 1.1s: val_loss: 70.61512 - diff: 25.60ml

Epoch 123: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.6ms/batch - loss: 55.96468 - diff: 27.14mlTrain batch 2/32 - 242.2ms/batch - loss: 40.24490 - diff: 20.44mlTrain batch 3/32 - 237.6ms/batch - loss: 46.34681 - diff: 22.38mlTrain batch 4/32 - 237.6ms/batch - loss: 44.32031 - diff: 22.04mlTrain batch 5/32 - 237.7ms/batch - loss: 39.35311 - diff: 20.63mlTrain batch 6/32 - 238.0ms/batch - loss: 38.06053 - diff: 20.48mlTrain batch 7/32 - 236.9ms/batch - loss: 36.06619 - diff: 19.88mlTrain batch 8/32 - 237.8ms/batch - loss: 34.28993 - diff: 19.28mlTrain batch 9/32 - 236.7ms/batch - loss: 33.49554 - diff: 18.88mlTrain batch 10/32 - 238.8ms/batch - loss: 35.37739 - diff: 19.41mlTrain batch 11/32 - 236.8ms/batch - loss: 34.68883 - diff: 19.19mlTrain batch 12/32 - 239.5ms/batch - loss: 34.24827 - diff: 19.12mlTrain batch 13/32 - 237.5ms/batch - loss: 35.93076 - diff: 19.34mlTrain batch 14/32 - 240.0ms/batch - loss: 35.04948 - diff: 18.99mlTrain batch 15/32 - 237.5ms/batch - loss: 36.87391 - diff: 19.34mlTrain batch 16/32 - 239.5ms/batch - loss: 36.27015 - diff: 19.24mlTrain batch 17/32 - 237.0ms/batch - loss: 36.57434 - diff: 19.39mlTrain batch 18/32 - 239.8ms/batch - loss: 35.62401 - diff: 19.05mlTrain batch 19/32 - 237.1ms/batch - loss: 34.88114 - diff: 18.69mlTrain batch 20/32 - 239.3ms/batch - loss: 35.57064 - diff: 18.82mlTrain batch 21/32 - 237.7ms/batch - loss: 34.76566 - diff: 18.56mlTrain batch 22/32 - 239.6ms/batch - loss: 34.54038 - diff: 18.59mlTrain batch 23/32 - 237.4ms/batch - loss: 33.75061 - diff: 18.30mlTrain batch 24/32 - 238.1ms/batch - loss: 33.37067 - diff: 18.21mlTrain batch 25/32 - 237.5ms/batch - loss: 35.03961 - diff: 18.62mlTrain batch 26/32 - 237.8ms/batch - loss: 34.87194 - diff: 18.59mlTrain batch 27/32 - 237.1ms/batch - loss: 34.79768 - diff: 18.56mlTrain batch 28/32 - 239.5ms/batch - loss: 34.61735 - diff: 18.49mlTrain batch 29/32 - 236.7ms/batch - loss: 34.49791 - diff: 18.48mlTrain batch 30/32 - 238.4ms/batch - loss: 33.85595 - diff: 18.28mlTrain batch 31/32 - 237.2ms/batch - loss: 35.32819 - diff: 18.63mlTrain batch 32/32 - 79.2ms/batch - loss: 37.77199 - diff: 18.71mlTrain batch 32/32 - 10.6s 79.2ms/batch - loss: 37.77199 - diff: 18.71ml
Test 1.1s: val_loss: 65.54877 - diff: 23.15ml

Epoch 124: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.6ms/batch - loss: 42.40952 - diff: 19.39mlTrain batch 2/32 - 238.1ms/batch - loss: 38.72916 - diff: 20.13mlTrain batch 3/32 - 236.8ms/batch - loss: 33.68466 - diff: 17.69mlTrain batch 4/32 - 238.4ms/batch - loss: 33.44118 - diff: 17.87mlTrain batch 5/32 - 236.9ms/batch - loss: 29.40481 - diff: 16.77mlTrain batch 6/32 - 238.6ms/batch - loss: 27.09355 - diff: 16.15mlTrain batch 7/32 - 236.4ms/batch - loss: 30.73326 - diff: 17.21mlTrain batch 8/32 - 242.3ms/batch - loss: 32.01541 - diff: 17.61mlTrain batch 9/32 - 236.9ms/batch - loss: 29.43345 - diff: 16.67mlTrain batch 10/32 - 239.0ms/batch - loss: 28.53803 - diff: 16.52mlTrain batch 11/32 - 237.5ms/batch - loss: 28.46136 - diff: 16.71mlTrain batch 12/32 - 238.9ms/batch - loss: 29.33666 - diff: 17.04mlTrain batch 13/32 - 236.9ms/batch - loss: 29.17862 - diff: 17.03mlTrain batch 14/32 - 239.8ms/batch - loss: 28.97678 - diff: 16.95mlTrain batch 15/32 - 236.7ms/batch - loss: 28.65937 - diff: 16.94mlTrain batch 16/32 - 237.1ms/batch - loss: 28.13012 - diff: 16.70mlTrain batch 17/32 - 237.5ms/batch - loss: 29.89503 - diff: 17.29mlTrain batch 18/32 - 237.5ms/batch - loss: 29.12594 - diff: 17.10mlTrain batch 19/32 - 237.5ms/batch - loss: 29.02031 - diff: 17.10mlTrain batch 20/32 - 237.7ms/batch - loss: 29.80087 - diff: 17.38mlTrain batch 21/32 - 236.9ms/batch - loss: 30.34172 - diff: 17.41mlTrain batch 22/32 - 238.7ms/batch - loss: 30.01949 - diff: 17.36mlTrain batch 23/32 - 237.1ms/batch - loss: 29.49886 - diff: 17.18mlTrain batch 24/32 - 239.7ms/batch - loss: 29.52566 - diff: 17.23mlTrain batch 25/32 - 236.8ms/batch - loss: 30.67524 - diff: 17.44mlTrain batch 26/32 - 239.0ms/batch - loss: 30.46517 - diff: 17.47mlTrain batch 27/32 - 237.4ms/batch - loss: 30.39514 - diff: 17.46mlTrain batch 28/32 - 240.1ms/batch - loss: 31.26992 - diff: 17.62mlTrain batch 29/32 - 237.3ms/batch - loss: 31.69685 - diff: 17.80mlTrain batch 30/32 - 240.0ms/batch - loss: 31.54216 - diff: 17.74mlTrain batch 31/32 - 236.6ms/batch - loss: 31.30376 - diff: 17.63mlTrain batch 32/32 - 78.0ms/batch - loss: 37.27430 - diff: 17.87mlTrain batch 32/32 - 10.6s 78.0ms/batch - loss: 37.27430 - diff: 17.87ml
Test 1.1s: val_loss: 75.18074 - diff: 26.29ml

Epoch 125: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.6ms/batch - loss: 19.35778 - diff: 14.45mlTrain batch 2/32 - 237.9ms/batch - loss: 22.51772 - diff: 15.15mlTrain batch 3/32 - 237.4ms/batch - loss: 24.40623 - diff: 15.82mlTrain batch 4/32 - 239.3ms/batch - loss: 31.10020 - diff: 18.09mlTrain batch 5/32 - 236.9ms/batch - loss: 31.95482 - diff: 18.51mlTrain batch 6/32 - 239.4ms/batch - loss: 31.97415 - diff: 18.53mlTrain batch 7/32 - 236.9ms/batch - loss: 31.86170 - diff: 18.49mlTrain batch 8/32 - 240.2ms/batch - loss: 34.11380 - diff: 19.18mlTrain batch 9/32 - 237.5ms/batch - loss: 34.77045 - diff: 19.48mlTrain batch 10/32 - 239.7ms/batch - loss: 34.14073 - diff: 19.36mlTrain batch 11/32 - 237.3ms/batch - loss: 33.14433 - diff: 18.65mlTrain batch 12/32 - 240.7ms/batch - loss: 32.49496 - diff: 18.53mlTrain batch 13/32 - 236.8ms/batch - loss: 31.33163 - diff: 18.16mlTrain batch 14/32 - 239.9ms/batch - loss: 31.16791 - diff: 18.14mlTrain batch 15/32 - 237.7ms/batch - loss: 31.03656 - diff: 17.98mlTrain batch 16/32 - 238.3ms/batch - loss: 32.52060 - diff: 18.36mlTrain batch 17/32 - 237.7ms/batch - loss: 31.47325 - diff: 18.04mlTrain batch 18/32 - 237.8ms/batch - loss: 31.68162 - diff: 18.11mlTrain batch 19/32 - 237.2ms/batch - loss: 31.52205 - diff: 18.16mlTrain batch 20/32 - 239.1ms/batch - loss: 32.69949 - diff: 18.57mlTrain batch 21/32 - 236.9ms/batch - loss: 32.68437 - diff: 18.58mlTrain batch 22/32 - 240.6ms/batch - loss: 32.94541 - diff: 18.68mlTrain batch 23/32 - 236.8ms/batch - loss: 32.92551 - diff: 18.61mlTrain batch 24/32 - 239.9ms/batch - loss: 34.73650 - diff: 19.07mlTrain batch 25/32 - 237.3ms/batch - loss: 34.01391 - diff: 18.81mlTrain batch 26/32 - 241.4ms/batch - loss: 33.92368 - diff: 18.79mlTrain batch 27/32 - 237.9ms/batch - loss: 34.15359 - diff: 18.86mlTrain batch 28/32 - 240.8ms/batch - loss: 33.69014 - diff: 18.68mlTrain batch 29/32 - 238.7ms/batch - loss: 34.11019 - diff: 18.76mlTrain batch 30/32 - 240.5ms/batch - loss: 35.23846 - diff: 19.11mlTrain batch 31/32 - 237.7ms/batch - loss: 35.61298 - diff: 19.26mlTrain batch 32/32 - 78.6ms/batch - loss: 38.09896 - diff: 19.40mlTrain batch 32/32 - 10.6s 78.6ms/batch - loss: 38.09896 - diff: 19.40ml
Test 1.1s: val_loss: 72.55126 - diff: 23.89ml
Epoch   126: reducing learning rate of group 0 to 3.1250e-05.

Epoch 126: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.7ms/batch - loss: 64.72621 - diff: 25.67mlTrain batch 2/32 - 240.0ms/batch - loss: 39.45642 - diff: 19.10mlTrain batch 3/32 - 236.8ms/batch - loss: 36.11552 - diff: 18.41mlTrain batch 4/32 - 240.4ms/batch - loss: 37.84771 - diff: 18.86mlTrain batch 5/32 - 237.8ms/batch - loss: 34.62439 - diff: 18.16mlTrain batch 6/32 - 240.4ms/batch - loss: 36.25484 - diff: 18.99mlTrain batch 7/32 - 238.0ms/batch - loss: 33.53442 - diff: 18.21mlTrain batch 8/32 - 240.6ms/batch - loss: 31.59860 - diff: 17.72mlTrain batch 9/32 - 237.7ms/batch - loss: 31.23845 - diff: 17.58mlTrain batch 10/32 - 240.9ms/batch - loss: 33.93063 - diff: 18.36mlTrain batch 11/32 - 237.1ms/batch - loss: 32.76895 - diff: 18.11mlTrain batch 12/32 - 238.0ms/batch - loss: 32.28968 - diff: 18.16mlTrain batch 13/32 - 237.8ms/batch - loss: 31.73325 - diff: 17.89mlTrain batch 14/32 - 237.9ms/batch - loss: 30.79210 - diff: 17.68mlTrain batch 15/32 - 236.6ms/batch - loss: 30.30127 - diff: 17.57mlTrain batch 16/32 - 241.1ms/batch - loss: 30.39107 - diff: 17.56mlTrain batch 17/32 - 236.8ms/batch - loss: 31.53289 - diff: 17.87mlTrain batch 18/32 - 240.8ms/batch - loss: 30.40222 - diff: 17.47mlTrain batch 19/32 - 237.4ms/batch - loss: 29.95314 - diff: 17.32mlTrain batch 20/32 - 241.2ms/batch - loss: 30.79714 - diff: 17.53mlTrain batch 21/32 - 237.0ms/batch - loss: 34.36287 - diff: 18.01mlTrain batch 22/32 - 240.6ms/batch - loss: 33.33323 - diff: 17.64mlTrain batch 23/32 - 237.6ms/batch - loss: 32.77219 - diff: 17.50mlTrain batch 24/32 - 241.2ms/batch - loss: 33.42871 - diff: 17.82mlTrain batch 25/32 - 238.3ms/batch - loss: 33.96041 - diff: 17.91mlTrain batch 26/32 - 240.7ms/batch - loss: 33.50500 - diff: 17.82mlTrain batch 27/32 - 237.6ms/batch - loss: 33.11344 - diff: 17.68mlTrain batch 28/32 - 240.2ms/batch - loss: 32.94067 - diff: 17.65mlTrain batch 29/32 - 237.9ms/batch - loss: 32.53356 - diff: 17.58mlTrain batch 30/32 - 240.0ms/batch - loss: 32.14475 - diff: 17.49mlTrain batch 31/32 - 237.3ms/batch - loss: 32.11079 - diff: 17.50mlTrain batch 32/32 - 79.0ms/batch - loss: 33.01640 - diff: 17.51mlTrain batch 32/32 - 10.6s 79.0ms/batch - loss: 33.01640 - diff: 17.51ml
Test 1.1s: val_loss: 70.71017 - diff: 24.18ml

Epoch 127: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.8ms/batch - loss: 14.14511 - diff: 12.24mlTrain batch 2/32 - 240.0ms/batch - loss: 15.47009 - diff: 12.53mlTrain batch 3/32 - 237.3ms/batch - loss: 19.84421 - diff: 14.15mlTrain batch 4/32 - 238.0ms/batch - loss: 23.94611 - diff: 15.44mlTrain batch 5/32 - 237.4ms/batch - loss: 27.39557 - diff: 16.70mlTrain batch 6/32 - 237.7ms/batch - loss: 31.47365 - diff: 17.71mlTrain batch 7/32 - 236.4ms/batch - loss: 29.57333 - diff: 16.77mlTrain batch 8/32 - 240.6ms/batch - loss: 29.60473 - diff: 16.86mlTrain batch 9/32 - 237.4ms/batch - loss: 28.74791 - diff: 16.70mlTrain batch 10/32 - 240.9ms/batch - loss: 30.03175 - diff: 17.22mlTrain batch 11/32 - 237.3ms/batch - loss: 35.03013 - diff: 18.36mlTrain batch 12/32 - 240.7ms/batch - loss: 34.93055 - diff: 18.44mlTrain batch 13/32 - 236.9ms/batch - loss: 35.75735 - diff: 18.51mlTrain batch 14/32 - 240.3ms/batch - loss: 34.06856 - diff: 18.01mlTrain batch 15/32 - 237.0ms/batch - loss: 34.21665 - diff: 18.11mlTrain batch 16/32 - 240.8ms/batch - loss: 32.47096 - diff: 17.54mlTrain batch 17/32 - 237.8ms/batch - loss: 31.44250 - diff: 17.12mlTrain batch 18/32 - 241.4ms/batch - loss: 32.53783 - diff: 17.29mlTrain batch 19/32 - 237.7ms/batch - loss: 32.17283 - diff: 17.20mlTrain batch 20/32 - 240.9ms/batch - loss: 32.23358 - diff: 17.13mlTrain batch 21/32 - 237.8ms/batch - loss: 31.84620 - diff: 17.00mlTrain batch 22/32 - 239.7ms/batch - loss: 31.31098 - diff: 16.86mlTrain batch 23/32 - 237.4ms/batch - loss: 30.67375 - diff: 16.70mlTrain batch 24/32 - 238.4ms/batch - loss: 30.13969 - diff: 16.57mlTrain batch 25/32 - 237.8ms/batch - loss: 29.71938 - diff: 16.50mlTrain batch 26/32 - 237.9ms/batch - loss: 29.60896 - diff: 16.51mlTrain batch 27/32 - 236.4ms/batch - loss: 29.26286 - diff: 16.44mlTrain batch 28/32 - 240.2ms/batch - loss: 29.15621 - diff: 16.45mlTrain batch 29/32 - 237.1ms/batch - loss: 28.88940 - diff: 16.40mlTrain batch 30/32 - 241.5ms/batch - loss: 29.36985 - diff: 16.58mlTrain batch 31/32 - 237.7ms/batch - loss: 29.24638 - diff: 16.58mlTrain batch 32/32 - 80.0ms/batch - loss: 31.82886 - diff: 16.73mlTrain batch 32/32 - 10.6s 80.0ms/batch - loss: 31.82886 - diff: 16.73ml
Test 1.1s: val_loss: 73.48305 - diff: 25.55ml

Epoch 128: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.7ms/batch - loss: 72.05098 - diff: 25.31mlTrain batch 2/32 - 237.9ms/batch - loss: 46.46379 - diff: 19.85mlTrain batch 3/32 - 236.6ms/batch - loss: 35.20231 - diff: 16.79mlTrain batch 4/32 - 239.7ms/batch - loss: 38.73616 - diff: 18.64mlTrain batch 5/32 - 236.6ms/batch - loss: 35.18557 - diff: 17.96mlTrain batch 6/32 - 239.9ms/batch - loss: 33.56067 - diff: 17.24mlTrain batch 7/32 - 237.2ms/batch - loss: 40.38542 - diff: 19.39mlTrain batch 8/32 - 241.1ms/batch - loss: 36.02552 - diff: 17.97mlTrain batch 9/32 - 237.2ms/batch - loss: 34.93949 - diff: 17.84mlTrain batch 10/32 - 239.9ms/batch - loss: 37.65885 - diff: 18.58mlTrain batch 11/32 - 237.9ms/batch - loss: 39.77476 - diff: 19.50mlTrain batch 12/32 - 240.3ms/batch - loss: 40.95315 - diff: 19.73mlTrain batch 13/32 - 237.9ms/batch - loss: 41.61077 - diff: 19.90mlTrain batch 14/32 - 240.2ms/batch - loss: 41.19308 - diff: 19.82mlTrain batch 15/32 - 238.0ms/batch - loss: 41.01877 - diff: 19.75mlTrain batch 16/32 - 240.0ms/batch - loss: 40.00023 - diff: 19.50mlTrain batch 17/32 - 237.7ms/batch - loss: 42.15776 - diff: 19.75mlTrain batch 18/32 - 237.9ms/batch - loss: 41.45554 - diff: 19.65mlTrain batch 19/32 - 237.4ms/batch - loss: 40.33648 - diff: 19.40mlTrain batch 20/32 - 237.9ms/batch - loss: 38.78016 - diff: 18.98mlTrain batch 21/32 - 237.2ms/batch - loss: 37.66944 - diff: 18.63mlTrain batch 22/32 - 240.0ms/batch - loss: 38.48494 - diff: 18.89mlTrain batch 23/32 - 236.6ms/batch - loss: 38.50432 - diff: 18.97mlTrain batch 24/32 - 240.7ms/batch - loss: 38.92180 - diff: 19.20mlTrain batch 25/32 - 236.7ms/batch - loss: 38.25615 - diff: 19.03mlTrain batch 26/32 - 241.4ms/batch - loss: 37.55276 - diff: 18.83mlTrain batch 27/32 - 237.8ms/batch - loss: 36.82661 - diff: 18.64mlTrain batch 28/32 - 240.3ms/batch - loss: 36.47003 - diff: 18.50mlTrain batch 29/32 - 237.6ms/batch - loss: 37.83834 - diff: 18.83mlTrain batch 30/32 - 239.9ms/batch - loss: 38.04849 - diff: 18.93mlTrain batch 31/32 - 238.1ms/batch - loss: 37.75586 - diff: 18.84mlTrain batch 32/32 - 78.6ms/batch - loss: 37.50468 - diff: 18.72mlTrain batch 32/32 - 10.6s 78.6ms/batch - loss: 37.50468 - diff: 18.72ml
Test 1.1s: val_loss: 67.63645 - diff: 23.52ml

Epoch 129: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.7ms/batch - loss: 28.83378 - diff: 15.52mlTrain batch 2/32 - 238.9ms/batch - loss: 22.77706 - diff: 14.33mlTrain batch 3/32 - 236.7ms/batch - loss: 23.85973 - diff: 14.95mlTrain batch 4/32 - 239.9ms/batch - loss: 23.92485 - diff: 15.03mlTrain batch 5/32 - 237.2ms/batch - loss: 25.48535 - diff: 15.82mlTrain batch 6/32 - 240.1ms/batch - loss: 23.76753 - diff: 15.42mlTrain batch 7/32 - 237.1ms/batch - loss: 24.37703 - diff: 15.92mlTrain batch 8/32 - 239.2ms/batch - loss: 25.27707 - diff: 16.13mlTrain batch 9/32 - 237.3ms/batch - loss: 26.33594 - diff: 16.62mlTrain batch 10/32 - 238.9ms/batch - loss: 25.98656 - diff: 16.47mlTrain batch 11/32 - 237.3ms/batch - loss: 27.54490 - diff: 16.68mlTrain batch 12/32 - 237.8ms/batch - loss: 28.96569 - diff: 17.09mlTrain batch 13/32 - 236.9ms/batch - loss: 30.34917 - diff: 17.63mlTrain batch 14/32 - 240.1ms/batch - loss: 30.03535 - diff: 17.58mlTrain batch 15/32 - 236.4ms/batch - loss: 30.25899 - diff: 17.39mlTrain batch 16/32 - 240.1ms/batch - loss: 29.64388 - diff: 17.28mlTrain batch 17/32 - 237.3ms/batch - loss: 28.53856 - diff: 16.85mlTrain batch 18/32 - 240.0ms/batch - loss: 30.85450 - diff: 17.21mlTrain batch 19/32 - 237.6ms/batch - loss: 31.08950 - diff: 17.35mlTrain batch 20/32 - 239.2ms/batch - loss: 30.74975 - diff: 17.29mlTrain batch 21/32 - 237.8ms/batch - loss: 29.95758 - diff: 17.02mlTrain batch 22/32 - 240.5ms/batch - loss: 29.96862 - diff: 16.93mlTrain batch 23/32 - 238.0ms/batch - loss: 30.42715 - diff: 17.10mlTrain batch 24/32 - 239.7ms/batch - loss: 30.35674 - diff: 17.09mlTrain batch 25/32 - 237.4ms/batch - loss: 30.13629 - diff: 17.04mlTrain batch 26/32 - 239.4ms/batch - loss: 30.40687 - diff: 17.12mlTrain batch 27/32 - 237.3ms/batch - loss: 31.47335 - diff: 17.37mlTrain batch 28/32 - 239.0ms/batch - loss: 31.25878 - diff: 17.33mlTrain batch 29/32 - 237.3ms/batch - loss: 30.93946 - diff: 17.20mlTrain batch 30/32 - 237.7ms/batch - loss: 30.98132 - diff: 17.23mlTrain batch 31/32 - 236.3ms/batch - loss: 31.04998 - diff: 17.28mlTrain batch 32/32 - 79.9ms/batch - loss: 33.76346 - diff: 17.42mlTrain batch 32/32 - 10.7s 79.9ms/batch - loss: 33.76346 - diff: 17.42ml
Test 1.1s: val_loss: 64.92432 - diff: 24.00ml

Epoch 130: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.8ms/batch - loss: 29.31158 - diff: 16.68mlTrain batch 2/32 - 239.4ms/batch - loss: 36.65286 - diff: 18.91mlTrain batch 3/32 - 237.8ms/batch - loss: 37.21101 - diff: 18.91mlTrain batch 4/32 - 237.8ms/batch - loss: 34.14924 - diff: 17.77mlTrain batch 5/32 - 237.3ms/batch - loss: 33.48691 - diff: 17.73mlTrain batch 6/32 - 248.1ms/batch - loss: 34.16732 - diff: 18.09mlTrain batch 7/32 - 237.0ms/batch - loss: 33.27082 - diff: 17.76mlTrain batch 8/32 - 237.6ms/batch - loss: 30.97775 - diff: 17.11mlTrain batch 9/32 - 236.6ms/batch - loss: 31.95045 - diff: 17.57mlTrain batch 10/32 - 239.3ms/batch - loss: 30.48377 - diff: 17.17mlTrain batch 11/32 - 236.7ms/batch - loss: 31.95886 - diff: 17.66mlTrain batch 12/32 - 239.6ms/batch - loss: 30.75022 - diff: 17.41mlTrain batch 13/32 - 237.6ms/batch - loss: 30.12245 - diff: 17.38mlTrain batch 14/32 - 240.7ms/batch - loss: 29.22255 - diff: 17.08mlTrain batch 15/32 - 237.3ms/batch - loss: 29.64022 - diff: 17.26mlTrain batch 16/32 - 239.9ms/batch - loss: 31.00130 - diff: 17.50mlTrain batch 17/32 - 238.0ms/batch - loss: 31.83228 - diff: 17.72mlTrain batch 18/32 - 239.8ms/batch - loss: 33.47462 - diff: 18.19mlTrain batch 19/32 - 237.6ms/batch - loss: 32.28499 - diff: 17.76mlTrain batch 20/32 - 240.1ms/batch - loss: 31.22879 - diff: 17.38mlTrain batch 21/32 - 236.7ms/batch - loss: 33.11187 - diff: 17.95mlTrain batch 22/32 - 240.8ms/batch - loss: 32.74097 - diff: 17.83mlTrain batch 23/32 - 236.8ms/batch - loss: 32.25450 - diff: 17.63mlTrain batch 24/32 - 237.5ms/batch - loss: 33.84156 - diff: 17.96mlTrain batch 25/32 - 236.6ms/batch - loss: 32.72666 - diff: 17.56mlTrain batch 26/32 - 239.2ms/batch - loss: 32.73724 - diff: 17.59mlTrain batch 27/32 - 236.4ms/batch - loss: 32.07131 - diff: 17.44mlTrain batch 28/32 - 240.2ms/batch - loss: 33.19780 - diff: 17.73mlTrain batch 29/32 - 236.6ms/batch - loss: 32.53629 - diff: 17.54mlTrain batch 30/32 - 241.1ms/batch - loss: 33.16060 - diff: 17.70mlTrain batch 31/32 - 237.9ms/batch - loss: 32.99962 - diff: 17.72mlTrain batch 32/32 - 79.6ms/batch - loss: 33.29604 - diff: 17.70mlTrain batch 32/32 - 10.6s 79.6ms/batch - loss: 33.29604 - diff: 17.70ml
Test 1.1s: val_loss: 65.06425 - diff: 22.76ml

Epoch 131: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.5ms/batch - loss: 31.14307 - diff: 17.28mlTrain batch 2/32 - 237.9ms/batch - loss: 27.22973 - diff: 16.75mlTrain batch 3/32 - 237.2ms/batch - loss: 28.01561 - diff: 16.84mlTrain batch 4/32 - 239.7ms/batch - loss: 27.36015 - diff: 16.48mlTrain batch 5/32 - 236.7ms/batch - loss: 27.29114 - diff: 16.63mlTrain batch 6/32 - 239.5ms/batch - loss: 28.76035 - diff: 16.85mlTrain batch 7/32 - 237.4ms/batch - loss: 27.23970 - diff: 16.40mlTrain batch 8/32 - 239.4ms/batch - loss: 29.91524 - diff: 17.02mlTrain batch 9/32 - 236.5ms/batch - loss: 30.19696 - diff: 17.29mlTrain batch 10/32 - 237.6ms/batch - loss: 31.21901 - diff: 17.38mlTrain batch 11/32 - 237.7ms/batch - loss: 31.47823 - diff: 17.60mlTrain batch 12/32 - 240.2ms/batch - loss: 30.80132 - diff: 17.34mlTrain batch 13/32 - 237.2ms/batch - loss: 35.31801 - diff: 17.87mlTrain batch 14/32 - 237.8ms/batch - loss: 33.96010 - diff: 17.48mlTrain batch 15/32 - 237.1ms/batch - loss: 33.89245 - diff: 17.35mlTrain batch 16/32 - 237.6ms/batch - loss: 33.44335 - diff: 17.20mlTrain batch 17/32 - 237.3ms/batch - loss: 33.25497 - diff: 17.14mlTrain batch 18/32 - 239.7ms/batch - loss: 33.09452 - diff: 17.20mlTrain batch 19/32 - 237.7ms/batch - loss: 32.34644 - diff: 17.00mlTrain batch 20/32 - 239.6ms/batch - loss: 33.65973 - diff: 17.44mlTrain batch 21/32 - 236.5ms/batch - loss: 32.93298 - diff: 17.26mlTrain batch 22/32 - 239.3ms/batch - loss: 32.53876 - diff: 17.15mlTrain batch 23/32 - 237.1ms/batch - loss: 32.32835 - diff: 17.14mlTrain batch 24/32 - 238.6ms/batch - loss: 31.65381 - diff: 16.94mlTrain batch 25/32 - 236.4ms/batch - loss: 31.92302 - diff: 16.97mlTrain batch 26/32 - 239.6ms/batch - loss: 32.92341 - diff: 17.09mlTrain batch 27/32 - 236.8ms/batch - loss: 32.82530 - diff: 17.08mlTrain batch 28/32 - 238.9ms/batch - loss: 32.76553 - diff: 17.18mlTrain batch 29/32 - 237.0ms/batch - loss: 32.64335 - diff: 17.10mlTrain batch 30/32 - 239.5ms/batch - loss: 33.44481 - diff: 17.24mlTrain batch 31/32 - 236.7ms/batch - loss: 34.22337 - diff: 17.51mlTrain batch 32/32 - 78.3ms/batch - loss: 35.30166 - diff: 17.53mlTrain batch 32/32 - 10.8s 78.3ms/batch - loss: 35.30166 - diff: 17.53ml
Test 1.1s: val_loss: 62.86133 - diff: 22.83ml

Epoch 132: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.6ms/batch - loss: 22.16805 - diff: 15.97mlTrain batch 2/32 - 238.3ms/batch - loss: 30.01557 - diff: 18.75mlTrain batch 3/32 - 236.5ms/batch - loss: 88.53336 - diff: 29.77mlTrain batch 4/32 - 239.6ms/batch - loss: 88.04416 - diff: 29.35mlTrain batch 5/32 - 237.4ms/batch - loss: 74.94022 - diff: 26.81mlTrain batch 6/32 - 237.6ms/batch - loss: 66.40455 - diff: 25.13mlTrain batch 7/32 - 236.6ms/batch - loss: 60.50657 - diff: 23.79mlTrain batch 8/32 - 237.7ms/batch - loss: 56.71118 - diff: 22.77mlTrain batch 9/32 - 236.4ms/batch - loss: 57.76598 - diff: 23.03mlTrain batch 10/32 - 238.3ms/batch - loss: 55.85091 - diff: 22.17mlTrain batch 11/32 - 236.4ms/batch - loss: 53.71496 - diff: 21.83mlTrain batch 12/32 - 238.8ms/batch - loss: 51.50707 - diff: 21.38mlTrain batch 13/32 - 236.8ms/batch - loss: 49.31035 - diff: 20.80mlTrain batch 14/32 - 239.4ms/batch - loss: 48.07868 - diff: 20.66mlTrain batch 15/32 - 236.8ms/batch - loss: 45.80741 - diff: 20.12mlTrain batch 16/32 - 239.2ms/batch - loss: 48.55808 - diff: 20.48mlTrain batch 17/32 - 237.1ms/batch - loss: 47.03039 - diff: 20.19mlTrain batch 18/32 - 239.3ms/batch - loss: 46.28768 - diff: 20.09mlTrain batch 19/32 - 237.0ms/batch - loss: 45.12898 - diff: 19.81mlTrain batch 20/32 - 239.5ms/batch - loss: 46.18599 - diff: 20.17mlTrain batch 21/32 - 236.0ms/batch - loss: 44.97116 - diff: 19.84mlTrain batch 22/32 - 241.3ms/batch - loss: 44.26110 - diff: 19.80mlTrain batch 23/32 - 237.3ms/batch - loss: 43.19469 - diff: 19.53mlTrain batch 24/32 - 237.8ms/batch - loss: 42.40246 - diff: 19.39mlTrain batch 25/32 - 236.8ms/batch - loss: 41.99028 - diff: 19.32mlTrain batch 26/32 - 238.8ms/batch - loss: 41.65141 - diff: 19.22mlTrain batch 27/32 - 236.8ms/batch - loss: 41.91300 - diff: 19.32mlTrain batch 28/32 - 238.1ms/batch - loss: 41.84885 - diff: 19.34mlTrain batch 29/32 - 237.0ms/batch - loss: 41.54165 - diff: 19.36mlTrain batch 30/32 - 239.3ms/batch - loss: 41.18067 - diff: 19.29mlTrain batch 31/32 - 237.0ms/batch - loss: 40.30178 - diff: 19.07mlTrain batch 32/32 - 77.7ms/batch - loss: 41.13936 - diff: 19.09mlTrain batch 32/32 - 10.7s 77.7ms/batch - loss: 41.13936 - diff: 19.09ml
Test 1.1s: val_loss: 69.63653 - diff: 24.10ml

Epoch 133: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.0ms/batch - loss: 42.83321 - diff: 19.80mlTrain batch 2/32 - 238.1ms/batch - loss: 37.19830 - diff: 19.13mlTrain batch 3/32 - 236.8ms/batch - loss: 32.91887 - diff: 17.75mlTrain batch 4/32 - 238.1ms/batch - loss: 37.04099 - diff: 18.84mlTrain batch 5/32 - 237.3ms/batch - loss: 33.29806 - diff: 18.11mlTrain batch 6/32 - 238.8ms/batch - loss: 34.12517 - diff: 18.32mlTrain batch 7/32 - 237.3ms/batch - loss: 33.30659 - diff: 18.05mlTrain batch 8/32 - 238.6ms/batch - loss: 32.13996 - diff: 17.80mlTrain batch 9/32 - 236.3ms/batch - loss: 30.50381 - diff: 17.39mlTrain batch 10/32 - 239.8ms/batch - loss: 29.41876 - diff: 16.94mlTrain batch 11/32 - 236.5ms/batch - loss: 36.10260 - diff: 18.16mlTrain batch 12/32 - 239.0ms/batch - loss: 34.81356 - diff: 17.89mlTrain batch 13/32 - 237.6ms/batch - loss: 35.51886 - diff: 18.33mlTrain batch 14/32 - 238.5ms/batch - loss: 35.74560 - diff: 18.50mlTrain batch 15/32 - 237.2ms/batch - loss: 36.31717 - diff: 18.69mlTrain batch 16/32 - 237.7ms/batch - loss: 36.95418 - diff: 18.88mlTrain batch 17/32 - 236.9ms/batch - loss: 36.89009 - diff: 18.82mlTrain batch 18/32 - 237.9ms/batch - loss: 37.81984 - diff: 19.14mlTrain batch 19/32 - 236.5ms/batch - loss: 37.69321 - diff: 19.04mlTrain batch 20/32 - 240.1ms/batch - loss: 37.98644 - diff: 19.11mlTrain batch 21/32 - 237.5ms/batch - loss: 37.17191 - diff: 18.92mlTrain batch 22/32 - 240.8ms/batch - loss: 37.14551 - diff: 18.96mlTrain batch 23/32 - 237.0ms/batch - loss: 38.06731 - diff: 19.29mlTrain batch 24/32 - 239.9ms/batch - loss: 38.57896 - diff: 19.36mlTrain batch 25/32 - 237.6ms/batch - loss: 37.89946 - diff: 19.22mlTrain batch 26/32 - 239.4ms/batch - loss: 37.58101 - diff: 19.08mlTrain batch 27/32 - 237.8ms/batch - loss: 37.22636 - diff: 19.03mlTrain batch 28/32 - 239.7ms/batch - loss: 37.62939 - diff: 19.08mlTrain batch 29/32 - 237.5ms/batch - loss: 36.93543 - diff: 18.88mlTrain batch 30/32 - 239.8ms/batch - loss: 36.75541 - diff: 18.86mlTrain batch 31/32 - 238.0ms/batch - loss: 36.11292 - diff: 18.71mlTrain batch 32/32 - 78.5ms/batch - loss: 37.10379 - diff: 18.73mlTrain batch 32/32 - 10.6s 78.5ms/batch - loss: 37.10379 - diff: 18.73ml
Test 1.1s: val_loss: 72.68853 - diff: 24.17ml

Epoch 134: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 237.0ms/batch - loss: 29.87134 - diff: 18.27mlTrain batch 2/32 - 238.1ms/batch - loss: 30.13151 - diff: 17.55mlTrain batch 3/32 - 236.5ms/batch - loss: 32.34155 - diff: 18.60mlTrain batch 4/32 - 239.7ms/batch - loss: 31.23439 - diff: 18.53mlTrain batch 5/32 - 237.6ms/batch - loss: 30.19656 - diff: 17.82mlTrain batch 6/32 - 237.7ms/batch - loss: 27.83402 - diff: 16.88mlTrain batch 7/32 - 237.2ms/batch - loss: 28.01338 - diff: 16.70mlTrain batch 8/32 - 237.9ms/batch - loss: 29.57563 - diff: 17.02mlTrain batch 9/32 - 236.7ms/batch - loss: 29.37737 - diff: 17.16mlTrain batch 10/32 - 238.3ms/batch - loss: 31.70836 - diff: 17.86mlTrain batch 11/32 - 236.7ms/batch - loss: 30.87881 - diff: 17.76mlTrain batch 12/32 - 239.8ms/batch - loss: 30.48543 - diff: 17.43mlTrain batch 13/32 - 236.9ms/batch - loss: 29.77247 - diff: 17.31mlTrain batch 14/32 - 240.3ms/batch - loss: 28.36192 - diff: 16.84mlTrain batch 15/32 - 236.9ms/batch - loss: 29.37679 - diff: 17.05mlTrain batch 16/32 - 240.1ms/batch - loss: 28.77508 - diff: 16.96mlTrain batch 17/32 - 236.8ms/batch - loss: 28.42490 - diff: 16.85mlTrain batch 18/32 - 239.9ms/batch - loss: 27.73680 - diff: 16.67mlTrain batch 19/32 - 237.4ms/batch - loss: 28.17991 - diff: 16.88mlTrain batch 20/32 - 239.3ms/batch - loss: 28.18191 - diff: 16.94mlTrain batch 21/32 - 237.1ms/batch - loss: 29.53276 - diff: 17.28mlTrain batch 22/32 - 240.3ms/batch - loss: 29.79647 - diff: 17.34mlTrain batch 23/32 - 237.0ms/batch - loss: 29.54985 - diff: 17.29mlTrain batch 24/32 - 237.8ms/batch - loss: 29.19686 - diff: 17.26mlTrain batch 25/32 - 237.6ms/batch - loss: 30.21990 - diff: 17.56mlTrain batch 26/32 - 237.8ms/batch - loss: 30.40887 - diff: 17.72mlTrain batch 27/32 - 236.7ms/batch - loss: 30.09273 - diff: 17.60mlTrain batch 28/32 - 239.5ms/batch - loss: 29.58802 - diff: 17.41mlTrain batch 29/32 - 237.3ms/batch - loss: 30.96825 - diff: 17.82mlTrain batch 30/32 - 240.0ms/batch - loss: 30.83223 - diff: 17.77mlTrain batch 31/32 - 237.0ms/batch - loss: 30.34568 - diff: 17.65mlTrain batch 32/32 - 77.6ms/batch - loss: 33.34956 - diff: 17.74mlTrain batch 32/32 - 10.7s 77.6ms/batch - loss: 33.34956 - diff: 17.74ml
Test 1.1s: val_loss: 64.17166 - diff: 23.49ml

Epoch 135: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.0ms/batch - loss: 15.00467 - diff: 13.75mlTrain batch 2/32 - 238.6ms/batch - loss: 27.16126 - diff: 16.69mlTrain batch 3/32 - 236.6ms/batch - loss: 28.28779 - diff: 17.21mlTrain batch 4/32 - 238.6ms/batch - loss: 39.89391 - diff: 20.14mlTrain batch 5/32 - 237.6ms/batch - loss: 40.93255 - diff: 20.40mlTrain batch 6/32 - 238.6ms/batch - loss: 37.12600 - diff: 19.21mlTrain batch 7/32 - 236.3ms/batch - loss: 35.15465 - diff: 18.65mlTrain batch 8/32 - 239.1ms/batch - loss: 32.97408 - diff: 17.93mlTrain batch 9/32 - 236.6ms/batch - loss: 34.57810 - diff: 18.39mlTrain batch 10/32 - 239.9ms/batch - loss: 33.49607 - diff: 18.07mlTrain batch 11/32 - 237.0ms/batch - loss: 31.60300 - diff: 17.40mlTrain batch 12/32 - 237.1ms/batch - loss: 30.01508 - diff: 16.91mlTrain batch 13/32 - 237.3ms/batch - loss: 29.66211 - diff: 16.86mlTrain batch 14/32 - 237.7ms/batch - loss: 29.41477 - diff: 16.79mlTrain batch 15/32 - 236.3ms/batch - loss: 32.38500 - diff: 17.58mlTrain batch 16/32 - 238.0ms/batch - loss: 32.45334 - diff: 17.60mlTrain batch 17/32 - 236.7ms/batch - loss: 33.21938 - diff: 17.70mlTrain batch 18/32 - 238.8ms/batch - loss: 32.54232 - diff: 17.47mlTrain batch 19/32 - 236.8ms/batch - loss: 32.19733 - diff: 17.38mlTrain batch 20/32 - 240.4ms/batch - loss: 32.16622 - diff: 17.37mlTrain batch 21/32 - 237.1ms/batch - loss: 31.65636 - diff: 17.30mlTrain batch 22/32 - 240.2ms/batch - loss: 31.67266 - diff: 17.37mlTrain batch 23/32 - 236.9ms/batch - loss: 30.99207 - diff: 17.21mlTrain batch 24/32 - 240.0ms/batch - loss: 30.78392 - diff: 17.22mlTrain batch 25/32 - 237.0ms/batch - loss: 30.00850 - diff: 16.92mlTrain batch 26/32 - 240.1ms/batch - loss: 30.29177 - diff: 17.02mlTrain batch 27/32 - 237.1ms/batch - loss: 29.52435 - diff: 16.78mlTrain batch 28/32 - 239.2ms/batch - loss: 29.23194 - diff: 16.66mlTrain batch 29/32 - 236.5ms/batch - loss: 29.59553 - diff: 16.79mlTrain batch 30/32 - 240.3ms/batch - loss: 30.25653 - diff: 17.03mlTrain batch 31/32 - 237.8ms/batch - loss: 30.38336 - diff: 17.12mlTrain batch 32/32 - 78.4ms/batch - loss: 32.64922 - diff: 17.23mlTrain batch 32/32 - 10.7s 78.4ms/batch - loss: 32.64922 - diff: 17.23ml
Test 1.1s: val_loss: 70.19872 - diff: 23.63ml

Epoch 136: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.7ms/batch - loss: 10.53250 - diff: 10.72mlTrain batch 2/32 - 239.0ms/batch - loss: 31.27955 - diff: 15.26mlTrain batch 3/32 - 237.0ms/batch - loss: 29.85747 - diff: 15.65mlTrain batch 4/32 - 237.6ms/batch - loss: 27.65586 - diff: 15.14mlTrain batch 5/32 - 237.1ms/batch - loss: 26.85556 - diff: 15.26mlTrain batch 6/32 - 237.7ms/batch - loss: 27.82324 - diff: 15.78mlTrain batch 7/32 - 236.3ms/batch - loss: 31.44653 - diff: 16.63mlTrain batch 8/32 - 238.2ms/batch - loss: 33.39661 - diff: 17.35mlTrain batch 9/32 - 236.9ms/batch - loss: 32.80728 - diff: 17.34mlTrain batch 10/32 - 239.2ms/batch - loss: 32.19769 - diff: 17.35mlTrain batch 11/32 - 236.9ms/batch - loss: 30.13021 - diff: 16.58mlTrain batch 12/32 - 238.4ms/batch - loss: 30.38870 - diff: 16.79mlTrain batch 13/32 - 237.0ms/batch - loss: 30.54777 - diff: 17.03mlTrain batch 14/32 - 238.9ms/batch - loss: 30.30134 - diff: 16.99mlTrain batch 15/32 - 237.7ms/batch - loss: 31.33183 - diff: 17.28mlTrain batch 16/32 - 239.0ms/batch - loss: 31.22998 - diff: 17.20mlTrain batch 17/32 - 236.9ms/batch - loss: 31.33440 - diff: 17.19mlTrain batch 18/32 - 239.7ms/batch - loss: 32.83144 - diff: 17.56mlTrain batch 19/32 - 236.7ms/batch - loss: 33.01331 - diff: 17.66mlTrain batch 20/32 - 237.2ms/batch - loss: 33.16244 - diff: 17.67mlTrain batch 21/32 - 237.0ms/batch - loss: 32.81641 - diff: 17.64mlTrain batch 22/32 - 237.7ms/batch - loss: 32.13701 - diff: 17.41mlTrain batch 23/32 - 237.3ms/batch - loss: 32.31152 - diff: 17.52mlTrain batch 24/32 - 238.0ms/batch - loss: 32.45769 - diff: 17.60mlTrain batch 25/32 - 236.9ms/batch - loss: 31.89779 - diff: 17.35mlTrain batch 26/32 - 238.8ms/batch - loss: 32.12192 - diff: 17.42mlTrain batch 27/32 - 237.1ms/batch - loss: 34.03294 - diff: 17.88mlTrain batch 28/32 - 240.4ms/batch - loss: 34.12323 - diff: 17.90mlTrain batch 29/32 - 237.2ms/batch - loss: 33.77843 - diff: 17.79mlTrain batch 30/32 - 239.4ms/batch - loss: 33.11110 - diff: 17.58mlTrain batch 31/32 - 237.6ms/batch - loss: 33.41298 - diff: 17.71mlTrain batch 32/32 - 78.3ms/batch - loss: 34.54757 - diff: 17.75mlTrain batch 32/32 - 10.7s 78.3ms/batch - loss: 34.54757 - diff: 17.75ml
Test 1.1s: val_loss: 64.34865 - diff: 23.10ml
Epoch   137: reducing learning rate of group 0 to 1.5625e-05.

Epoch 137: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.9ms/batch - loss: 43.37184 - diff: 22.20mlTrain batch 2/32 - 238.0ms/batch - loss: 44.26693 - diff: 22.32mlTrain batch 3/32 - 237.1ms/batch - loss: 35.99678 - diff: 19.45mlTrain batch 4/32 - 239.1ms/batch - loss: 33.77944 - diff: 18.47mlTrain batch 5/32 - 236.5ms/batch - loss: 33.86221 - diff: 18.59mlTrain batch 6/32 - 239.2ms/batch - loss: 32.25233 - diff: 18.43mlTrain batch 7/32 - 236.9ms/batch - loss: 31.81696 - diff: 18.16mlTrain batch 8/32 - 239.5ms/batch - loss: 30.98310 - diff: 17.84mlTrain batch 9/32 - 237.2ms/batch - loss: 29.85384 - diff: 17.61mlTrain batch 10/32 - 238.3ms/batch - loss: 30.40962 - diff: 17.45mlTrain batch 11/32 - 237.3ms/batch - loss: 29.52961 - diff: 17.32mlTrain batch 12/32 - 237.9ms/batch - loss: 28.65814 - diff: 17.12mlTrain batch 13/32 - 236.5ms/batch - loss: 28.52714 - diff: 17.02mlTrain batch 14/32 - 238.8ms/batch - loss: 28.47518 - diff: 16.94mlTrain batch 15/32 - 236.4ms/batch - loss: 28.09184 - diff: 16.78mlTrain batch 16/32 - 239.2ms/batch - loss: 30.13000 - diff: 16.98mlTrain batch 17/32 - 237.1ms/batch - loss: 29.59796 - diff: 16.90mlTrain batch 18/32 - 239.2ms/batch - loss: 29.73226 - diff: 16.98mlTrain batch 19/32 - 237.0ms/batch - loss: 29.09943 - diff: 16.76mlTrain batch 20/32 - 240.1ms/batch - loss: 28.84693 - diff: 16.70mlTrain batch 21/32 - 236.6ms/batch - loss: 27.94933 - diff: 16.40mlTrain batch 22/32 - 239.0ms/batch - loss: 27.93778 - diff: 16.48mlTrain batch 23/32 - 236.7ms/batch - loss: 27.44402 - diff: 16.29mlTrain batch 24/32 - 238.9ms/batch - loss: 27.44255 - diff: 16.17mlTrain batch 25/32 - 237.4ms/batch - loss: 27.26069 - diff: 16.13mlTrain batch 26/32 - 238.8ms/batch - loss: 27.62685 - diff: 16.32mlTrain batch 27/32 - 237.0ms/batch - loss: 27.56265 - diff: 16.37mlTrain batch 28/32 - 238.0ms/batch - loss: 27.28095 - diff: 16.23mlTrain batch 29/32 - 236.7ms/batch - loss: 28.53093 - diff: 16.52mlTrain batch 30/32 - 237.8ms/batch - loss: 28.13082 - diff: 16.38mlTrain batch 31/32 - 236.7ms/batch - loss: 27.99600 - diff: 16.38mlTrain batch 32/32 - 78.3ms/batch - loss: 28.50662 - diff: 16.38mlTrain batch 32/32 - 10.7s 78.3ms/batch - loss: 28.50662 - diff: 16.38ml
Test 1.1s: val_loss: 64.63792 - diff: 23.14ml

Epoch 138: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.2ms/batch - loss: 23.04833 - diff: 15.59mlTrain batch 2/32 - 238.0ms/batch - loss: 26.49948 - diff: 15.83mlTrain batch 3/32 - 236.2ms/batch - loss: 23.33600 - diff: 14.71mlTrain batch 4/32 - 239.2ms/batch - loss: 27.32589 - diff: 15.50mlTrain batch 5/32 - 236.4ms/batch - loss: 26.96690 - diff: 15.43mlTrain batch 6/32 - 238.3ms/batch - loss: 30.43181 - diff: 16.49mlTrain batch 7/32 - 237.0ms/batch - loss: 28.50677 - diff: 16.15mlTrain batch 8/32 - 238.7ms/batch - loss: 27.50589 - diff: 15.85mlTrain batch 9/32 - 236.8ms/batch - loss: 26.28357 - diff: 15.60mlTrain batch 10/32 - 239.2ms/batch - loss: 25.16991 - diff: 15.43mlTrain batch 11/32 - 236.8ms/batch - loss: 24.48423 - diff: 15.26mlTrain batch 12/32 - 239.4ms/batch - loss: 24.28931 - diff: 15.24mlTrain batch 13/32 - 237.0ms/batch - loss: 24.55118 - diff: 15.46mlTrain batch 14/32 - 239.6ms/batch - loss: 25.12676 - diff: 15.71mlTrain batch 15/32 - 236.6ms/batch - loss: 26.39648 - diff: 15.94mlTrain batch 16/32 - 239.4ms/batch - loss: 26.97210 - diff: 16.12mlTrain batch 17/32 - 237.7ms/batch - loss: 25.94647 - diff: 15.72mlTrain batch 18/32 - 237.8ms/batch - loss: 25.31263 - diff: 15.49mlTrain batch 19/32 - 237.1ms/batch - loss: 24.58289 - diff: 15.24mlTrain batch 20/32 - 238.4ms/batch - loss: 25.46817 - diff: 15.32mlTrain batch 21/32 - 236.3ms/batch - loss: 25.43066 - diff: 15.37mlTrain batch 22/32 - 240.0ms/batch - loss: 24.96672 - diff: 15.24mlTrain batch 23/32 - 237.2ms/batch - loss: 24.84956 - diff: 15.27mlTrain batch 24/32 - 240.7ms/batch - loss: 25.52758 - diff: 15.35mlTrain batch 25/32 - 236.6ms/batch - loss: 25.63436 - diff: 15.41mlTrain batch 26/32 - 240.2ms/batch - loss: 25.60674 - diff: 15.42mlTrain batch 27/32 - 237.9ms/batch - loss: 25.30882 - diff: 15.31mlTrain batch 28/32 - 239.7ms/batch - loss: 24.86340 - diff: 15.15mlTrain batch 29/32 - 237.4ms/batch - loss: 24.58140 - diff: 15.10mlTrain batch 30/32 - 240.6ms/batch - loss: 25.14807 - diff: 15.22mlTrain batch 31/32 - 237.6ms/batch - loss: 25.41427 - diff: 15.31mlTrain batch 32/32 - 78.1ms/batch - loss: 25.66919 - diff: 15.29mlTrain batch 32/32 - 10.7s 78.1ms/batch - loss: 25.66919 - diff: 15.29ml
Test 1.1s: val_loss: 66.87234 - diff: 23.25ml

Epoch 139: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.7ms/batch - loss: 27.74436 - diff: 16.12mlTrain batch 2/32 - 238.8ms/batch - loss: 28.77773 - diff: 15.65mlTrain batch 3/32 - 236.7ms/batch - loss: 28.66654 - diff: 15.44mlTrain batch 4/32 - 237.3ms/batch - loss: 25.21705 - diff: 14.39mlTrain batch 5/32 - 236.9ms/batch - loss: 34.10446 - diff: 16.95mlTrain batch 6/32 - 238.2ms/batch - loss: 29.99088 - diff: 15.91mlTrain batch 7/32 - 236.7ms/batch - loss: 30.24303 - diff: 16.15mlTrain batch 8/32 - 239.7ms/batch - loss: 28.90948 - diff: 15.93mlTrain batch 9/32 - 237.2ms/batch - loss: 30.51056 - diff: 16.54mlTrain batch 10/32 - 240.1ms/batch - loss: 30.58819 - diff: 16.81mlTrain batch 11/32 - 236.8ms/batch - loss: 29.60521 - diff: 16.60mlTrain batch 12/32 - 240.1ms/batch - loss: 31.21581 - diff: 17.09mlTrain batch 13/32 - 237.7ms/batch - loss: 32.25161 - diff: 17.36mlTrain batch 14/32 - 239.2ms/batch - loss: 31.62057 - diff: 17.18mlTrain batch 15/32 - 236.9ms/batch - loss: 30.71349 - diff: 16.95mlTrain batch 16/32 - 239.1ms/batch - loss: 30.24951 - diff: 16.93mlTrain batch 17/32 - 237.3ms/batch - loss: 30.48786 - diff: 17.09mlTrain batch 18/32 - 237.7ms/batch - loss: 29.45473 - diff: 16.77mlTrain batch 19/32 - 236.7ms/batch - loss: 29.59905 - diff: 16.86mlTrain batch 20/32 - 238.3ms/batch - loss: 29.81723 - diff: 16.76mlTrain batch 21/32 - 237.1ms/batch - loss: 29.39276 - diff: 16.64mlTrain batch 22/32 - 240.8ms/batch - loss: 29.91709 - diff: 16.85mlTrain batch 23/32 - 236.9ms/batch - loss: 29.40546 - diff: 16.75mlTrain batch 24/32 - 240.1ms/batch - loss: 28.67999 - diff: 16.49mlTrain batch 25/32 - 237.4ms/batch - loss: 29.40893 - diff: 16.71mlTrain batch 26/32 - 240.1ms/batch - loss: 29.50988 - diff: 16.74mlTrain batch 27/32 - 237.7ms/batch - loss: 30.75260 - diff: 17.13mlTrain batch 28/32 - 239.7ms/batch - loss: 30.35208 - diff: 17.05mlTrain batch 29/32 - 237.8ms/batch - loss: 30.21870 - diff: 17.03mlTrain batch 30/32 - 239.9ms/batch - loss: 34.32109 - diff: 17.82mlTrain batch 31/32 - 236.8ms/batch - loss: 33.73006 - diff: 17.68mlTrain batch 32/32 - 77.3ms/batch - loss: 36.91120 - diff: 17.81mlTrain batch 32/32 - 10.7s 77.3ms/batch - loss: 36.91120 - diff: 17.81ml
Test 1.1s: val_loss: 65.23082 - diff: 23.54ml

Epoch 140: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.9ms/batch - loss: 21.47870 - diff: 15.41mlTrain batch 2/32 - 239.2ms/batch - loss: 18.79715 - diff: 14.00mlTrain batch 3/32 - 237.0ms/batch - loss: 23.16325 - diff: 15.42mlTrain batch 4/32 - 237.7ms/batch - loss: 22.94163 - diff: 15.30mlTrain batch 5/32 - 237.2ms/batch - loss: 20.98722 - diff: 14.33mlTrain batch 6/32 - 238.7ms/batch - loss: 25.70130 - diff: 15.29mlTrain batch 7/32 - 236.7ms/batch - loss: 27.51029 - diff: 15.81mlTrain batch 8/32 - 237.7ms/batch - loss: 30.52010 - diff: 16.85mlTrain batch 9/32 - 236.6ms/batch - loss: 39.79654 - diff: 18.95mlTrain batch 10/32 - 237.9ms/batch - loss: 38.40525 - diff: 18.73mlTrain batch 11/32 - 237.0ms/batch - loss: 39.73599 - diff: 19.12mlTrain batch 12/32 - 240.0ms/batch - loss: 41.87598 - diff: 19.89mlTrain batch 13/32 - 237.1ms/batch - loss: 41.39297 - diff: 19.69mlTrain batch 14/32 - 240.9ms/batch - loss: 40.51200 - diff: 19.67mlTrain batch 15/32 - 237.0ms/batch - loss: 39.62291 - diff: 19.46mlTrain batch 16/32 - 240.0ms/batch - loss: 37.82037 - diff: 18.97mlTrain batch 17/32 - 236.9ms/batch - loss: 36.51961 - diff: 18.66mlTrain batch 18/32 - 240.3ms/batch - loss: 36.91254 - diff: 18.94mlTrain batch 19/32 - 237.1ms/batch - loss: 36.45517 - diff: 18.84mlTrain batch 20/32 - 240.1ms/batch - loss: 38.53921 - diff: 19.52mlTrain batch 21/32 - 237.7ms/batch - loss: 39.48595 - diff: 19.70mlTrain batch 22/32 - 240.8ms/batch - loss: 38.39754 - diff: 19.32mlTrain batch 23/32 - 237.9ms/batch - loss: 38.74345 - diff: 19.39mlTrain batch 24/32 - 239.3ms/batch - loss: 41.59862 - diff: 20.06mlTrain batch 25/32 - 237.5ms/batch - loss: 41.21364 - diff: 19.96mlTrain batch 26/32 - 237.6ms/batch - loss: 41.09446 - diff: 20.08mlTrain batch 27/32 - 237.3ms/batch - loss: 41.10577 - diff: 20.13mlTrain batch 28/32 - 238.0ms/batch - loss: 40.46008 - diff: 19.99mlTrain batch 29/32 - 236.8ms/batch - loss: 40.28482 - diff: 19.94mlTrain batch 30/32 - 239.7ms/batch - loss: 39.62824 - diff: 19.75mlTrain batch 31/32 - 236.5ms/batch - loss: 39.29276 - diff: 19.68mlTrain batch 32/32 - 78.5ms/batch - loss: 40.55405 - diff: 19.73mlTrain batch 32/32 - 10.7s 78.5ms/batch - loss: 40.55405 - diff: 19.73ml
Test 1.1s: val_loss: 64.64579 - diff: 23.49ml

Epoch 141: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.7ms/batch - loss: 17.77001 - diff: 12.78mlTrain batch 2/32 - 238.1ms/batch - loss: 25.37983 - diff: 15.08mlTrain batch 3/32 - 236.7ms/batch - loss: 21.02244 - diff: 13.99mlTrain batch 4/32 - 238.4ms/batch - loss: 33.05382 - diff: 17.04mlTrain batch 5/32 - 237.2ms/batch - loss: 29.52878 - diff: 16.19mlTrain batch 6/32 - 239.7ms/batch - loss: 30.08621 - diff: 16.53mlTrain batch 7/32 - 236.6ms/batch - loss: 29.15010 - diff: 16.56mlTrain batch 8/32 - 239.4ms/batch - loss: 29.04202 - diff: 16.87mlTrain batch 9/32 - 236.8ms/batch - loss: 29.98770 - diff: 17.39mlTrain batch 10/32 - 239.3ms/batch - loss: 30.42687 - diff: 17.68mlTrain batch 11/32 - 237.0ms/batch - loss: 29.69439 - diff: 17.45mlTrain batch 12/32 - 239.7ms/batch - loss: 29.60877 - diff: 17.30mlTrain batch 13/32 - 236.9ms/batch - loss: 28.55019 - diff: 16.92mlTrain batch 14/32 - 237.7ms/batch - loss: 28.92340 - diff: 17.02mlTrain batch 15/32 - 237.4ms/batch - loss: 28.69847 - diff: 16.88mlTrain batch 16/32 - 240.6ms/batch - loss: 27.82572 - diff: 16.64mlTrain batch 17/32 - 237.1ms/batch - loss: 28.30967 - diff: 16.82mlTrain batch 18/32 - 237.8ms/batch - loss: 29.20154 - diff: 16.88mlTrain batch 19/32 - 237.3ms/batch - loss: 31.03506 - diff: 17.45mlTrain batch 20/32 - 237.7ms/batch - loss: 31.08088 - diff: 17.46mlTrain batch 21/32 - 236.8ms/batch - loss: 31.39101 - diff: 17.49mlTrain batch 22/32 - 237.7ms/batch - loss: 31.75369 - diff: 17.57mlTrain batch 23/32 - 236.9ms/batch - loss: 31.93273 - diff: 17.68mlTrain batch 24/32 - 239.4ms/batch - loss: 32.02583 - diff: 17.78mlTrain batch 25/32 - 236.6ms/batch - loss: 32.76185 - diff: 17.89mlTrain batch 26/32 - 239.7ms/batch - loss: 32.07745 - diff: 17.72mlTrain batch 27/32 - 237.6ms/batch - loss: 32.56813 - diff: 17.79mlTrain batch 28/32 - 241.0ms/batch - loss: 32.77085 - diff: 17.83mlTrain batch 29/32 - 237.3ms/batch - loss: 32.42996 - diff: 17.75mlTrain batch 30/32 - 240.4ms/batch - loss: 32.60170 - diff: 17.62mlTrain batch 31/32 - 237.7ms/batch - loss: 32.39353 - diff: 17.51mlTrain batch 32/32 - 78.4ms/batch - loss: 35.12446 - diff: 17.61mlTrain batch 32/32 - 10.6s 78.4ms/batch - loss: 35.12446 - diff: 17.61ml
Test 1.1s: val_loss: 67.52282 - diff: 23.49ml

Epoch 142: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.5ms/batch - loss: 20.12073 - diff: 15.16mlTrain batch 2/32 - 238.4ms/batch - loss: 37.80264 - diff: 19.84mlTrain batch 3/32 - 237.2ms/batch - loss: 33.16323 - diff: 18.41mlTrain batch 4/32 - 239.3ms/batch - loss: 31.68674 - diff: 18.85mlTrain batch 5/32 - 236.3ms/batch - loss: 30.23079 - diff: 17.81mlTrain batch 6/32 - 239.3ms/batch - loss: 31.20242 - diff: 18.20mlTrain batch 7/32 - 237.9ms/batch - loss: 31.51369 - diff: 18.26mlTrain batch 8/32 - 237.7ms/batch - loss: 29.41319 - diff: 17.48mlTrain batch 9/32 - 237.4ms/batch - loss: 28.11265 - diff: 17.01mlTrain batch 10/32 - 237.9ms/batch - loss: 28.84198 - diff: 17.18mlTrain batch 11/32 - 236.5ms/batch - loss: 29.74319 - diff: 17.40mlTrain batch 12/32 - 239.5ms/batch - loss: 29.86005 - diff: 17.52mlTrain batch 13/32 - 237.3ms/batch - loss: 29.50535 - diff: 17.36mlTrain batch 14/32 - 239.8ms/batch - loss: 28.75066 - diff: 17.02mlTrain batch 15/32 - 237.5ms/batch - loss: 28.56975 - diff: 17.06mlTrain batch 16/32 - 239.1ms/batch - loss: 27.66081 - diff: 16.69mlTrain batch 17/32 - 237.3ms/batch - loss: 28.58824 - diff: 16.98mlTrain batch 18/32 - 239.4ms/batch - loss: 27.64935 - diff: 16.67mlTrain batch 19/32 - 236.8ms/batch - loss: 27.14389 - diff: 16.56mlTrain batch 20/32 - 236.8ms/batch - loss: 26.92265 - diff: 16.53mlTrain batch 21/32 - 237.5ms/batch - loss: 26.98767 - diff: 16.63mlTrain batch 22/32 - 237.9ms/batch - loss: 26.72492 - diff: 16.51mlTrain batch 23/32 - 237.5ms/batch - loss: 26.35265 - diff: 16.46mlTrain batch 24/32 - 237.7ms/batch - loss: 26.60760 - diff: 16.47mlTrain batch 25/32 - 236.9ms/batch - loss: 27.33121 - diff: 16.75mlTrain batch 26/32 - 238.9ms/batch - loss: 30.55064 - diff: 17.55mlTrain batch 27/32 - 237.2ms/batch - loss: 30.46327 - diff: 17.48mlTrain batch 28/32 - 239.9ms/batch - loss: 30.57053 - diff: 17.45mlTrain batch 29/32 - 237.3ms/batch - loss: 30.06193 - diff: 17.32mlTrain batch 30/32 - 240.0ms/batch - loss: 29.91217 - diff: 17.35mlTrain batch 31/32 - 237.5ms/batch - loss: 29.56534 - diff: 17.22mlTrain batch 32/32 - 78.6ms/batch - loss: 31.27299 - diff: 17.31mlTrain batch 32/32 - 10.8s 78.6ms/batch - loss: 31.27299 - diff: 17.31ml
Test 1.1s: val_loss: 68.70399 - diff: 23.33ml

Epoch 143: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.6ms/batch - loss: 73.51376 - diff: 26.30mlTrain batch 2/32 - 238.2ms/batch - loss: 83.19437 - diff: 28.93mlTrain batch 3/32 - 236.7ms/batch - loss: 65.14394 - diff: 25.36mlTrain batch 4/32 - 238.7ms/batch - loss: 58.57564 - diff: 24.41mlTrain batch 5/32 - 236.5ms/batch - loss: 53.34270 - diff: 23.31mlTrain batch 6/32 - 238.8ms/batch - loss: 50.46434 - diff: 22.80mlTrain batch 7/32 - 236.5ms/batch - loss: 45.06352 - diff: 21.12mlTrain batch 8/32 - 239.2ms/batch - loss: 42.70267 - diff: 20.67mlTrain batch 9/32 - 237.6ms/batch - loss: 42.40637 - diff: 20.68mlTrain batch 10/32 - 237.6ms/batch - loss: 41.50323 - diff: 20.37mlTrain batch 11/32 - 237.1ms/batch - loss: 40.06386 - diff: 19.96mlTrain batch 12/32 - 237.4ms/batch - loss: 38.68239 - diff: 19.61mlTrain batch 13/32 - 237.0ms/batch - loss: 37.19207 - diff: 19.07mlTrain batch 14/32 - 238.4ms/batch - loss: 37.16433 - diff: 19.23mlTrain batch 15/32 - 236.2ms/batch - loss: 35.99562 - diff: 18.92mlTrain batch 16/32 - 238.7ms/batch - loss: 35.42908 - diff: 18.85mlTrain batch 17/32 - 237.4ms/batch - loss: 34.22211 - diff: 18.41mlTrain batch 18/32 - 240.2ms/batch - loss: 34.54953 - diff: 18.51mlTrain batch 19/32 - 236.7ms/batch - loss: 33.90251 - diff: 18.42mlTrain batch 20/32 - 239.4ms/batch - loss: 32.72334 - diff: 18.04mlTrain batch 21/32 - 237.0ms/batch - loss: 32.29731 - diff: 17.85mlTrain batch 22/32 - 239.6ms/batch - loss: 32.44102 - diff: 17.87mlTrain batch 23/32 - 236.9ms/batch - loss: 32.94244 - diff: 18.08mlTrain batch 24/32 - 239.2ms/batch - loss: 34.07317 - diff: 18.25mlTrain batch 25/32 - 237.1ms/batch - loss: 33.35522 - diff: 18.07mlTrain batch 26/32 - 239.7ms/batch - loss: 32.65925 - diff: 17.88mlTrain batch 27/32 - 236.3ms/batch - loss: 32.18328 - diff: 17.76mlTrain batch 28/32 - 239.1ms/batch - loss: 33.01438 - diff: 17.98mlTrain batch 29/32 - 237.4ms/batch - loss: 33.27392 - diff: 18.08mlTrain batch 30/32 - 237.7ms/batch - loss: 32.95085 - diff: 17.97mlTrain batch 31/32 - 248.2ms/batch - loss: 33.11133 - diff: 18.03mlTrain batch 32/32 - 78.8ms/batch - loss: 33.56788 - diff: 18.00mlTrain batch 32/32 - 10.7s 78.8ms/batch - loss: 33.56788 - diff: 18.00ml
Test 1.1s: val_loss: 64.87235 - diff: 23.31ml

Epoch 144: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.3ms/batch - loss: 64.72502 - diff: 26.50mlTrain batch 2/32 - 237.8ms/batch - loss: 40.46097 - diff: 19.88mlTrain batch 3/32 - 237.2ms/batch - loss: 32.83741 - diff: 17.52mlTrain batch 4/32 - 237.8ms/batch - loss: 27.92445 - diff: 16.12mlTrain batch 5/32 - 236.5ms/batch - loss: 33.16612 - diff: 18.02mlTrain batch 6/32 - 237.6ms/batch - loss: 33.92040 - diff: 18.19mlTrain batch 7/32 - 236.4ms/batch - loss: 31.17464 - diff: 17.28mlTrain batch 8/32 - 238.3ms/batch - loss: 34.46435 - diff: 18.26mlTrain batch 9/32 - 236.5ms/batch - loss: 33.18799 - diff: 17.98mlTrain batch 10/32 - 239.0ms/batch - loss: 39.58983 - diff: 19.49mlTrain batch 11/32 - 236.7ms/batch - loss: 39.33429 - diff: 19.38mlTrain batch 12/32 - 238.6ms/batch - loss: 38.36000 - diff: 19.18mlTrain batch 13/32 - 237.5ms/batch - loss: 37.74978 - diff: 18.96mlTrain batch 14/32 - 238.9ms/batch - loss: 36.99191 - diff: 18.85mlTrain batch 15/32 - 236.5ms/batch - loss: 35.86712 - diff: 18.55mlTrain batch 16/32 - 239.2ms/batch - loss: 36.06209 - diff: 18.57mlTrain batch 17/32 - 237.3ms/batch - loss: 34.77928 - diff: 18.25mlTrain batch 18/32 - 238.9ms/batch - loss: 35.54905 - diff: 18.59mlTrain batch 19/32 - 237.3ms/batch - loss: 35.36205 - diff: 18.58mlTrain batch 20/32 - 237.8ms/batch - loss: 34.15957 - diff: 18.14mlTrain batch 21/32 - 236.8ms/batch - loss: 33.97030 - diff: 18.09mlTrain batch 22/32 - 237.5ms/batch - loss: 34.26018 - diff: 18.24mlTrain batch 23/32 - 236.5ms/batch - loss: 33.91487 - diff: 18.17mlTrain batch 24/32 - 239.5ms/batch - loss: 32.89716 - diff: 17.81mlTrain batch 25/32 - 237.5ms/batch - loss: 33.16193 - diff: 17.90mlTrain batch 26/32 - 239.3ms/batch - loss: 33.52411 - diff: 17.92mlTrain batch 27/32 - 236.9ms/batch - loss: 33.45220 - diff: 17.84mlTrain batch 28/32 - 239.6ms/batch - loss: 33.97146 - diff: 17.96mlTrain batch 29/32 - 237.5ms/batch - loss: 33.71354 - diff: 17.94mlTrain batch 30/32 - 239.0ms/batch - loss: 33.20269 - diff: 17.78mlTrain batch 31/32 - 236.9ms/batch - loss: 33.43400 - diff: 17.79mlTrain batch 32/32 - 78.0ms/batch - loss: 34.90465 - diff: 17.85mlTrain batch 32/32 - 10.7s 78.0ms/batch - loss: 34.90465 - diff: 17.85ml
Test 1.1s: val_loss: 67.34541 - diff: 24.16ml

Epoch 145: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.6ms/batch - loss: 58.28580 - diff: 23.05mlTrain batch 2/32 - 238.1ms/batch - loss: 34.80814 - diff: 16.72mlTrain batch 3/32 - 236.8ms/batch - loss: 36.90902 - diff: 18.39mlTrain batch 4/32 - 238.8ms/batch - loss: 32.82391 - diff: 17.14mlTrain batch 5/32 - 236.7ms/batch - loss: 31.15471 - diff: 17.04mlTrain batch 6/32 - 238.7ms/batch - loss: 30.12346 - diff: 16.91mlTrain batch 7/32 - 236.4ms/batch - loss: 30.71460 - diff: 16.91mlTrain batch 8/32 - 239.2ms/batch - loss: 29.98772 - diff: 16.88mlTrain batch 9/32 - 237.6ms/batch - loss: 28.17906 - diff: 16.31mlTrain batch 10/32 - 237.4ms/batch - loss: 27.80214 - diff: 16.33mlTrain batch 11/32 - 237.4ms/batch - loss: 27.72195 - diff: 16.29mlTrain batch 12/32 - 237.6ms/batch - loss: 27.51564 - diff: 16.31mlTrain batch 13/32 - 237.4ms/batch - loss: 27.57727 - diff: 16.32mlTrain batch 14/32 - 237.3ms/batch - loss: 28.43182 - diff: 16.58mlTrain batch 15/32 - 236.6ms/batch - loss: 27.86835 - diff: 16.39mlTrain batch 16/32 - 237.9ms/batch - loss: 27.59337 - diff: 16.44mlTrain batch 17/32 - 236.7ms/batch - loss: 31.22948 - diff: 17.07mlTrain batch 18/32 - 239.1ms/batch - loss: 30.24609 - diff: 16.77mlTrain batch 19/32 - 236.4ms/batch - loss: 30.67820 - diff: 16.94mlTrain batch 20/32 - 238.6ms/batch - loss: 31.65956 - diff: 17.23mlTrain batch 21/32 - 237.1ms/batch - loss: 30.51817 - diff: 16.79mlTrain batch 22/32 - 239.9ms/batch - loss: 30.30315 - diff: 16.77mlTrain batch 23/32 - 237.2ms/batch - loss: 29.72375 - diff: 16.62mlTrain batch 24/32 - 239.0ms/batch - loss: 29.21876 - diff: 16.57mlTrain batch 25/32 - 237.2ms/batch - loss: 29.06302 - diff: 16.56mlTrain batch 26/32 - 239.6ms/batch - loss: 28.82837 - diff: 16.51mlTrain batch 27/32 - 237.6ms/batch - loss: 28.13843 - diff: 16.28mlTrain batch 28/32 - 239.0ms/batch - loss: 27.87483 - diff: 16.22mlTrain batch 29/32 - 236.9ms/batch - loss: 28.22953 - diff: 16.29mlTrain batch 30/32 - 239.7ms/batch - loss: 27.86932 - diff: 16.22mlTrain batch 31/32 - 237.1ms/batch - loss: 28.33212 - diff: 16.41mlTrain batch 32/32 - 78.0ms/batch - loss: 28.26662 - diff: 16.34mlTrain batch 32/32 - 10.6s 78.0ms/batch - loss: 28.26662 - diff: 16.34ml
Test 1.1s: val_loss: 69.04940 - diff: 23.72ml

Epoch 146: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.6ms/batch - loss: 52.16553 - diff: 23.83mlTrain batch 2/32 - 238.3ms/batch - loss: 31.14977 - diff: 16.98mlTrain batch 3/32 - 236.5ms/batch - loss: 33.40551 - diff: 17.91mlTrain batch 4/32 - 238.4ms/batch - loss: 31.11281 - diff: 17.02mlTrain batch 5/32 - 236.4ms/batch - loss: 28.43335 - diff: 16.42mlTrain batch 6/32 - 240.9ms/batch - loss: 27.74134 - diff: 16.17mlTrain batch 7/32 - 237.0ms/batch - loss: 29.59732 - diff: 16.88mlTrain batch 8/32 - 237.5ms/batch - loss: 31.25371 - diff: 17.64mlTrain batch 9/32 - 237.0ms/batch - loss: 30.52252 - diff: 17.32mlTrain batch 10/32 - 237.6ms/batch - loss: 31.20917 - diff: 17.42mlTrain batch 11/32 - 236.3ms/batch - loss: 30.90151 - diff: 17.25mlTrain batch 12/32 - 237.6ms/batch - loss: 30.13821 - diff: 17.04mlTrain batch 13/32 - 237.0ms/batch - loss: 29.45708 - diff: 16.92mlTrain batch 14/32 - 238.9ms/batch - loss: 29.41583 - diff: 17.01mlTrain batch 15/32 - 237.0ms/batch - loss: 28.03675 - diff: 16.56mlTrain batch 16/32 - 241.0ms/batch - loss: 27.04415 - diff: 16.25mlTrain batch 17/32 - 236.9ms/batch - loss: 27.53727 - diff: 16.43mlTrain batch 18/32 - 240.1ms/batch - loss: 28.55116 - diff: 16.77mlTrain batch 19/32 - 236.8ms/batch - loss: 30.46203 - diff: 17.28mlTrain batch 20/32 - 239.8ms/batch - loss: 30.56640 - diff: 17.32mlTrain batch 21/32 - 237.0ms/batch - loss: 30.56419 - diff: 17.34mlTrain batch 22/32 - 239.6ms/batch - loss: 30.45377 - diff: 17.22mlTrain batch 23/32 - 237.4ms/batch - loss: 30.71506 - diff: 17.39mlTrain batch 24/32 - 239.0ms/batch - loss: 30.05193 - diff: 17.20mlTrain batch 25/32 - 236.6ms/batch - loss: 29.45588 - diff: 17.04mlTrain batch 26/32 - 240.3ms/batch - loss: 29.71607 - diff: 17.21mlTrain batch 27/32 - 236.6ms/batch - loss: 30.57914 - diff: 17.52mlTrain batch 28/32 - 240.2ms/batch - loss: 30.13762 - diff: 17.36mlTrain batch 29/32 - 237.2ms/batch - loss: 29.53812 - diff: 17.16mlTrain batch 30/32 - 237.8ms/batch - loss: 29.34583 - diff: 17.09mlTrain batch 31/32 - 237.1ms/batch - loss: 29.20449 - diff: 17.11mlTrain batch 32/32 - 78.4ms/batch - loss: 31.58757 - diff: 17.21mlTrain batch 32/32 - 10.6s 78.4ms/batch - loss: 31.58757 - diff: 17.21ml
Test 1.1s: val_loss: 68.34624 - diff: 22.99ml

Epoch 147: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.2ms/batch - loss: 14.59178 - diff: 12.50mlTrain batch 2/32 - 238.8ms/batch - loss: 17.11432 - diff: 12.82mlTrain batch 3/32 - 237.4ms/batch - loss: 30.16231 - diff: 15.22mlTrain batch 4/32 - 237.7ms/batch - loss: 31.28438 - diff: 16.11mlTrain batch 5/32 - 237.4ms/batch - loss: 31.05047 - diff: 16.39mlTrain batch 6/32 - 237.9ms/batch - loss: 28.59281 - diff: 15.77mlTrain batch 7/32 - 236.8ms/batch - loss: 29.90164 - diff: 16.58mlTrain batch 8/32 - 238.1ms/batch - loss: 31.63296 - diff: 17.41mlTrain batch 9/32 - 236.4ms/batch - loss: 29.69490 - diff: 16.94mlTrain batch 10/32 - 239.1ms/batch - loss: 27.72114 - diff: 16.24mlTrain batch 11/32 - 236.5ms/batch - loss: 29.58789 - diff: 16.95mlTrain batch 12/32 - 239.9ms/batch - loss: 29.21055 - diff: 16.87mlTrain batch 13/32 - 237.6ms/batch - loss: 28.93109 - diff: 16.90mlTrain batch 14/32 - 239.8ms/batch - loss: 27.97466 - diff: 16.58mlTrain batch 15/32 - 237.8ms/batch - loss: 27.17693 - diff: 16.27mlTrain batch 16/32 - 239.6ms/batch - loss: 26.95085 - diff: 16.26mlTrain batch 17/32 - 237.5ms/batch - loss: 26.89903 - diff: 16.35mlTrain batch 18/32 - 239.7ms/batch - loss: 26.60219 - diff: 16.27mlTrain batch 19/32 - 236.6ms/batch - loss: 27.43104 - diff: 16.52mlTrain batch 20/32 - 241.4ms/batch - loss: 27.01782 - diff: 16.48mlTrain batch 21/32 - 237.7ms/batch - loss: 28.17549 - diff: 16.74mlTrain batch 22/32 - 237.9ms/batch - loss: 28.18391 - diff: 16.77mlTrain batch 23/32 - 237.0ms/batch - loss: 28.27182 - diff: 16.81mlTrain batch 24/32 - 238.1ms/batch - loss: 28.05002 - diff: 16.75mlTrain batch 25/32 - 236.5ms/batch - loss: 31.01850 - diff: 17.31mlTrain batch 26/32 - 239.9ms/batch - loss: 31.06736 - diff: 17.28mlTrain batch 27/32 - 237.3ms/batch - loss: 30.62728 - diff: 17.21mlTrain batch 28/32 - 240.1ms/batch - loss: 30.59467 - diff: 17.30mlTrain batch 29/32 - 237.5ms/batch - loss: 30.12704 - diff: 17.12mlTrain batch 30/32 - 239.8ms/batch - loss: 30.26632 - diff: 17.22mlTrain batch 31/32 - 237.4ms/batch - loss: 30.03146 - diff: 17.16mlTrain batch 32/32 - 78.6ms/batch - loss: 31.30941 - diff: 17.23mlTrain batch 32/32 - 10.7s 78.6ms/batch - loss: 31.30941 - diff: 17.23ml
Test 1.1s: val_loss: 62.75348 - diff: 22.99ml
Epoch   148: reducing learning rate of group 0 to 7.8125e-06.

Epoch 148: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.8ms/batch - loss: 44.46844 - diff: 25.35mlTrain batch 2/32 - 237.8ms/batch - loss: 39.22119 - diff: 21.65mlTrain batch 3/32 - 236.6ms/batch - loss: 33.35525 - diff: 18.91mlTrain batch 4/32 - 238.9ms/batch - loss: 33.31952 - diff: 18.63mlTrain batch 5/32 - 237.5ms/batch - loss: 32.01127 - diff: 18.12mlTrain batch 6/32 - 238.8ms/batch - loss: 30.54829 - diff: 17.75mlTrain batch 7/32 - 236.5ms/batch - loss: 31.56902 - diff: 18.35mlTrain batch 8/32 - 240.0ms/batch - loss: 34.18214 - diff: 19.03mlTrain batch 9/32 - 236.6ms/batch - loss: 34.19276 - diff: 19.14mlTrain batch 10/32 - 240.4ms/batch - loss: 33.88003 - diff: 19.08mlTrain batch 11/32 - 237.4ms/batch - loss: 36.95920 - diff: 19.96mlTrain batch 12/32 - 239.0ms/batch - loss: 36.48639 - diff: 19.79mlTrain batch 13/32 - 237.3ms/batch - loss: 35.49876 - diff: 19.55mlTrain batch 14/32 - 237.7ms/batch - loss: 36.37843 - diff: 19.78mlTrain batch 15/32 - 236.9ms/batch - loss: 38.15893 - diff: 20.09mlTrain batch 16/32 - 237.4ms/batch - loss: 37.71710 - diff: 19.95mlTrain batch 17/32 - 236.6ms/batch - loss: 36.25087 - diff: 19.44mlTrain batch 18/32 - 239.5ms/batch - loss: 36.42654 - diff: 19.44mlTrain batch 19/32 - 237.4ms/batch - loss: 36.37201 - diff: 19.46mlTrain batch 20/32 - 240.0ms/batch - loss: 35.92401 - diff: 19.31mlTrain batch 21/32 - 236.8ms/batch - loss: 35.13454 - diff: 19.11mlTrain batch 22/32 - 239.6ms/batch - loss: 34.83504 - diff: 18.97mlTrain batch 23/32 - 237.5ms/batch - loss: 36.22038 - diff: 19.28mlTrain batch 24/32 - 239.7ms/batch - loss: 35.56302 - diff: 18.98mlTrain batch 25/32 - 237.4ms/batch - loss: 34.69776 - diff: 18.71mlTrain batch 26/32 - 239.6ms/batch - loss: 35.52162 - diff: 18.93mlTrain batch 27/32 - 237.5ms/batch - loss: 35.80015 - diff: 19.02mlTrain batch 28/32 - 239.0ms/batch - loss: 37.14724 - diff: 19.38mlTrain batch 29/32 - 237.3ms/batch - loss: 36.94518 - diff: 19.29mlTrain batch 30/32 - 237.6ms/batch - loss: 36.69833 - diff: 19.24mlTrain batch 31/32 - 236.6ms/batch - loss: 36.05709 - diff: 19.05mlTrain batch 32/32 - 79.4ms/batch - loss: 36.23538 - diff: 19.02mlTrain batch 32/32 - 10.7s 79.4ms/batch - loss: 36.23538 - diff: 19.02ml
Test 1.1s: val_loss: 62.71382 - diff: 23.31ml

Epoch 149: current best loss = 56.70134, at epoch 81
Train batch 1/32 - 236.9ms/batch - loss: 31.42557 - diff: 17.75mlTrain batch 2/32 - 238.6ms/batch - loss: 26.35731 - diff: 16.19mlTrain batch 3/32 - 237.3ms/batch - loss: 23.64768 - diff: 15.19mlTrain batch 4/32 - 237.2ms/batch - loss: 25.14766 - diff: 15.37mlTrain batch 5/32 - 236.7ms/batch - loss: 24.39085 - diff: 15.39mlTrain batch 6/32 - 239.1ms/batch - loss: 24.59286 - diff: 15.59mlTrain batch 7/32 - 236.3ms/batch - loss: 23.15357 - diff: 15.08mlTrain batch 8/32 - 239.4ms/batch - loss: 24.73467 - diff: 15.72mlTrain batch 9/32 - 237.3ms/batch - loss: 24.83927 - diff: 15.98mlTrain batch 10/32 - 239.5ms/batch - loss: 23.08440 - diff: 15.23mlTrain batch 11/32 - 236.8ms/batch - loss: 23.44428 - diff: 15.41mlTrain batch 12/32 - 239.6ms/batch - loss: 23.91251 - diff: 15.65mlTrain batch 13/32 - 238.2ms/batch - loss: 23.86307 - diff: 15.55mlTrain batch 14/32 - 239.3ms/batch - loss: 23.23539 - diff: 15.25mlTrain batch 15/32 - 237.1ms/batch - loss: 23.91456 - diff: 15.24mlTrain batch 16/32 - 240.2ms/batch - loss: 24.47933 - diff: 15.40mlTrain batch 17/32 - 236.5ms/batch - loss: 25.15370 - diff: 15.53mlTrain batch 18/32 - 239.3ms/batch - loss: 27.73042 - diff: 16.32mlTrain batch 19/32 - 237.6ms/batch - loss: 29.71241 - diff: 16.92mlTrain batch 20/32 - 239.6ms/batch - loss: 30.78652 - diff: 17.28mlTrain batch 21/32 - 237.4ms/batch - loss: 30.18613 - diff: 17.12mlTrain batch 22/32 - 237.8ms/batch - loss: 30.39733 - diff: 17.26mlTrain batch 23/32 - 237.1ms/batch - loss: 30.95441 - diff: 17.30mlTrain batch 24/32 - 238.2ms/batch - loss: 30.74504 - diff: 17.27mlTrain batch 25/32 - 236.6ms/batch - loss: 30.39826 - diff: 17.19mlTrain batch 26/32 - 239.2ms/batch - loss: 30.24595 - diff: 17.20mlTrain batch 27/32 - 236.7ms/batch - loss: 29.59226 - diff: 17.02mlTrain batch 28/32 - 240.5ms/batch - loss: 29.70419 - diff: 17.11mlTrain batch 29/32 - 236.7ms/batch - loss: 29.53142 - diff: 17.09mlTrain batch 30/32 - 240.3ms/batch - loss: 29.11119 - diff: 16.96mlTrain batch 31/32 - 237.0ms/batch - loss: 28.40439 - diff: 16.69mlTrain batch 32/32 - 79.6ms/batch - loss: 30.72215 - diff: 16.78mlTrain batch 32/32 - 10.6s 79.6ms/batch - loss: 30.72215 - diff: 16.78ml
Test 1.1s: val_loss: 65.26223 - diff: 23.42ml

Traceback (most recent call last):
  File "train.py", line 266, in <module>
    plot_results(train_losses, test_losses, title=f"Loss {target_label}", save_as=exp_name + "_loss")
  File "/home/allochi/tfm/workspace/lib/utils.py", line 345, in plot_results
    plt.plot(train_values, "r", label="train")
  File "/home/allochi/.conda/envs/DL_env/lib/python3.8/site-packages/matplotlib/pyplot.py", line 2824, in plot
    return gca().plot(
  File "/home/allochi/.conda/envs/DL_env/lib/python3.8/site-packages/matplotlib/pyplot.py", line 2352, in gca
    return gcf().gca(**kwargs)
  File "/home/allochi/.conda/envs/DL_env/lib/python3.8/site-packages/matplotlib/pyplot.py", line 731, in gcf
    return figure()
  File "/home/allochi/.conda/envs/DL_env/lib/python3.8/site-packages/matplotlib/pyplot.py", line 671, in figure
    figManager = new_figure_manager(num, figsize=figsize,
  File "/home/allochi/.conda/envs/DL_env/lib/python3.8/site-packages/matplotlib/pyplot.py", line 299, in new_figure_manager
    return _backend_mod.new_figure_manager(*args, **kwargs)
  File "/home/allochi/.conda/envs/DL_env/lib/python3.8/site-packages/matplotlib/backend_bases.py", line 3494, in new_figure_manager
    return cls.new_figure_manager_given_figure(num, fig)
  File "/home/allochi/.conda/envs/DL_env/lib/python3.8/site-packages/matplotlib/backends/_backend_tk.py", line 868, in new_figure_manager_given_figure
    window = tk.Tk(className="matplotlib")
  File "/home/allochi/.conda/envs/DL_env/lib/python3.8/tkinter/__init__.py", line 2261, in __init__
    self.tk = _tkinter.create(screenName, baseName, className, interactive, wantobjects, useTk, sync, use)
_tkinter.TclError: couldn't connect to display "localhost:10.0"
